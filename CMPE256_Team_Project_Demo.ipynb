{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CMPE 256 Team Project Demo\n",
        "Team Name: Bay Area Rockers\n",
        "Team Members: Shawn Chumbar, Dhruval Shah, Sajal Agarwal\n",
        "\n"
      ],
      "metadata": {
        "id": "MArR4qakBONv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "rPThH7wuBRGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wheel setuptools pip --upgrade\n",
        "# !pip3 install wheel setuptools pip --upgrade\n",
        "# !pip install wheel setuptools pip --upgrade\n",
        "\n"
      ],
      "metadata": {
        "id": "13np2TtxBN6v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install dotenv"
      ],
      "metadata": {
        "id": "ekBK_gdJXz-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7908319d-68a3-4e22-a156-28aad18c74eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.6-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.6\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.2)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.1-cp37-abi3-manylinux_2_28_x86_64.whl (699 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from chromadb)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
            "Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=8c745967303947c2846951e390870e3e3d41be310376d5ba7e618c475cd8f910\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, urllib3, typing-extensions, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, asgiref, posthog, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, kubernetes, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.1 chroma-hnswlib-0.7.3 chromadb-0.4.18 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.104.1 httptools-0.6.1 humanfriendly-10.0 kubernetes-28.1.0 mmh3-4.0.1 monotonic-1.6 onnxruntime-1.16.3 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 typing-extensions-4.8.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.343-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.7 (from langchain)\n",
            "  Downloading langchain_core-0.0.7-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.343 langchain-core-0.0.7 langsmith-0.0.67 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.0.5.tar.gz (2.4 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6qke9Et73LEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4768f59a-7875-4317-e79a-7370f57acd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.9.9-py3-none-any.whl (914 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.3/914.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.6)\n",
            "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
            "  Downloading aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.2 (from llama-index)\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.6.3)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.2.14)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.25.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (3.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index) (0.14.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index) (1.26.18)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index) (3.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>=1.1.0->llama-index) (1.1.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Installing collected packages: beautifulsoup4, aiostream, tiktoken, llama-index\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiostream-0.5.2 beautifulsoup4-4.12.2 llama-index-0.9.9 tiktoken-0.5.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.1-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.6/277.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.1\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.7.1-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.104.1)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.7.0 (from gradio)\n",
            "  Downloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.8.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio)\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=c111d2ed0e39f6591f9b327d8f8bc0b9277aedfa88480594cf768e321373f94e\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, python-multipart, pydantic-core, orjson, colorama, annotated-types, aiofiles, pydantic, gradio-client, gradio\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 12.0\n",
            "    Uninstalling websockets-12.0:\n",
            "      Successfully uninstalled websockets-12.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 ffmpy-0.3.1 gradio-4.7.1 gradio-client-0.7.0 orjson-3.9.10 pydantic-2.5.2 pydantic-core-2.14.5 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 shellingham-1.5.4 tomlkit-0.12.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip3 install llama-index\n",
        "!pip3 install pypdf\n",
        "!pip3 install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai pypdf faiss-cpu"
      ],
      "metadata": {
        "id": "3uKRozlBKPU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbca1e9f-53df-41a3-e923-751abc5c5d9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.343)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.7)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = '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'"
      ],
      "metadata": {
        "id": "2o_RrnDTSMlz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Dl_Ln3JwCBoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a875ad5-fcad-475f-abf7-4377c79b7d40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "import os"
      ],
      "metadata": {
        "id": "vEXcaeY8J-hH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path where the PDFs Reside\n",
        "directory_path = \"/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/\""
      ],
      "metadata": {
        "id": "SNWvMr7WN4ya"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "Lwi9qY0VBSbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions"
      ],
      "metadata": {
        "id": "4baAJ0JlNptS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_filenames(directory):\n",
        "    try:\n",
        "        # List all files and directories in the given directory\n",
        "        filenames = os.listdir(directory)\n",
        "        # Filter out directories if you want only files\n",
        "        filenames = [file for file in filenames if os.path.isfile(os.path.join(directory, file))]\n",
        "        return filenames\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "W_I6c0X2Ebd7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file\n",
        "def extract_text(file_path):\n",
        "  pdf_file_path = file_path\n",
        "  with open(pdf_file_path, 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "  # Create a PDF object\n",
        "    pdf_text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "      page = pdf_reader.pages[page_num]\n",
        "      pdf_text += page.extract_text()\n",
        "  return pdf_text\n"
      ],
      "metadata": {
        "id": "ux_5VocyEtRU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_all_PDF_files(file_list):\n",
        "  all_text = ''\n",
        "  full_directory_prefix = directory_path\n",
        "  for each_file_name in file_list:\n",
        "    print_name = full_directory_prefix + each_file_name\n",
        "    print(print_name)\n",
        "    all_text = all_text + extract_text(full_directory_prefix + each_file_name)\n",
        "  return all_text"
      ],
      "metadata": {
        "id": "Cl1-itGkEyFZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = list_filenames(directory_path)"
      ],
      "metadata": {
        "id": "y6WeTeTwN23x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDl3UviNN-Gw",
        "outputId": "47cf8614-b125-4b59-9ed9-3e944e680349"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RP_1.pdf', 'RP_2.pdf', 'RP_3.pdf', 'RP_4.pdf', 'RP_5.pdf', 'RP_6.pdf', 'RP_7.pdf', 'RP_8.pdf', 'RP_9.pdf', 'RP_10.pdf', 'RP_11.pdf', 'RP_12.pdf', 'RP_13.pdf', 'RP_14.pdf', 'RP_15.pdf', 'RP_16.pdf', 'RP_17.pdf', 'RP_18.pdf', 'RP_19.pdf', 'RP_20.pdf', 'RP_21.pdf', 'RP_22.pdf', 'RP_23.pdf', 'RP_24.pdf', 'RP_25.pdf', 'RP_26.pdf', 'RP_27.pdf', 'RP_28.pdf', 'RP_29.pdf', 'RP_30.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = extract_text_from_all_PDF_files(file_list)"
      ],
      "metadata": {
        "id": "n7u0mtkbbCSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d6b45c-7287-4eb7-cc4c-9a44852e5ad8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_1.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_2.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_3.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_4.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_5.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_6.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_7.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_8.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_9.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_10.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_11.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_12.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_13.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_14.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_15.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_16.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_17.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_18.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_19.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_20.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_21.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_22.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_23.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_24.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_25.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_26.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_27.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_28.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_29.pdf\n",
            "/content/drive/MyDrive/Colab Notebooks/CMPE256_Team_Project_Demo/Data/PDF/RP_30.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_text)"
      ],
      "metadata": {
        "id": "q5dYSaUfFGja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa57254a-f433-4a45-d966-ebc124dfbd23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Towards Hetero-Client Federated Multi-Task Learning\n",
            "Yuxiang Lu*, Suizhi Huang*, Yuwen Yang, Shalayiding Sirejiding, Yue Ding, Hongtao Lu\n",
            "Department of Computer Science and Engineering, Shanghai Jiao Tong University\n",
            "Abstract\n",
            "Federated Learning (FL) enables joint training across\n",
            "distributed clients using their local data privately. Fed-\n",
            "erated Multi-Task Learning (FMTL) builds on FL to han-\n",
            "dle multiple tasks, assuming model congruity that identical\n",
            "model architecture is deployed in each client. To relax this\n",
            "assumption and thus extend real-world applicability, we in-\n",
            "troduce a novel problem setting, Hetero-Client Federated\n",
            "Multi-Task Learning (HC-FMTL), to accommodate diverse\n",
            "task setups. The main challenge of HC-FMTL is the model\n",
            "incongruity issue that invalidates conventional aggregation\n",
            "methods. It also escalates the difficulties in accurate model\n",
            "aggregation to deal with data and task heterogeneity inher-\n",
            "ent in FMTL. To address these challenges, we propose the\n",
            "FEDHCA2framework, which allows for federated training\n",
            "of personalized models by modeling relationships among\n",
            "heterogeneous clients. Drawing on our theoretical insights\n",
            "into the difference between multi-task and federated opti-\n",
            "mization, we propose the Hyper Conflict-Averse Aggrega-\n",
            "tion scheme to mitigate conflicts during encoder updates.\n",
            "Additionally, inspired by task interaction in MTL, the Hyper\n",
            "Cross Attention Aggregation scheme uses layer-wise cross\n",
            "attention to enhance decoder interactions while alleviating\n",
            "model incongruity. Moreover, we employ learnable Hyper\n",
            "Aggregation Weights for each client to customize person-\n",
            "alized parameter updates. Extensive experiments demon-\n",
            "strate the superior performance of FEDHCA2in various\n",
            "HC-FMTL scenarios compared to representative methods.\n",
            "Our code will be made publicly available.\n",
            "1. Introduction\n",
            "Federated Learning (FL) [31] has emerged as a prominent\n",
            "paradigm in distributed training, gaining attention in both\n",
            "academic and industrial fields [4, 5, 17, 27, 79]. The FL\n",
            "framework empowers to collaboratively train models across\n",
            "multiple clients, like mobile devices or distributed data cen-\n",
            "ters, while preserving data privacy and reducing communi-\n",
            "cation costs. The impetus behind FL lies in the recogni-\n",
            "tion that harnessing a broader dataset can improve model\n",
            "*Equal contribution.\n",
            "C1 C2\n",
            "Task 1C3 C4\n",
            "Task 2&3 Task 2&3\n",
            "(b)\n",
            "C1 C2 C3 C4\n",
            "Task 1 Task 2 Task 3&4 Task 5&6&7\n",
            "(c)C1 C2\n",
            "(a)Task 1 Task 2 Task 1\n",
            "Aggregation\n",
            "Task SetupsFigure 1. Comparison of different settings in FMTL. (a) Each\n",
            "client is dedicated to a single task. (b) Clients are grouped with\n",
            "peers, and peers in the same group share identical task setting. (c)\n",
            "Our proposed HC-FMTL setting that enables flexible collabora-\n",
            "tion among clients with different task setups.\n",
            "performance, but it also introduces the data heterogeneity\n",
            "issue, as clients often collect samples from non-i.i.d. data\n",
            "distributions. Nevertheless, most FL research is centered on\n",
            "single-task scenarios, overlooking applications that demand\n",
            "simultaneous multi-task processing, e.g., autonomous driv-\n",
            "ing [23]. This gap has led to the integration of Multi-Task\n",
            "Learning (MTL) with FL, giving rise to Federated Multi-\n",
            "Task Learning (FMTL) [22, 54]. While existing FMTL ap-\n",
            "proaches primarily address statistical challenges [51, 65],\n",
            "recent studies [8, 12, 92] have highlighted the importance of\n",
            "task heterogeneity , particularly for dense predictions such\n",
            "as semantic segmentation and depth estimation [11, 58, 74].\n",
            "However, these FMTL methods often assume model\n",
            "congruity among clients, i.e., all participants either engage\n",
            "in a single task or aggregate with peers handling identical\n",
            "task sets, as shown in Fig. 1a, 1b. Considering the dis-\n",
            "crepancy of heterogeneous tasks in practical applications as\n",
            "well as the expensive labor of annotating task-specific la-\n",
            "bels, clients often have different task setups in different en-\n",
            "vironments. Here task setup describes a set of tasks that can\n",
            "vary in both number and type. We define this as a new prob-\n",
            "lem setting: Hetero-Client Federated Multi-Task Learning\n",
            "(HC-FMTL), as depicted in Fig. 1c. HC-FMTL relaxes the\n",
            "constraints on model congruity, facilitating more flexible\n",
            "collaborative learning of various tasks across diverse private\n",
            "1arXiv:2311.13250v1  [cs.CV]  22 Nov 2023data domains and making FMTL scenarios more universally\n",
            "applicable.\n",
            "As a more pervasive setting relaxed from FMTL, HC-\n",
            "FMTL introduces an additional challenge of model incon-\n",
            "gruity , which exacerbates client heterogeneity. This issue\n",
            "arises from clients having different task setups, coupled\n",
            "with the prevalent use of encoder-decoder architectures in\n",
            "vision tasks, leading to a disparity in multi-task model struc-\n",
            "tures. Model incongruity not only increases the complexity\n",
            "of model aggregation but also coexists with the data and\n",
            "task heterogeneity inherent in FMTL. Data heterogeneity\n",
            "is a consequence of clients encountering distinct data do-\n",
            "mains, as clients tend to use data from different domains\n",
            "to handle different target tasks without any overlap, which\n",
            "can result in performance degradation of collective learning.\n",
            "Meanwhile, task heterogeneity, which assigns different ob-\n",
            "jectives for each task, could impede joint optimization and\n",
            "magnify the influence of data heterogeneity.\n",
            "In this paper, we propose a novel framework named\n",
            "FEDHCA2, designed for HC-FMTL. Our goal is to adap-\n",
            "tively discern the relationships among heterogeneous clients\n",
            "and learn personalized yet globally collaborative models\n",
            "that benefit from both synergies and distinctions among\n",
            "clients and tasks. Since model incongruity precludes\n",
            "the straightforward application of conventional aggregation\n",
            "methods in FL, our approach involves the server disassem-\n",
            "bling client models into encoders and decoders for inde-\n",
            "pendent aggregation. For the encoders, we design the Hy-\n",
            "per Conflict-Averse Aggregation scheme to alleviate up-\n",
            "date conflicts among clients. The motivation behind this\n",
            "is grounded in our theoretical analysis (see Theorem 1) that\n",
            "the optimization processes of MTL and FL are closely con-\n",
            "nected and share similarities. By incorporating an approxi-\n",
            "mated gradient smoothing technique, we can find an appro-\n",
            "priate update direction for all clients that mitigates the nega-\n",
            "tive effects of conflicting parameter updates caused by data\n",
            "and task heterogeneity. When aggregating the decoders,\n",
            "we devise the Hyper Cross Attention Aggregation scheme\n",
            "to accommodate client heterogeneity. We draw inspiration\n",
            "from the modeling of task interaction in MTL [55, 77] and\n",
            "apply it to FL. Specifically, we implement a layer-wise cross\n",
            "attention mechanism to model the interplay between client\n",
            "decoders, enabling the capture of both the commonalities\n",
            "and discrepancies among different tasks in a fine-grained\n",
            "manner and thereby alleviating the incongruity at the model\n",
            "level. In addition, the personalized parameter updates for\n",
            "each client are tailored by learnable Hyper Aggregation\n",
            "Weights, which encourage encoders and decoders to adap-\n",
            "tively assimilate knowledge from peers that offer helpful\n",
            "complementary information.\n",
            "Our contributions are summarized as follows:\n",
            "• We introduce a novel setting of Hetero-Client Fed-\n",
            "erated Multi-Task Learning (HC-FMTL) alongside theFEDHCA2framework. It supports collaborative training\n",
            "across clients, each with its unique task setups, address-\n",
            "ing the complexities of data and task heterogeneity, and\n",
            "the newly identified challenge of model incongruity. The\n",
            "relaxed setting broadens the FMTL’s applicability to in-\n",
            "clude a wider variety of clients, tasks, and data situations.\n",
            "• We reveal the connection between the optimization of\n",
            "MTL and FL in Theorem 1 and underscore the impor-\n",
            "tance of circumventing update conflicts among clients,\n",
            "which are exacerbated by data and task heterogeneity in\n",
            "HC-FMTL. We propose a Hyper Conflict-Averse Aggre-\n",
            "gation scheme, designed to alleviate the adverse effects\n",
            "on encoders when absorbing shared knowledge.\n",
            "• We develop a Hyper Cross Attention Aggregation scheme\n",
            "to facilitate task interaction in decoders by modeling the\n",
            "fine-grained cross-task relationships among each decoder\n",
            "layer, tackling both intra- and inter-client heterogeneity.\n",
            "• We evaluate F EDHCA2using a composite of two bench-\n",
            "mark datasets, PASCAL-Context and NYUD-v2, for vari-\n",
            "ous HC-FMTL scenarios. Extensive experiments demon-\n",
            "strate that our approach outperforms existing methods.\n",
            "2. Related Work\n",
            "2.1. Personalized Federated Learning\n",
            "Federated Learning (FL) can be broadly classified into tra-\n",
            "ditional and personalized types, depending on the charac-\n",
            "teristics of data distribution [42, 68]. Traditional Feder-\n",
            "ated Learning, exemplified by the widely used FedAvg [53],\n",
            "has undergone refinements to tackle challenges such as data\n",
            "heterogeneity [1, 33, 34, 72, 73, 83, 85, 91], communi-\n",
            "cation efficiency [18, 29, 32, 52, 90], and privacy con-\n",
            "cerns [3, 25]. In contrast, personalized Federated Learn-\n",
            "ing (pFL) emerges as a specialized variant designed to\n",
            "cater to individual client needs and address data hetero-\n",
            "geneity more effectively [65, 68]. Techniques like meta-\n",
            "learning [19], regularization [24, 35, 67], personalized-\n",
            "head methods [2, 10, 15, 59], and other innovative ap-\n",
            "proaches [36, 37, 76] are widely employed in pFL. In\n",
            "essence, both traditional FL and pFL aim to grapple with\n",
            "the inherent challenge of data heterogeneity.\n",
            "2.2. Multi-Task Learning\n",
            "Multi-Task Learning (MTL) aims to improve overall per-\n",
            "formance while reducing parameters and speeding up train-\n",
            "ing or inference compared to training individual models\n",
            "for each task in isolation [9, 16, 60, 70]. The main di-\n",
            "rections of MTL research can be roughly categorized into\n",
            "network architecture design and multi-task optimization\n",
            "strategy [16]. Network structure design employs methods\n",
            "such as parameter sharing [28, 47, 50, 61, 66], task in-\n",
            "teraction [44, 64, 78, 86, 87, 89], and prediction distilla-\n",
            "tion [69, 81, 82, 88]. Regarding multi-task optimization,\n",
            "2  Client   Client   Client \n",
            "EncoderDecoderHead\n",
            "EncoderDecoderHead\n",
            "EncoderDecoderHead\n",
            "Local Dataset Local Dataset Local Dataset Task Task Task \n",
            "... ...Task\n",
            "Heterogeneity   Client \n",
            "EncoderDecoderHead\n",
            "DecoderHead\n",
            "Local Dataset Task Task       Client \n",
            "EncoderDecoderHead\n",
            "DecoderHead\n",
            "DecoderHead\n",
            "Local Dataset Task Task Task \n",
            "...\n",
            "...Hyper Cross Attention AggregationServerHyper Conflict-Averse Aggregation\n",
            "... ...\n",
            "updateupdate\n",
            "... ...Task\n",
            "Interaction\n",
            "...\n",
            "Model\n",
            "Incongruity\n",
            "Data\n",
            "HeterogeneityHyper Aggregation Weights:layer-wiseFigure 2. Illustration of the HC-FMTL setting and our proposed F EDHCA2framework. HC-FMTL enables clients to have different\n",
            "task setups, from single-task ( e.g. client C1, C2, C3) to multi-task ( e.g. client Ci, CN). HC-FMTL faces three main challenges: model\n",
            "incongruity due to different client model structures, data heterogeneity from different local data domains, and task heterogeneity from\n",
            "varied target tasks. The FL system includes a server and several clients. Our framework decomposes model aggregation into two parts:\n",
            "Hyper Conflict-Averse Aggregation for encoders and Hyper Cross Attention Aggregation for decoders. Learnable Hyper Aggregation\n",
            "Weights are employed to customize personalized parameter updates and are iteratively updated by local model updates from clients.\n",
            "strategies are differentiated into loss balancing and gradient\n",
            "balancing. Loss balancing techniques are designed to pro-\n",
            "duce suitable loss weights to reduce conflicts among mul-\n",
            "tiple tasks [30, 40, 41, 80]. Gradient balancing, on the\n",
            "other hand, addresses task interference by directly adjust-\n",
            "ing gradients, with recent methods concentrating on the for-\n",
            "mulation of a unified gradient vector subject to diverse con-\n",
            "straints [13, 14, 26, 38, 62, 75, 84]. In essence, MTL is\n",
            "dedicated to addressing the intrinsic challenges associated\n",
            "with the heterogeneity of tasks.\n",
            "2.3. Federated Multi-Task Learning\n",
            "It is essential to note that conventional Federated Multi-\n",
            "Task Learning (FMTL) is a branch of personalized Fed-\n",
            "erated Learning that primarily deals with data heterogene-\n",
            "ity across clients [22, 39, 54]. Representative works like\n",
            "MOCHA [65] and FedEM [51] attempt to train models\n",
            "across clients with diverse data distributions within an MTL\n",
            "setting. Recent advancements, including FedBone [12],\n",
            "MAS [92], and MaT-FL [8], have aimed to address both task\n",
            "and data heterogeneity in FMTL. FedBone aggregates the\n",
            "encoders by gradients uploaded from each client, enhancing\n",
            "feature extraction capability. MaT-FL uses dynamic group-\n",
            "ing to combine different client models. MAS distributes\n",
            "varied multi-task models to clients and aggregates models\n",
            "among those with the same task sets. Nevertheless, Fed-\n",
            "Bone and MaT-FL are limited to each client managing a\n",
            "single task (Fig. 1a). MAS supports multi-task clients but isstill limited to identical task sets for aggregation (Fig. 1b).\n",
            "In contrast, our proposed framework enables aggregation\n",
            "across clients with varying numbers and types of tasks, of-\n",
            "fering a more flexible collaboration.\n",
            "3. Methodology\n",
            "3.1. Preliminary\n",
            "Within Hetero-Client Federated Multi-Task Learning (HC-\n",
            "FMTL), clients are assigned flexible task setups, spanning\n",
            "from single-task to multi-task configurations, with an arbi-\n",
            "trary number of tasks per client. Formally, given a pool of\n",
            "Nclients, with client Ciaddressing task sets Tion a cor-\n",
            "responding local dataset Di={(xn,yn)}|Di|\n",
            "n=1, where xnis\n",
            "the input sample and yn=S\n",
            "t∈Tiyn,tcontains the ground-\n",
            "truth labels for all tasks in Ti.\n",
            "In line with FMTL, the objective of HC-FMTL is to train\n",
            "client-specific models θ={θ1, . . . , θ N}that benefit from\n",
            "collaborative optimization with other clients, thus improv-\n",
            "ing performance on their local tasks. The learning objective\n",
            "is to optimize personalized client models with Multi-Task\n",
            "Learning, formulated as follows:\n",
            "min\n",
            "θiX\n",
            "t∈TiLi,t(θi),∀i∈ {1, . . . , N }, (1)\n",
            "where Li,tis the loss function computed over client Ci’s\n",
            "local dataset Difor task t.\n",
            "33.2. Architecture Overview\n",
            "The overall architecture of our proposed F EDHCA2is de-\n",
            "picted in Fig. 2. It contains a pool of clients that perform\n",
            "local training on their private datasets and a server that coor-\n",
            "dinates the aggregation of models from these clients. Con-\n",
            "cerning dense prediction tasks, each client Ciutilizes an\n",
            "encoder-decoder structure consisting of a shared encoder\n",
            "θE\n",
            "i, task-specific decoders {θD,1\n",
            "i, . . . , θD,|Ti|\n",
            "i}and predic-\n",
            "tion heads for each task type they handle. In each commu-\n",
            "nication round r, after all clients finish their local training,\n",
            "they send the model parameters of previous round θ(r−1)\n",
            "and the updates in current round ∆θ(r)to the server. The\n",
            "server first disassembles these models into encoders and\n",
            "decoders and then performs independent aggregation pro-\n",
            "cesses. The prediction heads, due to their varying parame-\n",
            "ter dimensions tailored to specific task outputs, are excluded\n",
            "from the aggregation process and remain localized to indi-\n",
            "vidual clients. The encoder parameters from all Nclients\n",
            "undergo Hyper Conflict-Averse Aggregation. Meanwhile,\n",
            "the server aggregates the parameters of all K=PN\n",
            "i=1Ti\n",
            "decoders through Hyper Cross Attention Aggregation. The\n",
            "entire pipeline of our framework is outlined in Algorithm 1.\n",
            "Algorithm 1 Pseudo-codes for F EDHCA2\n",
            "Input: Nclients {C1, . . . , C N}with private local datasets\n",
            "{D1, . . . ,DN}, client Ciaddresses tasks Ti, total communi-\n",
            "cation rounds R, local epoch E, learning rate η\n",
            "Output: Trained models θ(R)={θ(R)\n",
            "1, . . . , θ(R)\n",
            "N}\n",
            "1:Clients initialize models θ(0)={θ(0)\n",
            "1, . . . , θ(0)\n",
            "N}, each model\n",
            "θiconsists of a shared encoder θE\n",
            "iand|Ti|task-specific de-\n",
            "codersS|Ti|\n",
            "j=1θD,j\n",
            "iand heads\n",
            "2:Server initializes Hyper Aggregation Weights αandβ\n",
            "3:procedure SERVER UPDATE\n",
            "4: foreach communication round r∈ {1, . . . , R }do\n",
            "5: foreach client Ciin parallel do\n",
            "6: ∆θ(r)\n",
            "i←CLIENT UPDATE (θ(r−1)\n",
            "i)\n",
            "7: end for\n",
            "8: Server gathers updates of client models ∆θ(r)\n",
            "9: Update α,βusing ∆θ(r)with Eq. (17)\n",
            "10: θ(r)←AGGREGATION (θ(r−1),∆θ(r))\n",
            "11: end for\n",
            "12:end procedure\n",
            "13:procedure CLIENT UPDATE (θ(r−1)\n",
            "i )\n",
            "14: θi←θ(r−1)\n",
            "i\n",
            "15: foreach local epoch e∈ {1, . . . , E }do\n",
            "16: formini-batch Bi⊂ D ido\n",
            "17: Compute losses Li=P|Ti|\n",
            "j=1Lj\n",
            "i(θi;Bi)\n",
            "18: Update model θi←θi−η∇θiLi\n",
            "19: end for\n",
            "20: end for\n",
            "21: return ∆θ(r)\n",
            "i=θi−θ(r−1)\n",
            "i\n",
            "22:end procedure\n",
            "Shared\n",
            "EncoderDecoder1Head1\n",
            "Decoder2Head2\n",
            "Encoder1Decoder1Head1\n",
            "Decoder2Head2\n",
            "Encoder2Client 1 Client 2\n",
            "AggregationBackward\n",
            "(a) (b)Figure 3. Comparison of optimization in MTL and FL. (a) The\n",
            "shared encoder in MTL is updated by gradient accumulation from\n",
            "all tasks. (b) The clients’ encoders are updated independently and\n",
            "then aggregated in FL.\n",
            "3.3. Hyper Conflict-Averse Aggregation\n",
            "In MTL, the encoder typically employs a parameter-sharing\n",
            "mechanism to capture common task-agnostic information,\n",
            "thereby serving as a general feature extractor for all tasks\n",
            "and enhancing their generalization capabilities. Within our\n",
            "encoder aggregation process, we anticipate that encoders\n",
            "from various clients, each addressing distinct tasks on dif-\n",
            "ferent data domains, are able to acquire general knowledge\n",
            "from other client encoders akin to MTL. To elucidate this,\n",
            "we begin with a theoretical analysis of the correlation be-\n",
            "tween the optimization process of MTL and FL.\n",
            "As depicted in Fig. 3a, consider an MTL scenario where\n",
            "Ntasks are learned simultaneously using a standard multi-\n",
            "decoder architecture. In each mini-batch, the network back-\n",
            "propagates the loss functions L1, . . . ,LNonto the shared\n",
            "encoder θEto calculate its gradient and update:\n",
            "g=NX\n",
            "i=1gi=NX\n",
            "i=1∇θELi, (2)\n",
            "∆θE=−ηg=−ηNX\n",
            "i=1gi, (3)\n",
            "where grepresents the cumulative gradient on the encoder\n",
            "andηsignifies the learning rate. By updating through the\n",
            "summation of gradients from diverse tasks, the encoder as-\n",
            "similates knowledge from various task domains, aligning\n",
            "with the objective of MTL. Meanwhile, in an FL setting\n",
            "shown in Fig. 3b, suppose there are Nclients, each using\n",
            "separate networks with the same architectures as the multi-\n",
            "task model but with independent encoders θE\n",
            "1, . . . , θE\n",
            "Nto\n",
            "learn the same Ntasks. Assuming identical initial weights\n",
            "θEwith MTL, and they are trained for only one mini-batch\n",
            "to obtain gradients gi=∇θELi. FL typically aggregates\n",
            "these encoders by averaging the parameters of all clients:\n",
            "˜θE\n",
            "i=1\n",
            "NNX\n",
            "i=1θE\n",
            "i, (4)\n",
            "where ˜θE\n",
            "iis the aggregated encoder parameters. Consider-\n",
            "4ing its change from the initial weight:\n",
            "∆˜θE\n",
            "i=1\n",
            "NNX\n",
            "i=1∆θE\n",
            "i=1\n",
            "N(−η)NX\n",
            "i=1gi, (5)\n",
            "it means the update for the aggregated encoder mirrors the\n",
            "update of the shared encoder in MTL, if we regard the opti-\n",
            "mizer as capable of automatically scaling the learning rate η\n",
            "in Eq. (3). While FL typically aggregates client models af-\n",
            "ter several local training epochs in a communication round,\n",
            "this implies that there are differences between the learning\n",
            "processes of MTL and FL:\n",
            "Theorem 1 (Difference in optimizing MTL and FL)\n",
            "Given clients with a shared encoder and task-specific\n",
            "decoder structure, the gradient descent in the shared\n",
            "encoder of MTL is equivalent to averaging parameter\n",
            "aggregation in FL, adding an extra term that maximizes\n",
            "the inner product of gradients between all pairs of tasks in\n",
            "each iteration.\n",
            "We provide proofs and in-depth analysis in Appendix A. As\n",
            "the inner product of gradients is a measurement of accor-\n",
            "dance, maximizing the inner product is equal to reducing\n",
            "the conflict of gradients [48]. Hence, Theorem 1 states the\n",
            "necessity of integrating optimization techniques to mitigate\n",
            "gradient conflicts during encoder aggregation in HC-FMTL.\n",
            "Inspired by CAGrad [38], for each communication round,\n",
            "we aim to find an optimal aggregated update ˜Ufor the en-\n",
            "coder that minimizes conflicts while optimizing the main\n",
            "objective with optimization problem:\n",
            "max\n",
            "˜Umin\n",
            "i⟨∆θE\n",
            "i,˜U⟩s.t.∥˜U−∆¯θE∥ ≤c∥∆¯θE∥,(6)\n",
            "where ∆¯θE=1\n",
            "NPN\n",
            "i=1∆θE\n",
            "iis the average parameter up-\n",
            "date and c∈[0,1)is a hyper-parameter controlling the\n",
            "convergence rate. Here mini⟨∆θE\n",
            "i,˜U⟩measures the maxi-\n",
            "mum conflict between client updates and the target update,\n",
            "which is an approximation to the conflict between gradi-\n",
            "ents, as the server only receives parameter updates after\n",
            "several local training epochs rather than the gradients in\n",
            "each iteration. Therefore, maximizing this term can min-\n",
            "imize the conflict in parameter optimization, which is con-\n",
            "sistent with our findings in Theorem 1. With constraintPN\n",
            "i=1wi= 1, wi≥0, solving this problem using La-\n",
            "grangian simplifies to:\n",
            "min\n",
            "wF(w) =U⊤\n",
            "w∆¯θE+p\n",
            "ϕ∥Uw∥, (7)\n",
            "where Uw=1\n",
            "NNX\n",
            "i=1wi∆θE\n",
            "i, ϕ=c2∥∆¯θE∥2.(8)\n",
            "Upon finding the optimum w∗and the optimal λ∗=\n",
            "||Uw∗||/ϕ1/2, we have the unified aggregated update:\n",
            "˜U= ∆¯θE+Uw∗/λ∗= ∆¯θE+√ϕ\n",
            "∥Uw∥Uw. (9)3.4. Hyper Cross Attention Aggregation\n",
            "The significance of task interaction in MTL is well-\n",
            "established [7, 20, 55, 69, 77], as it allows for exchanging\n",
            "knowledge among tasks and benefiting from complemen-\n",
            "tary information. In representative methods [55, 77], task\n",
            "interaction is facilitated by adding the target task’s feature\n",
            "with those from source tasks in decoders, formulated as:\n",
            "zD\n",
            "i=NX\n",
            "j=1γi,j(θD\n",
            "j)⊤zE,∀i∈ {1, . . . , N }, (10)\n",
            "where zEdenotes the output feature of the shared encoder,\n",
            "θD\n",
            "jrepresents the decoder of task j, and (θD\n",
            "j)⊤zEyields\n",
            "task-specific feature from the decoder. The coefficient γi,j\n",
            "manages the flow of features from source to target tasks\n",
            "within the interaction and is usually a learnable parameter.\n",
            "To emulate this task interaction within the FL context, we\n",
            "intuitively aggregate the decoder parameters as follows:\n",
            "˜θD\n",
            "i=NX\n",
            "j=1γi,jθD\n",
            "j. (11)\n",
            "Due to model incongruity in the HC-FMTL environment,\n",
            "the decoder parameters sent to the server originate from\n",
            "diverse clients with heterogeneous tasks. This intricacy\n",
            "leads to a complex landscape where decoders may align\n",
            "or diverge in both data domain and task type, requiring\n",
            "the aggregation process to discern the nuanced relation-\n",
            "ships among them. Our approach improves the decoder ag-\n",
            "gregation by adopting a cross attention mechanism to fur-\n",
            "ther promote the exchange of inter-task knowledge among\n",
            "clients with model incongruity. It calculates dependencies\n",
            "among the local updates of Kdecoders, thereby model-\n",
            "ing the interplay among tasks. Recognizing that decoders\n",
            "often exhibit varied utilities across different network lay-\n",
            "ers [6, 21, 46, 71], we apply a layer-wise strategy [49] to\n",
            "precisely capture the cross-task attention at each decoder\n",
            "layer, allows for a more fine-grained personalized aggrega-\n",
            "tion that can benefit the transfer of task-specific knowledge.\n",
            "The computation of cross attention is defined as:\n",
            "Vl= [∆θD\n",
            "1,l, . . . , ∆θD\n",
            "K,l]⊤, (12)\n",
            "˜Ai,l=Softmax (∆θD\n",
            "i,lV⊤\n",
            "l/√\n",
            "d)Vl, (13)\n",
            "where [·,·]indicates concatenation, ∆θi,land˜Ai,lare the\n",
            "original update and aggregated update for the l-th layer of\n",
            "thei-th decoder, with a dimension of d.\n",
            "3.5. Hyper Aggregation Weights\n",
            "As pointed out by pFL, a unified update for all clients is re-\n",
            "stricted in addressing client heterogeneity. Hence, we pro-\n",
            "pose Hyper Aggregation Weights, which adaptively assess\n",
            "the importance of the aggregated parameters from peers and\n",
            "empower clients with analogous data domains and task ob-\n",
            "5Table 1. Comparison to representative methods using PASCAL-Context for five Single- Task clients and NYUD-v2 for one Multi-Task\n",
            "client. ‘ ↑’ means higher is better and ‘ ↓’ means lower is better. ‘ ∆m%’ denotes the average performance drop w.r.t. local baseline.\n",
            "MethodPASCAL-Context ( ST) NYUD-v2 ( MT)\n",
            "∆m%↑ SemSeg Parts Sal Normals Edge SemSeg Depth Normals Edge\n",
            "mIoU ↑ mIoU ↑ maxF ↑ mErr ↓ odsF↑ mIoU ↑ RMSE ↓ mErr ↓ odsF↑\n",
            "Local 51.69 49.94 80.91 15.76 71.95 41.86 0.6487 20.59 76.46 0.00\n",
            "FedAvg [53] 39.98 37.33 77.56 18.27 69.17 38.94 0.7858 21.62 75.77 -11.76\n",
            "FedProx [34] 44.42 38.10 77.26 18.03 69.39 39.19 0.8068 21.52 76.03 -10.68\n",
            "FedPer [2] 54.51 46.56 78.85 16.95 71.00 44.02 0.6467 21.19 76.61 -1.11\n",
            "Ditto [35] 46.23 39.69 77.99 17.52 69.77 41.49 0.6508 20.60 76.45 -5.57\n",
            "FedAMP [24] 55.98 52.05 80.79 15.74 72.02 41.67 0.6428 20.54 76.40 1.47\n",
            "MaT-FL [8] 57.45 48.63 79.26 17.26 71.23 40.99 0.6352 20.65 76.59 -0.46\n",
            "FEDHCA257.55 52.30 80.71 15.60 72.08 41.47 0.6281 20.53 76.50 2.18\n",
            "jectives to have higher aggregation weights. This enhance-\n",
            "ment reinforces the mutual contribution from complemen-\n",
            "tary information, thus serving as high-level guidance in har-\n",
            "monizing the local updates with the collaborative updates.\n",
            "Specifically, the server maintains a dedicated set of weights\n",
            "for each client, which are applied as follows in the person-\n",
            "alized aggregation:\n",
            "θ(r)\n",
            "i=θ(r−1)\n",
            "i + ∆θ(r)\n",
            "i+ψi˜θi, (14)\n",
            "where ψidenotes the hyper weights for client Ci,i.e.αifor\n",
            "encoder or βifor decoder, and ˜θiis the aggregated update\n",
            "˜Ufrom Eq. (9) or ˜Ai,lfrom Eq. (13). It is worth noting\n",
            "that we implement distinct weights for each decoder layer\n",
            "rather than a single weight value to be consistent with the\n",
            "layer-wise computation of cross attention.\n",
            "Furthermore, we design Hyper Aggregation Weights\n",
            "to be learnable parameters that are dynamically updated\n",
            "throughout the training phase. This adaptability ensures that\n",
            "the weights are optimized in conjunction with the system’s\n",
            "overall objective. By employing the chain rule, we can de-\n",
            "rive the gradient of ψias follows:\n",
            "∇ψiLi= (∇ψiθ(r)\n",
            "i)⊤∇θ(r)\n",
            "iLi= (˜θi)⊤∇θ(r)\n",
            "iLi.(15)\n",
            "To better align this update rule with the FL paradigm, we\n",
            "can reformulate Eq. (15) by substituting gradients with\n",
            "model updates, which is the negative accumulation of gra-\n",
            "dients over batches:\n",
            "∆αi= (˜U(r))⊤∆θE,(r)\n",
            "i, (16)\n",
            "∆βi,l= (˜A(r)\n",
            "i,l)⊤∆θD,(r)\n",
            "i,l. (17)\n",
            "It indicates that the update of Hyper Aggregation Weights\n",
            "can be attained by the alteration in model parameters fol-\n",
            "lowing local training in subsequent communication rounds.\n",
            "4. Experiments\n",
            "4.1. Experimental Setup\n",
            "Datasets. We conduct experiments with two estab-\n",
            "lished benchmark datasets for multi-task dense prediction:\n",
            "PASCAL-Context [56] and NYUD-v2 [63]. The PASCAL-\n",
            "Context dataset contains 4,998 images for training and\n",
            "/g481a /g482/g481 b /g482Figure 4. Evaluation results during training. (a) Parts from\n",
            "PASCAL-Context on single-task client. (b) Normals from NYUD-\n",
            "v2 on multi-task client.\n",
            "5,105 for testing, annotated for five tasks: edge detection\n",
            "(‘Edge’), semantic segmentation (‘SemSeg’), human parts\n",
            "segmentation (‘Parts’), surface normal estimation (‘Nor-\n",
            "mals’), and saliency detection (‘Sal’). The NYUD-v2\n",
            "dataset consists of 795 training images and 654 testing im-\n",
            "ages, all depicting indoor scenes, and provides annotations\n",
            "for four tasks: edge detection, semantic segmentation, sur-\n",
            "face normal estimation, and depth estimation (‘Depth’).\n",
            "To evaluate our algorithm, we configure two HC-FMTL\n",
            "benchmark scenarios: 1) Five single-task clients address\n",
            "five tasks in PASCAL-Context, and one multi-task client ad-\n",
            "dresses four tasks in NYUD-v2; 2) Conversely, four single-\n",
            "task clients address four tasks in NYUD-v2, and one multi-\n",
            "task client addresses five tasks in PASCAL-Context. Fol-\n",
            "lowing MaT-FL [8], we set an equal number of data samples\n",
            "among the respective clients through random partitioning.\n",
            "Implementation. Our client architecture employs a pre-\n",
            "trained Swin-T [43] backbone coupled with simple FCN\n",
            "decoders and heads. Considering the varying capacities of\n",
            "datasets, we use one local epoch for PASCAL-Context and\n",
            "four for NYUD-v2, setting the total number of communi-\n",
            "cation rounds to 100 and the batch size to 8. We train all\n",
            "models using AdamW optimizer [45] with an initial learn-\n",
            "ing rate and weight decay rate set at 1e-4. We implement\n",
            "all methods with PyTorch [57] and run experiments on two\n",
            "NVIDIA RTX4090 GPUs. To adapt existing methods to the\n",
            "HC-FMTL setting, we decouple the models into encoders\n",
            "and decoders for separate aggregation across all methods.\n",
            "Metrics. We adhere to established evaluation metrics.\n",
            "Specifically, we measure semantic segmentation and hu-\n",
            "man parts segmentation using the mean Intersection over\n",
            "6Table 2. Comparison to representative methods using NYUD-v2 for four single-task clients and PASCAL-Context for one multi-task client.\n",
            "MethodNYUD-v2 ( ST) PASCAL-Context ( MT)\n",
            "∆m%↑ SemSeg Depth Normals Edge SemSeg Parts Sal Normals Edge\n",
            "mIoU ↑ RMSE ↓ mErr ↓ odsF↑ mIoU ↑ mIoU ↑ maxF ↑ mErr ↓ odsF↑\n",
            "Local 33.59 0.7129 23.22 75.02 65.80 55.01 83.23 14.21 71.89 0.00\n",
            "FedAvg [53] 25.80 0.8295 24.85 75.31 64.63 52.88 81.08 15.56 68.95 -7.56\n",
            "FedProx [34] 25.96 0.8316 25.20 75.34 64.97 50.78 81.29 15.83 69.81 -8.12\n",
            "FedPer [2] 35.93 0.7460 23.75 75.53 67.78 54.75 82.50 14.75 71.90 -0.16\n",
            "Ditto [35] 28.15 0.7482 23.96 75.42 65.99 51.45 81.74 15.29 69.96 -4.67\n",
            "FedAMP [24] 34.75 0.7103 23.31 75.03 66.08 54.10 83.35 14.20 71.88 0.27\n",
            "MaT-FL [8] 35.05 0.7504 23.39 75.33 67.90 54.78 82.84 14.58 71.94 -0.16\n",
            "FEDHCA234.95 0.7018 23.19 75.03 65.81 55.01 83.18 14.08 71.97 0.75\n",
            "Table 3. Ablation study on our proposed aggregation schemes. ‘+Enc’ and ‘+Dec’ denote the integration of Hyper Conflict-Averse\n",
            "Aggregation for the encoders and Hyper Cross Attention Aggregation for the decoders, respectively.\n",
            "MethodPASCAL-Context ( ST) NYUD-v2 ( MT)∆m%↑SemSeg ↑ Parts↑ Sal↑ Normals ↓ Edge ↑ SemSeg ↑ Depth ↓ Normals ↓ Edge ↑\n",
            "Local 51.69 49.94 80.91 15.76 71.95 41.86 0.6487 20.59 76.46 0.00\n",
            "+Enc 58.38 51.64 80.44 15.65 72.09 41.21 0.6377 20.55 76.50 1.89\n",
            "+Dec 57.39 51.65 80.75 15.69 72.06 41.48 0.6344 20.56 76.41 1.80\n",
            "+Enc+Dec 57.55 52.30 80.71 15.60 72.08 41.47 0.6281 20.53 76.50 2.18\n",
            "Union (mIoU). Saliency detection is evaluated with the\n",
            "maximum F-measure (maxF), while surface normal esti-\n",
            "mation is assessed by the mean error (mErr). Edge de-\n",
            "tection utilizes the optimal-dataset-scale F-measure (odsF),\n",
            "and depth estimation uses the Root Mean Square Error\n",
            "(RMSE). To provide an overall evaluation of different al-\n",
            "gorithms, we calculate the average per-task performance\n",
            "drop [50] relative to the local training baseline, which is\n",
            "trained without aggregation. The formula is as follows:\n",
            "∆m=1\n",
            "NPN\n",
            "i=1(−1)liMFed,i−MLocal,i\n",
            "MLocal,i,where Nis the count\n",
            "of tasks, MFed,iandMLocal ,icorrespond to the perfor-\n",
            "mance of task ifor federated methods and the local base-\n",
            "line, respectively. li= 1if a lower metric value is better for\n",
            "taski, and li= 0otherwise.\n",
            "4.2. Main Results\n",
            "To evaluate the performance of our method, we compare\n",
            "with representative works including two traditional FL ap-\n",
            "proaches FedAvg [53] and FedProx [34], three pFL meth-\n",
            "ods FedPer [2], Ditto [35], FedAMP [24], and one FMTL\n",
            "method MaT-FL [8]. The results presented in Tab. 1 and\n",
            "Tab. 2 demonstrate that F EDHCA2consistently delivers the\n",
            "best performance across most metrics. More importantly,\n",
            "it outperforms all representative methods when considering\n",
            "the average per-task performance drop, which is a widely\n",
            "acknowledged indicator for assessing the overall perfor-\n",
            "mance of MTL. In addition, Fig. 4 shows that F EDHCA2\n",
            "converges faster to a better result on different tasks.\n",
            "4.3. Indepth Analysis\n",
            "Ablation Study. An ablation study is conducted to dis-\n",
            "cern the individual contributions of each component within\n",
            "FEDHCA2, as shown in Tab. 3. The results indicate that in-\n",
            "corporating either encoder or decoder aggregation enhances\n",
            "performance relative to the baseline. The simultaneous em-Table 4. Comparison between different settings. ‘ST+Local’\n",
            "and ‘ST+Ours’ denote the setting with four single-task clients on\n",
            "NYUD-v2, trained with local baseline and F EDHCA2, respec-\n",
            "tively. ‘ST+MT+Ours’ denotes the setting in Tab. 2 trained with\n",
            "our framework. ‘ ∆m’ is calculated w.r.t. ‘ST+Local’ baseline.\n",
            "Setting SemSeg ↑Depth ↓Normals ↓Edge ↑∆m%↑\n",
            "ST+Local 33.59 0.7129 23.22 75.02 0.00\n",
            "ST+Ours 34.71 0.7170 23.25 74.98 0.64\n",
            "ST+MT+Ours 34.95 0.7018 23.19 75.03 1.44\n",
            "Table 5. Comparison to local baseline on the setting with only\n",
            "multi-task clients on two datasets.\n",
            "MethodPASCAL-Context ( MT) NYUD-v2 ( MT)∆m%↑SemSeg ↑Parts↑Normals ↓SemSeg ↑Normals ↓\n",
            "Local 64.87 53.34 14.07 39.81 20.65 0\n",
            "Ours 64.17 54.25 14.01 40.26 20.55 0.53\n",
            "ployment of both Hyper Conflict-Averse and Hyper Cross\n",
            "Attention Aggregations enables F EDHCA2to achieve opti-\n",
            "mal performance across the evaluated configurations. This\n",
            "result supports the idea that using these two aggregation\n",
            "schemes together enhances cooperation among different\n",
            "clients while simultaneously reducing negative conflicts be-\n",
            "tween various tasks.\n",
            "Impact of different FMTL scenarios. To further verify\n",
            "the necessity of introducing our new setting, we conduct ex-\n",
            "periments comparing two scenarios: 1) each client handles\n",
            "a single task, and 2) HC-FMTL encompasses both single-\n",
            "task and multi-task clients. As Tab. 4 illustrates, while\n",
            "FEDHCA2improves upon the local baseline in the single-\n",
            "task client scenario, integrating the multi-task client results\n",
            "in a greater enhancement. This improvement is attributed\n",
            "to the expanded pool of data and the knowledge jointly\n",
            "learned from additional tasks. Further experiments are car-\n",
            "ried out on another scenario of HC-FMTL setting which\n",
            "exclusively involves multi-task clients. Specifically, we se-\n",
            "lect three tasks from PASCAL-Context and two tasks from\n",
            "NYUD-v2 to create two multi-task client setups. The out-\n",
            "7Clients ScaleFigure 5. The performance changes of different methods with the\n",
            "number of clients scaling to 2 and 4 times. ‘ ∆m’ is calculated w.r.t.\n",
            "corresponding local baseline of 1C, 2C, or 4C. When the number\n",
            "of clients increases, our method can consistently provide superior\n",
            "performance, and an overall growth trend could be observed.\n",
            "/g481a /g482mutual\n",
            "speci ﬁc\n",
            "/g481b /g482\n",
            "Figure 6. Hyper Aggregation Weights αfor encoders of the client\n",
            "models. (a) Weights of five single-task clients. (b) Weights of the\n",
            "multi-task client which differs in two stages.\n",
            "comes presented in Tab. 5 align with our primary findings\n",
            "in Sec. 4.2 that nearly all metrics surpass the local baseline,\n",
            "further confirming the efficacy of our approach in this spe-\n",
            "cialized setting.\n",
            "Impact of the number of clients. To assess the effective-\n",
            "ness of F EDHCA2across varying client counts, we conduct\n",
            "tests by scaling the number of clients per task by factors of\n",
            "2 and 4, with the datasets evenly split. As depicted in Fig. 5,\n",
            "FEDHCA2consistently outperforms all comparative meth-\n",
            "ods, exhibiting a positive correlation between the number of\n",
            "clients and performance improvement. This trend contrasts\n",
            "with the performance decline seen with other methods as the\n",
            "client count increases—a result typically attributed to the\n",
            "diminished dataset available to each client and the increased\n",
            "decentralization within the federated learning system. The\n",
            "success of F EDHCA2substantiates the efficacy of the Hy-\n",
            "per Conflict-Averse Aggregation and Hyper Cross Attention\n",
            "Aggregation schemes, especially in scenarios characterized\n",
            "by pronounced data and task heterogeneity.\n",
            "Interaction between tasks. We investigate the dynamic\n",
            "learning process of Hyper Aggregation Weights for both\n",
            "encoders and decoders, aiming to understand their role in\n",
            "facilitating personalized aggregation for different clients.\n",
            "Fig. 6a reveals that the evolution of weights for encoders in\n",
            "single-task clients shows a rising trend, suggesting a consis-\n",
            "L1\n",
            "L2\n",
            "L3L4\n",
            "L5L6\n",
            "STSemSeg STParts\n",
            "STNormals STEdgeSTSal\n",
            "MTDepthMTEdge\n",
            "MTNormalMTSemSegFigure 7. Learned Hyper Aggregation Weights βacross decoders\n",
            "for different tasks, spanning layers from L1 to L6.\n",
            "tent uptake of knowledge from peers throughout the train-\n",
            "ing period. In contrast, the encoder weight of the multi-\n",
            "task client, as depicted in Fig. 6b, exhibits two stages. Ini-\n",
            "tially, the multi-task client mutually assimilates knowledge\n",
            "from single-task clients, a process that is crucial for rapid\n",
            "model convergence. The mutual learning for the multi-task\n",
            "client reaches its peak at about 20 rounds when the encoder\n",
            "weights are comparable. Subsequently, in the second phase,\n",
            "due to the heterogeneity in data and tasks, the multi-task\n",
            "client tends to enhance its feature extraction capabilities\n",
            "specific to its own data domain.\n",
            "Weights for decoders, as shown in Fig. 7, vary signif-\n",
            "icantly across different tasks and decoder layers. From\n",
            "a layer-oriented perspective, the layer closest to the out-\n",
            "put head, i.e., L6, depends least on cross-task information,\n",
            "which ensures that the final output is finely tuned to the\n",
            "specific task. In terms of task-related differences, a phe-\n",
            "nomenon markedly distinct from encoders is observed. For\n",
            "decoders of multi-task client, there is a persistent informa-\n",
            "tion integration from other tasks until the end of training.\n",
            "This empirical evidence substantiates the significance of\n",
            "employing task interaction in decoder aggregation.\n",
            "5. Conclusion\n",
            "In conclusion, this paper addresses the challenges of het-\n",
            "erogeneity in the novel Hetero-Client Federated Multi-Task\n",
            "Learning (HC-FMTL) setting through the novel F EDHCA2\n",
            "framework. By recognizing and tackling the issues of\n",
            "model incongruity, data heterogeneity, and task heterogene-\n",
            "ity, F EDHCA2learns personalized models with synergies\n",
            "of the proposed Hyper Conflict-Averse Aggregation, Hy-\n",
            "per Cross Attention Aggregation, and Hyper Aggregation\n",
            "Weights. Theoretical insights and extensive experiments\n",
            "confirm the effectiveness of our methodology. Our work\n",
            "opens possibilities for more flexible FL systems in diverse\n",
            "and realistic settings. For future work, we aim to delve into\n",
            "greater model heterogeneity that accommodates varied net-\n",
            "work structures across clients, and to integrate specific mod-\n",
            "ules into multi-task clients to further enhance task interac-\n",
            "tion, drawing on advancements in MTL.\n",
            "8References\n",
            "[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew\n",
            "Mattina, Paul Whatmough, and Venkatesh Saligrama. Fed-\n",
            "erated learning based on dynamic regularization. In ICLR ,\n",
            "2021. 2\n",
            "[2] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Ku-\n",
            "mar Singh, and Sunav Choudhary. Federated learning with\n",
            "personalization layers. CoRR , abs/1912.00818, 2019. 2, 6, 7\n",
            "[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah\n",
            "Estrin, and Vitaly Shmatikov. How to backdoor federated\n",
            "learning. In AISTATS , pages 2938–2948, 2020. 2\n",
            "[4] Xiang Bai, Hanchen Wang, Liya Ma, Yongchao Xu, et al.\n",
            "Advancing COVID-19 diagnosis with privacy-preserving\n",
            "collaboration in artificial intelligence. Nature Machine In-\n",
            "telligence , 3(12):1081–1089, 2021. 1\n",
            "[5] Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert, and\n",
            "Shadi Albarqouni. Federated disentangled representation\n",
            "learning for unsupervised brain anomaly detection. Nature\n",
            "Machine Intelligence , 4(8):685–695, 2022. 1\n",
            "[6] David Br ¨uggemann, Menelaos Kanakis, Stamatios Geor-\n",
            "goulis, and Luc Van Gool. Automated search for resource-\n",
            "efficient branched multi-task networks. In BMVC , page 359,\n",
            "2020. 5\n",
            "[7] David Br ¨uggemann, Menelaos Kanakis, Anton Obukhov,\n",
            "Stamatios Georgoulis, and Luc Van Gool. Exploring re-\n",
            "lational context for multi-task dense prediction. In ICCV ,\n",
            "pages 15869–15878, 2021. 5\n",
            "[8] Ruisi Cai, Xiaohan Chen, Shiwei Liu, Jayanth Srinivasa,\n",
            "Myungjin Lee, Ramana Kompella, and Zhangyang Wang.\n",
            "Many-task federated learning: A new problem setting and\n",
            "a simple baseline. In CVPR , pages 5037–5045, 2023. 1, 3,\n",
            "6, 7\n",
            "[9] Rich Caruana. Multitask learning. Machine learning , 28(1):\n",
            "41–75, 1997. 2\n",
            "[10] Hong-You Chen and Wei-Lun Chao. On bridging generic\n",
            "and personalized federated learning for image classification.\n",
            "InICLR , 2022. 2\n",
            "[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\n",
            "Schroff, and Hartwig Adam. Encoder-decoder with atrous\n",
            "separable convolution for semantic image segmentation. In\n",
            "ECCV , pages 801–818, 2018. 1\n",
            "[12] Yiqiang Chen, Teng Zhang, Xinlong Jiang, Qian Chen,\n",
            "Chenlong Gao, and Wuliang Huang. Fedbone: Towards\n",
            "large-scale federated multi-task learning. arXiv preprint\n",
            "arXiv:2306.17465 , 2023. 1, 3\n",
            "[13] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An-\n",
            "drew Rabinovich. Gradnorm: Gradient normalization for\n",
            "adaptive loss balancing in deep multitask networks. In ICML ,\n",
            "pages 794–803, 2018. 3\n",
            "[14] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong,\n",
            "Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov.\n",
            "Just pick a sign: Optimizing deep multitask models with gra-\n",
            "dient sign dropout. In NeurIPS , 2020. 3\n",
            "[15] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay\n",
            "Shakkottai. Exploiting shared representations for personal-\n",
            "ized federated learning. In ICML , pages 2089–2099, 2021.\n",
            "2[16] Michael Crawshaw. Multi-task learning with deep neural\n",
            "networks: A survey. arXiv preprint arXiv:2009.09796 , 2020.\n",
            "2\n",
            "[17] Ittai Dayan, Holger R. Roth, Aoxiao Zhong, Ahmed\n",
            "Harouni, Amilcare Gentili, et al. Federated learning for pre-\n",
            "dicting clinical outcomes in patients with COVID-19. Nature\n",
            "Medicine , 27(10):1735–1743, 2021. 1\n",
            "[18] Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Com-\n",
            "putation and communication efficient federated learning for\n",
            "heterogeneous clients. In ECCV , 2021. 2\n",
            "[19] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Per-\n",
            "sonalized federated learning with theoretical guarantees: A\n",
            "model-agnostic meta-learning approach. NeurIPS , 33:3557–\n",
            "3568, 2020. 2\n",
            "[20] Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L\n",
            "Yuille. Nddr-cnn: Layerwise feature fusing in multi-task\n",
            "cnns by neural discriminative dimensionality reduction. In\n",
            "CVPR , pages 3205–3214, 2019. 5\n",
            "[21] Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learn-\n",
            "ing to branch for multi-task learning. In ICML , pages 3854–\n",
            "3863, 2020. 5\n",
            "[22] Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Mu-\n",
            "rali Annavaram, and Salman Avestimehr. Spreadgnn: De-\n",
            "centralized multi-task federated learning for graph neural\n",
            "networks on molecular data. In AAAI , pages 6865–6873,\n",
            "2022. 1, 3\n",
            "[23] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,\n",
            "Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai\n",
            "Wang, et al. Planning-oriented autonomous driving. In\n",
            "CVPR , pages 17853–17862, 2023. 1\n",
            "[24] Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang,\n",
            "Jiangchuan Liu, Jian Pei, and Yong Zhang. Personalized\n",
            "cross-silo federated learning on non-iid data. In AAAI , pages\n",
            "7865–7873, 2021. 2, 6, 7\n",
            "[25] Yangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and\n",
            "Sanjeev Arora. Evaluating gradient inversion attacks and de-\n",
            "fenses in federated learning. NeurIPS , 34:7232–7241, 2021.\n",
            "2\n",
            "[26] Adri ´an Javaloy and Isabel Valera. Rotograd: Gradient ho-\n",
            "mogenization in multitask learning. In ICLR , 2022. 3\n",
            "[27] Peter Kairouz, H. Brendan McMahan, Brendan Avent, et al.\n",
            "Advances and open problems in federated learning. Found.\n",
            "Trends Mach. Learn. , 14(1-2):1–210, 2021. 1\n",
            "[28] Menelaos Kanakis, David Bruggemann, Suman Saha, Sta-\n",
            "matios Georgoulis, Anton Obukhov, and Luc Van Gool.\n",
            "Reparameterizing convolutions for incremental multi-task\n",
            "learning without task interference. In ECCV , pages 689–707,\n",
            "2020. 2\n",
            "[29] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,\n",
            "Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha\n",
            "Suresh. SCAFFOLD: stochastic controlled averaging for\n",
            "federated learning. In ICML , pages 5132–5143, 2020. 2\n",
            "[30] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\n",
            "learning using uncertainty to weigh losses for scene geome-\n",
            "try and semantics. In CVPR , pages 7482–7491, 2018. 3\n",
            "[31] Jakub Kone ˇcn´y, Brendan McMahan, and Daniel Ramage.\n",
            "Federated optimization: Distributed optimization beyond the\n",
            "datacenter. CoRR , abs/1511.03575, 2015. 1\n",
            "9[32] Jakub Kone ˇcn´y, H. Brendan McMahan, Felix X. Yu, Peter\n",
            "Richt ´arik, Ananda Theertha Suresh, and Dave Bacon. Fed-\n",
            "erated learning: Strategies for improving communication ef-\n",
            "ficiency. CoRR , abs/1610.05492, 2016. 2\n",
            "[33] Qinbin Li, Bingsheng He, and Dawn Song. Model-\n",
            "contrastive federated learning. In CVPR , pages 10713–\n",
            "10722, 2021. 2\n",
            "[34] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,\n",
            "Ameet Talwalkar, and Virginia Smith. Federated optimiza-\n",
            "tion in heterogeneous networks. In MLSys , 2020. 2, 6, 7\n",
            "[35] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia\n",
            "Smith. Ditto: Fair and robust federated learning through per-\n",
            "sonalization. In ICML , pages 6357–6368, 2021. 2, 6, 7\n",
            "[36] Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp,\n",
            "and Qi Dou. FedBN: Federated learning on non-IID features\n",
            "via local batch normalization. In ICLR , 2021. 2\n",
            "[37] Xin-Chun Li, De-Chuan Zhan, Yunfeng Shao, Bingshuai Li,\n",
            "and Shaoming Song. Fedphp: Federated personalization\n",
            "with inherited private models. In ECML PKDD , pages 587–\n",
            "602, 2021. 2\n",
            "[38] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang\n",
            "Liu. Conflict-averse gradient descent for multi-task learning.\n",
            "InNeurIPS , pages 18878–18890, 2021. 3, 5\n",
            "[39] Ken Liu, Shengyuan Hu, Steven Z Wu, and Virginia Smith.\n",
            "On privacy and personalization in cross-silo federated learn-\n",
            "ing. NeurIPS , 35:5925–5940, 2022. 3\n",
            "[40] Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin\n",
            "Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang.\n",
            "Towards impartial multi-task learning. In ICLR , 2021. 3\n",
            "[41] Shikun Liu, Edward Johns, and Andrew J. Davison. End-\n",
            "to-end multi-task learning with attention. In CVPR , pages\n",
            "1871–1880, 2019. 3\n",
            "[42] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin\n",
            "He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, and Qiang\n",
            "Yang. Vertical federated learning. CoRR , abs/2211.12814,\n",
            "2022. 2\n",
            "[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\n",
            "Zhang, Stephen Lin, and Baining Guo. Swin transformer:\n",
            "Hierarchical vision transformer using shifted windows. In\n",
            "ICCV , pages 10012–10022, 2021. 6\n",
            "[44] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S\n",
            "Yu. Learning multiple tasks with multilinear relationship net-\n",
            "works. NeurIPS , 30, 2017. 2\n",
            "[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\n",
            "regularization. In ICLR , 2019. 6\n",
            "[46] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng,\n",
            "Tara Javidi, and Rogerio Feris. Fully-adaptive feature shar-\n",
            "ing in multi-task networks with applications in person at-\n",
            "tribute classification. In CVPR , pages 5334–5343, 2017. 5\n",
            "[47] Yuxiang Lu, Shalayiding Sirejiding, Yue Ding, Chun-\n",
            "lin Wang, and Hongtao Lu. Prompt guided trans-\n",
            "former for multi-task dense prediction. arXiv preprint\n",
            "arXiv:2307.15362 , 2023. 2\n",
            "[48] Linhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan\n",
            "Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, and\n",
            "Shirui Pan. MAMDR: A model agnostic learning framework\n",
            "for multi-domain recommendation. In ICDE , pages 3079–\n",
            "3092, 2023. 5[49] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu.\n",
            "Layer-wised model aggregation for personalized federated\n",
            "learning. In CVPR , pages 10092–10101, 2022. 5\n",
            "[50] Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas\n",
            "Kokkinos. Attentive single-tasking of multiple tasks. In\n",
            "CVPR , pages 1851–1860, 2019. 2, 7\n",
            "[51] Othmane Marfoq, Giovanni Neglia, Aur ´elien Bellet, Laetitia\n",
            "Kameni, and Richard Vidal. Federated multi-task learning\n",
            "under a mixture of distributions. In NeurIPS , pages 15434–\n",
            "15447, 2021. 1, 3\n",
            "[52] Brendan McMahan, Eider Moore, Daniel Ramage, Seth\n",
            "Hampson, and Blaise Aguera y Arcas. Communication-\n",
            "efficient learning of deep networks from decentralized data.\n",
            "InAISTATS , pages 1273–1282, 2017. 2\n",
            "[53] Brendan McMahan, Eider Moore, Daniel Ramage, et al.\n",
            "Communication-efficient learning of deep networks from de-\n",
            "centralized data. In AISTATS , pages 1273–1282, 2017. 2, 6,\n",
            "7\n",
            "[54] Jed Mills, Jia Hu, and Geyong Min. Multi-task federated\n",
            "learning for personalised deep neural networks in edge com-\n",
            "puting. TPDS , 33(3):630–641, 2021. 1, 3\n",
            "[55] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-\n",
            "tial Hebert. Cross-stitch networks for multi-task learning. In\n",
            "CVPR , pages 3994–4003, 2016. 2, 5\n",
            "[56] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\n",
            "Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\n",
            "Alan Yuille. The role of context for object detection and se-\n",
            "mantic segmentation in the wild. In CVPR , pages 891–898,\n",
            "2014. 6\n",
            "[57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\n",
            "James Bradbury, Gregory Chanan, Trevor Killeen, Zem-\n",
            "ing Lin, Natalia Gimelshein, and Luca Antiga. Pytorch:\n",
            "An imperative style, high-performance deep learning library.\n",
            "NeurIPS , 32, 2019. 6\n",
            "[58] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\n",
            "sion transformers for dense prediction. In ICCV , pages\n",
            "12179–12188, 2021. 1\n",
            "[59] Jiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi,\n",
            "et al. Balanced meta-softmax for long-tailed visual recogni-\n",
            "tion. NeurIPS , 33:4175–4186, 2020. 2\n",
            "[60] Sebastian Ruder. An overview of multi-task learning in deep\n",
            "neural networks. arXiv preprint arXiv:1706.05098 , 2017. 2\n",
            "[61] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and\n",
            "Anders Søgaard. Latent multi-task architecture learning. In\n",
            "AAAI , pages 4822–4829, 2019. 2\n",
            "[62] Ozan Sener and Vladlen Koltun. Multi-task learning as\n",
            "multi-objective optimization. In NeurIPS , pages 525–536,\n",
            "2018. 3\n",
            "[63] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\n",
            "Fergus. Indoor segmentation and support inference from\n",
            "rgbd images. In ECCV , pages 746–760, 2012. 6\n",
            "[64] Shalayiding Sirejiding, Yuxiang Lu, Hongtao Lu, and Yue\n",
            "Ding. Scale-aware task message transferring for multi-task\n",
            "learning. In ICME , pages 1859–1864, 2023. 2\n",
            "[65] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and\n",
            "Ameet Talwalkar. Federated multi-task learning. In NeurIPS ,\n",
            "pages 4424–4434, 2017. 1, 2, 3\n",
            "10[66] Guolei Sun, Thomas Probst, Danda Pani Paudel, Nikola\n",
            "Popovi ´c, Menelaos Kanakis, Jagruti Patel, Dengxin Dai, and\n",
            "Luc Van Gool. Task switching network for multi-task learn-\n",
            "ing. In ICCV , pages 8291–8300, 2021. 2\n",
            "[67] Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personal-\n",
            "ized federated learning with moreau envelopes. NeurIPS ,\n",
            "33:21394–21405, 2020. 2\n",
            "[68] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.\n",
            "Towards Personalized Federated Learning. IEEE Transac-\n",
            "tions on Neural Networks and Learning Systems , pages 1–\n",
            "17, 2022. 2\n",
            "[69] Simon Vandenhende, Stamatios Georgoulis, and Luc Van\n",
            "Gool. Mti-net: Multi-scale task interaction networks for\n",
            "multi-task learning. In ECCV , pages 527–543, 2020. 2, 5\n",
            "[70] Simon Vandenhende, Stamatios Georgoulis, Wouter\n",
            "Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc\n",
            "Van Gool. Multi-task learning for dense prediction tasks: A\n",
            "survey. IEEE TPAMI , 44(7):3614–3633, 2021. 2\n",
            "[71] Matthew Wallingford, Hao Li, Alessandro Achille, Avinash\n",
            "Ravichandran, Charless Fowlkes, Rahul Bhotika, and Ste-\n",
            "fano Soatto. Task adaptive parameter sharing for multi-task\n",
            "learning. In CVPR , pages 7561–7570, 2022. 5\n",
            "[72] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris S.\n",
            "Papailiopoulos, and Yasaman Khazaeni. Federated learning\n",
            "with matched averaging. In ECCV , 2020. 2\n",
            "[73] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and\n",
            "H. Vincent Poor. Tackling the objective inconsistency prob-\n",
            "lem in heterogeneous federated optimization. In NeurIPS ,\n",
            "2020. 2\n",
            "[74] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\n",
            "Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\n",
            "Pyramid vision transformer: A versatile backbone for dense\n",
            "prediction without convolutions. In ICCV , pages 568–578,\n",
            "2021. 1\n",
            "[75] Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.\n",
            "Gradient vaccine: Investigating and improving multi-task\n",
            "optimization in massively multilingual models. In ICLR ,\n",
            "2021. 3\n",
            "[76] Chulin Xie, De-An Huang, Wenda Chu, Daguang Xu,\n",
            "Chaowei Xiao, Bo Li, and Anima Anandkumar. Perada:\n",
            "Parameter-efficient and generalizable federated learning per-\n",
            "sonalization with guarantees. CoRR , abs/2302.06637, 2023.\n",
            "2\n",
            "[77] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe.\n",
            "Pad-net: Multi-tasks guided prediction-and-distillation net-\n",
            "work for simultaneous depth estimation and scene parsing.\n",
            "InCVPR , pages 675–684, 2018. 2, 5\n",
            "[78] Yangyang Xu, Xiangtai Li, Haobo Yuan, Yibo Yang, and\n",
            "Lefei Zhang. Multi-task learning with multi-query trans-\n",
            "former for dense prediction. IEEE TCSVT , 2023. 2\n",
            "[79] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.\n",
            "Federated machine learning: Concept and applications. ACM\n",
            "Trans. Intell. Syst. Technol. , 10(2):12:1–12:19, 2019. 1\n",
            "[80] Feiyang Ye, Baijiong Lin, Zhixiong Yue, Pengxin Guo, Qiao\n",
            "Xiao, and Yu Zhang. Multi-objective meta learning. In\n",
            "NeurIPS , pages 21338–21351, 2021. 3[81] Hanrong Ye and Dan Xu. Inverted pyramid multi-task trans-\n",
            "former for dense scene understanding. In ECCV , pages 514–\n",
            "530, 2022. 2\n",
            "[82] Hanrong Ye and Dan Xu. Invpt++: Inverted pyramid multi-\n",
            "task transformer for visual scene understanding. arXiv\n",
            "preprint arXiv:2306.04842 , 2023. 2\n",
            "[83] Fuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di Wang,\n",
            "Chenchen Liu, Zhi Tian, and Xiang Chen. Fed2: Feature-\n",
            "aligned federated learning. In ACM SIGKDD , pages 2066–\n",
            "2074, 2021. 2\n",
            "[84] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine,\n",
            "Karol Hausman, and Chelsea Finn. Gradient surgery for\n",
            "multi-task learning. In NeurIPS , 2020. 3\n",
            "[85] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh,\n",
            "Kristjan H. Greenewald, Trong Nghia Hoang, and Yasaman\n",
            "Khazaeni. Bayesian nonparametric federated learning of\n",
            "neural networks. In ICML , pages 7252–7261, 2019. 2\n",
            "[86] Xiaoya Zhang, Ling Zhou, Yong Li, Zhen Cui, Jin Xie, and\n",
            "Jian Yang. Transfer vision patterns for multi-task pixel learn-\n",
            "ing. In ACM MM , pages 97–106, 2021. 2\n",
            "[87] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang\n",
            "Li, and Jian Yang. Joint task-recursive learning for semantic\n",
            "segmentation and depth estimation. In ECCV , pages 235–\n",
            "251, 2018. 2\n",
            "[88] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe,\n",
            "and Jian Yang. Pattern-affinitive propagation across depth,\n",
            "surface normal and semantic segmentation. In CVPR , pages\n",
            "4106–4115, 2019. 2\n",
            "[89] Ling Zhou, Zhen Cui, Chunyan Xu, Zhenyu Zhang, Chaoqun\n",
            "Wang, Tong Zhang, and Jian Yang. Pattern-structure diffu-\n",
            "sion for multi-task learning. In CVPR , pages 4514–4523,\n",
            "2020. 2\n",
            "[90] Ligeng Zhu, Hongzhou Lin, Yao Lu, Yujun Lin, and Song\n",
            "Han. Delayed gradient averaging: Tolerate the communi-\n",
            "cation latency for federated learning. NeurIPS , 34:29995–\n",
            "30007, 2021. 2\n",
            "[91] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free\n",
            "knowledge distillation for heterogeneous federated learning.\n",
            "InICML , pages 12878–12889, 2021. 2\n",
            "[92] Weiming Zhuang, Yonggang Wen, Lingjuan Lyu, and Shuai\n",
            "Zhang. Mas: Towards resource-efficient federated multiple-\n",
            "task learning. In ICCV , pages 23414–23424, 2023. 1, 3\n",
            "11Enhancing Robotic Manipulation: Harnessing the Power\n",
            "of Multi-Task Reinforcement Learning\n",
            "and Single Life Reinforcement Learning in Meta-World\n",
            "Ghadi Nehme1, Ishan Sabane1, Tejas Y. Deo1\n",
            "Abstract\n",
            "At present, robots typically require extensive training to successfully accomplish a\n",
            "single task. However, to truly enhance their usefulness in real-world scenarios, robots\n",
            "should possess the capability to perform multiple tasks effectively. To address this\n",
            "need, various multi-task reinforcement learning (RL) algorithms have been developed\n",
            "[1], including multi-task proximal policy optimization (PPO), multi-task trust region\n",
            "policy optimization (TRPO), and multi-task soft-actor critic (SAC). Nevertheless,\n",
            "these algorithms demonstrate optimal performance only when operating within an\n",
            "environment or observation space that exhibits a similar distribution. In reality, such\n",
            "conditions are often not the norm, as robots may encounter scenarios or observations\n",
            "that differ from those on which they were trained.\n",
            "For instance, if a robot is trained to pick and place a sphere from a specific set of\n",
            "locations, it should also possess the capability, during testing, to successfully pick and\n",
            "place a cube from different positions, at least those that are in close proximity to the\n",
            "trained positions. Addressing this challenge, algorithms like Q-Weighted Adversarial\n",
            "Learning (QWALE) [ 2] attempt to tackle the issue by training the base algorithm (gen-\n",
            "erating prior data) solely for a particular task, rendering it unsuitable for generalization\n",
            "across tasks.\n",
            "And so, the aim of this research project is to enable a robotic arm to successfully\n",
            "execute seven distinct tasks within the Meta World environment. These tasks encompass\n",
            "Pick and Place, Window Open and Close, Drawer Open and Close, Push, and Button\n",
            "Press. To achieve this, a multi-task soft actor critic (MT-SAC) is employed to train\n",
            "the robotic arm. Subsequently, the trained model will serve as a source of prior data\n",
            "for the single-life RL algorithm. The effectiveness of this MT-QWALE algorithm will\n",
            "be assessed by conducting tests on various target positions (novel positions), i.e., for\n",
            "example, different window positions in the Window Open and Close task.\n",
            "At the end, a comparison is provided between the trained MT-SAC and the MT-\n",
            "QWALE algorithm where the MT-QWALE performs better. An ablation study demon-\n",
            "strates that MT-QWALE successfully completes tasks with slightly more number of\n",
            "steps even after hiding the final goal position.\n",
            "1Stanford University\n",
            "1arXiv:2311.12854v1  [cs.AI]  23 Oct 20231 Introduction\n",
            "Reinforcement learning (RL) has made significant strides in various domains, including\n",
            "robotic manipulation, Atari games, etc. However, existing state-of-the-art RL methods\n",
            "require significantly more experience than humans to acquire even a single narrowly-defined\n",
            "skill. To make robots genuinely useful in realistic environments, it is crucial to develop\n",
            "algorithms that can reliably and efficiently learn a wide range of skills. Fortunately, in specific\n",
            "domains like robotic manipulation or locomotion, many tasks share common underlying\n",
            "structures, which can be leveraged to learn related tasks more efficiently.\n",
            "For example, most robotic manipulation tasks involve actions such as grasping, or moving\n",
            "objects within the workspace. Although current methods can learn individual skills like pick\n",
            "and place or hanging a mug or hammering a screw, we require algorithms that can efficiently\n",
            "learn and utilize shared structures across multiple related tasks. These algorithms should be\n",
            "able to learn new skills quickly by leveraging the learned structure, such as screwing a jar lid\n",
            "or hanging a bag. Recent advancements in machine learning have demonstrated exceptional\n",
            "generalization capabilities in domains like speech, implying that similar generalizations\n",
            "should be achievable in RL settings encompassing diverse tasks.\n",
            "Emergingapproachesinmeta-learningandmulti-taskreinforcementlearningshowpromise\n",
            "in addressing this challenge. Multi-task RL methods aim to learn a single policy capable of\n",
            "efficiently solving multiple tasks, surpassing the performance of learning individual tasks\n",
            "independently. On the other hand, meta-learning methods train on a multitude of tasks and\n",
            "optimize for rapid adaptation to new tasks.\n",
            "In the context of multi-task and meta-RL settings, it is crucial to address the challenge\n",
            "of agent failure when confronted with new and unseen situations. While a multi-task RL\n",
            "algorithm may exhibit excellent performance across a set of 50 tasks, it can still struggle\n",
            "significantly when faced with an observation space different from its training data. This\n",
            "limitation becomes evident in scenarios where a robotic manipulator, trained to open\n",
            "drawers at specific locations, encounters a completely different drawer position during testing.\n",
            "Similarly, for search-and-rescue disaster relief robots navigating through buildings, the\n",
            "possibility of encountering unfamiliar obstacles further highlights the need for adaptability\n",
            "and effective handling of new circumstances.\n",
            "Traditional RL algorithms aim to optimize the solution for a fixed set of tasks, without\n",
            "considering on-the-fly adaptations without human intervention. On the other hand, the\n",
            "Single Life RL (SLRL) setting encourages the autonomous and successful resolution of novel\n",
            "scenarios within a single trial. However, the current SLRL agents are primarily designed\n",
            "to handle novelties within a single environment, such as pick and place tasks or opening\n",
            "drawers.\n",
            "2 Related Work\n",
            "Many prior works and frameworks have been developed to test the effectiveness of the\n",
            "multi-task RL framework. In Meta-world: A benchmark and evaluation for multi-task and\n",
            "meta reinforcement learning [ 1], a framework has been developed to evaluate the capabilities\n",
            "of current multi-task and meta-reinforcement learning methods and make it feasible to design\n",
            "new algorithms that actually generalize and adapt quickly on meaningfully distinct tasks.\n",
            "Evaluation protocols and task suites in Metaworld are broad enough to enable this sort of\n",
            "generalization while containing sufficient shared structure for generalization to be possible.\n",
            "2You Only Live Once: Single-Life Reinforcement Learning [ 2] introduces a challenge where\n",
            "an agent must successfully complete a task within a single trial without human intervention,\n",
            "adapting to novel situations. Standard episodic RL algorithms struggle in this setting,\n",
            "prompting the proposal of Q-weighted adversarial learning (QWALE). QWALE leverages\n",
            "distribution matching and prior experience to guide the agent in novel states, resulting in\n",
            "20-60% higher success rates in single-life continuous control problems. This work provides\n",
            "valuable insights into autonomously adapting to unfamiliar situations, highlighting the\n",
            "effectiveness of distribution matching-based methods in SLRL. The major drawback of this\n",
            "approach is that the prior data for each task is collected by training a task-specific algorithm\n",
            "like SAC and thus is not able to generalize well on multiple tasks.\n",
            "Multi-task reinforcement learning with soft modularization [ 3] addresses the challenge\n",
            "of multi-task learning in reinforcement learning, emphasizing the complexity of optimizing\n",
            "shared parameters across tasks. To overcome this, an explicit modularization technique\n",
            "is introduced, utilizing a routing network to reconfigure the base policy network for each\n",
            "task. The task-specific policy employs soft modularization, combining possible routes for\n",
            "sequential tasks. Experimental results on robotics manipulation tasks demonstrate significant\n",
            "improvements in sample efficiency and performance compared to strong baselines. Even\n",
            "though the performance was significantly improved due to the soft modularization approach,\n",
            "the agent will still fail badly when there is some distribution shift in the observations.\n",
            "Actor-mimic: Deep multitask and transfer reinforcement learning [4] introduces a novel\n",
            "approach to enable agents to learn and transfer knowledge across multiple tasks. By utilizing\n",
            "deep reinforcement learning and model compression techniques, the proposed \"Actor-Mimic\"\n",
            "method trains a single policy network guided by expert teachers. The learned representations\n",
            "demonstrate the ability to generalize to new tasks without prior expert guidance, speeding\n",
            "up learning in novel environments. Atari games serve as a testing ground to showcase the\n",
            "effectiveness of the approach. Even though the approach is computationally efficient, it does\n",
            "not focus on the behavior of the agent on the same task with novel scenarios/observations.\n",
            "Comparing task simplifications to learn closed-loop object picking using deep reinforce-\n",
            "ment learning [ 5] compares reinforcement learning-based approaches for object picking in\n",
            "unstructured environments with a robotic manipulator. It shows that policies learned through\n",
            "curriculum learning and sparse rewards achieve similar success rates to those initialized on\n",
            "simplified tasks, with successful transfer to the real robot. The robotics arm is controlled by\n",
            "moving the end-effector by a certain displacement vector and changing the torque that the\n",
            "grippy fingers should apply. Thus, the robotics arm is controlled with inverse kinematics.\n",
            "3 Background\n",
            "For each task T, we examine a finite horizon Markov decision process (MDP) where Mtasks\n",
            "exist in total. These MDPs can be denoted as (S, A, P, R, H, γ ), where both the state s∈S\n",
            "and action a∈Aare continuous. The stochastic transition dynamics are represented by\n",
            "P(st+1|st, at), and the reward function is denoted as R(st, at). The horizon is defined as H,\n",
            "andγrepresents the discount factor. To represent the policy parameterized by ϕ, we utilize\n",
            "πϕ(at|st), with the objective being to learn a policy that maximizes the expected return.\n",
            "In the context of multi-task RL, tasks are randomly selected from a distribution p(T), and\n",
            "each task corresponds to a distinct MDP.\n",
            "33.1 Meta-World\n",
            "Meta-World presents an open-source simulated benchmark designed for meta-reinforcement\n",
            "learning and multi-task learning. It encompasses 50 unique robotic manipulation tasks that\n",
            "involve the interaction of a robotic arm with diverse objects possessing varying shapes, joints,\n",
            "and connectivity. Each task necessitates the robot to perform a combination of reaching,\n",
            "pushing, and grasping actions, depending on the specific task at hand.\n",
            "Figure 1: Multi-Task 10 (MT-10) Environment in Meta-World\n",
            "3.1.1 Action Space\n",
            "The action space for Meta-World can be represented with the following vector:\n",
            "a=\u0014δx:3D space of the end-effector\n",
            "τ:normalized torque that the gripper fingers should apply\u0015\n",
            "∈R4\n",
            "In the reinforcement learning context, the action is the output of a trained agent.\n",
            "3.1.2 Observation Space\n",
            "The observation space for Meta-World can be represented with the following vector:\n",
            "o=\n",
            "x(i)\n",
            "E:3D Cartesian coordinates of End-Effector\n",
            "δ(i)\n",
            "G:Measurement of how open the gripper is\n",
            "x(i)\n",
            "1:3D position of the first object\n",
            "λ(i)\n",
            "1:Quaternion of the first object\n",
            "x(i)\n",
            "2:3D position of the second object\n",
            "λ(i)\n",
            "2:Quaternion of the second object\n",
            "xGoal:3D position of the goal\n",
            "∈R39, i∈ {t, t−1}\n",
            "In the reinforcement learning context, the observation is the output of a trained agent.\n",
            "3.1.3 Reward Functions\n",
            "In Meta-World, reward functions have two key factors. First, tasks must be manageable\n",
            "by existing single-task reinforcement learning algorithms. This is crucial for evaluating\n",
            "4multi-task and meta-reinforcement learning algorithms. Second, the reward functions should\n",
            "exhibit shared structure across tasks. This promotes the transfer of knowledge and skills\n",
            "between tasks, enhancing the effectiveness of multi-task and meta-reinforcement learning in\n",
            "Meta-World.\n",
            "3.2 Soft Actor-Critic for Reinforcement Learning\n",
            "This paper introduces Soft Actor-Critic (SAC) as the training method for policy optimization.\n",
            "SAC is a deep reinforcement learning approach that combines off-policy actor-critic techniques\n",
            "and aims to achieve task success while maximizing randomness in action selection. The\n",
            "parameterized soft Q-function, denoted as Qθ(st, at), where θrepresents the parameters, is\n",
            "considered. SAC involves optimizing three types of parameters: the policy parameters ϕ, the\n",
            "parameters of the Q-function θ, and a temperature parameter α. The objective of policy\n",
            "optimization can be defined as follows:\n",
            "Jπ(ϕ) =Est∼D\u0002\n",
            "Eat∼πϕ[αlogπϕ(at|st)−Qθ(st, at)]\u0003\n",
            "(1)\n",
            "Here, αis a learnable temperature that serves as an entropy penalty coefficient. It can\n",
            "be learned to maintain the desired entropy level of the policy, using the following equation:\n",
            "J(α) =Eat∼πϕh\n",
            "−αlogπϕ(at|st)−α˜Hi\n",
            "where ˜Hrepresents the desired minimum expected entropy. If the optimization of\n",
            "logπt(at|st)increases its value and reduces entropy, αis adjusted accordingly to increase\n",
            "the process.\n",
            "3.3 Multi-task Reinforcement Learning\n",
            "To extend SAC from single-task to multi-task scenarios, we introduce a single, task-\n",
            "conditioned policy π(a|s, z), where zrepresents a task embedding. The policy is optimized\n",
            "to maximize the average expected return across all tasks sampled from the distribution p(T).\n",
            "The objective of policy optimization is defined as follows:\n",
            "Jπ(ϕ) =ET ∼p(T)[Jπ,T(ϕ)],\n",
            "where Jπ,T(ϕ)is directly adopted from Equation 1, incorporating the specific task T.\n",
            "Similarly, for the Q-function, the objective is as follows:\n",
            "JQ(θ) =ET ∼p(T)[JQ,T(θ)]\n",
            "3.4 Single Life Reinforcement Learning\n",
            "Single-life reinforcement learning enables autonomous task completion in a single trial, in\n",
            "the presence of a novel distribution shift, by leveraging prior data.\n",
            "The prior data Dpriorconsists of transition data from transitions from source MDP\n",
            "Msource. The agent interacts with the target MDP defined by Mtarget (S, A, ˜P, R, ˜ρ, γ)\n",
            "The target MDP is assumed to have a novelty that is not present in the source MDP\n",
            "such as different dynamics or differences in the initial and final state distribution ˜ρ. The\n",
            "more similar the source and the target domains are, the effectiveness of the SLRL algorithm\n",
            "5would be much better. The main aim in the SLRL setting is to maximize J=Ph\n",
            "t=0γtR(st)\n",
            "whilst minimizing the number of steps taken to solve the task.\n",
            "Figure 2: Single Life Reinforcement learning [2]\n",
            "3.5 Q-weighted Adversarial Learning (QWALE)\n",
            "The desired behavior of the algorithm is to guide the agent toward the distribution of prior\n",
            "data, facilitating recovery from out-of-distribution states and encouraging task completion\n",
            "simultaneously. SLRL assumes access to have sub-optimal offline prior data making it\n",
            "agnostic to the quality of the prior data. The main aim of SLRL is to match the state-\n",
            "action pairs that will lead to task completion rather than matching the entire state-action\n",
            "distribution. By learning a discriminator which can classify whether states belong to prior\n",
            "data or not, the algorithm can down-weigh states which are not in the prior data using the Q\n",
            "values for the state action pairs. Thus, from the paper [ 2], Q-weighted adversarial learning\n",
            "(QWALE), minimizes DJS\u0000\n",
            "ρπ∥ρ∗\n",
            "target\u0001\n",
            "as follows:\n",
            "min\n",
            "πDJS\u0000\n",
            "ρπ(s, a)∥ρ∗\n",
            "target (s, a)\u0001\n",
            "= min\n",
            "πmax\n",
            "DEs,a∼ρ∗\n",
            "target[logD(s, a)] +Es,a∼ρπ[log(1 −D(s, a))]\n",
            "= min\n",
            "πmax\n",
            "DE\n",
            "s,a∼ρβ\u0014exp (Qπtarget (s, a)−Vπtarget (s))\n",
            "Eρβ[exp ( Qπtarget (s, a)−Vπtarget (s))]logD(s, a)\u0015\n",
            "+E\n",
            "s,a∼ρπ[log(1 −D(s, a))]\n",
            "≡min\n",
            "πmax\n",
            "DEs,a∼ρβ[exp ( Qπtarget(s, a)−Vπtarget(s)) log D(s, a)] +Es,a∼ρπ[log(1 −D(s, a))],\n",
            "where Eρs[exp ( Qπtarget(s, a)−Vπtarget(s))]canbeignored.\n",
            "QWALE trains a weighted discriminator (D) function using a fixed Q-function Q(s, a)\n",
            "trained in the source MDP to distinguish between useful transitions and the ones that are\n",
            "less useful. The discriminator outputs essentially act as rewards for the agent (actor) taking\n",
            "actions in the target MDP where +1 represents that the agent’s transitions are useful and\n",
            "0 represents that the transitions were not so useful. The reward for the given state action\n",
            "pair is updated using the negative log likelihood of the probability that the current state\n",
            "does not belong to the prior data. The reward for the given state action pair reduces when\n",
            "the state does not fall closer to the prior data, and using this, the online agent learns the\n",
            "dynamically changing environment to accomplish its newer task.\n",
            "64 Methodology\n",
            "4.1 Problem Statement\n",
            "We want to train a robotic arm to be capable of achieving multiple tasks of the MT-10 in a\n",
            "single life. We select seven tasks from the MT-10 (Figure 3). To do that, we first train an\n",
            "MT-SAC agent on these seven tasks and save the final replay buffer. We will explore different\n",
            "task embedding and select the one that optimizes the performance of the Multi-Task SAC\n",
            "on the seven tasks. Then, we will use this trained policy and replay buffer as the prior data\n",
            "of a Multi-Task QWALE.\n",
            "Replay Buffer \n",
            "Meta-World \n",
            "Environment \n",
            "Actor Critic Sample \n",
            "Tuples Sample \n",
            "Tuples \n",
            "Store Tuples \n",
            "Policy Network Improve Policy \n",
            "Evaluate Policy Soft  Update Q𝜃1 ( s, a), Q𝜃2( s, a)\n",
            "at st \n",
            "Q𝜃1 ( s, a), Q𝜃2( s, a)𝜋𝝓( a| s ) Task Embedding \n",
            "Target Q-networks Main Q-networks Tasks MT-SAC \n",
            "MT-QWALE Prior Data Discriminator \n",
            "Replay Buffer \n",
            "Meta-World \n",
            "Environment \n",
            "Actor Critic Sample \n",
            "Tuples Sample \n",
            "Tuples \n",
            "Store Tuples \n",
            "Policy Network Improve Policy \n",
            "Evaluate Policy Soft  Update Q𝜃1 ( s, a), Q𝜃2( s, a)\n",
            "at st \n",
            "Q𝜃1 ( s, a), Q𝜃2( s, a)Task Embedding \n",
            "Target Q-networks Main Q-networks Initialize Update \n",
            "𝜋 ( a| s ) q𝝓(prior | s ) \n",
            "Update rewards \n",
            "Figure 3: Training Multi-Task QWALE\n",
            "Toevaluatethemodel, wewillruntheMulti-TaskQWALEintheseventasksenvironments\n",
            "where the objects are randomly positioned, to introduce novelty in the environments. We\n",
            "will then record the number of successes and the average number of steps to completion and\n",
            "compare it to the performance of MTSAC. Finally, we will do an ablation study by hiding\n",
            "the final goal and observe whether the Multi-Task Agent is still able of achieving the tasks.\n",
            "74.2 Multi-Task SAC\n",
            "4.2.1 Model Architecture\n",
            "We use multi-task SAC as the algorithm to optimize our multi-task agent. For our critic,\n",
            "value and agent networks we use the architectures shown in Figure 4.\n",
            "Thenactions = 4, since the dimension of the action space of Meta-World is 4. Moreover,\n",
            "the dimension of the state space or observation space is 46 since we concatenate the Meta-\n",
            "World observation (39) with the task embedding (7). For training, this model, γ= 0.99,\n",
            "β= 0.0003andα= 0.0003which are the learning rates for the critic and value networks,\n",
            "and the actor-network respectively. We finally use a reward scale rscale= 2andτ= 0.005\n",
            "for the factor by which we are going to modulate the soft update of our target networks.\n",
            "Figure 4: Model Architectures of the Critic, Value, and Actor Networks.\n",
            "4.2.2 Task Embedding\n",
            "We concatenate the observation of the Meta-World with three different types of task embed-\n",
            "ding:\n",
            "•One-Hot Encoding which associates each task a unit basis vector ek.\n",
            "•Sine Encoding whichassociateseachtask ktothecorrespondingvector: [sin(k),sin(2k), . . . , sin(mk)],\n",
            "given that we have mtasks.\n",
            "•Learned Encoding which associates to a each task kthe following task embedding:\n",
            "Mek, where ekis a unit basis vector and Mis a learned matrix.\n",
            "84.3 Multi-task QWALE\n",
            "We introduce Muti-Task QWALE: Leveraging the Multi-Task SAC algorithm which performs\n",
            "poorly on novel tasks. We tweak the QWALE algorithm to allow the agent to handle a\n",
            "variety of tasks. In this approach, we generate the prior data using the trained MT-SAC\n",
            "algorithm on the different tasks in the 7 environments. For a given task, the Multitask\n",
            "QWALE weights the prior data which contains the same task ID more as compared to the\n",
            "data from the other task. The prior data unrelated to the task is weighted 50% less than\n",
            "the task-specific data in the prior buffer. The discriminator training remains the same and\n",
            "continues to classify the state as useful followed by weighting the data point using the Q\n",
            "values. Using this, the Multi-Task QWALE allows us to recover from bad states and to keep\n",
            "proceeding toward the goal states based on the prior data.\n",
            "The discriminator output is used to update the reward for the given state-action pair. The\n",
            "Multi-Task SAC keeps on training online on the observed states using this updated reward\n",
            "function. This allows the model to learn the environment continuously while preferring to\n",
            "visit states which are good if it gets stuck in a bad position or keeps on going to the same\n",
            "states.\n",
            "5 Experiments, Results & Discussion\n",
            "We are building a model capable of completing multiple tasks in asingle trial , in the\n",
            "presence of novel distribution shifts . In our case, the tasks are defined in Fig. 3. The\n",
            "novelties are to randomly vary the positions of the objects in the environments within a\n",
            "perimeter of radius 0.3 around the position it was trained on.\n",
            "5.1 Multi-Task SAC\n",
            "We first train an MT-SAC on the 7 tasks, for 10000 episodes per task to collect the prior\n",
            "data needed for MT-QWALE. We repeat this experiment for each task embedding and select\n",
            "the task embedding that leads to the best performance to use for the Multi-Task QWALE.\n",
            "We observe that using the sine encoding leads to the best performance (Fig. 5). In fact,\n",
            "using a sine encoding leads to sending more signals to the model, compared to a one-hot\n",
            "encoding, which leads to more shared structure between the tasks while still being linearly\n",
            "independent task embedding. Nonetheless, it is quite surprising to see that the learned\n",
            "embeddings achieved a very low success rate. In fact, we expected the model to learn the\n",
            "best representation of each task that will maximize the performance of the algorithm in each\n",
            "task.\n",
            "Figure 5: Success Rates of MTSAC in different environments\n",
            "9We also observe that the algorithm is less accurate when executing tasks that combine\n",
            "grasping and moving. In fact, the model was able to perform better on simple tasks that\n",
            "require the arm to only move for example Window Close or Drawer Close. On the other\n",
            "side, the model performs poorly on more complex tasks that require the combination of one\n",
            "or more elementary tasks like Pick and Place or Drawer Open which both require some sort\n",
            "of grasping and moving actions.\n",
            "5.2 Multi-Task QWALE\n",
            "Now that the prior data is collected, we run MT-QWALE in the 7 environments where\n",
            "objects are initialized in random positions. We run each experiment 10 times and collect the\n",
            "success rates of MT-QWALE in each environment as well as the average number of steps to\n",
            "completion. We obtain the following plots:\n",
            "Figure 6: Success Rate of single life RL experiments in different environments\n",
            "Figure7: AveragenumberofstepsofsinglelifeRLexperimentsindifferentenvironments\n",
            "We observe that all the tasks achieve almost 100% success rate except for the Pick and\n",
            "Place task which failed every single time (Fig. 6). Pick and Place is the most complex task\n",
            "in the subset of Meta-World tasks which explains the observed results. It is a combination\n",
            "of moving, grasping, and placing which is much more complex than the other tasks which\n",
            "require just movement and sometimes some grasping.\n",
            "On the other side, when looking at the average number of steps required to achieve a task\n",
            "10there is a close correlation between the number of elementary tasks required to achieve a task\n",
            "and the average number of steps to complete the task. In fact, in Fig. 7, simple tasks that\n",
            "require only one to two elementary tasks like Window Open, Drawer Close, or Push require\n",
            "less number of steps to complete the task. We also observe that more complex tasks that\n",
            "require a combination of grasping and moving like Drawer Open need more steps to complete\n",
            "the task. An unexpected result is the Window Close Task, which despite being a simple\n",
            "task requiring only a linear movement of the arm in the opposite direction of Window Open\n",
            "takes much more steps to complete compared to Window Open. The reason behind such\n",
            "behavior might be that the prior data as a whole contains knowledge that is more relevant\n",
            "to solving the task Window Open compared to Window Close. Thus, QWALE would be\n",
            "able to complete the former in fewer steps than the latter.\n",
            "5.2.1 Comparing MT-QWALE and MT-SAC in environments with novelty\n",
            "(a) Window Open\n",
            " (b) Drawer Open\n",
            " (c) Press Button\n",
            "(d) Window Close\n",
            " (e) Drawer Close\n",
            " (f) Push\n",
            "Figure 8: Trajectories of end-effector comparing the performance of MT-SAC and MT-\n",
            "QWALE in different environments with novelty\n",
            "We observe that MT-QWALE provides a big improvement in the performance of MT-SAC\n",
            "in environments with novelty, except for the Pick and Place task. In fact, in rare cases, we\n",
            "observe that both algorithms achieve the tasks in a similar number of steps as we see in\n",
            "the window open environment (Fig 8a). In the cases where MT-SAC fails in environments\n",
            "with novelty, we observe two different behaviors of MT-QWALE. In some cases, we observe\n",
            "that MT-QWALE achieves the task in a very little number of steps like in the Drawer Close\n",
            "environment (Fig 8e). This might be the case due to the fact that this is a simple task that\n",
            "requires the arm to just move and thus realize an elementary task. In other cases, we observe\n",
            "that MT-QWALE achieves the task with many steps like in the Drawer Open Environment\n",
            "(Fig (Fig 8b)), where the robotic arm had to both grasp the handle and move to achieve this\n",
            "task. This is a more complex task that requires two elementary tasks which may have led to\n",
            "11this higher number of steps to achieve the task.\n",
            "5.2.2 Ablation Study: MT-QWALE with hidden end goal\n",
            "Figure 9: Success Rate of single life RL experiments in different environments with\n",
            "Goal Masking\n",
            "Figure 10: Average number of steps of single life RL experiments in different environ-\n",
            "ments with Goal Masking\n",
            "In this ablation study, we conducted an experiment by concealing the end goal from the\n",
            "observation vector during action selection. Specifically, we assigned zero values to the end\n",
            "goal position. Our aim was to examine how the performance of the MT-QWALE agent is\n",
            "affected by the absence of end goal information.\n",
            "Interestingly, we found that both masking and not masking the end goal resulted in a\n",
            "comparable success rate. This indicates that the MT-QWALE agent is capable of accomplish-\n",
            "ing the task without direct knowledge of the end goal, relying solely on reward feedback to\n",
            "adjust its actions. However, we did observe that when the end goal was masked, the average\n",
            "number of steps taken was greater compared to when it was not masked. This outcome\n",
            "aligns with our expectations.\n",
            "Essentially, by reducing the input signal pertaining to the end goal, we anticipated\n",
            "that the model would make more errors before successfully completing the tasks, thereby\n",
            "necessitating a greater number of steps.\n",
            "126 Conclusion\n",
            "Inconclusion, wepresentMT-QWALE,anovelapproachformulti-taskQ-weightedadversarial\n",
            "learning. MT-QWALE builds upon QWALE, adapting it to handle multiple tasks within a\n",
            "single trial. We conduct experiments using seven tasks from Meta-World, namely Window\n",
            "Open and Close, Drawer Open and Close, Pick and Place, Push, and Button Press.\n",
            "To collect prior data for MT-QWALE, we train MT-SAC on the seven tasks using the\n",
            "best task embedding method we explored, which is sine encoding. We then evaluate the\n",
            "performance of MT-QWALE in these seven environments, incorporating a novelty factor by\n",
            "randomly placing objects within the environment. In comparison to MTSAC, MT-QWALE\n",
            "demonstrates successful completion of all tasks except for the most complex one, Pick and\n",
            "Place.\n",
            "To further investigate the impact of goal position on MT-QWALE’s performance, we\n",
            "conduct an ablation study. We find that hiding the end goal generally leads to an increase\n",
            "in the number of steps required to complete the task, highlighting MT-QWALE’s ability to\n",
            "navigate solely based on reward feedback.\n",
            "For future research, we have several plans in mind. Firstly, we intend to extend this study\n",
            "to include additional tasks and compare the results of MT-QWALE with QWALE using only\n",
            "prior data from a single task. Additionally, we aim to explore different model architectures\n",
            "for the value, actor, and critic networks, as well as investigate alternative methods for task\n",
            "ID embedding. Another avenue of exploration involves conducting ablation studies on other\n",
            "components of the observation vector.\n",
            "Finally, we are interested in introducing various types of novelties into the environment,\n",
            "such as changing gravity or adding wind, to further enhance the capabilities of our ap-\n",
            "proach. These future endeavors will allow us to deepen our understanding and improve the\n",
            "performance of MT-QWALE in a broader range of scenarios.\n",
            "References\n",
            "[1]Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn,\n",
            "and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta\n",
            "reinforcement learning. In Conference on robot learning , pages 1094–1100. PMLR, 2020.\n",
            "[2]Annie Chen, Archit Sharma, Sergey Levine, and Chelsea Finn. You only live once: Single-\n",
            "life reinforcement learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\n",
            "and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35,\n",
            "pages 14784–14797. Curran Associates, Inc., 2022.\n",
            "[3]Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning\n",
            "with soft modularization. Advances in Neural Information Processing Systems , 33:4767–\n",
            "4777, 2020.\n",
            "[4]Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask\n",
            "and transfer reinforcement learning. arXiv preprint arXiv:1511.06342 , 2015.\n",
            "[5]Michel Breyer, Fadri Furrer, Tonci Novkovic, Roland Siegwart, and Juan Nieto. Com-\n",
            "paring task simplifications to learn closed-loop object picking using deep reinforcement\n",
            "learning. IEEE Robotics and Automation Letters , 4(2):1549–1556, 2019.\n",
            "13MULTI LORA: D EMOCRATIZING LORA FOR BETTER\n",
            "MULTI -TASK LEARNING\n",
            "Yiming Wang, Yu Lin, Xiaodong Zeng, Guannan Zhang\n",
            "Ant Group\n",
            "Shanghai, China\n",
            "ABSTRACT\n",
            "LoRA achieves remarkable resource efficiency and comparable performance when adapting LLMs\n",
            "for specific tasks. Since ChatGPT demonstrated superior performance on various tasks, there\n",
            "has been a growing desire to adapt one model for all tasks. However, the explicit low-rank of\n",
            "LoRA limits the adaptation performance in complex multi-task scenarios. LoRA is dominated by a\n",
            "small number of top singular vectors while fine-tuning decomposes into a set of less important unitary\n",
            "transforms. In this paper, we propose MultiLoRA for better multi-task adaptation by reducing the\n",
            "dominance of top singular vectors observed in LoRA. MultiLoRA scales LoRA modules horizontally\n",
            "and change parameter initialization of adaptation matrices to reduce parameter dependency, thus\n",
            "yields more balanced unitary subspaces. We unprecedentedly construct specialized training data\n",
            "by mixing datasets of instruction follow, natural language understanding, world knowledge, to\n",
            "cover semantically and syntactically different samples. With only 2.5% of additional parameters,\n",
            "MultiLoRA outperforms single LoRA counterparts and fine-tuning on multiple benchmarks and\n",
            "model scales. Further investigation into weight update matrices of MultiLoRA exhibits reduced\n",
            "dependency on top singular vectors and more democratic unitary transform contributions1.\n",
            "1 Introduction\n",
            "In recent years, Large Language Models (LLMs) have manifested unprecedentedly superior performance in various\n",
            "natural language processing tasks[ 1,2,3,4]. As model scales up, high-level multi-task capabilities[ 5] emerges from\n",
            "LLMs. Capabilities such as real-world knowledge, logic reasoning and arithmetic skills from one LLM proves the\n",
            "feasibility of \"one model for all tasks\". However, scaling up LLMs by adding billions of parameters not only bring\n",
            "emergent abilities and grokking, but also dramatically increase training and down-stream adaptation costs. For instance,\n",
            "parameter counts of LLaMA[ 4] series range from 7 billion to 65 billion, and GPT-3[ 2] contains up to 175 billion\n",
            "parameters. Full parameter fine-tuning these models for down-stream adaptation yields huge amount of memory\n",
            "footprint and thus requires prohibitively expensive hardwares.\n",
            "To address the issue of hardware requirements for LLM adaptation, a solution called Parameter Efficient Fine-Tuning\n",
            "(PEFT) has been proposed. PEFT methods reduce VRAM usage of cached optimizer states[ 6] by only optimizing a\n",
            "fraction of model parameters while keeping the rest frozen. Various PEFT methods, such as adapter[ 7], p-tuning[ 8],\n",
            "IA3[9] and LoRA[ 6], have been suggested. Compared to other PEFT methods, LoRA possesses the advantages of: 1)\n",
            "high modularity for distribution, 2) mergeable weights for zero inference overhead. While LoRA has proven successful\n",
            "in single-task adaptation, its performance in more intricate multi-task settings of generative AI remains unexplored.\n",
            "Thus, a crucial question lingers: Can LoRA effectively adapts LLMs to complex multi-task scenarios as full parameter\n",
            "fine-tuning does?\n",
            "Works on applying PEFT methods on multi-task learning scenarios are in literature, albeit with certain limitations[ 10,\n",
            "11,12,13]. These proposed methods manage to improve multi-task benchmark performances with task information\n",
            "sharing or activation routing[ 13,10,11]. However, these dedicated modules add unaffordable overhead to transformer\n",
            "inference, which hinders their industrial application[ 6,14]. Another limitation is that the prior works focused on Natural\n",
            "1Our code is coming to GitHub soon.arXiv:2311.11501v1  [cs.LG]  20 Nov 2023PRIME AI paper\n",
            "Language Understanding (NLU), which may not be suitable for current generative LLMs. A mixture of NLU tasks\n",
            "are commonly used[ 10,11] despite that data samples of these tasks do not present much semantical or syntactical\n",
            "difference among them. More benchmarks on tasks of interest of generative LLMs, like instruction following, logic\n",
            "reasoning should be taken into consideration.\n",
            "Therefore, the research goal of this paper is to adapt LoRA for better multi-task learning while maintaining modularity\n",
            "and zero inference overhead of LoRA. We firstly reveal the fundamental difference between LoRA and full parameter\n",
            "fine-tuning with the help of Singular Value Decomposition (SVD, Section 3.2). We found dominance of top singular\n",
            "vectors in LoRA while fine-tuning is more democratic, as the residual weight decomposes into larger sets of unitary\n",
            "transforms of smaller importance. In order to mitigate the observed dominance, we propose to horizontally scale\n",
            "LoRAmodules (Section 3.3). MultiLoRA horizontally scales lora modules to reduce parameter dependency. Multi-\n",
            "LoRA divides LoRA along the rank, add learnable scaling factor and change the parameter initialization to enhance\n",
            "expressiveness of lora modules. Compared to conventional LoRA, MultiLoRA produces more democratic weight\n",
            "update matrices as those of full parameter fine-tuning.\n",
            "To better demonstrate the effectiveness of MultiLoRA, we constructed a comprehensive dataset composed of various\n",
            "tasks relevant to generative LLMs. A series of datasets from different domains are selected including instruction\n",
            "following[ 15], world knowledge[ 16], arithmetic reasoning[ 17] and NLU[ 18]. Both context and target of samples in\n",
            "aforementioned datasets exhibit strong semantical and syntactical differences, thus augmenting adaptation difficulty.\n",
            "With our multi-task datasets, we conducted extensive empirical experiments on LLaMA ranging from 7B to 65B.\n",
            "On benchmarks of MMLU[ 16] and SuperGLUE[ 18], we found MultiLoRA consistently outperforms LoRA even\n",
            "under smaller parameter budget and can perform on-par with full-parameter fine-tuning. We further dive into obtained\n",
            "weight update matrices with SVD. Side by side comparison to full parameter fine-tuning suggests that MultiLoRA ex-\n",
            "hibit a higher degree of subspace overlapping and more similar singular value distribution, indicating successful\n",
            "democratization of unitary transforms contribution.\n",
            "Therefore, our main contributions can be summarized as follows:\n",
            "•We find dominance of unitary transforms in weight update matrices of LoRA, while fine-tuning produces more\n",
            "democratic contribution distribution.\n",
            "•We propose MultiLoRA to mitigate dominance seen in LoRAand democratize contributions of its unitary\n",
            "transforms.\n",
            "•We propose a multi-task learning scheme based on mixture of tasks of interest of generative LLMs, to cover\n",
            "semantically and syntactically different samples. Our proposed MultiLoRA exhibits stronger consistency than\n",
            "LoRA and can outperform full parameter fine-tuning on various tasks and model scales.\n",
            "2 Related Work\n",
            "2.1 PEFT\n",
            "PEFT methods lowers hardware requirement of model fine-tuning by significantly reducing trainable parameters\n",
            "and consequently optimizer states cached in VRAM. By exploiting the local optimum of a pretrained model, a\n",
            "much smaller solution space brought by reduce trainable parameters helps PEFT methods achieve comparable tuning\n",
            "performance[ 19,20]. PEFT can be classified into two categories: 1) reparameterization-based methods[ 21,22] that\n",
            "retrain a portion of the parameters and 2) addition-based methods that train additional parameters[ 6,23,7]. Recent\n",
            "works in PEFT focus on resource efficiency[ 7,9,6,23]. LoRA[ 6] fits incremental weights by decomposing them\n",
            "into low-rank matrices. (IA)3tunes hidden states with learned multipliers. AdaLoRA[ 23] adds importance-aware\n",
            "pruning mechanisms to further improve resource efficiency. There’re also work focusing on ensemble learning with\n",
            "adapters. AdaMix[ 13] and UniPELT[ 24] integrate existing PEFT methods into a unified framework to boost adaptation\n",
            "performance.\n",
            "2.2 Multi-Task Learning with PEFT\n",
            "In multi-task learning with PEFT, adapter is utilized for code summarization across different programming languages[ 12].\n",
            "HyperFormer[ 10] assigns task-related weights to adapter[ 7] activations using shared hypernets across layers and tasks.\n",
            "Multitask Prompt Tuning[ 11] extends prompt tuning by firstly distilling from source prompts adapted for various tasks\n",
            "and further finetunes with low rank updates. While these methods have shown effectiveness, the additional weights\n",
            "cannot be seamlessly integrated into the base model, resulting in inevitable inference latency that is impractical for LLM\n",
            "serving[ 6,14]. Moreover, the emphasis in the multi-task setting has predominantly been on NLU tasks, disregarding\n",
            "the tasks that are of interest to generative LLMs.\n",
            "2PRIME AI paper\n",
            "3 Method\n",
            "3.1 Background\n",
            "Before formal explanation on design choices of MultiLoRA, a few notations are proposed base on LLaMA and LoRA.\n",
            "3.1.1 LLaMA\n",
            "LLaMA model consists of Lstacked decoder layers, where each block contains two submodules: a multi-head attention\n",
            "(MHA) and a fully connected FFN. Given the input sequence x∈Rn×d, MHA performs the attention function in\n",
            "parallel h heads:\n",
            "head i=Softmax (xWq_proj\n",
            "i (xWk_proj\n",
            "i )⊤\n",
            "√dn)xWv_proj\n",
            "i ,MHA (x) =Concat (head 1, . . . , head n)Wo_proj, (1)\n",
            "where Wq_proj\n",
            "i , Wk_proj\n",
            "i , Wv_proj\n",
            "i ∈Rd×dhare query, key and value projections of ithattention head and Wo_proj∈\n",
            "Rd×dis the output projection to aggregate multi-head outputs. dhis typically set to d/h. The other important module is\n",
            "a MLP which consists of three linear transformations, namely up_proj, down_proj, gate_proj with a SwiGLU activation\n",
            "in between:\n",
            "MLP(x) =SwiGLU (xWup_proj(xWgate _proj))Wdown _proj, (2)\n",
            "where Wup_proj, Wgate _proj∈Rd×dmid, dmid> d andWdown _proj∈Rdmid×d. Layer normalization is applied\n",
            "before and after the attention module.[4]\n",
            "3.1.2 Low-Rank Adaptation\n",
            "Given target module with weight W∈Rd×k, LoRA inserts two sequential low rank matrices to fit the residual weights\n",
            "for adaptation. The forward computation of adapted module writes as follow:\n",
            "y′=y+ ∆y=Wx+BAx, (3)\n",
            "where A∈Rd×r, B∈Rr×kwithr≪min(d, k). Either AorBis initialized with zeroes and the other is initialized\n",
            "with Kaiming Uniform[ 25] to force ∆y= 0at the very beginning. Analysis on weight update matrices suggest that\n",
            "LoRA work by enhancing existing feature transforms in original model weight[6].\n",
            "3.2 Difference between LoRA and fine-tuning\n",
            "Although LoRA achieves comparable performances to fine-tuning on many benchmarks, it is essential to understand the\n",
            "underlying differences between the two approaches. To shed light on this question, We train LLaMA-7B on Alpaca and\n",
            "MMLU using both methods2to get weight update matrices ∆Wand conduct an analysis of the weight update matrices\n",
            "using SVD.\n",
            "Figure 1 illustrates singular value distribution of ∆WFTand∆WLoRA. For better visualization, we plot the negative\n",
            "logarithms of the singular values ( −log(s)) . The empirical distribution of fine-tuning exhibits a bell-shaped curve\n",
            "while the distribution for LoRA falls at both ends of the spectrum. The extreme bimodal distribution of LoRA arises\n",
            "from the constraint that Rank (∆WLoRA)should not exceed r, resulting in at least ( k−r) singular values being zero.\n",
            "Interestingly, we also noticed an inverse trend in the counts of top singular values. In LoRA, the count increased with\n",
            "the magnitude of the singular values, while fine-tuning exhibited the opposite behavior. This suggests that LoRA\n",
            "predominantly relies on a small group of singular vectors, whereas fine-tuning distributes importance more evenly\n",
            "among singular vectors. Such phenomenon can also be observed on LoRA trained on other datasets or publicly available\n",
            "LoRA weights, indicating observed dominance arises from the structural design of LoRA (refer to Appendix B for\n",
            "more examples).\n",
            "Based on these findings, we can infer that the dominance observed in LoRA may limit its adaptation performance,\n",
            "particularly in complex multi-task scenarios that require enhancement of multiple distinct feature transforms. ∆Wof\n",
            "full parameter fine-tuning decomposes into a larger set (more specifically equals rank of original weight matrix) of\n",
            "unitary transforms. In contrast, LoRA’s explicit rank limitation restricts it to decompose into a smaller number ( r) of\n",
            "unitary transforms. As a result, the expressiveness of LoRA may be constrained compared to full parameter fine-tuning.\n",
            "3.3 Scaling LoRA to Democratize Unitary Transform Contribution\n",
            "2LoRA hyperparameters set to r= 64 andα= 64\n",
            "3PRIME AI paper\n",
            "1e1.0 1e2.0 1e3.0 1e4.0 1e5.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "0100200300400500Count\n",
            "LoRA Full Parameter Fine-tuning\n",
            "(a)\n",
            "1e1.0 1e1.5 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "050100150200250300350Count\n",
            "LoRA Full Parameter Fine-tuning (b)\n",
            "Figure 1: Top singular value distribution of weight update matrix of ∆Wv_proj. (a) Complete view of the histogram. (b)\n",
            "Close-up view on top singular values. Both histograms are plotted based on on the negative logarithms of the singular\n",
            "values −log(s), where left end of horizontal axis represents larger singular values. Bell-shape curved of full parameter\n",
            "fine-tuning indicates a democratic composition of a large number of relatively less important unitary transforms. On\n",
            "the hand, LoRA heavily relies on a small group of important unitary transforms, which could hurt complex multi-task\n",
            "adaptation.\n",
            "+\n",
            "Figure 2: Overview of MultiLoRA. Multiple\n",
            "parallel LoRA modules are used to adapt tar-\n",
            "get weight matrix. Parameter initialization and\n",
            "zero-initialized scaling factor are introduced to\n",
            "democratize residual weight updates.In Section 3.2, we observe a small group of top singular vectors dom-\n",
            "inate weight update matrices of LoRA ∆WLoRAwhile top singular\n",
            "vectors contribute more evenly to ∆WFT. In order to match fine-\n",
            "tuning in complex task adaptation, we propose MultiLoRA aiming at\n",
            "producing less polarized weight update matrices ∆W. MultiLoRA in-\n",
            "serts multiple parallel LoRAs to reduce parameter sharing, changes\n",
            "parameter initialization to enable larger optimization search space\n",
            "and implement starting point initialization with a learnable parameter.\n",
            "Figure 2 shows the overview of MultiLoRA. There’re 3 major differ-\n",
            "ence to original LoRA: horizontal scaling of LoRA modules, scaling\n",
            "factors and parameter initialization.\n",
            "3.3.1 Scaling LoRA Horizontally\n",
            "Given that LoRA performs closely despite scaling up the rank r, our\n",
            "key strategy of depolarization is parallelism. Through parallelism,\n",
            "incremental activation ∆yis further decomposed into a series of inde-\n",
            "pendent variables, which allows for more degrees of freedom during\n",
            "optimization. Bear in mind that under the same parameter budget,\n",
            "decomposing one large LoRA module into multiple small LoRAs can-\n",
            "not augment rank of ∆Wasrank(AB)≤min(rank(A), rank (B))\n",
            "andrank(A+B)≤rank(A) +rank(B). Corresponding weight\n",
            "matrices are noted as {Ai∈Rr×k}i∈[1,n],{Bi∈Rd×r}i∈[1,n]. Thus, the forward computation of MultiLoRA writes\n",
            "as:\n",
            "∆y=nX\n",
            "i=1scaling iBiAix, (4)\n",
            "Comparing with the forward of LoRA, MultiLoRA differs in that less parameter dependency brought to {Bi}. Paral-\n",
            "lelism has no effect on Aas intermediate mi=Aixis equivalent to reshape the result of [A⊤\n",
            "0, . . . , A⊤\n",
            "n]⊤x. But here\n",
            "comes the major difference.\n",
            "4PRIME AI paper\n",
            "3.3.2 Parameter Initialization\n",
            "Scaling LoRA horizontally allows for independent feature transform especially the up-projection of {Bi}. To further\n",
            "push the expressiveness of {Bi}, we change its parameter initialization to Kaiming-Uniform [ 25] instead of all zeroes\n",
            "and consequently introduce a learnable scaling factor to implement the starting point initialization.\n",
            "The zero initialization seen in Bof LoRA aims to keep activation unchanged before training. Such practice, we term as\n",
            "Starting Point Initialization , is commonly seen in PEFT methods but may be implemented differently. With starting\n",
            "point initialization, tuning a pretrained LLM essentially becomes optimizing in a much smaller parameter space around\n",
            "the local optimum of pretrained models. However, zero initialization is a double-edged sword. It is also infamous\n",
            "for introducing redundancy and breaks asymmetry[ 26,25], yielding limited expressiveness of networks despite faster\n",
            "convergence speed during adaptation.\n",
            "To take advantage of the starting point initialization and mitigate the drawbacks of zero initialization, Multi-\n",
            "LoRA changes initialization of {Bi}to Kaiming-Uniform and implements stating point initialization with zero-\n",
            "initialized learnable scaling factors scaling i∈Rk. Kaiming-Uniform has been shown to improve the generalization\n",
            "performance of neural networks and is the default parameter initialization method in PyTorch[27] implementation.\n",
            "4 Experiments\n",
            "In this section, we evaluate our proposed method from three aspects, namely memory profile, throughput and downstream\n",
            "performance. All our experiments are conducted with LLaMA series[4], ranging from 7B to 65B.\n",
            "4.1 Experiment Setups\n",
            "Model Size Method MMLU Boolq MultiRC RTE WIC\n",
            "7BZero-Shot 35.1 66.5 42.3 57.0 49.4\n",
            "FT 45.3 87.6 84.5 87.0 71.2\n",
            "LoRA r=96 44.7 86.0 81.7 86.6 67.6\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 45.1 88.7 83.8 85.6 70.2\n",
            "13BZero-Shot 46.9 65.0 43.4 60.6 49.5\n",
            "FT 51.3 87.1 85.7 90.8 74.3\n",
            "LoRA r=96 51.0 87.3 86.1 91.7 69.9\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 51.3 86.7 84.7 91.4 75.4\n",
            "30BZero-Shot 57.8 74.6 46.9 53.4 50.0\n",
            "FT 59.2 89.3 87.9 92.8 74.0\n",
            "LoRA r=96 58.8 89.7 87.0 91.0 74.1\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 59.1 89.5 88.1 93.1 74.1\n",
            "65BZero-Shot 63.5 73.6 48.3 59.6 51.3\n",
            "FT 64.6 91.6 90.1 93.9 75.4\n",
            "LoRA r=96 64.2 91.4 90.0 93.1 74.5\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 63.3 91.0 90.2 93.5 74.7\n",
            "Table 1: Main results on MMLU and SuperGLUE using LLaMA of all scales trained in conventional single dataset\n",
            "setup. MMLU is tested with 5-shot prompts and SuperGLUE are tested with zero-shot. MultiLoRA, LoRA and full\n",
            "parameter fine-tuningproduces similar results on single dataset setup.\n",
            "4.1.1 Training Data\n",
            "To evaluate on tasks of interest of generative LLMs, we build multi-task datasets encompassing Alpaca[ 15] for\n",
            "instruction following, MMLU[ 16] for world knowledge, GSM8K[ 17] for arithmetic reasoning and SuperGLUE[ 18] for\n",
            "NLU. Therefore, our mixture of tasks covers semantically and structurally different samples. In terms of source and\n",
            "target sequence length, samples from MMLU and SuperGLUE consist of single-choice questions with very short target\n",
            "lengths, typically one token. On the other hand, Alpaca and GSM8k contain longer target sequences. From the aspect\n",
            "of task semantic, the subjects covered by each dataset differ. MMLU encompasses real-world knowledge across various\n",
            "domains such as humanities, STEM, and social sciences, offering different levels of difficulty. In contrast, Alpaca\n",
            "focuses primarily on aligning model output with human preferences. GSM8k train the models to generate logical and\n",
            "step-by-step responses to questions.\n",
            "5PRIME AI paper\n",
            "To ensure consistency in evaluation, we follow QLoRA[ 28] and MeZO[ 29] to verbalize samples of MMLU and\n",
            "SuperGLUE, respectively. This verbalization process helps standardize input data across tasks, enabling fair comparisons.\n",
            "During training, we introduce random shuffling to enhance the learning process and prevent any bias that may arise\n",
            "from the ordering of the samples.\n",
            "4.1.2 Baselines\n",
            "We use models from LLaMA[ 4] series as the base model. In our comparative analysis, we consider two baselines: full\n",
            "parameter fine-tuning (referred to as FT) and single LoRA (referred to as LoRA). To establish a strong single LoRA\n",
            "baseline, we incorporate LoRA modules alongside all linear layers of LLaMA. Specifically, we insert LoRA modules in\n",
            "q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj modules in LLaMA. The more layers that are adapted by\n",
            "LoRA, the better down-stream task performances will be[28, 6].\n",
            "For Boolq, MultiRC, RTE and WIC, we report zero-shot performances and we report 5-shot results for MMLU. Instead\n",
            "of reporting the individual best task scores, we report the score of each task when the best average score is achieved to\n",
            "emphasize multi-task capability. The hyperparameter settings employed in our experiments are detailed in Appendix\n",
            "A. All experiments are conducted using 8 A100 80G GPUs . Python library PEFT[ 30] is used to help implement\n",
            "MultiLoRA and LoRA. We use Deepspeed ZeRO-3[ 31] for distributed training and offload optimizer states and model\n",
            "parameters for larger training throughput.\n",
            "4.2 Evaluation Results\n",
            "Model Size Method # Params MMLU Boolq MultiRC RTE WIC A VG.\n",
            "7BFT 100% 49.5 88.4 87.2 85.2 74.0 76.9\n",
            "LoRA r=96 3.6% 47.7 88.2 85.4 83.4 71.6 75.2\n",
            "LoRA r=160 5.9% 50.2 87.7 85.3 83.3 70.1 75.3\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 3.6% 51.2 87.8 88.7 89.7 70.8 77.6\n",
            "MultiLoRAn=5\n",
            "r=32(Ours) 6.0% 51.4 88.5 89.4 89.4 71.4 78.0\n",
            "13BFT 100% 51.4 89.2 89.3 91.3 75.1 79.2\n",
            "LoRA r=96 2.9% 49.7 89.7 88.5 87.0 71.5 77.2\n",
            "LoRA r=160 4.8% 50.4 89.4 88.4 87.6 72.1 77.5\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 2.9% 52.6 89.4 89.9 86.9 74.1 78.5\n",
            "MultiLoRAn=5\n",
            "r=32(Ours) 4.8% 52.9 89.3 89.5 90.3 74.3 79.4\n",
            "30BFT 100% 57.5 90.5 91.0 91.7 75.9 81.3\n",
            "LoRA r=96 2.2% 57.1 90.2 90.5 90.5 74.0 80.4\n",
            "LoRA r=160 3.7% 56.8 90.8 90.1 89.9 73.8 80.2\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 2.3% 58.4 90.6 90.5 91.5 74.9 81.1\n",
            "MultiLoRAn=5\n",
            "r=32(Ours) 3.8% 58.0 91.7 90.6 91.9 75.2 81.2\n",
            "65BFT 100% 66.4 91.7 91.3 93.9 76.5 83.9\n",
            "LoRA r=96 1.8% 65.9 91.3 90.8 92.4 75.1 83.1\n",
            "LoRA r=160 3.1% 65.8 90.9 90.4 93.6 75.5 83.2\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 1.8% 65.9 91.5 90.5 93.8 76.2 83.5\n",
            "MultiLoRAn=5\n",
            "r=32(Ours) 3.1% 66.3 91.8 90.1 93.3 76.6 83.6\n",
            "Table 2: Evaluation results on MMLU and SuperGLUE using LLaMA of all scales trained on our mixture. We report\n",
            "score of each task when the best average score is achieved throughout training. MMLU is tested with 5-shot prompts\n",
            "and SuperGLUE are tested with zero-shot. MultiLoRA produces better and more consistent results compared to LoRA.\n",
            "Benchmark performances trained on mixed data are listed in Table 2. Comparing these results to those obtained from\n",
            "single datasets (listed in Table 1), training on mixed datasets generally leads to benefits across all tasks and model\n",
            "scales, resulting in improved benchmark scores to varying extents. Based on these findings, we draw the following\n",
            "conclusions:.\n",
            "MultiLoRA consistently outperforms LoRA and achieves better results than full parameter fine-tuning on\n",
            "smaller models. Across all benchmarks and model scales, MultiLoRA demonstrates stronger data fitting capabilities\n",
            "and outperforms the LoRA counterpart with the same parameter budget by a notable margin. For instance, Multi-\n",
            "LoRA improves upon LoRA ’s performance by 3.5% on MMLU for LLaMA-7B and by 5.9% on RTE for the same\n",
            "model. On average, MultiLoRA surpasses LoRA in terms of the evaluated tasks’ average score by 2.8%. Notably,\n",
            "MultiLoRA even outperforms full parameter fine-tuning on smaller models (7B and 13B), only slightly falling behind\n",
            "on larger scales. Specifically, MultiLoRA achieves an average score improvement of 1.1% compared to full parameter\n",
            "6PRIME AI paper\n",
            "fine-tuning on LLaMA-7B, and a slight 0.3% decrease on LLaMA-65B. These significant improvements highlight\n",
            "MultiLoRA ’s superior capability in complex multi-task adaptation.\n",
            "MultiLoRA exhibits small performance fluctuations comparable to full parameter fine-tuning in complex multi-\n",
            "task learning scenarios. On smaller models, LoRA tends to show performance variability, with more frequent\n",
            "fluctuations between different tasks. For example, on LLaMA-7B, compared to full parameter fine-tuning , MultiRC,\n",
            "RTE, and WIC scores exhibit fluctuations of over 3% in LoRA . In contrast, both full parameter fine-tuning and\n",
            "MultiLoRA yield consistent individual task scores. The observed fluctuations in LoRA can be attributed to the\n",
            "dominance of top singular vectors, as noted in Section 3.2, where a small number of unitary transforms carry significant\n",
            "importance.\n",
            "In the single dataset setting, MultiLoRA performs similarity to full parameter fine-tuning and LoRA. Table 1\n",
            "shows evaluation results of models trained on single dataset. Across the 5 tested tasks and 4 scales, full parameter\n",
            "fine-tuning performs the best on 9 combinations, while MultiLoRA and LoRA perform the best in 7 and 5 combinations,\n",
            "respectively (MultiLoRA performs equally to LoRA on WIC for LLaMA-30B). Based on these observations, we\n",
            "cannot definitively declare one approach as superior. However, in the multi-task setting, MultiLoRA and full parameter\n",
            "fine-tuning demonstrate better performances compared to LoRA.\n",
            "4.3 Resources & Throughput Analysis\n",
            "Training throughput, VRAM usage and inference latency are crucial for generative LLMs. In this section, In this\n",
            "section, we thoroughly examine the resource usage and throughput of MultiLoRA as we scale up the number of parallel\n",
            "LoRA modules n. We primarily focus on VRAM usage and throughput during training as MultiLoRA inherits zero\n",
            "inference overhead from original LoRA. Our benchmark protocol involves training LLaMA-7B on sequences of 1024\n",
            "tokens using 8 A100 GPUs and recording the peak VRAM usage and throughput3. Deepspeed ZeRO-3 and model\n",
            "parameter offload are activated to better evaluate impacts brought by MultiLoRA.\n",
            "32 64 96 160 192\n",
            "n×r050100150200250300350400Tokens/(GPU Second)\n",
            "LoRA\n",
            "MultiLoRA\n",
            "Fine-tuning\n",
            "(a) Throughput\n",
            "32 64 96 160 192\n",
            "n×r0102030405060VRAM/GPU (GB)Fine-tuning\n",
            "LoRA\n",
            "MultiLoRA (b) VRAM\n",
            "Figure 3: (a) Throughput and (b) peak VRAM usage benchmarked when training LLaMA-7Bwith sequences of 1024\n",
            "tokens and batch size of 1. n×ron horizontal axis indicates total rank of LoRA and MultiLoRA. Thanks to high\n",
            "parallelism of MultiLoRA, training throughput is almost identical to LoRA. VRAM usage scales up linearly with the\n",
            "number of parallel LoRA modules.\n",
            "Results are listed in Figure 3. On the horizontal axis, n×rdenotes the equal total rank of MultiLoRA ( nparallel\n",
            "LoRA of rank r) and LoRA (one LoRA of rank n×r).\n",
            "Training throughput is one of the advantages of using MultiLoRA as other PEFT methods. A limitation with full\n",
            "parameter fine-tuning lies in the fact that cached optimizer states can consume a significant portion of the VRAM.\n",
            "Specifically, when training a 7B model with AdamW[ 32], cached optimizer states can occupy up to 70% of the available\n",
            "VRAM, and over 48% with SGD[ 33]. Thanks to the significantly reduced number of trainable parameters, finite VRAM\n",
            "can be leveraged to load more data samples, leading to larger training throughput. Additionally, due to the parallelism\n",
            "inherent in MultiLoRA, multiple LoRA modules do not introduce notable latency and the throughput remains close to\n",
            "3We train MultiLoRA with individual rank of 32.\n",
            "7PRIME AI paper\n",
            "that of LoRA, around 400 tokens per GPU per second. In our benchmarking, the throughput of MultiLoRA is almost\n",
            "twice that of full parameter fine-tuning (208 tokens per GPU per second).\n",
            "For VRAM usage, peak memory scales up much faster than LoRA. In order to optimize multiple parallel LoRA modules,\n",
            "multiple copies of activations should be cached in VRAM. Therefore, one major drawback of MultiLoRA is activation\n",
            "VRAM usage scales linearly with number of parallel LoRA modules which can be unaffordable in long sequence\n",
            "training. In our benchmark, training LLaMA-7B with sequences of 1024 tokens with n= 5would use more VRAM\n",
            "than full parameter fine-tuning.\n",
            "5 Understanding MultiLoRA\n",
            "In this section, we apply SVD on weight update matrices trained with LLaMA-7B in Section 4.1 to investigate why\n",
            "MultiLoRA outperforms LoRA in complex task adaptation. Specifically, subspace similarity and magnitudes of singular\n",
            "value are thoroughly studied for MultiLoRA, LoRA and fine-tuning.\n",
            "5.1 Comparison with Fine-tuning\n",
            "to demonstrate a higher degree of similarity to full parameter fine-tuning of MultiLoRA, we utilize SVD to compare\n",
            "weight update matrices ∆Wof LoRA and MultiLoRA. Specifically, we focus on comparing the subspace coverage of\n",
            "singular vectors and the magnitudes of singular values.\n",
            "As for subspace similarity of singular vectors, we follow [ 6] to use ϕ(∆W′,∆W, i, j )in Equation 5, the Frobenius\n",
            "norm of cosine similarity between top-i and top-j singular vectors of two weight update matrices.\n",
            "ϕ(∆W′,∆W, i, j ) =∥U⊤\n",
            "iU′\n",
            "j∥2\n",
            "F\n",
            "min(i, j)∈[0,1], (5)\n",
            "where Ui=U[:,:i]andU′\n",
            "j=U′[:,:j]are stacked top-i and top-j singular vectors.\n",
            "Moreover, the magnitudes of singular values offer valuable insights into the relative importance of each singular\n",
            "vector. Larger singular values signify a greater contribution to the overall data representation. In Section 3.2, we find\n",
            "∆WLoRAis very polarized as proportion of top singular values is largest. By comparing the singular value distribution,\n",
            "we want to find out whether MultiLoRA manages to balance contribution of each singular vectors.\n",
            "5.1.1 Subspace Comparison\n",
            "0 10 20\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "1×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "3×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "5×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "FT,WV\n",
            "FT)\n",
            "0.00.20.40.60.81.0\n",
            "Figure 4: Subspace similarity to fine-tuning of LoRA ( 1), MultiLoRA ( 2, 3) and fine-tuning with a different random seed\n",
            "(4). LoRA ( 2) and MultiLoRA ( 3) share same parameter budget but MultiLoRA exhibits stronger subspace similarity to\n",
            "fine-tuning. Heatmap of MultiLoRAn=3\n",
            "r=32does not differ much from that of MultiLoRAn=5\n",
            "r=32. Only i, j∈[1,30]are\n",
            "presented for better visibility.\n",
            "Orthonormal singular vectors define the \"direction\" of data transform. By measuring the subspace overlapping with\n",
            "ϕ(∆W′,∆W), we can measure the degree of similarity between two transforms. We randomly choose value projection\n",
            "of the 15 thdecoder layer to calculate ϕ(∆WLoRA,∆WFT)andϕ(∆WMultiLoRA,∆WFT). Similarity between\n",
            "fine-tuning of two different runs ϕ(∆WFT′,∆WFT)is also calculated for reference.\n",
            "MultiLoRA resembles fine-tuning more than LoRA in terms of subspace span. According to visualization in Figure\n",
            "4, MultiLoRAn=3\n",
            "r=32exhibits stronger resemblance to fine-tuning than LoRA r=96under the same parameter budget,\n",
            "8PRIME AI paper\n",
            "indicating that subspace of the weight update matrix of MultiLoRA is closer to that of fine-tuning. Heatmap of LoRA is\n",
            "generally dimmer but top singular vectors still present overlapping of subspaces to fine-tuning to some degree.\n",
            "Scaling up ndoes not necessarily augment MultiLoRA subspace similarity to fine-tuning. Another thing to\n",
            "behold is barely visible difference between MultiLoRAn=3\n",
            "r=32and MultiLoRAn=5\n",
            "r=32, meaning that increasing parallel\n",
            "LoRA number ndoes not necessarily make subspace closer to fine-tuning. The same trend can be observed on other\n",
            "weights of different depths in decoder stack (more at Appendix C).\n",
            "5.1.2 Singular Value Distribution Comparison\n",
            "1e1.0 1e1.5 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "101\n",
            "100101102Count\n",
            "1e1.0 1e1.5 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "100101102Count\n",
            "LoRAr=96 Fine-tuning MultiLoRAn=3\n",
            "r=32MultiLoRAn=5\n",
            "r=32\n",
            "Figure 5: Singular value distribution of weight update matrices ∆Wofk_proj (Left) and v_proj (Right ). Our proposed\n",
            "MultiLoRA exhibits higher degree of resemblance to fine-tuning. Scaling up nproduces more democratic unitary\n",
            "transform contributions.\n",
            "In previous Section 5.1.1, we measure the subspace similarity between unitary singular vectors but without knowing the\n",
            "importance of aforementioned singular vectors, we cannot conclude the higher resemblance between MultiLoRA and\n",
            "fine-tuning. Thus, we investigate into singular value distribution by plotting histogram of singular value as in Section\n",
            "3.2.\n",
            "ForΣ =diag(s)obtained from SVD(∆W), we count the number of sover a series of thresholds and average the\n",
            "statistics of the same module over different depths of decoder layers. We calculate negative logarithms −log(s)for\n",
            "better visibility since more than 95% of singular values are within [0,1]. Results are shown in Figure 5.\n",
            "MultiLoRA balances subspace contributions compared to LoRA. MultiLoRA shows similar distribution as fine-\n",
            "tuning where number of singular value of MultiLoRA decreases with its magnitude. Given the explicit low rank\n",
            "r << d , LoRA shows heavy reliance on a small group of top singular vectors but MultiLoRAdemocratizes contributions\n",
            "of singular vectors.\n",
            "Scaling up nmakes MultiLoRA amplify features at a more fine-grained level. Comparing MultiLoRAn=3\n",
            "r=32and\n",
            "MultiLoRAn=5\n",
            "r=32, histograms of {s| −log(s)>1e1.6}are almost identical but MultiLoRAn=5\n",
            "r=32shows wider spectrum\n",
            "as proportion of small singular values increases. A wider spectrum covering small singular values enables more\n",
            "fine-grained fitting of ∆Was fine-tuning.\n",
            "5.2 Comparison among MultiLoRA\n",
            "In this section, to demonstrate MultiLoRA accomplishes the design goal of depolarization, we compare in pair sub-\n",
            "LoRAs. From heatmap, subspace similarity between top-1 singular vectors is around 0.6. Comparison between ∆Wi=5\n",
            "and∆Wi=3shows relatively low similarity. The variance of subspace similarities indicates a more fine-grained pattern\n",
            "decomposition.\n",
            "9PRIME AI paper\n",
            "0 5 10 15\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15(Wn=5\n",
            "i=1,Wn=5\n",
            "i=3)\n",
            "0 5 10 15\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15(Wn=5\n",
            "i=3,Wn=5\n",
            "i=3)\n",
            "0 5 10 15\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15(Wn=5\n",
            "i=4,Wn=5\n",
            "i=3)\n",
            "0 5 10 15\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15(Wn=5\n",
            "i=5,Wn=5\n",
            "i=3)\n",
            "0.00.20.40.60.81.0\n",
            "Figure 6: Subspace similarity between parallel module of MultiLoRA. We analyze the MultiLoRA targeting down_proj\n",
            "in the first decoder layer. Each individual module produces close but not identical subspaces, thus augmenting the\n",
            "general expressiveness of MultiLoRA.\n",
            "5.3 Underlying Mechanisms of LoRA and MultiLoRA\n",
            "In previous sections, we study the difference in subspace similarity and singular value distribution by applying SVD\n",
            "on weight update matrices of experimented methods. Our observations shed light on underlying mechanisms of\n",
            "LoRA and MultiLoRA. From the singular value distribution of fine-tuning, we learn that fine-tuning fits residual\n",
            "weights by aggregating a large number (usually equals to rank of weight matrix) of relatively less important unitary\n",
            "transforms. Given the low rank limitation, LoRA and MultiLoRA fits residual weights with r≪min(d, k)unitary\n",
            "transforms. The low subspace similarity to fine-tuning and dominance in singular value distribution observed in\n",
            "LoRA show that LoRA tends to decompose the residual weights into unitary transforms of large importance. Meanwhile,\n",
            "MultiLoRA democratizes influences of unitary transforms by assigning smaller importance to its unitary transforms\n",
            "similar to fine-tuning. With democratized unitary subspaces, MultiLoRA produces better complex multi-task learning\n",
            "performance.\n",
            "6 Conclusion\n",
            "In conclusion, our study introduces MultiLoRA, a novel approach that enhances multi-task adaptation in language\n",
            "models. By mitigating the dominance of unitary transforms of LoRA, we successfully improve performance in complex\n",
            "multi-task scenarios. Our proposed method focuses on scaling LoRA modules horizontally and modifying parameter\n",
            "initialization to reduce parameter dependency, thereby creating more balanced unitary subspaces. Additionally,\n",
            "we construct a comprehensive dataset covering a wide range of tasks of interest for generative LLMs. Through\n",
            "extensive experimentation, we have demonstrated that MultiLoRA outperforms single LoRA and achieves comparable\n",
            "performance to fine-tuning across multiple benchmarks and model scales. MultiLoRA stabilizes multi-task adaptation\n",
            "especially for smaller models. Furthermore, our investigation into weight update matrices reveals a significant reduction\n",
            "in dependency on top singular vectors and a more equitable contribution of unitary subspaces in MultiLoRA. Overall,\n",
            "MultiLoRA provides an efficient and effective solution for multi-task adaptation in language models.\n",
            "10PRIME AI paper\n",
            "References\n",
            "[1]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional\n",
            "transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of\n",
            "the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis,\n",
            "MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages 4171–4186. Association for Computational\n",
            "Linguistics, 2019.\n",
            "[2]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\n",
            "lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger,\n",
            "Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher\n",
            "Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\n",
            "Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In\n",
            "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing\n",
            "Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\n",
            "[3]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\n",
            "moyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR , abs/1907.11692,\n",
            "2019.\n",
            "[4]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
            "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave,\n",
            "and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv , abs/2302.13971, 2023.\n",
            "[5]Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\n",
            "Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,\n",
            "and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res. , 2022, 2022.\n",
            "[6]Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. Lora:\n",
            "Low-rank adaptation of large language models, 2021.\n",
            "[7]Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo,\n",
            "Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. CoRR , abs/1902.00751, 2019.\n",
            "[8]Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be\n",
            "comparable to fine-tuning universally across scales and tasks. CoRR , abs/2110.07602, 2021.\n",
            "[9]Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\n",
            "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In NeurIPS , 2022.\n",
            "[10] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient\n",
            "multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of\n",
            "the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language\n",
            "Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 565–576.\n",
            "Association for Computational Linguistics, 2021.\n",
            "[11] Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogério Feris, Huan Sun, and Yoon Kim. Multitask prompt\n",
            "tuning enables parameter-efficient transfer learning. In The Eleventh International Conference on Learning\n",
            "Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\n",
            "[12] Deze Wang, Boxing Chen, Shanshan Li, Wei Luo, Shaoliang Peng, Wei Dong, and Xiangke Liao. One adapter for\n",
            "all programming languages? adapter tuning for code search and summarization. In 45th IEEE/ACM International\n",
            "Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023 , pages 5–16. IEEE,\n",
            "2023.\n",
            "[13] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and\n",
            "Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. In Proceedings of the 2022\n",
            "Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\n",
            "Emirates, December 7-11, 2022 , pages 5744–5760. Association for Computational Linguistics, 2022.\n",
            "[14] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji\n",
            "Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. Deepspeed- inference: Enabling efficient\n",
            "inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance\n",
            "Computing, Networking, Storage and Analysis, Dallas, TX, USA, November 13-18, 2022 , pages 46:1–46:15. IEEE,\n",
            "2022.\n",
            "[15] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\n",
            "Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/\n",
            "tatsu-lab/stanford_alpaca , 2023.\n",
            "11PRIME AI paper\n",
            "[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-\n",
            "suring massive multitask language understanding. In 9th International Conference on Learning Representations,\n",
            "ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021.\n",
            "[17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\n",
            "Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve\n",
            "math word problems. CoRR , abs/2110.14168, 2021.\n",
            "[18] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\n",
            "Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In\n",
            "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing\n",
            "Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pages 3261–3275, 2019.\n",
            "[19] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min\n",
            "Chan, Weize Chen, et al. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained\n",
            "language models. arXiv preprint arXiv:2203.06904 , 2022.\n",
            "[20] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si.\n",
            "On the effectiveness of adapter-based tuning for pretrained language model adaptation. In Proceedings of the 59th\n",
            "Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\n",
            "Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 ,\n",
            "pages 2208–2222. Association for Computational Linguistics, 2021.\n",
            "[21] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for\n",
            "transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association\n",
            "for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 1–9.\n",
            "Association for Computational Linguistics, 2022.\n",
            "[22] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in\n",
            "GPT. In NeurIPS , 2022.\n",
            "[23] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.\n",
            "Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on\n",
            "Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\n",
            "[24] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa.\n",
            "Unipelt: A unified framework for parameter-efficient language model tuning. In Proceedings of the 60th Annual\n",
            "Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\n",
            "May 22-27, 2022 , pages 6253–6264. Association for Computational Linguistics, 2022.\n",
            "[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level\n",
            "performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision, ICCV 2015,\n",
            "Santiago, Chile, December 7-13, 2015 , pages 1026–1034. IEEE Computer Society, 2015.\n",
            "[26] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In\n",
            "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010,\n",
            "Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010 , volume 9 of JMLR Proceedings , pages 249–256. JMLR.org,\n",
            "2010.\n",
            "[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\n",
            "Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary\n",
            "DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\n",
            "Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information\n",
            "Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,\n",
            "December 8-14, 2019, Vancouver, BC, Canada , pages 8024–8035, 2019.\n",
            "[28] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized\n",
            "llms. CoRR , abs/2305.14314, 2023.\n",
            "[29] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora.\n",
            "Fine-tuning language models with just forward passes. CoRR , abs/2305.17333, 2023.\n",
            "[30] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft:\n",
            "State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft , 2022.\n",
            "[31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training\n",
            "trillion parameter models. In Proceedings of the International Conference for High Performance Computing,\n",
            "Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020 , page 20.\n",
            "IEEE/ACM, 2020.\n",
            "12PRIME AI paper\n",
            "[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on\n",
            "Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.\n",
            "[33] Sebastian Ruder. An overview of gradient descent optimization algorithms. CoRR , abs/1609.04747, 2016.\n",
            "[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\n",
            "Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,\n",
            "Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M.\n",
            "Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on\n",
            "Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online, October 2020.\n",
            "Association for Computational Linguistics.\n",
            "13PRIME AI paper\n",
            "A Hyperparameters\n",
            "We list hyperparameters used in our experiments in the Table 3. Batch size of 32 is achieved for LLaMA-30B and\n",
            "LLaMA-65B with gradient accumulation. We use default values give by Huggingface transformers[ 34] trainer for most\n",
            "of the optimizer hyperparameters.\n",
            "Expeiment Hyperparameters Values\n",
            "Batch Size per GPU 32\n",
            "Number of Epochs 2\n",
            "Fine-TuneLearning Rate 5e-6\n",
            "LR Schedule Linear\n",
            "Optimizer AdamW\n",
            "Warmup Ratio 0.05\n",
            "LoRALearning Rate 5e-5\n",
            "LR Schedule Linear\n",
            "Optimizer AdamW\n",
            "Warmup Ratio 0.05\n",
            "MultiLoRALearning Rate 5e-5\n",
            "LR Schedule Linear\n",
            "Optimizer AdamW\n",
            "Warmup Ratio 0.05\n",
            "Table 3: Training Hyperparameters used in our experiments.\n",
            "B Singular Value Distribution\n",
            "1e1.0 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "05101520Count\n",
            "Guanacor=64Alpacar=96MMLUr=96\n",
            "(a)v_proj\n",
            "1e1.0 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "0.02.55.07.510.012.515.017.520.0Count\n",
            "Guanacor=64Alpacar=96MMLUr=96 (b)q_proj\n",
            "Figure 7: Singular Value Distribution of (a) v_proj and (b) q_proj of weight update matrices trained on different datasets.\n",
            "In Section 3.2, we find that LoRA’s weight update matrices of are dominated by small group of unitary transforms.\n",
            "To further support this, we analyzed LoRA modules obtained from training on various datasets or using publicly\n",
            "available resources. We use LoRA modules obtained by training on public available datasets (MMLU and Alpaca) or\n",
            "downloading publicly available resources (Guanaco4).\n",
            "4Downloadable at https://huggingface.co/timdettmers/guanaco-7b/tree/main\n",
            "14PRIME AI paper\n",
            "Figure 7 plots histograms of singular values of weight update matrices of q_proj andv_proj . To enhance visualization,\n",
            "the negative logarithm of the singular values (−log(s))is calculated, given that most values are smaller than 0.1\n",
            "Mean values are used to aggregate statistics across all decoder layers. The histograms for both modules exhibit a striking\n",
            "similarity. The triangular shape of the histograms indicates the dominance of the top singular vectors, as mentioned in\n",
            "Section 3.2. It is worth noting that this dominance arises from the inherent design of LoRA, as we do not deliberately\n",
            "alter its structure or use unconventional datasets.\n",
            "C Subspace Similarities of other modules of different depth\n",
            "In Section 5.1.1, we use cosine similarity between top singular vectors to measure subspace overlap of ∆W. Here, we\n",
            "present more visualizations on different modules from different depths of the decoder stack.\n",
            "0 10 20\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WQ\n",
            "1×LoRA,WQ\n",
            "FT)\n",
            "0 10 20\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WQ\n",
            "3×LoRA,WQ\n",
            "FT)\n",
            "0 10 20\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "5×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WQ\n",
            "FT,WQ\n",
            "FT)\n",
            "0.00.20.40.60.81.0\n",
            "(a)up_proj of MLP in the 28 thdecoder layer.\n",
            "0 10 20\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "1×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "3×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "5×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "FT,WV\n",
            "FT)\n",
            "0.00.20.40.60.81.0\n",
            "(b)q_proj of self attention in the 1 stdecoder layer.\n",
            "0 10 20\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "1×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "3×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "5×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "FT,WV\n",
            "FT)\n",
            "0.00.20.40.60.81.0\n",
            "(c)k_proj of self attention in the 14 thdecoder layer.\n",
            "Figure 8: Subspace similarity of LoRA and MultiLoRA to fine-tuningof different modules at different depths.\n",
            "We randomly choose up_proj ,q_proj andk_proj of MLP and self attention module at different depths to compare weight\n",
            "update matrices of LoRA and MultiLoRA to fine-tuning. The heatmap visualization shows higher degree of similarity\n",
            "of MultiLoRA as observed in Section 5.1.1. Our observation sheds light on the mechanism of LoRAthat the residual\n",
            "weight is decomposed into a small number group of unitary transform of large importance. Important unitary transforms\n",
            "15PRIME AI paper\n",
            "of small number hinders the model handling complicated multi-task learning. Meanwhile, MultiLoRA manages to fits\n",
            "the residual weight more similar to fine-tuning which gathers a larger number of relatively less important transforms.\n",
            "16Preprint\n",
            "MULTI -TASK REINFORCEMENT LEARNING WITH\n",
            "MIXTURE OF ORTHOGONAL EXPERTS\n",
            "Ahmed Hendawy1,2∗, Jan Peters1,2,3,4, Carlo D’Eramo1,2,5\n",
            "1Department of Computer Science, TU Darmstadt, Germany\n",
            "2Hessian Center for Artificial Intelligence (Hessian.ai), Germany\n",
            "3German Research Center for AI (DFKI), Systems AI for Robot Learning\n",
            "4Center for Cognitive Science, TU Darmstadt, Germany\n",
            "5Center for Artificial Intelligence and Data Science, University of W ¨urzburg, Germany\n",
            "ABSTRACT\n",
            "Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem\n",
            "of endowing agents with skills that generalize across a variety of problems. To\n",
            "this end, sharing representations plays a fundamental role in capturing both unique\n",
            "and common characteristics of the tasks. Tasks may exhibit similarities in terms\n",
            "of skills, objects, or physical properties while leveraging their representations\n",
            "eases the achievement of a universal policy. Nevertheless, the pursuit of learn-\n",
            "ing a shared set of diverse representations is still an open challenge. In this paper,\n",
            "we introduce a novel approach for representation learning in MTRL that encapsu-\n",
            "lates common structures among the tasks using orthogonal representations to pro-\n",
            "mote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE),\n",
            "leverages a Gram-Schmidt process to shape a shared subspace of representations\n",
            "generated by a mixture of experts. When task-specific information is provided,\n",
            "MOORE generates relevant representations from this shared subspace. We assess\n",
            "the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid\n",
            "and MetaWorld, showing that MOORE surpasses related baselines and establishes\n",
            "a new state-of-the-art result on MetaWorld.\n",
            "1 I NTRODUCTION\n",
            "Reinforcement Learning (RL) has shown outstanding achievements in a wide array of decision-\n",
            "making problems, including Atari games (Mnih et al., 2013; Hessel et al., 2018a), board games (Sil-\n",
            "ver et al., 2016; 2017), high-dimensional continuous control (Schulman et al., 2015; 2017; Haarnoja\n",
            "et al., 2018), and robot manipulation (Yu et al., 2019). Despite the success of RL, generalizing the\n",
            "learned policy to a broader set of related tasks remains an open challenge. Multi-Task Reinforce-\n",
            "ment Learning (MTRL) is introduced to scale up the RL framework, holding the promise of enabling\n",
            "learning a universal policy capable of addressing multiple tasks concurrently. To this end, sharing\n",
            "knowledge is key in MTRL (Teh et al., 2017; D’Eramo et al., 2020; Sodhani et al., 2021; Sun et al.,\n",
            "2022). However, deciding upon the kind of knowledge to share, and the set of tasks to share that\n",
            "knowledge, is crucial for designing an efficient MTRL algorithm. Human beings exhibit remark-\n",
            "able adaptability across a multitude of tasks by mastering some essential skills as well as having the\n",
            "intuition of physical laws. Similarly, MTRL can benefit from sharing representations that capture\n",
            "unique and diverse properties across multiple tasks, easing the learning of an effective policy.\n",
            "Recently, sharing compositional knowledge (Devin et al., 2017; Calandriello et al., 2014; Sodhani\n",
            "et al., 2021; Sun et al., 2022) has shown potential as an effective form of knowledge transfer in\n",
            "MTRL. For example, Devin et al. (2017) investigates the challenges of knowledge transfer between\n",
            "distinct robots and tasks by sharing a modular structure of the policy. This approach leverages task-\n",
            "specific and robot-specific modules, enabling effective transfer of knowledge. Nevertheless, this ap-\n",
            "proach requires manual intervention to determine the allocation of responsibilities for each module,\n",
            "given some prior knowledge. In contrast, we aim for an end-to-end approach that implicitly learns\n",
            "and shares the prominent components of the tasks for acquiring a universal policy. Furthermore,\n",
            "∗Ahmed Hendawy (ahmed.hendawy@tu-darmstadt.de) is the corresponding author.\n",
            "1arXiv:2311.11385v1  [cs.LG]  19 Nov 2023Preprint\n",
            "CARE (Sodhani et al., 2021) adopts a different strategy by focusing on learning representations of\n",
            "different skills and objects encountered by the tasks through the utilization of context information.\n",
            "However, there is no inherent guarantee of achieving diversity among the learned representations.\n",
            "In this work, our goal is to ensure the diversity of the learned representations to maximize the rep-\n",
            "resentation capacity and avoid collapsing to similar representations.\n",
            "Consequently, we propose a novel approach for representation learning in MTRL to share a set\n",
            "of representations that capture unique and common properties shared by all the tasks. To ensure\n",
            "the richness and diversity of these shared representations, our approach solves a constrained opti-\n",
            "mization problem that orthogonalizes the representations generated by a mixture of experts via the\n",
            "application of the Gram-Schmidt process, thus favoring independence between the representations.\n",
            "Hence, we name our approach, Mixture OfORthogonal Experts (MOORE). Notably, the orthogo-\n",
            "nal representations act as bases that span a subspace of representations leveraged by all tasks where\n",
            "task-relevant properties can be interpolated. More formally, we show that these orthogonal repre-\n",
            "sentations are a set of orthogonal vectors belonging to a particular Riemannian manifold where the\n",
            "inner product is defined, known as Stiefel manifold (James, 1977). Interestingly, the Stiefel mani-\n",
            "fold has recently drawn substantial attention within the field of machine learning (Ozay & Okatani,\n",
            "2016; Huang et al., 2018a; Li et al., 2019; Chaudhry et al., 2020). For example, several works focus\n",
            "on enhancing the generalization and stability of neural networks by solving an optimization prob-\n",
            "lem to learn parameters lying in the Stiefel manifold. Another line of work aims at reducing the\n",
            "redundancy of the learned features by forcing the weights to inhabit the Stiefel manifold. Addition-\n",
            "ally, Chaudhry et al. (2020) proposes a continual learning method that forces each task to learn in a\n",
            "different subspace, thus reducing task interference through orthogonalizing the weights.\n",
            "In this paper, our objective is to ensure diversity among the shared representations across tasks by\n",
            "imposing a constraint that forces these representations to exist within the Stiefel manifold. Thus, we\n",
            "aim to leverage the extracted representations, in combination with deep RL algorithms, to enhance\n",
            "the generalization capabilities of MTRL policies. In the following, we provide a rigorous mathemat-\n",
            "ical formulation of the problem in the form of a Block Contextual Markov Decision Process (MDP)\n",
            "based on latent representations belonging to the Stiefel manifold. Then, we devise our Mixture Of\n",
            "Orthogonal Experts (MOORE) approach for obtaining orthogonal task representations through the\n",
            "application of a Gram-Schmidt process on the latent features extracted from a mixture of experts.\n",
            "We empirically validate MOORE on two widely used and challenging MTRL problems, namely\n",
            "MiniGrid (Chevalier-Boisvert et al., 2023) and MetaWorld (Yu et al., 2019), comparing to recent\n",
            "baselines for MTRL. Remarkably, MOORE establishes a new state-of-the-art performance on the\n",
            "MetaWorld MT10-rand and MT50-rand collections of tasks.\n",
            "To recap, the contribution of this work is threefold: (i) We propose a mathematical formulation of\n",
            "the Block Contextual MDP, that describes the MTRL problem where the state is encoded in the\n",
            "Stiefel manifold through a mapping function. (ii) We devise a novel representation learning method\n",
            "for Multi-Task Reinforcement Learning, that leverages a modular structure of the shared repre-\n",
            "sentations to capture common components across multiple tasks. Our approach, named MOORE,\n",
            "learns a mixture of orthogonal experts by encouraging diversity through the orthogonality of their\n",
            "corresponding representations. (iii) Our approach outperforms related baselines and achieves state-\n",
            "of-the-art results on the MetaWorld benchmark.\n",
            "2 P RELIMINARIES\n",
            "Consider an MDP (Bellman, 1957; Puterman, 1995) defined as a tuple M=<S,A,P, r, ρ, γ > ,\n",
            "where Sis the state space, Ais the action space, P:S × A → S is the transition distribution\n",
            "where P(s′|s, a)is the probability of reaching s′when being in state sand performing action a,\n",
            "r:S × A → Ris the reward function, ρis the initial state distribution, and γ∈(0,1]is the\n",
            "discount factor. A policy πmaps each state sto a probability distribution over the action space\n",
            "A. The goal of RL is to learn a policy that maximizes the expected cumulative discounted return\n",
            "J(π) =Eπ[P∞\n",
            "t=0γtr(st, at)]. We parameterize the policy πθ(at|st)and optimize the parameters θ\n",
            "to maximize J(πθ) =J(θ).\n",
            "2Preprint\n",
            "2.1 M ULTI -TASK REINFORCEMENT LEARNING\n",
            "In MTRL, the agent interacts with different tasks τ∈ T , where each task τis a different MDP\n",
            "Mτ=<Sτ,Aτ,Pτ, rτ, ρτ, γτ>. The goal of MTRL is to learn a single policy πthat maximizes\n",
            "the expected accumulated discounted return averaged across all tasks J(θ) =P\n",
            "τJτ(θ). Tasks can\n",
            "differ in one or more components of the MDP. A class of problems in MTRL assumes only a change\n",
            "in the reward function rτ. This can be exemplified by a navigation task where the agent learns to\n",
            "reach multiple goal positions or a robotic manipulation task where the object’s position changes.\n",
            "In this class, the goal position is usually augmented to the state representation. Besides the reward\n",
            "function, a bigger set of problems deals with changes in other components. In this category, tasks\n",
            "access a subset of the state space Sτ, while the true state space Sis unknown. For example, learning\n",
            "a universal policy that performs multiple manipulation tasks interacting with different objects (Yu\n",
            "et al., 2019). Task information should be provided either in the form of task ID (e.g. one-hot vector)\n",
            "or metadata, e.g., task description (Sodhani et al., 2021).\n",
            "Following Sodhani et al. (2021), we define our MTRL problem as a Block Contextual MDP (BC-\n",
            "MDP). It is defined by 5-tuple <C,S,A, γ,M′>where Cis the context space, Sis the true state\n",
            "space, Ais the action space, while M′is a mapping function that provides the task-specific MDP\n",
            "components given the context c∈ C,M′(c) ={rc,Pc,Sc, ρc}. As of now, we refer to the task τ\n",
            "and its components by the context parameter denoted as c.\n",
            "3 R ELATED WORKS\n",
            "Sharing knowledge among tasks is a key benefit of MTRL over single-task learning, as broadly ana-\n",
            "lyzed by several works that propose disparate ways to leverage the relations between tasks (D’Eramo\n",
            "et al., 2020; Sodhani et al., 2021; Sun et al., 2022; Calandriello et al., 2014; Devin et al., 2017; Yang\n",
            "et al., 2020). Among many, D’Eramo et al. (2020) establishes a theoretical benefit of MTRL over\n",
            "single-task learning as the number of tasks increases, and Teh et al. (2017) learn individual policies\n",
            "while sharing a prior among tasks. However, naive sharing may exhibit negative transfer since not\n",
            "all knowledge should be shared by all tasks. An interesting line of work investigates the task in-\n",
            "terference issue in MTRL from the gradient perspective. For example, Yu et al. (2020) propose a\n",
            "gradient projection method where each task’s gradient is projected to an orthogonal direction of the\n",
            "others. Nevertheless, these approaches are sensitive to the high variance of the gradients. Another\n",
            "approach, known as PopArt (Hessel et al., 2018b), examines task interference focusing on the insta-\n",
            "bility caused by different reward magnitudes, addressing this issue by a normalizing technique on\n",
            "the output of the value function.\n",
            "Recently, sharing knowledge in a modular form has been advocated for reducing task interference.\n",
            "Yang et al. (2020) share a base model among tasks while having a routing network that generates\n",
            "task-specific models. Moreover, Devin et al. (2017) divides the responsibilities of the policy by\n",
            "sharing two policies, allocating one to different robots and the other to different tasks. Additionally,\n",
            "Sun et al. (2022) propose a parameter composition technique where a subspace of policy is shared\n",
            "by a group of related tasks. Moreover, CARE Sodhani et al. (2021) highlight the importance of\n",
            "using metadata for learning a mixture of state encoders that are shared among tasks, based on the\n",
            "claim that the learned encoders produce diverse and interpretable representations through an atten-\n",
            "tion mechanism. Despite the potential of this work, the method is highly dependent on the context\n",
            "information as shown in this recent work (Cheng et al., 2023). However, we argue that all of these\n",
            "approaches lack the guarantee of learning diverse representations.\n",
            "In this work, we promote diversity across a mixture of experts by enforcing orthogonality among\n",
            "their representations. The mixture-of-experts has been well-studied in the RL literature (Akrour\n",
            "et al., 2022; Ren et al., 2021). Moreover, some works dedicate attention to maximizing the diversity\n",
            "of the learned skills in RL (Eysenbach et al., 2018). Previous works leverage orthogonality for dis-\n",
            "parate purposes (Mackey et al., 2018). For example, Bansal et al. (2018) promote orthogonality on\n",
            "the weights by a regularized loss to stabilize training in deep convolutional neural networks. Simi-\n",
            "larly, Huang et al. (2018a) employ orthogonality among the weights for stabilizing the distribution\n",
            "of activation in neural networks. In the context of MTRL, Paredes et al. (2012) enforce the represen-\n",
            "tation obtained from a set of similar tasks to be orthogonal to the one obtained from selected tasks\n",
            "known to be unrelated. Recently, Chaudhry et al. (2020) alleviate catastrophic forgetting in con-\n",
            "tinual learning by organizing task representations in orthogonal subspaces. Finally, Mashhadi et al.\n",
            "(2021) favor diversity in an ensemble of learners via a Gram-Schmidt process. As opposed to it,\n",
            "3Preprint\n",
            "Task\n",
            " Encoderc\n",
            "wc\n",
            "vcV Gram-\n",
            "Schmidt\n",
            "Stiefel Manifold Output Head Mixture of Expertsfθ\n",
            "Figure 1: MOORE illustrative diagram. A state representation is encoded as a set of representa-\n",
            "tions using a mixture of experts. The Gram-Schmidt process orthogonalizes the representations to\n",
            "encourage diversity. Then, the output head processes the representations Vby interpolating the task-\n",
            "specific representations vcusing the task-specific weights wc, from which we compute the output\n",
            "using the output module fθ. In our approach, we employ this architecture in an actor-critic setting\n",
            "for both the actor and the critic.\n",
            "our primary focus lies in the acquisition of a set of orthogonal representations that span a subspace\n",
            "shared by a group of tasks where task-relevant representations can be interpolated.\n",
            "4 S HARING ORTHOGONAL REPRESENTATIONS\n",
            "We aim to obtain a set of rich and diverse representations that can be leveraged to find a universal\n",
            "policy that accomplishes multiple tasks. To this end, we propose to enforce the orthogonality of the\n",
            "representations extracted by a mixture of experts.\n",
            "In the following, we first provide a mathematical formulation from which we derive our approach. In\n",
            "particular, we highlight the connection between our method and the Stiefel manifold theory (Huang\n",
            "et al., 2018b; Chaudhry et al., 2020; Li et al., 2020), together with the description of the role played\n",
            "by the Gram-Schmidt process. Then, we proceed to devise our novel method for Multi-Task Rein-\n",
            "forcement Learning on orthogonal representation obtained from mixture of experts.\n",
            "4.1 O RTHOGONALITY IN CONTEXTUAL MARKOV DECISION PROCESSES\n",
            "We study the optimization of a policy π, given a set of k-orthonormal representations in Rdfor the\n",
            "states. We define the orthonormal representations of state sas a matrix Vs= [v1, ..., v k]∈Rd×k\n",
            "where vi∈Rd,∀i≤k. It can be shown that the orthonormal representations Vsbelong to a\n",
            "topological space known as Stiefel manifold, a smooth and differentiable manifold largely used in\n",
            "machine learning (Huang et al., 2018b; Chaudhry et al., 2020; Li et al., 2020).\n",
            "Definition 4.1 (Stiefel Manifold) Stiefel manifold Vk(Rd)is defined as the set of all orthonormal\n",
            "k-frames in the Euclidean space Rd, where k≤d,Vk(Rd) ={Vs∈Rd×k:VT\n",
            "sVs=Ik,∀s∈ S} .\n",
            "Under this lens, our goal can be interpreted as finding a set of orthogonal representations belonging\n",
            "to the Stiefel manifold, that capture the common components in the true state space. Thus, we\n",
            "propose a novel MDP formulation for MTRL, which we call Stiefel Contextual MDP (SC-MDP),\n",
            "that is inspired by the BC-MDP introduced in Sodhani et al. (2021). An SC-MDP includes a function\n",
            "that maps the state stok-orthonormal representations Vs∈ Vk(Rd), and it is defined as follows.\n",
            "Definition 4.2 (Stiefel Contextual Markov Decision Process) A Stiefel Contextual Markov Decision\n",
            "Process (SC-MDP) is defined as a tuple <C,S,A,M′, φ > whereCis the context space, Sis the\n",
            "state space, Ais the action space. M′is a function that maps a context c∈ Cto MDP parameters\n",
            "and observation space M′(c) ={rc,Pc,Sc, ρc},φis a function that maps every state s∈ S to a\n",
            "k-orthonormal representations Vs∈ Vk(Rd),Vs=φ(s).\n",
            "4Preprint\n",
            "We use a compositional form of the universal policy π, defined as π(a|s, c) =θ(Vswc), where\n",
            "wc∈Rkis the task-specific weight that combines the k-orthogonal representations into a single\n",
            "one and θ∈R|A|×dcombines the task-specific representations for generating actions. To leverage\n",
            "a diverse set of representations across tasks, the mapping function φplays a crucial role. Hence,\n",
            "we learn a mixture of experts hϕ= [hϕ1, ..., h ϕk]with learnable parameters ϕ= [ϕ1, ..., ϕ k]that\n",
            "generate k-representations Us∈Rd×kof state s, while ensuring that the generated representations\n",
            "are orthogonal to enforce diversity. Conveniently, this objective finds a rigorous formulation as a\n",
            "constrained optimization problem where we impose a hard constraint to enforce orthogonality:\n",
            "max\n",
            "Θ={ϕ,θ}J(Θ)\n",
            "s.t. hT\n",
            "ϕ(s)hϕ(s) =Ik∀s∈ S,(1)\n",
            "where h ϕ(s)∈Rd×krepresents a k-orthonormal representations in Rd, and Ik∈Rk×kis the\n",
            "identity matrix. Instead of solving the constrained optimization problem in Eq. 1, we ensure the\n",
            "diversity across experts through the application of the Gram-Schmidt (GS) process to orthogonalize\n",
            "thek-representations Us.\n",
            "Definition 4.3 (Gram-Schmidt Process) Gram-Schmidt process is a method for orthogonalizing a\n",
            "set of linearly independent U={u1, ..., u k:ui∈Rd,∀i≤k}. It maps the vectors to a set of\n",
            "k-orthonormal vectors V={v1, ..., v k:vi∈Rd,∀i≤k}defined as\n",
            "vk=uk−k−1X\n",
            "i=1⟨vi, uk⟩\n",
            "⟨vi, vi⟩vi. (2)\n",
            "where the representation of the i-th expert uiis projected in the orthogonal direction to the represen-\n",
            "tations of all i−1experts. Therefore, we apply GS process to map the generated representations by\n",
            "the mixture of experts Us=hϕ(s)to a set of orthonormal representations Vs=GS(Us), satisfying\n",
            "the hard constraint in Eq. 1.\n",
            "4.2 M ULTI -TASK REINFORCEMENT LEARNING WITH ORTHOGONAL REPRESENTATIONS\n",
            "Following the compositional form of the policy π(a|s, c), each task can interpolate its relevant repre-\n",
            "sentation from the space spanned by the k-orthonormal representations Vs. We train a task encoder\n",
            "to produce the task-specific weights wc∈Rkgiven task information (e.g. task ID). The orthonor-\n",
            "mal representations are combined using the task-specific weight to produce relevant representations\n",
            "vc∈Rdto the task as vc=V wc, where sis dropped for simplicity. The interpolated representation\n",
            "vccaptures the relevant components of the task that can be utilized by the RL algorithm and fed to\n",
            "an output module fθ. The output module can be learned for each task separately (multi-head) or\n",
            "shared by all tasks (single-head) to compute the action components from the representations vc. In\n",
            "conclusion, this approach results in a Mixture OfORthogonal Experts, thus, we call it MOORE,\n",
            "whose extracted representation is used to learn universal policies for MTRL.\n",
            "We adopt two different RL algorithms, namely Proximal Policy Optimization (PPO) and Soft Actor-\n",
            "Critic (SAC), with the purpose of demonstrating that our approach is agnostic to the used RL algo-\n",
            "rithms. PPO (Schulman et al., 2017) is a policy gradient algorithm that has the merit of obtaining\n",
            "satisfactory performance in a wide range of problems while being easy to implement. It is a first-\n",
            "order method that enhances the policy update given the current data by limiting the deviation of\n",
            "the new policy from the current one. In this work, we impose the same compositional structure\n",
            "formerly mentioned for both actor and critic. Moreover, we integrate our approach to SAC, a high-\n",
            "performing off-policy RL algorithm that leverages entropy maximization to enhance exploration.\n",
            "Similar to PPO, the compositional structure is utilized for both the actor (Alg. 1) and critic (Alg. 2).\n",
            "A visual demonstration of our approach is shown in Fig.1.\n",
            "5 E XPERIMENTAL RESULTS\n",
            "In this section, we evaluate MOORE against related baselines on two challenging MTRL bench-\n",
            "marks, namely MiniGrid (Chevalier-Boisvert et al., 2023), a set of visual goal-oriented tasks, and\n",
            "5Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT3\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.60.8Average ReturnMT5\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnMT7\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "(a) Multi-Head Architecture\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT3\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT5\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.6Average ReturnMT7\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "(b) Single-Head Architecture\n",
            "Figure 2: Average return on the three MTRL scenarios of MiniGrid. We utilize both, multi-head and\n",
            "single-head architectures, for our approach MOORE as well as the related baselines. For MOORE\n",
            "and MOE, the number of experts kis 2, 3, and 4 for MT3, MT5, and MT7, respectively. The black\n",
            "dashed line represents the final single-task performance of PPO averaged across all tasks. For the\n",
            "evaluation metric, we compute the accumulated return averaged across all tasks. We report the mean\n",
            "and the 95% confidence interval across 30 different runs.\n",
            "MetaWorld (Yu et al., 2019), a collection of robotic manipulation tasks. The objective is to assess the\n",
            "adaptability of our approach in handling different types of state observations and tackling a variable\n",
            "number of tasks. Moreover, the flexibility of MOORE is evinced by using it in on-policy (PPO for\n",
            "MiniGrid) and off-policy RL (SAC for MetaWorld) algorithms. Additionally, we conduct ablation\n",
            "studies that support the effectiveness of MOORE in various aspects. We want to assess the follow-\n",
            "ing points: the benefit of using Gram-Schmidt to impose diversity across experts, the quality of the\n",
            "learned representations, as well as the transfer capabilities, and the interpretability of the diverse\n",
            "experts.\n",
            "5.1 M INIGRID\n",
            "We consider different tasks in MiniGrid (Chevalier-Boisvert et al., 2023), which is a suite of 2D goal-\n",
            "oriented environments that require solving different mazes while interacting with different objects\n",
            "like doors, keys, or boxes of several colors, shapes, and roles. MiniGrid allows the use of a visual\n",
            "representation of the state, which we adopt for our multi-task setting. We consider the multi-task\n",
            "setting from Jin et al. (2023) that includes three multi-task scenarios. The first scenario, MT3 , in-\n",
            "volves the three tasks: LavaGap, RedBlueDoors, and Memory; the second scenario, MT5 , includes\n",
            "the five tasks: DoorKey, LavaGap, Memory, SimpleCrossing, and MultiRoom. Finally, MT7 com-\n",
            "prises the seven tasks: DoorKey, DistShift, RedBlueDoors, LavaGap, Memory, SimpleCrossing, and\n",
            "MultiRoom. In Sec. A.1, we provide a description of the considered tasks.\n",
            "We compare MOORE against four baselines. The first one is PPO , which is considered as a refer-\n",
            "ence for comparing to single-task performance. The second baseline is Multi-Task PPO (MTPPO) ,\n",
            "an adaptation of PPO (Schulman et al., 2017) for MTRL. Then, we consider MOE , which employs\n",
            "a mixture of experts to generate representations without enforcing diversity across experts. Ad-\n",
            "ditionally, we have PCGrad (Yu et al., 2020), which is an MTRL approach that tackles the task\n",
            "6Preprint\n",
            "0 20 40 60 80 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnMT3  MT5\n",
            "0 20 40 60 80 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnMT5  MT7\n",
            "MOE Transfer-MOE MOORE (ours) Transfer-MOORE (ours)\n",
            "Figure 3: Evaluating MOORE against MOE on the transfer setting. The study is conducted on the\n",
            "two transfer learning scenarios in MiniGrid, employing a multi-head architecture. The number of\n",
            "experts kis 2 and 3 for MT3 →MT5 and MT5 →MT7, respectively. For the evaluation metric,\n",
            "we compute the accumulated return averaged across all tasks. We report the mean and the 95%\n",
            "confidence interval across 30 different runs.\n",
            "interference issue by manipulating the gradient. For a fair comparison, we integrate PCGrad on top\n",
            "of the MOE baseline. As for the MTRL architecture, we utilize multi-head and single-head archi-\n",
            "tectures for all methods, showing their average return across all tasks in Fig. 2(a), and Fig. 2(b)\n",
            "respectively. MOORE outperforms the aforementioned baselines in almost all the MTRL scenar-\n",
            "ios. Notably, our method exhibits faster convergence than the baselines. It is interesting to observe\n",
            "that MOORE outperforms the single-task performance with a significant margin in comparison to\n",
            "the other baselines (Fig.2(a)), which is usually considered as an upper-bound of the MTRL perfor-\n",
            "mance in previous works. This highlights the quality of the learned representations and the role of\n",
            "MOORE representation learning process in MTRL.\n",
            "5.1.1 A BLATION STUDIES\n",
            "2 3 4 5 6 7 8 9 10\n",
            "Number of Experts0.40.50.60.7Average ReturnMOE\n",
            "MOORE (ours)\n",
            "Figure 4: Ablation study on the effect of changing\n",
            "the number of experts. We compare the perfor-\n",
            "mance of MOE and MOORE (ours) on MiniGrid\n",
            "MT7 using a single-head architecture. We report\n",
            "the mean of the evaluation metric across 30 seeds.\n",
            "For the evaluation metric, we compute the accu-\n",
            "mulated return averaged across all tasks.We examine the advantage of transferring the\n",
            "trained experts, on a set of base tasks, to novel\n",
            "tasks, in order to assess the quality and general-\n",
            "ization of these learned experts in comparison\n",
            "to the MOE baseline. We refer to the transfer\n",
            "variant of our approach as Transfer-MOORE\n",
            "while Transfer-MOE for the baseline. More-\n",
            "over, we include the performance of MOORE\n",
            "and MOE as a MTRL reference for learning\n",
            "only the novel tasks, completely isolated from\n",
            "the base tasks. In Fig.3, we show the empir-\n",
            "ical results on two transfer learning scenarios\n",
            "where we transfer a set of experts learned on\n",
            "MT3 to MT5 ( MT3→MT5 ), and on MT5\n",
            "to MT7 ( MT5→MT7 ). MT3 is a subset of\n",
            "MT5 while MT5 is a subset of MT7. First, we\n",
            "train on the base tasks (intersection of the two\n",
            "sets), and then we transfer the learned experts to\n",
            "the novel tasks (the difference between the two\n",
            "sets). As illustrated in Fig. 3, Transfer-MOORE\n",
            "outperforms Transfer-MOE in the two scenar-\n",
            "ios, showing the quality of the learned representations in the context of transfer learning. Moreover,\n",
            "the study demonstrates the ability of our approach as an effective MTRL algorithm, that provides\n",
            "competitive results against the transfer variant (Transfer-MOORE). In contrast, MOE struggles to\n",
            "beat the transfer variant as in the MT3 →MT5 scenario. Consequently, this study advocates the\n",
            "diversification of the shared representations in transfer learning as well as MTRL. We highlight\n",
            "more details in B.2.\n",
            "7Preprint\n",
            "Total Env Steps 1M 2M 3M 5M 10M 15M 20M\n",
            "SAC (Yu et al., 2019) 10.0±8.2 17.7 ±2.1 18.7 ±1.1 20.0 ±2.0 48.0 ±9.5 57.7 ±3.1 61.9 ±3.3\n",
            "MTSAC (Yu et al., 2019) 34.9±12.9 49.3 ±9.0 57.1 ±9.8 60.2 ±9.6 61.6 ±6.7 65.6 ±10.4 62.9 ±8.0\n",
            "SAC + FiLM (Perez et al., 2017) 32.7±6.5 46.9 ±9.4 52.9 ±6.4 57.2 ±4.2 59.7 ±4.6 61.7 ±5.4 58.3 ±4.3\n",
            "PCGrad (Yu et al., 2020) 32.2±6.8 46.6 ±9.3 54.0 ±8.4 60.2 ±9.7 62.6 ±11.0 62.6 ±10.5 61.7 ±10.9\n",
            "Soft-Module (Yang et al., 2020) 24.2±4.8 41.0 ±2.9 47.4 ±5.3 51.4 ±6.8 53.6 ±4.9 56.6 ±4.8 63.0 ±4.2\n",
            "CARE (Sodhani et al., 2021) 26.0±9.1 52.6 ±9.3 63.8 ±7.9 66.5 ±8.3 69.8 ±5.1 72.2 ±7.1 76.0 ±6.9\n",
            "PaCo (Sun et al., 2022) 30.5±9.5 49.8 ±8.2 65.7 ±4.5 64.7 ±4.2 71.0 ±5.5 81.0 ±5.9 85.4 ±4.5\n",
            "MOORE (ours) 37.2±9.9 63.0 ±7.2 68.6 ±6.9 77.3 ±9.6 82.7 ±7.3 88.2 ±5.6 88.7 ±5.6\n",
            "Table 1: Results on MetaWorld MT10 Yu et al. (2019) with random goals (MT10-rand). The results\n",
            "of the baselines are borrowed from Sun et al. (2022). For MOORE, the number of experts kis 4. For\n",
            "all methods, we report the mean and standard deviation of the evaluation metric across 10 different\n",
            "runs. The evaluation metric is the average success rate across all tasks. We highlight with bold text\n",
            "thebest so far.\n",
            "Additionally, we focus on the impact of changing the number of experts on the performance of\n",
            "our approach, as well as on MOE. In Fig.4, we consider different numbers of experts on the MT7\n",
            "scenario. We observe the effect of utilizing more experts in MOORE algorithm compared to MOE.\n",
            "The study shows that MOORE exhibits a noticeable advantage, on average, for an increasing number\n",
            "of experts. On the contrary, a slower enhancement of the performance is encountered by MOE. It\n",
            "is also worth noting that the performance of MOORE with k= 4 slightly outperforms MOE with\n",
            "k= 10 while being comparable to MOE with k= 8(the best setting for MOE). This supports our\n",
            "claim about the efficient utilization of the expert capacity through enforcing diversity.\n",
            "5.2 M ETAWORLD\n",
            "Finally, we evaluate our approach on another challenging MTRL setting with a large number of\n",
            "manipulation tasks. We benchmark against MetaWorld Yu et al. (2019), a widely adopted robotic\n",
            "manipulation benchmark for Multi-Task and Meta Reinforcement Learning. We consider the MT10\n",
            "setting, where a set of 10related manipulation tasks has to be performed by a single robot.\n",
            "For the baselines, we compare our approach against the following algorithms. First, SAC (Haarnoja\n",
            "et al., 2018) is the off-policy RL that is trained on each task separately, thus being a reference\n",
            "for the single-task setting. Second, Multi-Task SAC (MTSAC) is the adaptation of SAC to the\n",
            "MTRL setting, where we employ a single-head architecture with a one-hot vector concatenated with\n",
            "the state. Then, SAC+FiLM is a task-conditional policy that employs the FiLM module (Perez\n",
            "et al., 2017). Furthermore, PCGrad (Yu et al., 2020) is an MTRL approach that tackles the task\n",
            "interference issue by manipulating the gradient. Soft-Module (Yang et al., 2020) utilizes a routing\n",
            "network that proposes weights for soft combining of activations for each task. CARE (Sodhani\n",
            "et al., 2021) is an attention-based approach that learns a mixture of experts for encoding the state\n",
            "while utilizing context information. Finally, PaCo (Sun et al., 2022) is the recent state-of-the-art\n",
            "method for MetaWorld that learns a compositional policy where task-specific weights are utilized\n",
            "for interpolating task-specific policies. On the other hand, our approach uses a similar framework as\n",
            "in the MiniGrid experiment and employs a multi-head architecture.\n",
            "Following Sun et al. (2022), we benchmark against MT10-rand where each task is trained with\n",
            "random goal positions. The goal position is concatenated with the state representation. As a per-\n",
            "formance metric, we compute the success rate averaged across all tasks. For a fair comparison, we\n",
            "run our approach for 10different runs. In Tab. 1, we report the mean and the standard deviations of\n",
            "the metric across the 10different runs and at different learning steps. As stated in Tab.1, MOORE\n",
            "outperforms all the baselines both in terms of convergence speed and asymptotic performance. It is\n",
            "important to mention that all the MTRL uses tricks to enhance the stability of the learning process.\n",
            "For instance, PaCo avoids task and gradient explosion by proposing two empirical tricks, named loss\n",
            "maskout andw-reset , where pruning every task loss that reaches above a certain threshold, besides\n",
            "resetting the task-specific weight for that task. Also, as in Sun et al. (2022), the other baselines re-\n",
            "sort to more expensive tricks, such as terminating and re-launching the training session when a loss\n",
            "explosion is encountered. On the contrary, our approach does not need such tricks to improve the\n",
            "stability of the learning process which can be an indication of the stability of the chosen architecture\n",
            "and the importance of learning distinct experts.\n",
            "8Preprint\n",
            "0 5 10 15 20\n",
            "#Epochs0.000.250.500.75Success Rate\n",
            "MOE\n",
            "MOORE (ours)\n",
            "(a) Success rate in MT10-rand.\n",
            "0 2 4 6 8\n",
            "#Epochs0.00.20.40.6Success RateMOE\n",
            "MOORE (ours) (b) Success rate in MT50-rand.\n",
            "Figure 6: (a) Success rate on MetaWorld MT10-rand comparing MOORE, against MOE, using 4\n",
            "experts. (b) Success rate on MetaWorld MT50-rand comparing MOORE, against MOE, given 6\n",
            "experts. We show the average success rate across all tasks and the 95% confidence interval across\n",
            "10and5different runs for MT10-rand and MT50-rand, respectively.\n",
            "5.2.1 A BLATION STUDIES\n",
            "1.0\n",
            " 0.5\n",
            " 0.0 0.5 1.0 1.5\n",
            "Principal Component 10.6\n",
            "0.4\n",
            "0.2\n",
            "0.00.20.40.6Principal Component 2reach\n",
            "push\n",
            "pick-place\n",
            "door-open\n",
            "drawer-open\n",
            "drawer-close\n",
            "button-press-topdown\n",
            "peg-insert-side\n",
            "window-open\n",
            "window-close\n",
            "Figure 5: Principle Component Analysis (PCA)\n",
            "on the task-specific weights learned by MOORE\n",
            "on MetaWorld MT10-rand for a run with 100%\n",
            "success rate across all tasks. This shows the inter-\n",
            "pretability and diversity of the shared experts.Similarly, we want to evince the advantage\n",
            "of favoring diversity across experts. We con-\n",
            "sider the same architecture of MOORE, but\n",
            "without the Gram-Schmidt process, and refer\n",
            "to it as MOE. We evaluate MOORE against\n",
            "MOE on two MTRL scenarios in MetaWorld.\n",
            "In addition to MT10-rand, we benchmark on\n",
            "MT50-rand, a large-scale MTRL scenario in\n",
            "MetaWorld with 50different but related ma-\n",
            "nipulation tasks. In Fig. 6(a), MOORE ex-\n",
            "hibits superior sample-efficiency compared to\n",
            "MOE. Moreover, MOORE significantly out-\n",
            "performs the baseline also in MT50-rand (Fig.\n",
            "6(b)), evincing the scalability of our approach\n",
            "to large-scale MTRL problems. This study il-\n",
            "lustrates the importance of enforcing diversity\n",
            "across experts in MTRL algorithms.\n",
            "Additionally, we verify the interpretability of\n",
            "the learned representations. Fig. 5 shows an\n",
            "application of PCA on the learned task-specific\n",
            "weights wcthat are used to combine the repre-\n",
            "sentations of the experts. On the one hand, as\n",
            "shown, the pick-place task is close to the peg-\n",
            "insert-side since both tasks require picking up an object. On the other hand, the weights of door-open\n",
            "and window-open tasks are similar as they share the open skill. Therefore, enforcing diversity across\n",
            "experts distributes the responsibilities across them in capturing common components across tasks\n",
            "(e.g. objects or skills). This confirms that the learned experts have some roles that can be inter-\n",
            "pretable.\n",
            "6 C ONCLUSION AND DISCUSSION\n",
            "We proposed a novel MTRL approach for diversifying a mixture of shared experts across tasks.\n",
            "Mathematically, we formulate our objective as a constrained optimization problem where a hard\n",
            "constraint is explicitly imposed to ensure orthogonality between the representations. As a result,\n",
            "the orthogonal representations live on a smooth and differentiable manifold called the Stiefel man-\n",
            "9Preprint\n",
            "ifold. We formulate our MTRL as a novel contextual MDP while mapping each state to the Stiefel\n",
            "manifold using a mapping function, which we learn through a mixture of experts while enforcing\n",
            "orthogonality across their representations with the Gram-Schmidt process, hence satisfying the hard\n",
            "constraint. Our approach demonstrates superior performance against related baselines on two chal-\n",
            "lenging MTRL baselines.\n",
            "Taking advantage of all the experts during inference, our approach has the limitation of potentially\n",
            "suffering from high time complexity, in comparison, for instance, to a sparse selection of one expert.\n",
            "This leads to a trade-off between the representation capacity and time complexity which could be\n",
            "investigated in the future through a selection of a few orthogonal experts. In addition to the transfer\n",
            "learning study we conducted, we are interested in investigating extensions of our approach into a\n",
            "continual learning setting.\n",
            "REFERENCES\n",
            "Riad Akrour, Davide Tateo, and Jan Peters. Continuous action reinforcement learning from a mixture\n",
            "of interpretable experts. IEEE Transactions on Pattern Analysis and Machine Intelligence , 44(10):\n",
            "6795–6806, 2022. doi: 10.1109/TPAMI.2021.3103132.\n",
            "Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regu-\n",
            "larizations in training deep networks? Advances in Neural Information Processing Systems , 31,\n",
            "2018.\n",
            "Richard Bellman. Dynamic Programming . Princeton University Press, Princeton, NJ, USA, 1 edi-\n",
            "tion, 1957.\n",
            "Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli. Sparse multi-task reinforcement\n",
            "learning. In Advances in Neural Information Processing Systems , 2014.\n",
            "Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual learning in low-\n",
            "rank orthogonal subspaces. Advances in Neural Information Processing Systems , 33:9900–9911,\n",
            "2020.\n",
            "Guangran Cheng, Lu Dong, Wenzhe Cai, and Changyin Sun. Multi-task reinforcement learning\n",
            "with attention-based mixture of experts. IEEE Robotics and Automation Letters , 8(6):3812–3819,\n",
            "2023. doi: 10.1109/LRA.2023.3271445.\n",
            "Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem\n",
            "Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modu-\n",
            "lar & customizable reinforcement learning environments for goal-oriented tasks. arXiv preprint\n",
            "arXiv:2306.13831 , 2023.\n",
            "Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowl-\n",
            "edge in multi-task deep reinforcement learning. In International Conference on Learning Repre-\n",
            "sentations , 2020.\n",
            "Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular\n",
            "neural network policies for multi-task and multi-robot transfer. In International Conference on\n",
            "Robotics and Automation , 2017.\n",
            "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\n",
            "Learning skills without a reward function. arXiv preprint arXiv:1802.06070 , 2018.\n",
            "Gene H Golub and Charles F Van Loan. Matrix computations . JHU press, 2013.\n",
            "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\n",
            "maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-\n",
            "ence on Machine Learning , 2018.\n",
            "Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan\n",
            "Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in\n",
            "deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence ,\n",
            "volume 32, 2018a.\n",
            "10Preprint\n",
            "Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van\n",
            "Hasselt. Multi-task deep reinforcement learning with popart. CoRR , abs/1809.04474, 2018b.\n",
            "Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. Orthogonal weight\n",
            "normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural\n",
            "networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018a.\n",
            "Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. Orthogonal weight\n",
            "normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural\n",
            "networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018b.\n",
            "I. M. James. The Topology of Stiefel Manifolds . London Mathematical Society Lecture Note Series.\n",
            "Cambridge University Press, 1977. doi: 10.1017/CBO9780511600753.\n",
            "Yonggang Jin, Chenxu Wang, Liuyu Xiang, Yaodong Yang, Jie Fu, and Zhaofeng He. Deep re-\n",
            "inforcement learning with multitask episodic memory based on task-conditioned hypernetwork.\n",
            "arXiv preprint arXiv:2306.10698 , 2023.\n",
            "Jun Li, Li Fuxin, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold\n",
            "via the cayley transform. arXiv preprint arXiv:2002.01113 , 2020.\n",
            "Shuai Li, Kui Jia, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal deep neural networks.\n",
            "IEEE transactions on pattern analysis and machine intelligence , 43(4):1352–1368, 2019.\n",
            "Lester Mackey, Vasilis Syrgkanis, and Ilias Zadik. Orthogonal machine learning: Power and limita-\n",
            "tions. In International Conference on Machine Learning , pp. 3375–3383. PMLR, 2018.\n",
            "Peyman Sheikholharam Mashhadi, Sławomir Nowaczyk, and Sepideh Pashami. Parallel orthogonal\n",
            "deep neural network. Neural Networks , 140:167–183, 2021.\n",
            "V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\n",
            "stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\n",
            "arXiv:1312.5602 , 2013.\n",
            "Mete Ozay and Takayuki Okatani. Optimization on submanifolds of convolution kernels in cnns.\n",
            "arXiv preprint arXiv:1610.07008 , 2016.\n",
            "Bernardino Romera Paredes, Andreas Argyriou, Nadia Berthouze, and Massimiliano Pontil. Ex-\n",
            "ploiting unrelated tasks in multi-task learning. In Artificial intelligence and statistics , pp. 951–\n",
            "959. PMLR, 2012.\n",
            "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. FiLM:\n",
            "Visual reasoning with a general conditioning layer. CoRR , abs/1709.07871, 2017.\n",
            "Martin L Puterman. Markov decision processes: Discrete stochastic dynamic programming. Journal\n",
            "of the Operational Research Society , 1995.\n",
            "Jie Ren, Yewen Li, Zihan Ding, Wei Pan, and Hao Dong. Probabilistic mixture-of-experts for\n",
            "efficient deep reinforcement learning. arXiv preprint arXiv:2104.09122 , 2021.\n",
            "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\n",
            "policy optimization. In International conference on machine learning , pp. 1889–1897. PMLR,\n",
            "2015.\n",
            "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n",
            "optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n",
            "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\n",
            "Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\n",
            "the game of go with deep neural networks and tree search. nature , 529(7587):484–489, 2016.\n",
            "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\n",
            "Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi\n",
            "by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815 ,\n",
            "2017.\n",
            "11Preprint\n",
            "Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with context-\n",
            "based representations. In International Conference on Machine Learning , 2021.\n",
            "Lingfeng Sun, Haichao Zhang, Wei Xu, and Masayoshi Tomizuka. Paco: Parameter-compositional\n",
            "multi-task reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\n",
            "Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL\n",
            "https://openreview.net/forum?id=LYXTPNWJLr .\n",
            "Yee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nico-\n",
            "las Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances\n",
            "in Neural Information Processing Systems , 2017.\n",
            "Ruihan Yang, Huazhe Xu, YI WU, and Xiaolong Wang. Multi-task reinforcement learning with soft\n",
            "modularization. In Advances in Neural Information Processing Systems , 2020.\n",
            "Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey\n",
            "Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\n",
            "InConference on Robot Learning , 2019.\n",
            "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\n",
            "Gradient surgery for multi-task learning. In Advances in Neural Information Processing Systems ,\n",
            "2020.\n",
            "12Preprint\n",
            "(a) DoorKey\n",
            " (b) DistShift\n",
            " (c) RedBlueDoors\n",
            " (d) LavaGap\n",
            "(e) Memory\n",
            " (f) SimpleCrossing\n",
            " (g) MultiRoom\n",
            "Figure 7: MiniGrid (Chevalier-Boisvert et al., 2023) Tasks, where the red triangle represents the\n",
            "agent, and the green square refers to the goal.\n",
            "A A DDITIONAL DETAILS ON THE EXPERIMENTS\n",
            "In this section, we elaborate on the implementation details of our approach, MOORE, for bench-\n",
            "marking against MiniGrid (Chevalier-Boisvert et al., 2023) and MetaWorld (Yu et al., 2019). Be-\n",
            "sides, we provide additional ablation studies that demonstrate various aspects of our approach.\n",
            "A.1 M INIGRID\n",
            "A.1.1 E NVIRONMENT DETAILS\n",
            "MiniGrid (Chevalier-Boisvert et al., 2023) is a collection of 2D goal-oriented environments where\n",
            "the agent learns how to solve different mazes while interacting with various objects in terms of\n",
            "shape, color, and role. The library of MiniGrid provides multiple choice for state representation.\n",
            "For our MTRL setting, we adopt the visual representation of the state where a 3-dimensional input\n",
            "of shape 7x7x3 is provided. As mentioned in Sec. 5.1, our MTRL setting consists of three scenarios\n",
            "that include 7 tasks in total that are distributed differently. A render example of each of the tasks is\n",
            "demonstrated in Fig. 7. Additionally, the description of each task is provided in Tab. 2.\n",
            "Task Description\n",
            "DoorKey Use the key to open the door and then get to the goal.\n",
            "DistShift Get to the green goal square.\n",
            "RedBlueDoors Open the red door and then the blue door\n",
            "LavaGap Avoid the lava and get to the green goal square.\n",
            "Memory Go to the matching object at the end of the hallway\n",
            "SimpleCrossing Find the opening and get to the green goal square\n",
            "MultiRoom Traverse the rooms to get to the goal.\n",
            "Table 2: MiniGrid (Chevalier-Boisvert et al., 2023) Task Descriptions.\n",
            "13Preprint\n",
            "A.1.2 I MPLEMENTATION DETAILS\n",
            "As an RL algorithm, we use PPO (Schulman et al., 2017), which is considered a state-of-the-art\n",
            "on-policy RL algorithm on many benchmarks. Moreover, it has been used in the official paper of\n",
            "the MiniGrid benchmark (Chevalier-Boisvert et al., 2023). We adapt PPO to the MTRL setting\n",
            "by computing the loss function of both the actor and critic averaged on transitions sampled from\n",
            "all tasks. We refer to this adapted algorithm as MTPPO. In Tab. 3, we highlight the important\n",
            "hyperparameters needed to reproduce the results on MiniGrid.\n",
            "Hyperparameter Value\n",
            "General Hyperparameters\n",
            "Discount factor γ 0.99\n",
            "Number of environments [3,5,7]\n",
            "Steps per environment 1 step per 1 environment\n",
            "Number of epochs 100\n",
            "Steps per epoch 2000\n",
            "Train frequency 2000\n",
            "Number of episodes for evaluation 16\n",
            "PPO Hyperparameters\n",
            "Lambda coefficient in GAE formula 0.95\n",
            "Entropy term coefficient 0.01\n",
            "Clipping Epsilon 0.2\n",
            "Number of epochs for Policy 8\n",
            "Batch Size for Policy 256\n",
            "Number of epochs for Critic 1\n",
            "Batch Size for Critic 2000\n",
            "Critic Loss Mean Squared Error\n",
            "Optimizer Adam\n",
            "Learning rate for Policy 0.001\n",
            "Learning rate for Critic 0.001\n",
            "Table 3: MiniGrid (Chevalier-Boisvert et al., 2023) hyperparameters.\n",
            "We use a similar network architecture for the actor and the critic of our approach as well as the related\n",
            "baselines. In general, the network architecture consists of two main parts, a representation block ,\n",
            "and an output head . For the representation block, we use a Convolutional Neural Network (CNN)\n",
            "to encode the visual input to a latent space. For our MOORE, MOE, and PCGrad, k-CNNs are used\n",
            "to represent the mixture of experts inside the representation block. On the other hand, the output\n",
            "head consists of a task-encoder that generates the task-specific weights wc, in addition to the output\n",
            "module for producing the output of the network.\n",
            "The output module can utilize a single-head or a multi-head architecture. For single-head archi-\n",
            "tecture, the output of the representation block Vis weighted by the task-specific weight wc, then\n",
            "the task representation vcis concatenated with the task information c(e.g. task ID) and fed to\n",
            "the output module fθ. On the contrary, the multi-head architecture has multiple output modules\n",
            "fθ= [fθ1, .., f θ|C|]that can be selected given the context c(e.g. task ID).\n",
            "For regular baselines, we use a single CNN in the representation block while having the same two\n",
            "options of single-head and multi-head for the output module. Since we are using a single expert,\n",
            "there is no need for a task-encoder inside the output head. In Tab. 4, we illustrate the hyperparameters\n",
            "of both the representation block and the output head. It is worth noting that MOORE, MOE, and\n",
            "PCGrad linearly combine the generated representations from different experts before applying the\n",
            "last activation function of the representation block vc=Tanh(V wc).\n",
            "14Preprint\n",
            "Hyperparameter Value\n",
            "Representation Block\n",
            "Number of Experts ( k) {MT3: k= 2, MT5: k= 3, MT7: k= 4}\n",
            "Number of convolution layers 3\n",
            "Channels per layer [16, 32, 64]\n",
            "Kernel size [(2,2), (2,2), (2,2)]\n",
            "Activation functions [ReLU, ReLU, Tanh]\n",
            "Output Module\n",
            "Number of linear layers 2 (x number of tasks |T |)\n",
            "Number of output units [128,|A|for actor and 1 for critic]\n",
            "Activation functions [Tanh, Linear]\n",
            "Task Encoder\n",
            "Number of linear layers 1\n",
            "Number of output units Number of Experts (k)\n",
            "Use bias False\n",
            "Activation function Linear\n",
            "Table 4: Actor and Critic Architecture for PPO\n",
            "Algorithm 1 MOORE for Actor\n",
            "Require: Mixture of experts hϕ, state s, context c,\n",
            "task-specific weights wc, output module fθ.\n",
            "1:Us=hϕ(s)\n",
            "2:Vs=GS(Us) ▷Apply Eq.2\n",
            "3:vc=Vswc\n",
            "4:a∼fθ(vc)\n",
            "5:Return: aAlgorithm 2 MOORE for Critic\n",
            "Require: Mixture of experts hϕ, state-action (s, a), con-\n",
            "textc, task-specific weights wc, output module fθ.\n",
            "1:Us,a=hϕ(s, a)\n",
            "2:Vs,a=GS(Us,a) ▷Apply Eq.2\n",
            "3:vc=Vs,awc\n",
            "4:q=fθ(vc)\n",
            "5:Return: q\n",
            "A.2 M ETAWORLD\n",
            "A.2.1 E NVIRONMENT DETAILS\n",
            "MetaWorld (Yu et al., 2019) is a suite of a large number of robotic manipulation tasks. All tasks\n",
            "require dealing with one or two objects. Moreover, they are similar in terms of the dimensionality\n",
            "of the state space, yet the semantics of the state components differ. The state space consists of\n",
            "the following: the 3D position of the end effector, a normalized measure of how much the gripper\n",
            "is open, the 3D position of the first object, the quaternion of the first object (4D), as well as the\n",
            "3D position and quaternion of the second object (zeroed out, if not needed). Two consecutive data\n",
            "frames are stacked together, in addition to the 3D goal position forming a 39-dimensional state\n",
            "space. On the other hand, the action space is the same which represents the 3D change of the end\n",
            "effector, in addition to the normalized torque applied by the gripper. We benchmark our approach\n",
            "against the MT10 and MT50 scenarios. Following Sun et al. (2022), we randomize the goal position\n",
            "or the object position across all tasks and refer to it as MT10-rand and MT50-rand.\n",
            "A.2.2 I MPLEMENTATION DETAILS\n",
            "In this benchmark, we use SAC (Haarnoja et al., 2018), a state-of-the-art off-policy algorithm that\n",
            "enhances the exploration of the agent by maximizing the entropy. Similar to Yu et al. (2019); Sun\n",
            "et al. (2022), we adapt SAC by computing the actor and the critic losses averaged on transitions\n",
            "sampled from all tasks. We have a replay buffer for each task from which we sample transitions\n",
            "equally. In addition, we disentangle the temperature parameter of SAC by learning separate temper-\n",
            "ature parameters for each task. We refer to this adapted algorithm as MTSAC. Tab. 5, we list the\n",
            "hyperparameters required for reproducing our results on MetaWorld.\n",
            "Similar to MiniGrid, we use a network architecture with a representation block and an output head.\n",
            "The difference is that we use a dense neural network to represent the representation block. For\n",
            "MOORE, MOE, we also use k-dense networks to represent the mixture of experts. We also use a\n",
            "15Preprint\n",
            "task encoder to aggregate the representations Vgenerated by the experts. We apply a Tanh activation\n",
            "function after the linear combination of the representation vc=Tanh(V wc). Then, we fed the task-\n",
            "specific representation vcto the output module f θwhere we employed a multi-head architecture\n",
            "of a single linear layer per task. We use the context cto select the corresponding task-specific\n",
            "output module fθc. We show the MOORE adaptation for the actor and critic in Alg. 1 and Alg. 2,\n",
            "respectively.\n",
            "Hyperparameter Value\n",
            "General Hyperparameters\n",
            "Horizon 150\n",
            "Discount factor γ 0.99\n",
            "Number of environments 10\n",
            "Steps per environment 1 step per 1 environment\n",
            "Number of epochs 20\n",
            "Steps per epoch 100000\n",
            "Train frequency 1\n",
            "Number of episodes for evaluation 10\n",
            "SAC Hyperparameters\n",
            "Batch Size 128\n",
            "Critic Loss Mean Squared Error\n",
            "Disentangled temperature Alpha αTrue\n",
            "Optimizer Adam\n",
            "Learning rate for Policy 3×10−4\n",
            "Learning rate for Critic 3×10−4\n",
            "Learning rate for Alpha 1×10−4\n",
            "Policy minimum standard e−10\n",
            "Policy maximum standard e2\n",
            "Sift target interpolation 5×10−3\n",
            "Exploration steps 1500\n",
            "Replay buffer steps 1×106\n",
            "Table 5: MetaWorld Yu et al. (2019) Hyperparameters.\n",
            "Hyperparameter Value\n",
            "Representation Block\n",
            "Number of Experts ( k) 4\n",
            "Number of Linear layers 3\n",
            "Number of output units [400, 400, 400]\n",
            "Activation functions [ReLU, ReLU, Linear]\n",
            "Output Block\n",
            "Number of linear layers 1 (x number of tasks |T |)\n",
            "Number of output units [|A|for actor and 1 for critic]\n",
            "Activation functions Linear\n",
            "Task Encoder\n",
            "Number of linear layers 1\n",
            "Number of output units Number of Experts (k)\n",
            "Use bias False\n",
            "Activation function Linear\n",
            "Table 6: Actor and Critic Architecture for SAC\n",
            "16Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.75Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.30.40.50.6Average ReturnMemory\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 8: Individual task average return on the MT3 scenario of MiniGrid. We utilize the multi-head\n",
            "architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE, the\n",
            "number of experts kis 2. The black dashed line represents the final single-task performance of PPO\n",
            "averaged across all tasks. For the evaluation metric, we compute the accumulated return averaged\n",
            "across all tasks. We report the mean and the 95% confidence interval across 30 different runs.\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnDoorKey\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.81.0Average ReturnDistShift\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.30.40.50.6Average ReturnMemory\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 9: Individual task average return on the MT5 scenario of MiniGrid. We utilize the multi-head\n",
            "architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE, the\n",
            "number of experts kis 3. The black dashed line represents the final single-task performance of PPO\n",
            "averaged across all tasks. For the evaluation metric, we compute the accumulated return averaged\n",
            "across all tasks. We report the mean and the 95% confidence interval across 30 different runs.\n",
            "B A DDITIONAL EMPIRICAL RESULTS\n",
            "B.1 M INIGRID\n",
            "In Sec.5.1, we present the performance averaged across all the tasks. Here, we want to show the\n",
            "individual task performance of all three scenarios of MiniGrid.\n",
            "17Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnDoorKey\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnDistShift\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.75Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMemory\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnSimpleCrossing\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnMultiRoom\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 10: Individual task average return on the MT7 scenario of MiniGrid. We utilize the multi-\n",
            "head architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE,\n",
            "the number of experts kis 4. The black dashed line represents the final single-task performance\n",
            "of PPO averaged across all tasks. For the evaluation metric, we compute the accumulated return\n",
            "averaged across all tasks. We report the mean and the 95% confidence interval across 30 different\n",
            "runs.\n",
            "B.2 T RANSFER LEARNING WITH MOORE\n",
            "Furthermore, we discuss the experimental details of the Transfer Learning ablation study in Fig 3. In\n",
            "this study, we assess the transfer capability of our approach in utilizing the diverse representations,\n",
            "learned on a set of base tasks, for a set of novel but related tasks. We evaluate our approach,\n",
            "MOORE, against the MOE baseline on MiniGrid. We refer to the transfer learning adaptation of our\n",
            "approach as Transfer-MOORE , and Transfer-MOE for the MOE baseline.\n",
            "We conducted two experiments based on the sets of tasks defined on MiniGrid (MT3, MT5, and\n",
            "MT7). In Fig.3, we show the empirical results on two transfer learning scenarios where we transfer\n",
            "a set of experts learned on MT3 to MT5 ( MT3→MT5 ), and on MT5 to MT7 ( MT5→MT7 ). It is\n",
            "worth noting that MT3 is a subset of MT5, and MT5 is a subset of MT7. We consider the intersection\n",
            "between every two sets (MT3 and MT5 or MT5 and MT7) as base tasks while considering the\n",
            "difference as novel tasks. For instance, in the MT3 →MT5 scenario, the base tasks are LavaGap,\n",
            "RedBlueDoors, and Memory (common for MT3 and MT5), while having DoorKey, and MultiRoom\n",
            "as novel tasks (only in MT5).\n",
            "For Transfer-MOORE, we train on the base tasks, then we use the learned mixture of experts in\n",
            "a frozen state, to learn the novel ones. On the contrary, MOORE is only trained on novel tasks\n",
            "from scratch. This also holds for MOE and Transfer-MOE. In this study, we employ a multi-head\n",
            "18Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.75Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.4Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMemory\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 11: Individual task average return on the MT3 scenario of MiniGrid. We utilize the single-\n",
            "head architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE,\n",
            "the number of experts kis 2. The black dashed line represents the final single-task performance\n",
            "of PPO averaged across all tasks. For the evaluation metric, we compute the accumulated return\n",
            "averaged across all tasks. We report the mean and the 95% confidence interval across 30 different\n",
            "runs.\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnDoorKey\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.6Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.81.0Average ReturnDistShift\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.30.40.50.6Average ReturnMemory\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 12: Individual task average return on the MT5 scenario of MiniGrid. We utilize the single-\n",
            "head architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE,\n",
            "the number of experts kis 3. The black dashed line represents the final single-task performance\n",
            "of PPO averaged across all tasks. For the evaluation metric, we compute the accumulated return\n",
            "averaged across all tasks. We report the mean and the 95% confidence interval across 30 different\n",
            "runs.\n",
            "architecture for the actor and critic, hence each task has a decoupled output head from other tasks,\n",
            "easing the transfer learning experiment. However, they all share the representation stage (mixture\n",
            "of experts). To learn the novel tasks, we add randomly initialized output heads while keeping the\n",
            "mixture of experts frozen. For MT3→MT5 , the number of experts kis 2. On the other hand, for\n",
            "MT5→MT7 , we use 3 experts.\n",
            "B.3 C OSINE SIMILARITY\n",
            "We investigate the ability of MOORE to diversify the shared representations, compared to relaxing\n",
            "the hard constraint in Eq.1. Therefore, we replace the hard constraint with a regularization term\n",
            "19Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnDoorKey\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnDistShift\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.6Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.75Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.30.40.50.6Average ReturnMemory\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.6Average ReturnSimpleCrossing\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnMultiRoom\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 13: Individual task average return on the MT7 scenario of MiniGrid. We utilize the single-\n",
            "head architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE,\n",
            "the number of experts kis 4. The black dashed line represents the final single-task performance of\n",
            "PPO averaged across all tasks. We show the accumulated return averaged across all tasks. We report\n",
            "the mean and the 95% confidence interval across 30 different runs.\n",
            "equivalent to a cosine similarity loss computed over the set of representations:\n",
            "l=Es∈S[hϕ(s)Thϕ(s)−Ik], (3)\n",
            "where we added a regularization weight which we set to 1. We benchmark MOORE against the\n",
            "Cosine-Similarity on the three scenarios of MiniGrid. As shown in Fig. 14, MOORE outperforms\n",
            "the baseline across all settings, highlighting the advantage of using Gram-Schmidt in diversifying\n",
            "the experts over regularization-based techniques. In addition, our approach is hyperparameter-free ,\n",
            "contrary to the regularization-based techniques that require delicate hyperparameter tuning.\n",
            "C C OMPUTATION AND MEMORY REQUIREMENTS\n",
            "The difference between MOORE and MOE is in the Gram-Schmidt stage, where we orthogonalize\n",
            "thekrepresentations. The time complexity of the Gram-Schmidt process is T=O(k2×d)(Golub\n",
            "& Van Loan, 2013; Mashhadi et al., 2021), where dis the representation dimension and kis the\n",
            "number of experts. MOORE and MOE can be considered as soft-MOE because they both compute\n",
            "the whole krepresentations from all the experts and then aggregate them. On the other hand, sparse-\n",
            "MOE approaches select top-k experts based on soft weights computed using a gating network. The\n",
            "trade-off between the representation capacity and time complexity is well-known. As a future work,\n",
            "we can investigate the adaptation of MOORE to pick only a few orthogonal experts, hence lowering\n",
            "the time complexity. MOORE is similar to the MOE baseline in terms of the memory required for\n",
            "20Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT3\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT5\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT7\n",
            "Cosine-Similarity MOORE (ours)\n",
            "Figure 14: Evaluating the diversity capabilities of our approach, MOORE, against using Cosine-\n",
            "Similarity. The study is conducted on the three MTRL scenarios of MiniGrid employing a single-\n",
            "head architecture. The number of experts kis2,3, and 4for MT3, MT5, and MT7, respectively.\n",
            "For the evaluation metric, we compute the accumulated return averaged across all tasks. We report\n",
            "the mean and the 95% confidence interval across 30different runs.\n",
            "storing all the experts. It is worth noting that it is also similar to PaCo (Sun et al., 2022) regarding the\n",
            "memory requirements; however, in the MetaWorld experiments, we used fewer experts than PaCo.\n",
            "D T HEGRAM -SCHMIDT PROCESS AND THE INITIAL EXPERT\n",
            "0 20 40 60 80 100\n",
            "#Epochs0.20.40.60.8Average ReturnMOORE-u1\n",
            "MOORE-u2\n",
            "MOORE-u3\n",
            "Figure 15: Ablation study on the effect of the\n",
            "initial expert selected for the Gram-Schmidt pro-\n",
            "cess with a multi-head architecture. The number\n",
            "of experts kis where u 1, u2, and u 3represent the\n",
            "representations of the three experts before apply-\n",
            "ing the Gram-Schmidt process. For the evalua-\n",
            "tion metric, we compute the accumulated return\n",
            "averaged across all tasks. We report the mean\n",
            "and the 95% confidence interval across 30differ-\n",
            "ent runs.In MOORE, we consider the first expert’s rep-\n",
            "resentation as the initial vector for the Gram-\n",
            "Schmidt process. In a normal setting, we can\n",
            "expect the process to yield a different set of or-\n",
            "thonormal vectors depending on the initial se-\n",
            "lected vector. We argue that it does not matter\n",
            "in our case since the representations are actually\n",
            "generated from the mixture of experts, that are\n",
            "being trained. We conduct an ablation study on\n",
            "the MT5 scenario of MiniGrid where we utilize\n",
            "3experts. We provide the variations of MOORE\n",
            "based on the initial vector selected for the Gram-\n",
            "Schmidt process. For instance, MOORE-u1 se-\n",
            "lects the representation of the first expert u1 as\n",
            "the initial vector of the Gram-Schmidt process\n",
            "(similar to the whole paper). On the other hand,\n",
            "MOORE-u2 and MOORE-u3 select the repre-\n",
            "sentation of the second u 2and third u 3expert,\n",
            "respectively, as the initial vector for the Gram-\n",
            "Schmidt process. As expected, Fig. 15 shows\n",
            "that the performance is comparable for different\n",
            "selected initial vectors.\n",
            "21Multi-Task Learning Approach for Unified\n",
            "Biometric Estimation from Fetal Ultrasound\n",
            "Anomaly Scans\n",
            "Mohammad Areeb Qazi1*, Mohammed Talha Alam1, Ibrahim Almakky1,\n",
            "Werner Gerhard Diehl2, Leanne Bricker2, and Mohammad Yaqub1\n",
            "1Mohamed bin Zayed University of Artificial Intelligence,\n",
            "Abu Dhabi, United Arab Emirates\n",
            "2Abu Dhabi Health Services Company (SEHA),\n",
            "United Arab Emirates\n",
            "*mohammad.qazi@mbzuai.ac.ae\n",
            "Abstract. Precise estimation of fetal biometry parameters from ultra-\n",
            "sound images is vital for evaluating fetal growth, monitoring health,\n",
            "and identifying potential complications reliably. However, the automated\n",
            "computerized segmentation of the fetal head, abdomen, and femur from\n",
            "ultrasound images, along with the subsequent measurement of fetal bio-\n",
            "metrics, remains challenging. In this work, we propose a multi-task learn-\n",
            "ing approach to classify the region into head, abdomen and femur as well\n",
            "as estimate the associated parameters. We were able to achieve a mean\n",
            "absolute error (MAE) of 1.08 mm on head circumference, 1.44 mm on\n",
            "abdomen circumference and 1.10 mm on femur length with a classifi-\n",
            "cation accuracy of 99.91% on a dataset of fetal Ultrasound images. To\n",
            "achieve this, we leverage a weighted joint classification and segmentation\n",
            "loss function to train a U-Net architecture with an added classification\n",
            "head. The code can be accessed through Github .\n",
            "Keywords: Ultrasound Images ·Segmentation ·Classification ·Fetal\n",
            "Biometry.\n",
            "1 Introduction\n",
            "Ultrasound is a widely used non-invasive imaging modality in obstetrics for mon-\n",
            "itoring pregnancy progression and assessing fetal development. The term “ges-\n",
            "tational age” (GA), measured in weeks, is commonly used during pregnancy to\n",
            "indicate the stage of pregnancy. Accurately estimating gestational age and fetal\n",
            "biometry parameters through ultrasound images plays a critical role in evaluat-\n",
            "ing fetal growth progress and identifying any concerns during development [1].\n",
            "However, manual measurement of biometry parameters from ultrasound images\n",
            "is a time-consuming and operator-dependent task, leading to inconsistent and\n",
            "inaccurate measurements that can affect the interpretation of fetal growth and\n",
            "development [2].arXiv:2311.09607v1  [eess.IV]  16 Nov 20232 Qazi et al.\n",
            "Deep learning methods have demonstrated impressive performance in vari-\n",
            "ous medical imaging tasks, such as segmentation, registration, and classification\n",
            "[3]. Despite these advancements, there remains a need to leverage deep learn-\n",
            "ing techniques for the accurate estimation of gestational age and fetal biometry\n",
            "parameters from ultrasound images. By automating the process, deep learning\n",
            "models can provide consistent and reliable measurements, overcoming the limi-\n",
            "tations associated with manual estimation. Furthermore, the integration of deep\n",
            "learning algorithms holds promise in enabling early detection of fetal growth ab-\n",
            "normalities, thereby improving prenatal care and the management of potential\n",
            "complications [4]. The automation of biometry estimation in ultrasound imaging\n",
            "has emerged as a compelling area of research with the potential to enhance the\n",
            "efficiency of prenatal care. Clinicians recognize the significance of accurately esti-\n",
            "mating biometric parameters, including head circumference (HC), femur length\n",
            "(FL), and abdominal circumference (AC), in assessing fetal growth and devel-\n",
            "opment. As a result, there has been a growing interest in leveraging machine\n",
            "learning methods to automate biometry estimation, primarily focusing on HC\n",
            "estimation.\n",
            "The current body of literature on biometry parameter estimation primarily\n",
            "focuses on HC estimation, with limited attention given to other essential bio-\n",
            "metric measurements such as FL and AC. Consequently, there exists a research\n",
            "gap regarding the comprehensive assessment of multiple biometric parameters\n",
            "in ultrasound analysis [5]. In this paper, we aim to fill this gap by developing\n",
            "an end-to-end network that incorporates the estimation of multiple biometric\n",
            "parameters. Instead of solely emphasizing HC estimation, our framework will\n",
            "incorporate the calculation of FL and AC measurements. This integrated ap-\n",
            "proach, encompassing anatomical region identification and subsequent biometric\n",
            "parameter estimation, has the potential to be a valuable tool for clinicians in\n",
            "their assessment of fetal development.\n",
            "In this work, our main contributions are:\n",
            "1. We introduce a comprehensive multi-task framework that combines classi-\n",
            "fication and accurate estimation of biometric parameters, namely, HC, AC\n",
            "and FL.\n",
            "2. We conduct an investigation into determining the optimal weight allocations\n",
            "for the losses within our multi-task setup. This analysis sheds light on the\n",
            "crucial interplay between losses, enhancing our understanding of the system’s\n",
            "overall performance.\n",
            "2 Methods\n",
            "This research introduces a multitask learning method for 2D ultrasound im-\n",
            "ages. The proposed model proposes an end-to-end Convolutional Neural Net-\n",
            "work (CNN) architecture that does classification as well as segmentation. The\n",
            "model predicts the class and the segmentation mask which are forwarded for\n",
            "post-processing, producing the bio-parameter associated with that class. TheMulti-Task Learning for Fetal US 3\n",
            "Softmax\n",
            "FCλ LCls + (1-λ) L Seg\n",
            "Conv\n",
            "Maxpool\n",
            "Up-conv\n",
            "Skip Connection\n",
            "Ellipse fitting\n",
            "Rectangle fittingMODELPOST \n",
            "PROCESSING\n",
            "B\n",
            "FA\n",
            "Fig. 1: (a) Overview of the proposed multi-task learning network. B: Brain, A:\n",
            "Abdomen, F: Femur.\n",
            "proposed framework is shown in Fig. 1. We use U-Net [6] as the backbone network\n",
            "for the task due to its excellent performance in 2D medical image segmentation.\n",
            "The U-Net design can be separated into three parts: an encoding path, a\n",
            "decoding path, and skip connections between them. U-Net incorporates stacked\n",
            "convolution layers in each level, as shown in Fig. 1. In our approach, we em-\n",
            "ployed five downsampling steps in the encoder to capture sufficient features.\n",
            "Correspondingly, the decoder used five upsampling steps to restore the features\n",
            "to the original size, leveraging the encoded information to predict the target’s\n",
            "segmentation mask. Skip connections linked feature maps from the encoder to\n",
            "the decoder, allowing spatial information to flow and improving segmentation\n",
            "results. All convolution layers used 3x3 kernels, followed by batch normaliza-\n",
            "tion and ReLU activation. Additionally, we utilized 2x2 pooling to downsample\n",
            "feature maps effectively.\n",
            "2.1 Multi-task Learning Network\n",
            "Taking inspiration from the established practice in image classification, where\n",
            "prominent CNN models like VGG [7] and ResNet [8] incorporate high-level fea-\n",
            "ture maps, our approach capitalizes on this insight. We extend the idea to our\n",
            "proposed multi-task learning network, enabling the extraction of shared features\n",
            "from U-Net for segmentation and classification. Within this network architec-\n",
            "ture, we introduce a classification branch at the base of the U-Net, as depicted\n",
            "in Fig. 1. This branch utilizes feature maps from the bottleneck layer, which4 Qazi et al.\n",
            "are flattened and directed through a fully connected (FC) layer, followed by a\n",
            "softmax layer. This ensemble empowers the network to predict the input image\n",
            "class into brain, abdomen, or femur regions, which also helps in the segmentation\n",
            "task.\n",
            "For the classification task, we employed the cross-entropy loss function. This\n",
            "loss function quantifies the dissimilarity between predicted and actual class prob-\n",
            "abilities, aiming to minimize the discrepancy during training, and it is formulated\n",
            "as follows:\n",
            "Lcls(x, y) =−1\n",
            "NNX\n",
            "n=1log \n",
            "exp (xn,yn)PC\n",
            "c=1exp (xn,c)!\n",
            "(1)\n",
            "where xrepresents the input, ysignifies the ground truth class, Cis the total\n",
            "number of classes, and Nis the batch size.\n",
            "For the segmentation task, we employed the Dice loss function, which is\n",
            "formulated as follows:\n",
            "Lseg(p, q) = 1−2P\n",
            "ip(i)·q(i)P\n",
            "ip(i)2+P\n",
            "iq(i)2(2)\n",
            "where the variable psignifies the ground truth binary mask, whereas qstands\n",
            "for the predicted segmentation mask. The index ipertains to the individual ele-\n",
            "ments within the mask. Within our methodology, we integrate the classification\n",
            "loss and the segmentation loss through a linear combination, regulated by a hy-\n",
            "perparameter denoted as λ. This combined loss, known as the multi-task loss, is\n",
            "precisely defined as:\n",
            "Ljoint=λ· Lcls+ (1−λ)· Lseg (3)\n",
            "The significance of λlies in its capacity to assign weights to individual losses.\n",
            "This strategic weight allocation hinges on the concept that tasks characterized by\n",
            "higher training losses merit increased attention during the optimization process.\n",
            "By systematically varying λand conducting an ablation study, we gain insights\n",
            "into its impact and determine the optimal value that aligns with our objectives.\n",
            "2.2 Estimating the Bio-Parameters\n",
            "To calculate the bio parameter, we post-process the segmentation masks. We ob-\n",
            "served that the estimation process of HC and AC differs from the FL estimation\n",
            "process. This distinction arises due to the distinct shape characteristics of these\n",
            "anatomical regions. The head and abdomen regions can be assumed to have an\n",
            "elliptical shape, allowing us to apply ellipse-fitting techniques to extract the bor-\n",
            "der of the head/abdomen from the segmented regions. Then the circumference\n",
            "can be approximated using the following equation:\n",
            "circumference =πa+b+ 3−(a−b)2\n",
            "10a+ 10b+√\n",
            "a2+ 14ab+b2(4)Multi-Task Learning for Fetal US 5\n",
            "where a, b, x, y is the semi-minor and semi-major axis lengths and the centre\n",
            "coordinate of the ellipse.\n",
            "In contrast to the elliptical shape of the brain and abdomen regions, we found\n",
            "the femur region to have a rectangular shape. To estimate the dimensions of the\n",
            "femur, we apply a rectangle fitting technique, allowing us to extract the length\n",
            "and breadth of the rectangular region. Subsequently, we calculate the perimeter\n",
            "of the rectangle using the obtained length and breadth measurements.\n",
            "3 Data and Experiments\n",
            "3.1 Dataset\n",
            "Our dataset consists of images collected from a hospital over one year. The\n",
            "dataset comprises a total of 5,872 images from 2206 subjects, with 2,023 images\n",
            "depicting the brain, 2,128 images representing the abdomen, and the remaining\n",
            "1,721 images focusing on the femur. To ensure comprehensive evaluation, we\n",
            "divided the dataset on the basis of subjects into an 80% training set and a\n",
            "20% testing set. Additionally, we allocated 10% of the training set as a separate\n",
            "validation set for fine-tuning and performance monitoring.\n",
            "The dataset included images and some associated information. Specifically,\n",
            "for the brain and abdomen, the dataset contained coordinates for the center,\n",
            "length of the the major and minor axes with an angle with the x-axis. Using these\n",
            "values, elliptical masks were created to represent these regions in the images. For\n",
            "the femur, the data had endpoints of it. These were connected to create a line\n",
            "with a fixed width, essentially forming a mask for the femur class. To make the\n",
            "data consistent, both the images and the masks were resized to a common size\n",
            "of 256 x 256 before using them for the model.\n",
            "3.2 Experiments\n",
            "To evaluate the effectiveness of the proposed method, we conducted ablation\n",
            "experiments. The results for each of these experiments are shown in Tab. 1. We\n",
            "started by training a separate U-Net model for each of the three classes which\n",
            "serves as the baseline and later helped us understand the benefit of training a\n",
            "single model for all three classes. Then, we added the classification head in the\n",
            "model and trained only the classification head by keeping the weight of the dice\n",
            "loss 0. Similarly, we also trained the model by keeping the classification loss\n",
            "weight as 0. Then we trained different models by tweaking the values of λ.\n",
            "All models were implemented using Pytorch [9] and were trained and tested\n",
            "on Quadro RTX 6000 GPUs (25G) using the ADAMax optimizer with a decaying\n",
            "learning rate initialized at 5e-4. The batch size and the number of epochs were\n",
            "set to 16 and 100 for each experiment respectively.\n",
            "In this context, considering the relatively manageable nature of the classi-\n",
            "fication task and the balanced dataset, we opted to utilize only accuracy as\n",
            "the metric for evaluating the classification component of the model. For the seg-\n",
            "mentation task, we used the biometric parameter error metric i.e. Mean Average6 Qazi et al.\n",
            "Table 1: The effects of λon Multi-Task Learning: The table presents an analysis\n",
            "of varying λvalues on the Multi-Task Learning framework. The performance\n",
            "metrics encompass accuracy percentages and MAE for distinct organ classes.\n",
            "Asλis adjusted, notable trends emerge in both accuracy and estimation errors\n",
            "across the brain, abdomen, and femur classes.\n",
            "λ Accuracy (%) Brain (MAE in mm) Abdomen (MAE in mm) Femur (MAE in mm)\n",
            "1 100 - - -\n",
            "0.8 100 1.50 ±1.6156 2.33 ±4.9390 1.50 ±2.1424\n",
            "0.6 100 1.88 ±1.7502 2.78 ±2.9089 1.65 ±2.6943\n",
            "0.4 99.91 1.39 ±1.2199 2.08 ±3.9290 1.53 ±6.3734\n",
            "0.2 100 1.41 ±1.2420 1.83 ±2.0092 1.13 ±1.6356\n",
            "0.1 100 1.19 ±1.0825 1.55 ±1.7488 1.14 ±1.5085\n",
            "0.05 99.91 1.46 ±1.2163 2.11 ±3.7542 1.14 ±1.7677\n",
            "0.025 100 1.24 ±1.1018 1.73 ±2.7895 1.31 ±1.9683\n",
            "0.01 100 1.22 ±1.1179 1.61 ±1.8859 1.19 ±1.4767\n",
            "0.001 99.91 1.08±0.9702 1.44 ±1.9269 1.13 ±1.4498\n",
            "0.00001 99.33 1.18 ±1.0052 1.54 ±1.7283 1.10±1.1298\n",
            "0 - 11.24 ±23.1575 14.94 ±26.8828 65.12 ±56.0734\n",
            "Separate Models 1.71 ±1.35 1.55 ±1.88 1.08 ±1.18\n",
            "Error (MAE). We apply the post-processing steps on the mask and then calculate\n",
            "MAE. MAE gauges the average magnitude of discrepancies between predicted\n",
            "and true values. The MAE values served as objective metrics for quantifying the\n",
            "results of our models in capturing these measurements. The formula for MAE\n",
            "is:\n",
            "MAE =1\n",
            "nnX\n",
            "i=1|y(i)\n",
            "true−y(i)\n",
            "pred| (5)\n",
            "Here, nstands for the number of samples, y(i)\n",
            "truerepresents the true biometric\n",
            "parameter value for the ith sample, and y(i)\n",
            "predis the corresponding predicted\n",
            "value.\n",
            "4 Results and Discussion\n",
            "Tab. 1 provides an overview of our ablation experiment outcomes. Initially, when\n",
            "we used separate models for each class, we obtained MAE values of 1.71 mm\n",
            "for the brain, 1.55 mm for the abdomen, and 1.08 mm for the femur. We then\n",
            "trained only the encoder for classification across all classes (see Tab. 1, λ= 1),\n",
            "successfully classifying all test images accurately. Moreover, we also trained the\n",
            "U-Net without the classification head (see Tab. 1, λ= 0) to examine its effect\n",
            "on segmentation when classification is excluded. The results were notably worse,\n",
            "yielding 11.24 mm MAE for the brain, 14.95 mm MAE for the abdomen, and\n",
            "65.12 mm MAE for the femur. These outcomes are considered the baseline for\n",
            "the ablation study, guiding us in finding the optimal balance between the weights\n",
            "of the two losses.Multi-Task Learning for Fetal US 7\n",
            "Fig. 2: Effect of λon the Bio-Parameters: The image illustrates the impact of\n",
            "varying λvalues on the bio-parameters. Noticeably, the model’s performance is\n",
            "better with lower λvalues.\n",
            "The value of λin Eq. (3) affects the trade-off between classification and\n",
            "segmentation performance in multi-task learning. We experimented with vari-\n",
            "ousλvalues and observed interesting trends in Tab. 1. Larger λvalues gave\n",
            "excellent classification accuracy but had some impact on segmentation. Lower-\n",
            "ingλimproved segmentation without significantly compromising classification.\n",
            "Surprisingly, very low λvalues produced good segmentation. Among all tested\n",
            "values, λ= 0.001 struck the best balance. Although classification performance\n",
            "was consistently strong across values near 1 and smaller, the MAE saw the most\n",
            "improvement from 1 to 0.001. Further, lowering λfurther led to diminishing\n",
            "returns. Fig. 2 illustrates this relationship graphically, showing how reducing λ\n",
            "enhanced MAEs.\n",
            "Fig. 3 visually presents the impact of varying λon the convergence of different\n",
            "losses. In Fig. 3a, we observe that a lower λvalue led to quicker convergence\n",
            "of the dice loss. This can be attributed to the model’s increased emphasis on\n",
            "the dice loss, which in turn accelerated the learning process. Conversely,higher\n",
            "λvalues caused greater fluctuations in the cross-entropy loss. This behavior\n",
            "suggests that the model found learning the classes relatively easier when the λ\n",
            "was low. Consequently, the overall loss, as depicted in Fig. 3b, exhibited smoother\n",
            "convergence when λwas set to a smaller value. The optimal λvalue is 0.001 at\n",
            "which learning was notably seamless for both tasks, and the results reached their\n",
            "peak performance.\n",
            "The pronounced improvement observed with lower values of λconcludes that\n",
            "the classification task is relatively more manageable. This is attributed to the\n",
            "distinct structural characteristics of the classes, allowing the model to effectively\n",
            "grasp features and distinguish them. The multi-task learning framework plays a8 Qazi et al.\n",
            "(a) Dice & Cross-Entropy Loss w.r.t λ\n",
            " (b) Total Loss w.r.t λ\n",
            "Fig. 3: Training Loss Graphs with various values of λ.\n",
            "pivotal role in enhancing performance across individual classes, leveraging the\n",
            "increased volume of training data to capture a broader spectrum of variations.\n",
            "The integration of a classification head yielded substantial advantages, partic-\n",
            "ularly evident in improved segmentation outcomes and more precise estimations\n",
            "of the bio-parameter. This enhancement was prominently visible in the error\n",
            "reductions for specific classes: the MAE for the brain class diminished from 1.71\n",
            "mm to 1.08 mm, and for the abdomen class, it decreased from 1.55 mm to 1.44\n",
            "mm. Interestingly, the femur class exhibited relatively consistent performance,\n",
            "similar to training a standalone model. This could be attributed to the struc-\n",
            "tural similarity between the brain and abdomen classes, allowing the model to\n",
            "leverage knowledge gained from one class to benefit the learning process for the\n",
            "other.\n",
            "Conversely, the distinct characteristics of the femur class prevented similar\n",
            "cross-class learning benefits. The output generated by our model when process-\n",
            "ing images from various classes is depicted in Fig. 4. In cases of success, as\n",
            "shown in Fig. 4a, the model’s predictions closely align with the ground truth,\n",
            "resulting in favorable Mean Average Error (MAE) values. However, challenges\n",
            "are evident in Fig. 4b, which showcases instances of the model struggling with\n",
            "accurate segmentation, subsequently affecting the estimation. Notably, for the\n",
            "brain class, the segmentation mask appears sound, yet the MAE reaches around\n",
            "6mm. Conversely, for the femur, complications arise due to mixed outputs with\n",
            "other classes, adversely impacting the predictions.\n",
            "5 Conclusion\n",
            "In this work, an integrated approach is proposed to simultaneously classify, seg-\n",
            "ment, and estimate parameters of various body organs in a fetus. The method\n",
            "utilizes a multi-task learning technique. By merging the cross-entropy loss withMulti-Task Learning for Fetal US 9\n",
            "(a) Success cases\n",
            "(b) Failure cases\n",
            "Fig. 4: Model Predictions: Each prediction incorporates printed estimations from\n",
            "the model. The prediction showcases the estimated biometric parameter (left)\n",
            "alongside the corresponding ground truth value (right). Further, the predicted\n",
            "class (left) and the actual class (right) are presented below these values.\n",
            "the dice loss, our model is designed to effectively estimate the parameters associ-\n",
            "ated with each class. Unlike previous studies, our approach involves the calcula-\n",
            "tion of multiple bio-parameters and capitalizes on insights from other classes to\n",
            "enhance overall performance. Additionally, we conduct experiments to evaluate\n",
            "the impact of varying loss weights within the multi-task learning framework.\n",
            "To evaluate the effectiveness of our approach, we compared it with individu-\n",
            "ally trained segmentation models for each organ class. The results demonstrated\n",
            "noticeable improvements in terms of MAE and classification accuracy. The inte-\n",
            "grated model outperformed the individual models across evaluated organ classes.\n",
            "One of the key advantages of our method is its versatility, as it can be applied\n",
            "to multiple body organs, rather than focusing on a single organ. This offers\n",
            "clinicians a comprehensive tool to assess various bio-parameters simultaneously.\n",
            "Additionally, the computational cost of our model remains manageable, as it\n",
            "covers multiple organ parameters within a single framework. Future work can\n",
            "be done by extending this framework to more organ estimations.\n",
            "References\n",
            "1. Jamie Perin, Amy Mulick, Diana Yeung, Francisco Villavicencio, Gerard Lopez,\n",
            "Kathleen L Strong, David Prieto-Merino, Simon Cousens, Robert E Black, and\n",
            "Li Liu. Global, regional, and national causes of under-5 mortality in 2000–19: an10 Qazi et al.\n",
            "updated systematic analysis with implications for the sustainable development goals.\n",
            "The Lancet Child & Adolescent Health , 6(2):106–115, 2022.\n",
            "2. Leo Joskowicz, D Cohen, N Caplan, and Jacob Sosna. Inter-observer variability of\n",
            "manual contour delineation of structures in ct. European radiology , 29:1391–1399,\n",
            "2019.\n",
            "3. Andr´ es Anaya-Isaza, Leonel Mera-Jim´ enez, and Martha Zequera-Diaz. An overview\n",
            "of deep learning in medical imaging. Informatics in medicine unlocked , 26:100723,\n",
            "2021.\n",
            "4. Mehmet Murat Seval and Bulut Varlı. Current developments in artificial intelligence\n",
            "from obstetrics and gynecology to urogynecology. Frontiers in Medicine , 10, 2023.\n",
            "5. Maria Chiara Fiorentino, Francesca Pia Villani, Mariachiara Di Cosmo, Emanuele\n",
            "Frontoni, and Sara Moccia. A review on deep-learning algorithms for fetal\n",
            "ultrasound-image analysis. Medical Image Analysis , 83:102629, jan 2023.\n",
            "6. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional net-\n",
            "works for biomedical image segmentation. In Nassir Navab, Joachim Hornegger,\n",
            "William M. Wells, and Alejandro F. Frangi, editors, Medical Image Computing\n",
            "and Computer-Assisted Intervention – MICCAI 2015 , pages 234–241, Cham, 2015.\n",
            "Springer International Publishing.\n",
            "7. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for\n",
            "large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014.\n",
            "8. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\n",
            "for image recognition. In Proceedings of the IEEE conference on computer vision\n",
            "and pattern recognition , pages 770–778, 2016.\n",
            "9. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\n",
            "gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.\n",
            "Pytorch: An imperative style, high-performance deep learning library. Advances in\n",
            "neural information processing systems , 32, 2019.Fast Detection of Phase Transitions\n",
            "with Multi-Task Learning-by-Confusion\n",
            "Julian Arnold\n",
            "Department of Physics\n",
            "University of Basel\n",
            "Klingelbergstrasse 82, 4056 Basel, Switzerland\n",
            "julian.arnold@unibas.chFrank Schäfer\n",
            "CSAIL\n",
            "Massachusetts Institute of Technology\n",
            "Cambridge, Massachusetts 02139, USA\n",
            "franksch@mit.edu\n",
            "Niels Lörch\n",
            "Department of Physics\n",
            "University of Basel\n",
            "Klingelbergstrasse 82, 4056 Basel, Switzerland\n",
            "niels.loerch@unibas.ch\n",
            "Abstract\n",
            "Machine learning has been successfully used to study phase transitions. One of\n",
            "the most popular approaches to identifying critical points from data without prior\n",
            "knowledge of the underlying phases is the learning-by-confusion scheme. As\n",
            "input, it requires system samples drawn from a grid of the parameter whose change\n",
            "is associated with potential phase transitions. Up to now, the scheme required\n",
            "training a distinct binary classifier for each possible splitting of the grid into two\n",
            "sides, resulting in a computational cost that scales linearly with the number of\n",
            "grid points. In this work, we propose and showcase an alternative implementation\n",
            "that only requires the training of a single multi-class classifier. Ideally, such multi-\n",
            "task learning eliminates the scaling with respect to the number of grid points.\n",
            "In applications to the Ising model and an image dataset generated with Stable\n",
            "Diffusion, we find significant speedups that closely correspond to the ideal case,\n",
            "with only minor deviations.\n",
            "1 Introduction\n",
            "An exciting application of machine learning in physics is the detection of phase transitions from\n",
            "data [ 1–11]: the state of a physical system is sampled at different values of a tuning parameter that\n",
            "determines the distribution of samples. The states are fed to an algorithm that identifies the parameter\n",
            "values at which the system undergoes a phase transition.\n",
            "One of the most popular methods to accomplish this task is learning-by-confusion [3], which has\n",
            "successfully revealed phase transitions in a large variety of physical systems using data from both\n",
            "simulation [ 3,12–26] and experiment [ 19]. So far, its implementation has been computationally\n",
            "costly, as it involves training Kdistinct binary classifiers and analyzing their accuracy, where K+ 1\n",
            "is the number of sampled values of the tuning parameter.\n",
            "In this work, we propose an alternative implementation of the learning-by-confusion scheme that\n",
            "works by training a single K-class classifier, which thus promises a speedup by a factor of Kin\n",
            "the ideal case. We demonstrate a significant speedup in two applications: First, the thermal phase\n",
            "transition of the Ising model, which is theoretically well-understood and allows for a comparison\n",
            "Machine Learning and the Physical Sciences Workshop, NeurIPS 2023.arXiv:2311.09128v1  [cs.LG]  15 Nov 2023with established results. Second, an image dataset generated with Stable Diffusion [ 27], which serves\n",
            "as a more challenging example where no prior knowledge of transitions is available.\n",
            "2 Unbiased Learning-by-Confusion with Multi-Task Learning\n",
            "In the simplest use case of the learning-by-confusion method [ 3], a physical system undergoes a\n",
            "phase transition as a function of a single real-valued parameter θ. To detect the critical point θ∗at\n",
            "which the phase transition occurs, the θ-axis is discretized into K+ 1different points and at each\n",
            "point, Msamples are drawn from the system. With K+ 1points, there are Kpossibilities to separate\n",
            "theθ-axis in two non-empty, contiguous regions Θ<\n",
            "k={θ|θ < θ∗\n",
            "k}andΘ>\n",
            "k={θ|θ > θ∗\n",
            "k}, each\n",
            "corresponding to a tentative location θ∗\n",
            "k= (θk+θk+1)/2of the phase transition that lies between\n",
            "the grid points θkandθk+1. For each of these splittings, a separate classifier is trained to distinguish\n",
            "the two corresponding classes of samples, see Fig. 1(a). Intuitively, whichever classifier kis least\n",
            "confused, i.e., achieves the lowest error rate on evaluation, must have been trained on the most natural\n",
            "splitting of the data. Therefore, the value θ∗\n",
            "kassociated with the lowest error rate is our best guess for\n",
            "the location of the phase transition.\n",
            "The loss function of the kth classifier is an unbiased binary cross-entropy loss\n",
            "Lk=−1\n",
            "2X\n",
            "y∈{>,<}1\n",
            "|Dy\n",
            "k|X\n",
            "x∈Dy\n",
            "klog [ˆpk(y|x)], (1)\n",
            "where |Dy\n",
            "k|is the size of the dataset Dy\n",
            "kof samples xcorresponding to Θy\n",
            "k, and ˆpkis the estimated\n",
            "class probability of the classifier. The normalization1\n",
            "|Dy|compensates for the imbalance of classes\n",
            "that would otherwise bias the signal and may lead to a failure of the confusion scheme [ 26]. Similarly,\n",
            "the error rate of the kth classifier can be estimated as\n",
            "perr\n",
            "k≈1\n",
            "2X\n",
            "y∈{>,<}1\n",
            "|Dy\n",
            "k|X\n",
            "x∈Dy\n",
            "kerrk(y,x), (2)\n",
            "where for each sample xthe error errk(y,x)is 0 if it is classified correctly and 1 if it is classified\n",
            "erroneously. During training, Dy\n",
            "kin Eq. (1)refers to a training set, while it typically refers to a\n",
            "separate evaluation set when estimating the error via Eq. (2).\n",
            "pK−1(x)pK−1(x)050100150200k10°310°210°11perrkcritical point (theory)histogram-based referencemulti-task learningneural netneural netneural netneural net...neural net...(a)(b)...p1(x)p2(x)...p1(x)p2(x)xxxxx(c)p0(x)p0(x)\n",
            "Figure 1: Schematic illustration of the learning-by-confusion method for detecting phase transitions\n",
            "(a) with original single-task architecture and (b) with our proposed multi-task architecture. (c) Classi-\n",
            "fication error for the Ising model at each node using a single network trained on spin configurations.\n",
            "The solid lines correspond to, from top to bottom, the result after 0, 1, 5, and 50 epochs of training\n",
            "averaged over 5 independent runs. The vertical dotted line indicates the true location of the phase\n",
            "transition and the dashed line corresponds to an estimate of the Bayes-optimal error rate obtained\n",
            "using a histogram-based generative classifier in energy space [20, 26].\n",
            "Instead of training a new classifier for each tentative splitting k, we propose to train a single classifier\n",
            "withKoutputs {ˆpk}K−1\n",
            "k=0, cf. Figs. 1(a) and (b). The loss function for this multi-task architecture\n",
            "isL=1\n",
            "KPK−1\n",
            "k=0Lk. The evaluation of the error rate remains the same as before. Multi-task\n",
            "learning [ 28,29] is expected to be highly efficient because the Kclassification tasks are very similar\n",
            "to each other and only differ in a slight alteration of the tentative splitting of the parameter space.\n",
            "Thus, the learned features are very much transferable between tasks.\n",
            "23 Benchmark and Application\n",
            "3.1 Ising Model\n",
            "As a test system that has been extensively analyzed theoretically and for which established benchmarks\n",
            "are available, we consider the two-dimensional square-lattice ferromagnetic Ising model. It is\n",
            "described by the energy function E(σ) =−JP\n",
            "⟨ij⟩σiσj,where the sum runs over all nearest-\n",
            "neighboring sites (with periodic boundary conditions), Jis the interaction strength (J >0), and\n",
            "σi∈ {+1,−1}denotes the discrete spin variable at lattice site i. The Ising model exhibits a phase\n",
            "transition between a paramagnetic phase at high temperature Tand a ferromagnetic phase at low\n",
            "temperature [30].\n",
            "To generate the dataset for the Ising model, we sample spin configurations σon a60×60lattice\n",
            "from Boltzmann distributions at 200 equally-spaced parameter values between 0.05kBT/J and10\n",
            "kBT/J via Markov chain Monte Carlo, see Appendix A.1.2 for details. Figure 1(c) shows how the\n",
            "learning-by-confusion signal of a multi-task convolutional neural network trained on this dataset\n",
            "evolves with training epochs, see Appendix A.1.2 for implementation details. Eventually, the node\n",
            "achieving the lowest error rate coincides with the critical point.\n",
            "To compare the single- and multi-task approach, we train 4-layer convolutional networks with\n",
            "otherwise identical architectures and training settings. Figure 2(a) shows how the error rate evolves\n",
            "as a function of the training epoch at nodes below, near, and above the critical point. In all cases,\n",
            "single-task learning-by-confusion shows a slightly faster rate of convergence early on during training.\n",
            "Below the phase transition, multi-task learning-by-confusion does not achieve an error rate as low\n",
            "as single-task learning-by-confusion. In contrast, near and above the phase transition, multi-task\n",
            "learning-by-confusion eventually catches up and even achieves lower error rates.\n",
            "We also studied the training behavior of multi-task networks that each have an output node corre-\n",
            "sponding to the true critical point as well as a varying number of additional output nodes. In particular,\n",
            "we recorded the number of epochs it takes to reach different error thresholds at their critical output\n",
            "node. At low thresholds, the difference in the number of epochs between different architectures\n",
            "was negligible. At high thresholds, we observed the number of epochs to marginally increase with\n",
            "the number of output nodes. However, we observed no clear scaling and any overhead was minor\n",
            "(at worst ≈ ×6for some combinations of training hyperparameters and model architectures) as\n",
            "compared to the speedup gained through multi-tasking.\n",
            "0 5 10 15 20 25 30\n",
            "epoch10−210−1perr\n",
            "(a) multi-task k = 10\n",
            "multi-task k = 40\n",
            "multi-task k = 180single-task k = 10\n",
            "single-task k = 40\n",
            "single-task k = 180\n",
            "0 10 20 30 40 50\n",
            "epoch3×10−210−13×10−1\n",
            "4×10−26×10−22×10−1perr\n",
            "(b) multi-task k = 29\n",
            "multi-task k = 56\n",
            "multi-task k = 121single-task k = 29\n",
            "single-task k = 56\n",
            "single-task k = 121\n",
            "Figure 2: Error rate at representative nodes as a function of training epoch for single-task and multi-\n",
            "task learning-by-confusion for (a) the Ising dataset averaged over 5 runs and (b) the Stable Diffusion\n",
            "dataset averaged over 4 runs. Error bars derived from the standard deviation are negligible (same\n",
            "scale as markers), and underestimate the true confidence intervals, e.g., due to the non-Gaussian\n",
            "nature of the data.\n",
            "3.2 Stable Diffusion\n",
            "We now consider an image dataset generated using Stable Diffusion [ 27], where for each integer\n",
            "θin[1900 ,2050) images are sampled with the prompt “technology of the year θ”. For example,\n",
            "node 0 corresponds to the separator between the years 1900 and 1901. While this dataset does not\n",
            "3feature phase transitions in the physical sense, the learning-by-confusion scheme can be used to\n",
            "identify points in parameter space at which the data distribution changes rapidly. Note that no prior\n",
            "benchmark is available for this data and the predicted change points cannot be verified by theory.\n",
            "As the images have complex features comparable to typical image datasets, we load the pretrained\n",
            "ResNet-50 [ 31] from PyTorch [32], and exchange its final layer to fit our task, see Appendix A.1.1\n",
            "for implementation details.\n",
            "Figure 3 shows (at least) three major local minima indicating rapid changes in the image dataset; one\n",
            "between the years 1929 and 1930, a second, broader one in the 1990s, and a third one between 2021\n",
            "and 2022. The Stable Diffusion model has only encountered images of actual technology from before\n",
            "and around 2022 in its training dataset LAION-5B [ 33], which may explain the third minimum. Due\n",
            "to the small dataset size and the resulting challenges in generalization, the signal is generally less\n",
            "reliable close to the edges.\n",
            "1920 1940 1960 1980 2000 2020 2040\n",
            "year0.050.10perr\n",
            "Figure 3: Error rate obtained using multi-task learning-by-confusion for the Stable Diffusion dataset\n",
            "as a function of θin prompt \"technology of the year θ\". The colored lines depict the results obtained\n",
            "by training on three separate datasets, each averaged over 10 runs as described in Appendix A.1 in\n",
            "more detail. The black line represents their mean.\n",
            "Figure 2(b) shows the error at representative points, the extrema at nodes 29, 56, 121 (corresponding\n",
            "to the transitions between years 1929-1930, 1956-1957, and 2021-2022) as a function of the training\n",
            "epoch for a single-task and multi-task network with otherwise identical network architecture and\n",
            "training parameters. As there is no significant overhead, the speedup of the multi-task approach is\n",
            "approximately given by the number of grid separators (here K= 149 ).\n",
            "4 Discussion and Conclusion\n",
            "In the limit of infinite model capacity, both multi-task and single-task learning models ultimately\n",
            "yield the same predictions, because the multi-task loss corresponds to the average of the single-task\n",
            "losses. However, in real-world scenarios where model expressivity, training time, and data are limited,\n",
            "the learning behavior depends on the particulars of the model and dataset at hand.\n",
            "For the Ising dataset analyzed with a shallow 4-layer convolutional net, we observed some differences\n",
            "in predictions between the single-task and multi-task architectures, but not near the critical point\n",
            "where it would matter most. At the critical point, we found a minor overhead with respect to the ideal\n",
            "speedup linear in the number of grid points.\n",
            "The analysis of the Stable Diffusion dataset with the 50-layer ResNet-50 demonstrates the viability\n",
            "of the multi-task learning-by-confusion algorithm to reveal rapid changes in the distribution of a\n",
            "complex dataset, where a theoretical description is not available and a larger model is required\n",
            "to learn the features. In this case, we found no signs of an overhead. This is in line with our\n",
            "general expectation that the relative overhead associated with the multi-task approach decreases as\n",
            "the underlying classification tasks get more complicated.\n",
            "In conclusion, we find the multi-task implementation of the learning-by-confusion algorithm to\n",
            "provide much faster execution on large parameter grids as compared to its single-task version.\n",
            "4Broader Impact\n",
            "The characterization of phases of matter and the study of critical phenomena are of great importance\n",
            "in physics. Our work contributes a faster variant of a highly popular unsupervised learning method\n",
            "for the data-driven detection of phase transitions.\n",
            "By revealing structure in the output images of the Stable Diffusion generative model, we demonstrate\n",
            "an application of the learning-by-confusion method for change point detection beyond physics.\n",
            "For datasets outside statistical physics, the demonstrated speedup of our multi-task approach is\n",
            "particularly impactful as their analysis typically requires a large amount of computational resources.\n",
            "Acknowledgments and Disclosure of Funding\n",
            "We thank Christoph Bruder for stimulating discussions and helpful suggestions on the manuscript.\n",
            "J.A. and N.L. acknowledge financial support from the Swiss National Science Foundation individual\n",
            "grant (grant no. 200020 200481). This material is based upon work supported by the National Science\n",
            "Foundation under grant no. OAC-1835443, grant no. OAC-2103804, and grant no. DMS-2325184.\n",
            "References\n",
            "[1] L. Wang, Phys. Rev. B 94, 195105 (2016).\n",
            "[2] J. Carrasquilla and R. G. Melko, Nat. Phys. 13, 431 (2017).\n",
            "[3] E. P. Van Nieuwenburg, Y .-H. Liu, and S. D. Huber, Nat. Phys. 13, 435 (2017).\n",
            "[4] S. J. Wetzel, Phys. Rev. E 96, 022140 (2017).\n",
            "[5]G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. V ogt-Maranto, and\n",
            "L. Zdeborová, Rev. Mod. Phys. 91, 045002 (2019).\n",
            "[6] F. Schäfer and N. Lörch, Phys. Rev. E 99, 062107 (2019).\n",
            "[7]E. Greplova, A. Valenti, G. Boschung, F. Schäfer, N. Lörch, and S. D. Huber, New J. Phys. 22,\n",
            "045003 (2020).\n",
            "[8] J. Carrasquilla, Adv. Phys.: X 5, 1797528 (2020).\n",
            "[9] J. Arnold, F. Schäfer, M. Žonda, and A. U. J. Lode, Phys. Rev. Res. 3, 033052 (2021).\n",
            "[10] J. Carrasquilla and G. Torlai, PRX Quantum 2, 040201 (2021).\n",
            "[11] A. Dawid, J. Arnold, B. Requena, A. Gresch, M. Płodzie ´n, K. Donatella, K. A. Nicoli, P. Stornati,\n",
            "R. Koch, M. Büttner, et al. , arXiv:2204.04198 (2022).\n",
            "[12] Y .-H. Liu and E. P. L. van Nieuwenburg, Phys. Rev. Lett. 120, 176401 (2018).\n",
            "[13] M. J. S. Beach, A. Golubeva, and R. G. Melko, Phys. Rev. B 97, 045207 (2018).\n",
            "[14] P. Suchsland and S. Wessel, Phys. Rev. B 97, 174435 (2018).\n",
            "[15] S. S. Lee and B. J. Kim, Phys. Rev. E 99, 043308 (2019).\n",
            "[16] W. Guo, B. Ai, and L. He, arXiv:2005.10505 (2020).\n",
            "[17] Y . A. Kharkov, V . E. Sotskov, A. A. Karazeev, E. O. Kiktenko, and A. K. Fedorov, Phys. Rev.\n",
            "B101, 064406 (2020).\n",
            "[18] I. Corte, S. Acevedo, M. Arlego, and C. Lamas, Comput. Mater. Sci. 198, 110702 (2021).\n",
            "[19] A. Bohrdt, S. Kim, A. Lukin, M. Rispoli, R. Schittko, M. Knap, M. Greiner, and J. Léonard,\n",
            "Phys. Rev. Lett. 127, 150504 (2021).\n",
            "[20] J. Arnold and F. Schäfer, Phys. Rev. X 12, 031044 (2022).\n",
            "5[21] M. Richter-Laskowska, M. Kurpas, and M. M. Ma ´ska, Phys. Rev. E 108, 024113 (2023).\n",
            "[22] M. A. Gavreev, A. S. Mastiukova, E. O. Kiktenko, and A. K. Fedorov, New J. Phys. 24, 073045\n",
            "(2022).\n",
            "[23] D. Zvyagintseva, H. Sigurdsson, V . K. Kozin, I. Iorsh, I. A. Shelykh, V . Ulyantsev, and\n",
            "O. Kyriienko, Commun. Phys. 5, 8 (2022).\n",
            "[24] H. Schlömer and A. Bohrdt, SciPost Phys. 15, 099 (2023).\n",
            "[25] W. Guo and L. He, New J. Phys. 25, 083037 (2023).\n",
            "[26] J. Arnold, F. Schäfer, A. Edelman, and C. Bruder, arXiv:2306.14894 (2023).\n",
            "[27] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, arXiv:2112.10752 (2021).\n",
            "[28] R. Caruana, Mach. Learn. 28, 41 (1997).\n",
            "[29] S. Ruder, arXiv:1706.05098 (2017).\n",
            "[30] L. Onsager, Phys. Rev. 65, 117 (1944).\n",
            "[31] K. He, X. Zhang, S. Ren, and J. Sun, in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern\n",
            "Recognit. (2016).\n",
            "[32] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\n",
            "N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\n",
            "S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, in Adv. Neural Inf. Process. Syst. ,\n",
            "V ol. 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and\n",
            "R. Garnett (Curran Associates, Inc., 2019).\n",
            "[33] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\n",
            "A. Katta, C. Mullis, M. Wortsman, et al. , Adv. Neural Inf. Process. Syst. 35, 25278 (2022).\n",
            "[34] J. Arnold, F. Schäfer, and N. Lörch, “Python implementation of multi-task learning-by-\n",
            "confusion,” (2023), https://github.com/multitaskLBC – GitHub repository.\n",
            "[35] D. Kingma and J. Ba, arXiv:1412.6980 (2014).\n",
            "A Appendix\n",
            "A.1 Implementation\n",
            "APython implementation of the single-task and multi-task learning by confusion method is available\n",
            "at [34]. The hyperparameters used to generate the figures in this article are summarized in Table 1.\n",
            "Here, MtrainandMvalidrefer to the number of samples per grid point within the training and validation\n",
            "set, respectively. For training, we use the Adam optimizer [ 35] with a learning rate given in Table 1.\n",
            "Table 1: Hyperparameters employed in this paper (default settings are used except where explicitly\n",
            "stated).\n",
            "Figure Mtrain Mvalid batch size learning rate training epochs\n",
            "1(c) 1000 1000 1024 1×10−450\n",
            "2(a) 1000 1000 1024 1×10−430\n",
            "2(b) 1000 1000 1024 5×10−450\n",
            "3 3×16 3×11 256 5×10−5150\n",
            "6A.1.1 Stable Diffusion\n",
            "The total Stable Diffusion dataset contains 3×27images per year. In Fig. 3, the assignment of\n",
            "training and validation set is randomized before each run, so the distinction between training set and\n",
            "validation set does not strictly apply. The number of training epochs was set to 150 as the overall\n",
            "validation loss started increasing again around that point. Because the accuracy at individual nodes\n",
            "may have peaked earlier already, each curve represents the minimal error across all epochs. In Fig. 2,\n",
            "we only use one of the three subsets making up the Stable Diffusion dataset corresponding to the seed\n",
            "numbers N < 27in the accompanying Python script, and the images with seed numbers N < 16\n",
            "are fixed as the training set. A Python script with seeds to generate the dataset and a notebook to\n",
            "perform training can be found at [34].\n",
            "A.1.2 Ising Model\n",
            "The Ising dataset is generated by sampling spin configurations from Boltzmann distributions at various\n",
            "temperatures via the Metropolis-Hastings algorithm. The lattice is initialized in a state with all spins\n",
            "pointing up and updated by drawing a random spin that is flipped with probability min(1 , e−∆E/kBT),\n",
            "where ∆Eis the energy difference resulting from the spin flip. In a thermalization period, we sweep\n",
            "the complete lattice 105times. Afterward, we collect samples, increase the temperature, and start\n",
            "another thermalization period.\n",
            "For the Ising model, we utilized fixed training and validation sets, as randomization in the split was\n",
            "unnecessary due to the abundance of data. The convolutional neural network architecture utilized to\n",
            "produce the results in Figs. 1(c) and 2(a) can be found in the accompanying code [34].\n",
            "A.2 Compute Resources\n",
            "For our computations, we use an NVIDIA GTX 3090 GPU and an Intel i9-10900K CPU, where a\n",
            "training and validation epoch finishes within a few seconds for both the Ising and Stable Diffusion\n",
            "dataset.\n",
            "7Proceedings of Machine Learning Research 222, 2023 ACML 2023\n",
            "FedOpenHAR: Federated Multi-Task Transfer Learning for\n",
            "Sensor-Based Human Activity Recognition\n",
            "Egemen ˙I¸ sg¨ uder and¨Ozlem Durmaz ˙Incel\n",
            "{egemen.isguder, ozlem.durmaz }@boun.edu.tr\n",
            "Bogazici University, Istanbul, Turkey\n",
            "Abstract\n",
            "Motion sensors integrated into wearable and mobile devices provide valuable information\n",
            "about the device users. Machine learning and, recently, deep learning techniques have been\n",
            "used to characterize sensor data. Mostly, a single task, such as recognition of activities,\n",
            "is targeted, and the data is processed centrally at a server or in a cloud environment.\n",
            "However, the same sensor data can be utilized for multiple tasks and distributed machine-\n",
            "learning techniques can be used without the requirement of the transmission of data to\n",
            "a centre. This paper explores Federated Transfer Learning in a Multi-Task manner for\n",
            "both sensor-based human activity recognition and device position identification tasks. The\n",
            "OpenHAR framework is used to train the models, which contains ten smaller datasets.\n",
            "The aim is to obtain model(s) applicable for both tasks in different datasets, which may\n",
            "include only some label types. Multiple experiments are carried in the Flower federated\n",
            "learning environment using the DeepConvLSTM architecture. Results are presented for\n",
            "federated and centralized versions under different parameters and restrictions. By utilizing\n",
            "transfer learning and training a task-specific and personalized federated model, we obtained\n",
            "a similar accuracy with training each client individually and higher accuracy than a fully\n",
            "centralized approach.\n",
            "Keywords: Federated transfer learning, multi-task learning, human activity recognition.\n",
            "1. Introduction\n",
            "Motion sensors available on wearable and mobile devices are commonly used to characterize\n",
            "their users: activities performed, who and where they are, and where they carry their\n",
            "devices. Particularly, monitoring people’s physical activity levels can help them stay active\n",
            "and reduce their risk of chronic diseases such as obesity. In the literature, machine learning\n",
            "and, recently, deep learning techniques are applied to the sensor data. Although there\n",
            "are efforts to train and run the learning algorithms on the device, still the wearables are\n",
            "resource-limited regarding computation and memory. Hence, processing is performed at a\n",
            "central server or a cloud environment. Wearable and mobile devices are personal devices,\n",
            "and the integrated sensors collect privacy-sensitive personal data. Centrally, the collection\n",
            "and processing of such data may violate the users’ privacy. In this respect, federated learning\n",
            "is an emerging approach.\n",
            "Federated learning is a novel technique in machine learning first announced by Google.\n",
            "The main goal of federated learning is to treat data in a privacy-preserving manner. Espe-\n",
            "cially with the recent updates on the General Data Protection Regulation (GDPR), data\n",
            "privacy has become a significant national and international concern. Federated learning\n",
            "©2023 E. ˙I¸ sg¨ uder & ¨O. Durmaz ˙Incel.arXiv:2311.07765v1  [cs.LG]  13 Nov 2023˙Is ¸g¨uder Durmaz ˙Incel\n",
            "offers a critical solution at this point. Instead of sharing data among learning participants\n",
            "(clients), model parameters are shared, and the learning is realized in a distributed manner.\n",
            "This way, data privacy is assured, and the wireless communication costs decrease because\n",
            "only model parameters are shared between participants instead of big chunks of data. This\n",
            "also helps devices with resource constraints like wearable devices, smartphones and IoT\n",
            "devices.\n",
            "An area where federated learning is utilized is sensor-based human activity recognition\n",
            "(HAR). Data from movement sensors such as accelerometers and gyroscopes are processed\n",
            "with machine learning techniques to monitor a user’s activities (walking, sitting, etc.) or\n",
            "special activities such as sports exercises or activities that may include life-threatening\n",
            "situations (e.g. falling).\n",
            "In this paper, for the recognition of human activities, DeepConvLSTM Ord´ o˜ nez and\n",
            "Roggen (2016) is applied for centralized and federated learning, both with various methods\n",
            "and settings. For the learning part, the OpenHAR framework Siirtola et al. (2018), which\n",
            "consists of ten smaller datasets, is used. In total, it contains accelerometer sensor data\n",
            "positioned in various parts of the body collected from 211 participants. Every dataset is\n",
            "owned by a different client in the federated learning experiments. In contrast, two versions\n",
            "exist in centralized learning: one similar to the federated setup, in which each dataset is\n",
            "used for centralized learning separately at each client, and another form where all data is\n",
            "combined in a big pile for centralized learning. Federated experiments are also categorized\n",
            "as one-task, multi-task and multi-task with a layered hierarchy. The tasks to learn during\n",
            "the experiments are the activities of the users and the positions of the devices. In the\n",
            "OpenHAR dataset, the devices were not located in a specific position. To the best of\n",
            "our knowledge, this paper is the first to implement a multi-task classification problem in\n",
            "multiple different datasets with a federated transfer learning method, where all datasets\n",
            "may not contain every type of label.\n",
            "For the federated learning part, the layered hierarchy for learning consists of pre-trained,\n",
            "common, task-specific and personalized layers inspired by Ke¸ ceci et al. (2022), where they\n",
            "focus on multiple image recognition tasks. One of the clients is selected to pre-train the\n",
            "initial weights, which constitutes the pre-trained layer, and then all clients, without any task\n",
            "restriction, participate in one federated loop, which creates the common layer. Afterwards,\n",
            "there are two federated loops (one for each task) to implement the task-specific layers.\n",
            "Ultimately, every client uses its own data for simple training to obtain the personalized\n",
            "layer. In the end, a model with an accuracy of 72.4% is obtained (which is 0.2% less than\n",
            "the baseline), which works on multiple tasks, 14 different labels, and opens the path to\n",
            "learning new labels and tasks with its layered hierarchy.\n",
            "The paper continues as follows: Section 2 examines the recent works in the area, Sec-\n",
            "tion 3 explains the used dataset in experiments, the selected classification algorithm and\n",
            "the federated learning architecture. Section 4 focuses on the results of experiments realized\n",
            "in different conditions with various challenges. Section 5 summarizes the results.\n",
            "2. Related Work\n",
            "FedHealth Chen et al. (2020) suggests a federated learning platform for wearable healthcare\n",
            "devices. The UCI public dataset for smartphones Garcia-Gonzalez et al. (2020) is used forFedOpenHAR\n",
            "training in the FedHealth study. This dataset is just one of the OpenHAR-integrated\n",
            "datasets used in our study. In FedHealth, machine learning techniques such as KNN and\n",
            "SVM are used instead of deep learning techniques.\n",
            "In another study, authors Ek et al. (2020) use yet again a dataset from OpenHAR-\n",
            "integrated datasets, which is RealWorldHAR Sztyler and Stuckenschmidt (2016), in order\n",
            "to train a deep artificial neural network based on CNN. In addition, they compare the success\n",
            "of different parameter aggregation algorithms (FedAvg, FedPer, FedMA). It is reported that\n",
            "the CNN classifier, alongside the FedAvg algorithm, has achieved the highest results. The\n",
            "FedAR study Presotto et al. (2022a), which focuses on the challenge of finding labelled data\n",
            "in human activity recognition applications and also semi-supervised machine learning in that\n",
            "context, designed experiments using two separate datasets from OpenHAR. Comparing\n",
            "the results of MLP and CNN-based architectures, the authors report that MLP achieves\n",
            "better success. The same authors Presotto et al. (2022b) also proposed a clustering-based\n",
            "federated learning approach. It is also reported that FedCLAR achieved better results than\n",
            "FedAvg and FedHealth within both datasets.\n",
            "In the study of Ke¸ ceci et al. (2022), the writers propose a novel federated learning\n",
            "architecture combining multi-task and federated learning. Their proposed algorithm is\n",
            "composed of four different types of layers: pre-trained, common, task-specific and person-\n",
            "alized. During their experiments, they use an artificially generated network consisting of\n",
            "face images. Their dataset’s labels are binary, containing only two different types. They\n",
            "achieve close results with their baseline (individually trained) and their proposed method.\n",
            "In our study, instead of dividing one dataset into separate clients, every client uses a whole\n",
            "different dataset, which may or may not contain all the types of labels, which is the crucial\n",
            "difference. Regarding similar works in the field, although federated multi-task learning was\n",
            "utilized for different tasks in the image recognition domain or federated transfer learning in\n",
            "the human activity recognition domain, this paper is the first to implement human activity\n",
            "recognition with a federated multi-task transfer learning architecture.\n",
            "3. Dataset and Federated Multi-Task Learning\n",
            "3.1. OpenHAR Framework\n",
            "OpenHAR Siirtola et al. (2018), offers a platform that contains ten public datasets. It\n",
            "consolidates the data from different datasets at different sampling rates to 10 Hz. Data is\n",
            "collected through accelerometers placed on various positions of the body. The whole data\n",
            "has 17 different daily activity labels collected from 211 participants and 14 different body\n",
            "positions.\n",
            "All datasets contain these columns: user ID information, type of activity, sensor position,\n",
            "timestamp and x-y-z axes accelerometer readings. After some initial training experiments,\n",
            "some labels’ success rates from the confusion matrix were observed to be less than the\n",
            "others. Since some labels are much scarcer than others, some of the similar labels are\n",
            "combined to obtain a better label distribution. In the data, there were five different labels\n",
            "for “walking” activity: “Walking”, “Walking inc. stairs”, “Walking stairs up”, “Walking\n",
            "stairs down”, and “Walking at stairs” which are all combined into one label,“Walking”.\n",
            "The same is applied to the position labels, i.e.“Foot, left”,“Foot, right” are all combined˙Is ¸g¨uder Durmaz ˙Incel\n",
            "Figure 1: Distribution of Activity (Left) and Position (Right) Labels.\n",
            "into“Leg/Foot” label. After preprocessing, the resulting distributions of the labels are given\n",
            "in Figure 1.\n",
            "3.2. Architecture and Federated Transfer Learning\n",
            "We used the DeepConvLSTM architecture Ord´ o˜ nez and Roggen (2016) in training the\n",
            "models. It combines Deep Convolutional Neural Network with LSTM (Long Short Term\n",
            "Memory) Network. The architecture of the algorithm consists of four convolutional layers,\n",
            "then two softmax layers and one softmax classifier layer at the end. The convolutional\n",
            "layers of the first part act as feature extractors and provide feature maps of the tabular\n",
            "data. In contrast, the recurrent layers model the temporality of the obtained feature maps.\n",
            "The algorithm is chosen as it is proven to be effective in HAR classification problems.\n",
            "FedAvg, FedProx, and Federated Matched Averaging are some of the most commonly\n",
            "used averaging algorithms in Federated Learning. This study uses FedAvg as the server’s\n",
            "aggregation algorithm McMahan et al. (2017). In FedAvg, the parameter aggregation is\n",
            "based on weighted averages. The central server aggregates all local model parameters into\n",
            "one new global model and then resends the model to a sub-group of clients. The main\n",
            "federated learning algorithm loops until the desired number of communication rounds.\n",
            "We use a Federated Multi-Task Transfer learning approach inspired by the architecture\n",
            "in Ke¸ ceci et al. (2022) for image recognition tasks. The main layered hierarchy can be seen\n",
            "in Figure 2. However, in this study, instead of using separate servers for each task, the\n",
            "same server trains each layer of the hierarchy, and every time a layer’s training is finished,\n",
            "its parameters are frozen, and the training is applied to the subsequent layers.FedOpenHAR\n",
            "Figure 2: Federated Multi-Task Transfer Learning Architecture (from Ke¸ ceci et al. (2022)\n",
            "4. Experiments\n",
            "We performed different experiments to evaluate the performance of the explained techniques\n",
            "individually and in combination. For all experiments, data is always split into train and test\n",
            "sets with 80%-20% ratio. Flower environment Beutel et al. (2020) is used in the experiments.\n",
            "4.1. Centralized, Individual and Federated Learning\n",
            "In centralized experiments, all data from all datasets are combined into one big pile, which\n",
            "does not conserve data privacy. Cross entropy loss is used as the default loss criteria, and\n",
            "stochastic gradient descent is used as the optimizer. For the second class of experiments,\n",
            "instead of combining data into one big pile, each client (each dataset) is treated individually,\n",
            "using only their data, conserving data privacy. However, they cannot benefit from other\n",
            "clients’ data. The position identification task is only trained with three clients because\n",
            "they are the only clients with more than two types of position labels in their data. The\n",
            "accuracy is reported in terms of the weighted average accuracy of all clients (each client’s\n",
            "data size is taken as weight). Individual training is considered as the baseline for further\n",
            "comparison with the federated experiments. Thirdly, in the federated experiments, the\n",
            "setup is similar to the individual training, but this time, there is model parameter sharing,\n",
            "hence the federated learning. For the one-task experiments, only one type of label’s clients\n",
            "are trained at a time. The percentages are the weighted average accuracies of each type of\n",
            "the client.\n",
            "The results are presented in Table 1. Position identification results are lower than the\n",
            "activity recognition results because the number of clients with position data is less than the\n",
            "number of activity clients, and position labels vary more between clients than activity label\n",
            "types. Federated learning results are the lowest because of the heterogeneity between label\n",
            "types in different clients, negatively affecting parameter sharing.\n",
            "Table 1: Accuracies of Centralized, Individual and Federated Experiments.\n",
            "Method/Label Type Activity Label Position Label\n",
            "Centralized in Bulk 69.8% 59.2%\n",
            "Individual Training 76.7% 65.0%\n",
            "Federated One Task 61.5% 57.2%˙Is ¸g¨uder Durmaz ˙Incel\n",
            "4.2. Federated Multi-Task and Federated Multi-Task Transfer Learning\n",
            "In the multi-task learning experiments, we considered two scenarios. During the simple\n",
            "federated-multi-task experiments, rather than learning one label at a time (separate simu-\n",
            "lations), both labels are learned in the same experiment. The other method is multi-task\n",
            "transfer learning, which combines federated learning, transfer learning and multi-task learn-\n",
            "ing, as presented in Figure 2. First, a simple dataset is trained for pre-trained layers. In\n",
            "this case, dataset-9 in OpenHAR (Sztyler and Stuckenschmidt (2016)) is used since, for\n",
            "position identification, it revealed the best results. Then, the next layer is trained with\n",
            "all 13 clients, which creates the common layers. Afterwards, two different training sessions\n",
            "were performed for activity (10 clients) and position (3 clients) tasks, which resulted in the\n",
            "task-specific layers. Finally, every client is fine-tuned, and the last layers of their models\n",
            "are trained with their own data. Before training each layer, the previous layers are frozen.\n",
            "Figure 3: Weighted Average of Accuracies of 13 Clients For Different Methods/Layers.\n",
            "The performance of training with different layers can be seen in Figure 3. The percent-\n",
            "ages are the weighted average accuracies of all the clients. The first three columns present\n",
            "the baselines, centralized, individual (one-task) and simple federated learning (multi-task).\n",
            "The latest personalized layer has an average weighted accuracy of 72.4%, whereas the indi-\n",
            "vidual learning’s accuracy is 72.6% and federated multi-task is 42.7%. From these results,\n",
            "we can say that personalization works quite well, similar to training the clients individ-\n",
            "ually, and the accuracy loss in task-specific layers is tolerable, but the pre-trained and\n",
            "common layers’ accuracies are not close to this individual training baseline. Also, the lay-\n",
            "ered hierarchy achieves more success than simple federated multi-task training. Federated\n",
            "multi-task transfer learning achieved similar results to individual training and better re-\n",
            "sults than centralized training. One may argue that each client can be trained with its own\n",
            "data. However, collecting labelled data to train a model is a difficult process, and by using\n",
            "a transfer learning approach, the clients can get similar results with a limited amount of\n",
            "labelled data, which is used in the personalization step. Due to space limitations, we cannot\n",
            "present the accuracy results for each client. However, we observe that at every layer, the\n",
            "accuracy increases and gets closer to the individual training baseline.FedOpenHAR\n",
            "5. Conclusion & Future Studies\n",
            "This paper focuses on the comparison between various models of centralized learning, feder-\n",
            "ated learning and federated multi-task transfer learning combined with the DeepConvLSTM\n",
            "based architecture for human activity classification and device position identification with\n",
            "motion sensor data. Models are trained using the integrated OpenHAR dataset containing\n",
            "ten smaller datasets. In multi-task federated transfer learning, the obtained success rates\n",
            "are similar to the baseline’s success (individual training) and are better than a fully central-\n",
            "ized approach. For future studies, different averaging algorithms than FedAvg and different\n",
            "classifiers’ effects on success will be analyzed. In addition, experiments can be implemented\n",
            "with different percentages of training data to analyze the effect of the data amount.\n",
            "References\n",
            "Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques, Yan Gao,\n",
            "Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusm˜ ao, et al. Flower:\n",
            "A friendly federated learning research framework. arXiv preprint arXiv:2007.14390 , 2020.\n",
            "Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao. Fedhealth: A federated transfer\n",
            "learning framework for wearable healthcare. IEEE Intelligent Systems , 35(4):83–93, 2020.\n",
            "Sannara Ek, Fran¸ cois Portet, Philippe Lalanda, and German Vega. Evaluation of federated learn-\n",
            "ing aggregation algorithms: Application to human activity recognition. In Adjunct Proceedings\n",
            "UbiComp/ISWC ’20 , page 638–643, 2020.\n",
            "Daniel Garcia-Gonzalez, Daniel Rivero, Enrique Fernandez-Blanco, and Miguel R. Luaces. A public\n",
            "domain dataset for real-life human activity recognition using smartphone sensors. Sensors , 20(8),\n",
            "2020.\n",
            "Cihat Ke¸ ceci, Mohammad Shaqfeh, Hayat Mbayed, and Erchin Serpedin. Multi-task and transfer\n",
            "learning for federated learning applications. arXiv preprint arXiv:2207.08147 , 2022.\n",
            "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\n",
            "Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-\n",
            "gence and Statistics , pages 1273–1282. PMLR, 2017.\n",
            "Francisco Javier Ord´ o˜ nez and Daniel Roggen. Deep convolutional and lstm recurrent neural networks\n",
            "for multimodal wearable activity recognition. Sensors , 16(1), 2016.\n",
            "Riccardo Presotto, Gabriele Civitarese, and Claudio Bettini. Semi-supervised and personalized\n",
            "federated activity recognition based on active learning and label propagation. Personal and\n",
            "Ubiquitous Computing , 26:1–18, 06 2022a.\n",
            "Riccardo Presotto, Gabriele Civitarese, and Claudio Bettini. Fedclar: Federated clustering for\n",
            "personalized sensor-based human activity recognition. In 2022 IEEE International Conference on\n",
            "Pervasive Computing and Communications (PerCom) , pages 227–236, 2022b.\n",
            "Pekka Siirtola, Heli Koskim¨ aki, and Juha R¨ oning. Openhar: A matlab toolbox for easy access\n",
            "to publicly open human activity data sets. In Adjunct Proceedings UbiComp/ISWC ’18 , page\n",
            "1396–1403, New York, NY, USA, 2018.\n",
            "Timo Sztyler and Heiner Stuckenschmidt. On-body localization of wearable devices: An investiga-\n",
            "tion of position-aware activity recognition. In 2016 IEEE International Conference on Pervasive\n",
            "Computing and Communications (PerCom) , pages 1–9, 2016.ML4H Findings Track Collection Machine Learning for Health (ML4H) 2023\n",
            "Attention-based Multi-task Learning for Base Editor Outcome\n",
            "Prediction\n",
            "Amina Mollaysa∗maolaaisha.aminanmu@uzh.ch\n",
            "University of Zurich, Switzerland\n",
            "Ahmed Allam∗ahmed.allam@uzh.ch\n",
            "University of Zurich, Switzerland\n",
            "Michael Krauthammer michael.krauthammer@uzh.ch\n",
            "University of Zurich, Switzerland\n",
            "Abstract\n",
            "Human genetic diseases often arise from point\n",
            "mutations, emphasizing the critical need for\n",
            "precise genome editing techniques. Among\n",
            "these, base editing stands out as it allows tar-\n",
            "geted alterations at the single nucleotide level.\n",
            "However, its clinical application is hindered by\n",
            "low editing efficiency and unintended muta-\n",
            "tions, necessitating extensive trial-and-error ex-\n",
            "perimentation in the laboratory. To speed up\n",
            "this process, we present an attention-based two-\n",
            "stage machine learning model that learns to pre-\n",
            "dict the likelihood of all possible editing out-\n",
            "comes for a given genomic target sequence. We\n",
            "further propose a multi-task learning schema to\n",
            "jointly learn multiple base editors (i.e. variants)\n",
            "at once. Our model’s predictions consistently\n",
            "demonstrated a strong correlation with the ac-\n",
            "tual experimental results on multiple datasets\n",
            "and base editor variants. These results provide\n",
            "further validation for the models’ capacity to\n",
            "enhance and accelerate the process of refining\n",
            "base editing designs.\n",
            "Keywords: Base editor, self-attention, multi-\n",
            "task learning, CRISPR, genome editing\n",
            "1. Introduction\n",
            "The landscape of human genetic diseases is largely\n",
            "dominated by a significant proportion of cases arising\n",
            "from point mutations (Landrum et al., 2014). These\n",
            "minor genetic alterations (substitution, deletion, or\n",
            "insertion of a single nucleotide), pose serious impli-\n",
            "cations for health ranging from rare monogenic con-\n",
            "ditions to more common chronic diseases. Genome\n",
            "editing approaches allow researchers to make targeted\n",
            "∗These authors contributed equallychanges to the DNA of living cells. Among these,\n",
            "base editing (Komor et al., 2016) shows promising\n",
            "results as it enables precise genome editing at the sin-\n",
            "gle nucleotide level without causing double-stranded\n",
            "breaks (DSBs) in the DNA. While base editors have\n",
            "great potential as genome editing tools for basic re-\n",
            "search and gene therapy, their application has been\n",
            "limited due to 1) low editing efficiency on specific se-\n",
            "quences or 2) unintended editing results with concur-\n",
            "rent mutations, especially where there are multiple\n",
            "substrate nucleotides within close proximity to the\n",
            "intended edit.\n",
            "Developing a robust machine learning model capa-\n",
            "ble of accurately predicting the potential editing out-\n",
            "comes of diverse base editors on various target sites\n",
            "could significantly enhance the field. It allows the bi-\n",
            "ologists to assess possible outcomes much faster and\n",
            "fine-tune their editing strategies with high efficiency.\n",
            "In this paper, we focus on estimating the probabil-\n",
            "ity of potential outcome sequences when various base\n",
            "editors are applied to specific DNA targets. Differ-\n",
            "ent editors exhibit varying behaviors when applied\n",
            "to the same target sequences due to factors such as\n",
            "binding affinities and editing window sizes leading to\n",
            "distributional shifts. Rather than training individ-\n",
            "ual models for each editor, we propose a multi-task\n",
            "learning framework to train a unified model and learn\n",
            "from multiple editors simultaneously. We train and\n",
            "test our models on six datasets corresponding to the\n",
            "experimental outcomes from six base editors applied\n",
            "on thousands of target sites (Table 6).\n",
            "2. Method\n",
            "Base editor Base editors (BEs) are created by fus-\n",
            "ing the Cas9 protein with DNA-modifying enzymes.\n",
            "©2023 A. Mollaysa, A. Allam & M. Krauthammer.arXiv:2311.07636v2  [q-bio.GN]  15 Nov 2023Multi-task learning for Base Editor Outcome Prediction\n",
            "ACAGAATTTGTTGAGGGCGA\n",
            "ACAGGATTTGTTGAGGGCGAACAGAGTTTGTTGAGGGCGAACAGGGTTTGTTGAGGGCGAACGGAATTTGTTGAGGGCGAACGGGATTTGTTGAGGGCGAACGGAGTTTGTTGAGGGCGAACGGGGTTTGTTGAGGGCGAACAGAATTTGTTGAGGGCGA0.520.180.100.060.040.050.0440.006ReferenceOutcomes\n",
            "Figure 1: An example of a reference sequence of 20 bases\n",
            "(i.e. nucleotides) and associated outcome se-\n",
            "quences when applying ABEmax base editor.\n",
            "The first row represents the reference (target)\n",
            "sequence, and the second row is the outcome\n",
            "sequence with no modification (i.e. wild-type)\n",
            "with a probability of occurrence of 0.52. The\n",
            "third row represents a possible outcome se-\n",
            "quence where the letter A is changed to G at\n",
            "position 5 with a probability of 0.35. The rest\n",
            "of the rows represent all possible changes of\n",
            "the reference sequence targeting letter A to G\n",
            "with their associated probabilities.\n",
            "They are directed by a 20-base pair guiding RNA\n",
            "molecule (sgRNA) that acts as a GPS to locate and\n",
            "bind to a matching DNA segment known as the pro-\n",
            "tospacer. The effectiveness of BEs largely depends on\n",
            "the composition of this protospacer sequence. BEs, in\n",
            "tandem with the sgRNA, can only bind to the DNA\n",
            "if there’s a protospacer adjacent motif (PAM) - a se-\n",
            "quence consisting of 2-6 nucleotides - present adja-\n",
            "cent to the protospacer. This PAM sequence further\n",
            "influences the activity of BEs (see section 5.8.1).\n",
            "Data representation Assume we have a tar-\n",
            "get/reference DNA sequence denoted by xref=\n",
            "[x1, x2, . . . , x T] where xi∈ {A, C, G, T }, and a set of\n",
            "DNA sequences Xout= [xout,1,xout,2, . . . ,xout,M]∈\n",
            "RM×Trepresenting corresponding outcomes when a\n",
            "specific base editor is applied to the reference se-\n",
            "quence xref. The associated probabilities for these\n",
            "outcomes are given by y= [y1, y2, . . . , y M] where\n",
            "yi=P(xout,i|xref)∈[0,1],fori= 1,2, . . . , M , in-\n",
            "dicating the likelihood of obtaining outcome xout,i\n",
            "through editing of xref. Here, Tis the reference se-\n",
            "quence length, and Mis the total possible outcomes,\n",
            "which vary with the reference sequence. Figure 1\n",
            "shows an example of a reference sequence and its\n",
            "outcomes. To represent the reference sequence, weconsider protospacer, PAM, and overhangs ( Figure\n",
            "4). Here, “overhangs” refer to adjacent nucleotides\n",
            "on both sides of the protospacer. For simplicity,\n",
            "we use xrefto denote the reference sequence which\n",
            "could refer to one of these representations: (a) pro-\n",
            "tospacer, (b) protospacer + PAM, or a (c) left over-\n",
            "hangs + protospacer + PAM + right overhangs where\n",
            "+ is the concatenation operator. Respectively, the\n",
            "outcome sequences match the reference sequence in\n",
            "length but differ in the modified target bases in the\n",
            "protospacer. The outcome sequence identical to the\n",
            "reference sequence (with no edits) is referred to as the\n",
            "wild-type. The training dataset comprises Npairs,\n",
            "each containing a reference sequence, its associated\n",
            "outcomes, and the corresponding probabilities, de-\n",
            "noted by D={xi\n",
            "ref,Xi\n",
            "out,yi}N\n",
            "i=1. To simplify, we\n",
            "omit instance-level indexing and use only xrefwhen\n",
            "referring to a specific reference sequence.\n",
            "2.1. Problem formulation\n",
            "Our objective is to predict the likelihood of potential\n",
            "outcomes resulting from a specific BE applied to a ref-\n",
            "erence sequence. One approach would be formulating\n",
            "it as a generative model where we directly model the\n",
            "conditional distribution P(Xout|xref) that we can use\n",
            "to sample different outcomes for a given reference se-\n",
            "quence and calculate the probability of each outcome.\n",
            "However, unlike typical generative models that must\n",
            "learn to generate entire output sequences, our sce-\n",
            "nario benefits from already knowing a portion of the\n",
            "output sequences. Due to the BEs specific target-\n",
            "ing of A-to-G or C-to-T transformations, a substan-\n",
            "tial portion of the output sequence remains consistent\n",
            "with the reference sequence, with only a few positions\n",
            "undergoing alteration.\n",
            "In the inference phase, for a given reference\n",
            "sequence, we can efficiently generate all possible\n",
            "outcomes by considering only the edit combina-\n",
            "tion of target bases (A/G) within the protospacer.\n",
            "Therefore, we only need to learn the distribution\n",
            "P(Xout|xref) such that we can evaluate the probabil-\n",
            "ity of a specific outcome for a given reference sequence\n",
            "P(Xout=xout,i|xref). However, there is a relatively\n",
            "higher probability often associated with the wild-type\n",
            "outcome (not edited) compared to the edited out-\n",
            "comes. This situation presents a challenge when di-\n",
            "rectly modeling P(Xout|xref)— as the model might\n",
            "easily learn the wild-type probability but struggle\n",
            "with outcomes that have extremely low probabilities.\n",
            "2Multi-task learning for Base Editor Outcome Prediction\n",
            "2.2. Two-stage model\n",
            "Therefore, we propose a two-stage model where we\n",
            "break down P(Xout|xref) as the product of two prob-\n",
            "abilities:\n",
            "P(xout,i|xref) =\n",
            "\n",
            "P(xout,i|xref,edited) P(edited |xref),\n",
            "ifxout,i̸=xref\n",
            "1−P(edited |xref),ifxout,i=xref\n",
            "(1)\n",
            "For a given reference sequence, we first predict the\n",
            "probability of overall efficiency (Eq.2). It provides\n",
            "the probability of the target sequence being edited,\n",
            "P(edited |xref). Next, we predict the probability of all\n",
            "possible edited outcomes, P(xout,i|xref, edited ).\n",
            "P(edited |xref) =Sum of the read count of all edited reads for the target\n",
            "Total read count of the target sequence (2)\n",
            "We estimate the overall efficiency of the given ref-\n",
            "erence sequence using fθ1(xref), denoted by the over-\n",
            "all efficiency model, and the conditional probabilities\n",
            "of all non wild-type outcomes using fθ2(xref,xout,i)\n",
            "which we denote by proportion model .\n",
            "fθ1(xref) =P(edited |xref) (3)\n",
            "fθ2(xref,xout,i) =P(xout,i|xref,edited ), (4)\n",
            "where xout,i̸=xref. Once fθ1andfθ2are learned, we\n",
            "can calculate P(X=xout,i|xref) where i= 1, . . . M\n",
            "for all outcome sequences, including wild-type and\n",
            "edited sequences using Eq 1. The final objective is\n",
            "composed of both losses (KL divergence measure)\n",
            "from the overall efficiency and proportion models (see\n",
            "appendix 5.3), with a weight regularization term on\n",
            "the model parameters represented by θ={θ1, θ2}:\n",
            "Lproportion (θ1;D) +Lefficiency (θ2, D) +λ\n",
            "2∥θ∥2\n",
            "2(5)\n",
            "2.3. Multi-task learning with multiple BEs\n",
            "There exists a diverse set of BEs, each distinguished\n",
            "by its unique design attributes, resulting in differ-\n",
            "ent editing efficiencies when applied to a given target\n",
            "sequence. Conventional approaches have often pro-\n",
            "posed training separate models for each editor. To\n",
            "leverage common patterns and relationships present\n",
            "across various datasets derived from various BEs, and\n",
            "reduce computational time, we propose a more effi-\n",
            "cient solution based on multi-task learning. Instead\n",
            "of training separate models for each editor, we train\n",
            "AAA C C CC T T TT G G\n",
            "reference sequenceOne-hot encoding\n",
            "1DCNN\n",
            "k=2\n",
            "s=1\n",
            "32 filtersReLUk=2\n",
            "s=1\n",
            "64 filtersReLU ReLU\n",
            "k=2\n",
            "s=1\n",
            "128 filtersFeed\n",
            "forwardp( ...AAA C C CC T T TT G G... edited )\n",
            "A AA C C C C T T T T G G ... ...Input embeddingPositional\n",
            "embedding+......\n",
            "Add &\n",
            " Norm....\n",
            "D1D2DT Add &\n",
            " Norm.\n",
            "Multi-head Self-\n",
            "attentionFeed\n",
            "forward= Encoder block x N\n",
            "reference sequenceA GA C C C C T T T T G GOutput embeddingPositional\n",
            "embedding+......\n",
            "Add &\n",
            " Norm....\n",
            "D1D2DT Add &\n",
            " Norm.\n",
            "Multi-head Self-\n",
            "attentionFeed\n",
            "forward=Encoder block x N\n",
            "output sequence... z1z2 zTout out out... z z2 zTref\n",
            "... ...concatenate\n",
            "ref ref...z1z2 zTFeed\n",
            "forward...\n",
            "y1y2 yTp(... ...A GA C C CC T T TTGG AAA C C CC T T TT G G... ... ) , editedmultiplyp(... ...A GA C C CC T T TTGG AAA C C CC T T TT G G... ... )\n",
            "1Enc1\n",
            "Enc2g(z)\n",
            "... ...\n",
            "(A) Overall Efficiency Model (B) Proportion ModelFigure 2: Two-stage Model overview\n",
            "zref\n",
            "zout\n",
            "zoutzout...\n",
            "......\n",
            "...\n",
            "...g(z)\n",
            "g(z)g(z)Enc1\n",
            "Enc2\n",
            "Enc2\n",
            "Enc2B1B2...BD\n",
            "B1\n",
            "B2\n",
            "BDreference sequences\n",
            "output sequencesz...\n",
            "...\n",
            "...1DCNN  layers\n",
            "B1B2...BDB2reference sequencesFeed\n",
            "forward\n",
            "Feed\n",
            "forward\n",
            "Feed\n",
            "forward\n",
            "...B1\n",
            "BD\n",
            "(A) Overall Efficiency Model\n",
            "(B) Proportion Model\n",
            "Figure 3: Multi-task learning model overview\n",
            "a single model capable of predicting the efficiency of\n",
            "various editors simultaneously.\n",
            "Given a total number of Deditors where each ed-\n",
            "itor has its own dataset Bi, we developed a multi-\n",
            "task learning model that uses shared encoding lay-\n",
            "ers to extract a common representation across all the\n",
            "datasets as well as individual branches that fine-tune\n",
            "the model specifically for each library, ensuring a bet-\n",
            "ter fit to their respective data. This approach implic-\n",
            "itly models P(Xout|xref, Bi) where Birepresents the\n",
            "3Multi-task learning for Base Editor Outcome Prediction\n",
            "base editor type applied on the reference sequence.\n",
            "To implement multi-task learning across all datasets,\n",
            "we extend our proposed two-stage model architecture\n",
            "for multi-task learning, as depicted in Figure 3.\n",
            "3. Experiments\n",
            "Dataset To comprehensively assess BEs efficiency\n",
            "across thousands of genomic sequences, we conducted\n",
            "high-throughput screening, resulting in the creation\n",
            "of six distinct datasets. Each dataset corresponds to\n",
            "the application of one of the following base editors:\n",
            "SpRY-ABE8e, SpCas9-ABE8e SpG-ABE8e, SpRY-\n",
            "ABEmax, SpCas9-ABEmax, and SpG-ABEmax (see\n",
            "appendix Table 6). Detailed descriptions of the used\n",
            "editors are provided in Appendix Section 5.9.\n",
            "Experiment setup All reported results are\n",
            "based on the average performance over the three\n",
            "runs(indicated by mean ±std). We use a one-stage\n",
            "model (appendix 5.2) that computes P(Xout|xref)\n",
            "using the proportion model architecture of the two-\n",
            "stag model as a baseline. In the first step, we use this\n",
            "one-stage model to identify optimal features for repre-\n",
            "senting the target/reference sequence. Subsequently,\n",
            "utilizing these selected features (i.e., protospacer +\n",
            "PAM, as detailed in appendix 5.11.1), we compare\n",
            "the two-stage model’s performance against the one-\n",
            "stage model. Finally, we extend the two-stage model\n",
            "to multi-task learning, comparing it with single-task\n",
            "learning, where separate models are trained for dif-\n",
            "ferent editors.\n",
            "3.1. Experiment results\n",
            "Two-stage Model Table 1 demonstrates slightly\n",
            "better Spearman correlation results for the two-stage\n",
            "model compared to the one-stage model. This im-\n",
            "provement is attributed to the two-stage model’s ap-\n",
            "proach, which initially predicts the wild-type and\n",
            "then refines predictions for edited outcomes. We fur-\n",
            "ther evaluated each model’s performance separately\n",
            "for both wild-type and edited outcomes. The two-\n",
            "stage model outperforms the one-stage model in most\n",
            "of the datasets when evaluating the performance on\n",
            "wild-type and edited outcomes separately (see Table\n",
            "2 and 3).\n",
            "Multi-task learning We compared the perfor-\n",
            "mance of multi-task learning model (see Figure 3)\n",
            "across all the datasets/editors with a single-taskOne-stage Model Two-stage Model\n",
            "Libraries Spearman Pearson Spearman Pearson\n",
            "SpRY-ABEmax 0 .854±0.006 0 .983±0.001 0 .873±0.001 0 .986±0.001\n",
            "SpCas9-ABEmax 0 .881±0.006 0 .989±0.0005 0 .879±0.004 0 .991±0.001\n",
            "SpG-ABEmax 0 .866±0.004 0 .989±0.0003 0 .887±0.003 0 .991±0.0006\n",
            "SpRY-ABE8e 0 .779±0.003 0 .968±0.002 0 .862±0.003 0 .974±0.001\n",
            "SpCas9-ABE8e 0 .857±0.007 0 .945±0.0006 0 .856±0.003 0 .937±0.002\n",
            "SpG-ABE8e 0 .820±0.005 0 .974±0.0009 0 .865±0.004 0 .978±0.0008\n",
            "Table 1: Performance comparison between One-stage\n",
            "and Two-stage models on all outcomes (i.e. in-\n",
            "cluding wild-type sequences).\n",
            "One-stage Model Two-stage Model\n",
            "Libraries Spearman Pearson Spearman Pearson\n",
            "SpRY-ABEmax 0 .745±0.015 0 .711±0.011 0 .799±0.007 0 .782±0.012\n",
            "SpCas9-ABEmax 0 .82±0.0003 0 .851±0.014 0 .838±0.009 0 .890±0.030\n",
            "SpG-ABEmax 0 .807±0.003 0 .752±0.014 0 .845±0.011 0 .822±0.014\n",
            "SpRY-ABE8e 0 .393±0.021 0 .508±0.025 0 .547±0.056 0 .669±0.051\n",
            "SpCas9-ABE8e 0 .855±0.007 0 .840±0.003 0 .866±0.0021 0 .858±0.021\n",
            "SpG-ABE8e 0 .712±0.002 0 .732±0.004 0 .774±0.005 0 .810±0.009\n",
            "Table 2: Performance comparison between One-stage\n",
            "and Two-stage model on wild-type outcomes\n",
            "only\n",
            "One-stage Model Two-stage Model\n",
            "Libraries Spearman Pearson Spearman Pearson\n",
            "SpRY-ABEmax 0 .740±0.007 0 .778±0.012 0 .798±0.003 0 .818±0.006\n",
            "SpCas9-ABEmax 0 .683±0.0003 0 .748±0.022 0 .728±0.006 0 .795±0.006\n",
            "SpG-ABEmax 0 .729±0.0043 0 .744±0.004 0 .778±0.005 0 .810±0.004\n",
            "SpRY-ABE8e 0 .707±0.010 0 .816±0.006 0 .809±0.004 0 .849±0.003\n",
            "SpCas9-ABE8e 0 .684±0.007 0 .729±0.008 0 .714±0.014 0 .753±0.007\n",
            "SpG-ABE8e 0 .719±0.004 0 .787±0.005 0 .789±0.004 0 .826±0.003\n",
            "Table 3: Performance comparison between One-stage\n",
            "and Two-stage model performance on non wild-\n",
            "type outcomes (i.e. edited outcome sequences)\n",
            "Single task learning Multi task learning\n",
            "Libraries Spearman Pearson Spearman Pearson\n",
            "SpRY-ABEmax 0 .877±0.001 0 .986±0.001 0 .872±0.002 0 .986±0.0002\n",
            "SpCas9-ABEmax 0 .879±0.004 0 .989±0.001 0 .864±0.0019 0 .992±0.0001\n",
            "SpG-ABEmax 0 .882±0.001 0 .991±0.0006 0 .889±0.0016 0 .992±0.0004\n",
            "SpRY-ABE8e 0 .861±0.0029 0 .974±0.001 0 .863±0.0011 0 .975±0.001\n",
            "SpCas9-ABE8e 0 .856±0.008 0 .938±0.0005 0 .852±0.002 0 .937±0.003\n",
            "SpG-ABE8e 0 .865±0.004 0 .980±0.0008 0 .871±0.003 0 .979±0.001\n",
            "Table 4: Multi-task model VS single task models\n",
            "setup where we trained one model per editor. Ta-\n",
            "ble 4 reports similar performance for both models.\n",
            "Although there wasn’t a substantial performance dif-\n",
            "ference, adopting a unified multi-task model offers ad-\n",
            "vantages such as reduced run-time (for training and\n",
            "inference) and smaller model size (fewer parameters)\n",
            "while maintaining consistent performance across all\n",
            "datasets. Moreover, with a unified model, we can si-\n",
            "multaneously predict the editing outcomes of all six\n",
            "editors at once for a given target sequence.\n",
            "Comparing to baselines in the literature We\n",
            "compared our model to BE-DICT (Marquart et al.,\n",
            "2021). It is computationally intensive due to its\n",
            "auto-regressive sequence-to-sequence decoding and is\n",
            "trained as a single-task model (one model per editor).\n",
            "We extended and retrained BE-DICT on two ran-\n",
            "domly chosen datasets and compared its predictions\n",
            "4Multi-task learning for Base Editor Outcome Prediction\n",
            "BE-DICT Ours\n",
            "reference sequence Libraries Spearman Pearson Spearman Pearson\n",
            "prrotospacerSpRY-ABEmax 0.801 0.943 0.835 0.981\n",
            "SpRY-ABE8e 0.746 0.861 0.776 0.965\n",
            "prrotospacer & PAMSpRY-ABEmax 0.804 0.951 0.870 0.987\n",
            "SpRY-ABE8e 0.762 0.850 0.860 0.975\n",
            "Table 5: Performance comparison with BE-DICT\n",
            "with our model’s results. Results in Table 5 show that\n",
            "our model consistently outperforms BE-DICT. Fur-\n",
            "thermore, considering computational efficiency dur-\n",
            "ing model training BE-DICT takes in the order of\n",
            "minute per epoch, while our single-task model accom-\n",
            "plishes the same task in the order of seconds ( 15 sec-\n",
            "onds wall clock time). Notably, the multi-task learn-\n",
            "ing model trained jointly on all six datasets takes 21\n",
            "seconds.\n",
            "This highlights the benefits of replacing the com-\n",
            "plex sequence-to-sequence architecture in favor of a\n",
            "streamlined encoder-encoder structure. This choice\n",
            "not only improves the computational efficiency but\n",
            "also leads to performance enhancements. Moreover,\n",
            "the introduction of a two-stage model and a multi-\n",
            "task framework amplifies these performance gains\n",
            "even further. We present additional results for com-\n",
            "parisons with other baselines in Table 8 in Appendix.\n",
            "4. Conclusion\n",
            "Our work provides a detailed assessment of the mod-\n",
            "eling approaches for BE outcome prediction. As\n",
            "the first machine learning-focused paper in the do-\n",
            "main of BE outcome prediction, our work represents\n",
            "a stepping stone toward a systematic modeling ap-\n",
            "proach to genome editing. We explored the different\n",
            "modeling decisions from one-stage to two-stage mod-\n",
            "els, and from single-task to multi-task learning. We\n",
            "evaluated the different sequence representations and\n",
            "benchmarked our best model with one of the main\n",
            "models developed for base editing outcome predic-\n",
            "tion.\n",
            "We believe that further work studying systemati-\n",
            "cally the different modeling decisions for genome edit-\n",
            "ing will help guide researchers toward more promis-\n",
            "ing editing strategies that in turn will bring advance-\n",
            "ments in gene therapy. For the future, given the cur-\n",
            "rent absence of standardized and systematic bench-\n",
            "mark datasets in the field, we aim to bridge this\n",
            "gap by creating standard benchmark datasets, estab-\n",
            "lishing baseline models, and proposing better per-\n",
            "formance metrics. This initiative will provide themachine-learning community with a solid foundation\n",
            "for testing a wide range of ideas.\n",
            "Acknowledgments\n",
            "We thank G. Schwank, K. Marquart, L. Kissling and\n",
            "S. Janjuha for input on the CRISPR-Cas and base\n",
            "editing technology and for data sharing and pre-\n",
            "processing. This work was supported by the URPP\n",
            "‘Human Reproduction Reloaded’ and ‘University\n",
            "Research Priority Programs’.\n",
            "5Multi-task learning for Base Editor Outcome Prediction\n",
            "References\n",
            "Zubaida Sa’id Ameen, Mehmet Ozsoz, Auwalu Saleh\n",
            "Mubarak, Fadi Al Turjman, and Sertan Serte. C-\n",
            "svr crispr: Prediction of crispr/cas12 guiderna ac-\n",
            "tivity using deep learning models. Alexandria En-\n",
            "gineering Journal , 60(4):3501–3508, 2021.\n",
            "Mandana Arbab, Max W Shen, Beverly Mok,\n",
            "Christopher Wilson, ˙Zaneta Matuszek, Christo-\n",
            "pher A Cassa, and David R Liu. Determinants of\n",
            "base editing outcomes from target library analysis\n",
            "and machine learning. Cell, 182(2):463–480, 2020.\n",
            "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\n",
            "Hinton. Layer normalization. arXiv preprint\n",
            "arXiv:1607.06450 , 2016.\n",
            "Nicole M Gaudelli, Alexis C Komor, Holly A Rees,\n",
            "Michael S Packer, Ahmed H Badran, David I\n",
            "Bryson, and David R Liu. Programmable base\n",
            "editing of a •t to g •c in genomic dna without dna\n",
            "cleavage. Nature , 551(7681):464–471, 2017.\n",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\n",
            "Sun. Deep residual learning for image recognition.\n",
            "InProceedings of the IEEE conference on com-\n",
            "puter vision and pattern recognition , pages 770–\n",
            "778, 2016.\n",
            "Nahye Kim, Sungchul Choi, Sungjae Kim, Myung-\n",
            "jae Song, Jung Hwa Seo, Seonwoo Min, Jinman\n",
            "Park, Sung-Rae Cho, and Hyongbum Henry Kim.\n",
            "Deep learning models to predict the editing efficien-\n",
            "cies and outcomes of diverse base editors. Nature\n",
            "Biotechnology , pages 1–14, 2023.\n",
            "Diederik P Kingma and Jimmy Ba. Adam: A\n",
            "method for stochastic optimization. arXiv preprint\n",
            "arXiv:1412.6980 , 2014.\n",
            "Alexis C Komor, Yongjoo B Kim, Michael S Packer,\n",
            "John A Zuris, and David R Liu. Programmable\n",
            "editing of a target base in genomic dna without\n",
            "double-stranded dna cleavage. Nature , 533(7603):\n",
            "420–424, 2016.\n",
            "Melissa J Landrum, Jennifer M Lee, George R Ri-\n",
            "ley, Wonhee Jang, Wendy S Rubinstein, Deanna M\n",
            "Church, and Donna R Maglott. Clinvar: public\n",
            "archive of relationships among sequence variation\n",
            "and human phenotype. Nucleic acids research , 42\n",
            "(D1):D980–D985, 2014.Yann LeCun, Yoshua Bengio, et al. Convolutional\n",
            "networks for images, speech, and time series. The\n",
            "handbook of brain theory and neural networks , 3361\n",
            "(10):1995, 1995.\n",
            "Kim F Marquart, Ahmed Allam, Sharan Janjuha,\n",
            "Anna Sintsova, Lukas Villiger, Nina Frey, Michael\n",
            "Krauthammer, and Gerald Schwank. Predict-\n",
            "ing base editing outcomes with an attention-\n",
            "based deep learning algorithm trained on high-\n",
            "throughput target library screens. Nature Com-\n",
            "munications , 12(1):5114, 2021.\n",
            "Nicolas Mathis, Ahmed Allam, Lucas Kissling,\n",
            "Kim Fabiano Marquart, Lukas Schmidheini,\n",
            "Cristina Solari, Zsolt Bal´ azs, Michael Krautham-\n",
            "mer, and Gerald Schwank. Predicting prime edit-\n",
            "ing efficiency and product purity by deep learning.\n",
            "Nature Biotechnology , pages 1–9, 2023.\n",
            "Holly A Rees and David R Liu. Base editing: pre-\n",
            "cision chemistry on the genome and transcriptome\n",
            "of living cells. Nature reviews genetics , 19(12):770–\n",
            "788, 2018.\n",
            "Myungjae Song, Hui Kwon Kim, Sungtae Lee,\n",
            "Younggwang Kim, Sang-Yeon Seo, Jinman Park,\n",
            "Jae Woo Choi, Hyewon Jang, Jeong Hong Shin,\n",
            "Seonwoo Min, et al. Sequence-specific prediction\n",
            "of the efficiencies of adenine and cytosine base edi-\n",
            "tors. Nature biotechnology , 38(9):1037–1043, 2020.\n",
            "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\n",
            "Ilya Sutskever, and Ruslan Salakhutdinov.\n",
            "Dropout: a simple way to prevent neural networks\n",
            "from overfitting. The journal of machine learning\n",
            "research , 15(1):1929–1958, 2014.\n",
            "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
            "Uszkoreit, Llion Jones, Aidan N Gomez,  Lukasz\n",
            "Kaiser, and Illia Polosukhin. Attention is all you\n",
            "need. Advances in neural information processing\n",
            "systems , 30, 2017.\n",
            "J Xie, M Liu, and L Zhou. Crispr-ote: Predic-\n",
            "tion of crispr on-target efficiency based on multi-\n",
            "dimensional feature fusion. IRBM , 44(1):100732,\n",
            "2023.\n",
            "Guishan Zhang, Tian Zeng, Zhiming Dai, and Xi-\n",
            "anhua Dai. Prediction of crispr/cas9 single guide\n",
            "rna cleavage efficiency and specificity by attention-\n",
            "based convolutional neural networks. Computa-\n",
            "tional and structural biotechnology journal , 19:\n",
            "1445–1457, 2021.\n",
            "6Multi-task learning for Base Editor Outcome Prediction\n",
            "5. Appendix\n",
            "5.1. Related Work\n",
            "In recent years, the intersection of deep learning\n",
            "and CRISPR-Cas9 systems has witnessed substan-\n",
            "tial interest from the bioinformatics community. Re-\n",
            "searchers have explored the applications of deep\n",
            "learning in predicting various aspects of CRISPR-\n",
            "Cas9 systems, including predicting gRNA activities\n",
            "(Ameen et al., 2021; Xie et al., 2023; Zhang et al.,\n",
            "2021) and editing outcomes for both base editing and\n",
            "prime editing scenarios (Mathis et al., 2023).\n",
            "Among those, one notable approach is the BE-\n",
            "Hive proposed by Arbab et al. (2020), which aims to\n",
            "predict base editing outcomes and efficiencies while\n",
            "considering sequence context, PAM compatibility,\n",
            "and cell-type-specific factors. The model employs\n",
            "a gradient boosting tree for predicting overall edit-\n",
            "ing efficiency and a deep conditional autoregressive\n",
            "model for predicting probability of edited outcome se-\n",
            "quences (denoted by bystander efficiency). Similarly,\n",
            "Song et al. (2020) presented DeepABE and Deep-\n",
            "CBE, that is based on convolutional neural networks\n",
            "to model both overall editing efficiency and bystander\n",
            "efficiency of adenine and cytosine base editors.\n",
            "Recently, Marquart et al. (2021) proposed BE-\n",
            "DICT, which predicts per-base editing efficiency (i.e.\n",
            "editing efficiency of each target base in a sequence)\n",
            "and bystander base-editing efficiency using attention-\n",
            "based deep learning. In a latest comprehensive\n",
            "study, Kim et al. (2023) developed DeepCas9variants\n",
            "and DeepBEs to predict editing efficiencies and out-\n",
            "comes of various BEs, taking into account different\n",
            "Cas9 variants. They build on and adapt the mod-\n",
            "els proposed in Song et al. (2020) (i.e. convolu-\n",
            "tional networks) to generate predictions for a range\n",
            "of CRISPR-Cas9 systems.\n",
            "While the surge of interest in applying machine\n",
            "learning to CRISPR-Cas9 systems is clear in recent\n",
            "literature, it’s noteworthy that many of these works\n",
            "have a primary emphasis on designing CRISPR-Cas9\n",
            "systems under various conditions and less focused on\n",
            "the analysis of ML models without offering a holis-\n",
            "tic and systematic analysis of model design. Given\n",
            "the intricate nature of CRISPR-Cas9 systems and the\n",
            "multitude of model paradigms adopted, deriving con-\n",
            "crete conclusions about optimal model design strate-\n",
            "gies remains elusive. In this context, our work aims to\n",
            "serve as model-first work that presents the base edit-\n",
            "ing outcome prediction through a modeling lens. We\n",
            "focus on model development and provide a systematicanalysis of each component of the models, providing\n",
            "a structured framework for problem formulation and\n",
            "model design specifically tailored to the prediction of\n",
            "base editing outcomes. Through this structured ex-\n",
            "amination of these critical aspects, our aim is to lay\n",
            "the groundwork for more informed and refined ap-\n",
            "proaches for using deep learning models to assist the\n",
            "design of base editors.\n",
            "5.2. One-stage Model\n",
            "In this setup, we tackle the problem by learning a\n",
            "function f(xref,xout,i)→ˆyiwhere i= 1, . . . , M ,\n",
            "andPM\n",
            "i=1ˆyi= 1, that takes as input the refer-\n",
            "ence sequence and one of its corresponding outcome\n",
            "and learns to approximate the probability of obtain-\n",
            "ing that specific outcome. Notably, this function\n",
            "fcharacterizes a categorical distribution P(Xout=\n",
            "xout,i|xref)∼Cat(M,ˆy), where ˆyis the vector con-\n",
            "taining probabilities for M outcomes. To learn the\n",
            "function f, we propose to use attention-based en-\n",
            "coder blocks to learn the encoding of both the ref-\n",
            "erence sequence and output sequence. Subsequently,\n",
            "we apply a prediction model on the learned encoded\n",
            "representation to output the probability of obtaining\n",
            "the outcome. The network architecture to learn fis\n",
            "reported in figure 2 (B: proportion model).\n",
            "5.3. Two-stage Model\n",
            "5.3.1. Overall efficiency model\n",
            "We formulate the overall efficiency model as a prob-\n",
            "abilistic classification task where fθ1parameterizes\n",
            "a binomial distribution P(C|xref) of a random vari-\n",
            "able C∈ {edited ,not edited }with the aim to learn\n",
            "to output the P(C=edited |xref) for a given ref-\n",
            "erence sequence. To learn fθ1, we first computed\n",
            "the overall editing efficiency for each reference se-\n",
            "quence by summing all probabilities attributed to the\n",
            "non wild-type outcomes as given in Eq 2, or equiva-\n",
            "lently, 1 −P(wild-type |xref). Then we use multiple\n",
            "1D-Convolutional layers (LeCun et al., 1995) on the\n",
            "one-hot-encoded representation of xrefto learn dis-\n",
            "criminative feature embedding that is passed to the\n",
            "multi-layer perceptron (MLP) layer to approximate\n",
            "the distribution P(C|xref). The model architecture\n",
            "is presented in Figure 2 (A). We trained fθ1using\n",
            "KL-divergence loss that is applied on the true distri-\n",
            "bution P(C|xref) and learned distribution ˆP(C|xref)\n",
            "7Multi-task learning for Base Editor Outcome Prediction\n",
            "for each reference sequence.\n",
            "Lefficiency (θ1, D) =PN\n",
            "i=1Dkl(P(C|xi\n",
            "ref)∥ˆP(C|xi\n",
            "ref))(6)\n",
            "5.3.2. Proportion model\n",
            "This model is designed to approximate the condi-\n",
            "tional distribution P(Xout|xref,edited ). To achieve\n",
            "this, we first remove the wild-type from each\n",
            "reference sequence’s corresponding output Xout.\n",
            "Then, we normalize the probabilities of the remain-\n",
            "ing outcomes to ensure a valid distribution effec-\n",
            "tively converting P(Xout|xref) into the distribution\n",
            "P(Xout|xref,edited ). The proportion model fθ2is de-\n",
            "signed to learn the parameters governing the dis-\n",
            "tribution P(Xout|xref,edited ). Similar to the one-\n",
            "stage model, fθ2is provided with both the reference\n",
            "sequence xrefand its associated outcome sequence\n",
            "xout,i. The model is then trained to estimate the like-\n",
            "lihood P(xout,i|xref,edited ), representing the proba-\n",
            "bility of reference sequence being edited, and result\n",
            "in the outcome sequence xout,i.\n",
            "As illustrated in Figure 2 (B), fθ2uses attention-\n",
            "based models comprised of two encoder networks,\n",
            "Enc1(xref), Enc2(xout), and one output network g.\n",
            "The design of the encoder networks adapts the trans-\n",
            "former encoder blocks architecture (Vaswani et al.,\n",
            "2017), characterized by multiple layers of multi-head\n",
            "self-attention modules. The two encoder networks\n",
            "process the reference sequence and one of its cor-\n",
            "responding output sequence xout,i, leading to the\n",
            "extraction of their respective latent representations,\n",
            "namely Zref∈RT×dandZout∈RT×d. Both vectors\n",
            "are then concatenated to form a unified learned repre-\n",
            "sentation Z∈RT×2d. Subsequently, the output net-\n",
            "work gembeds this unified representation Zto com-\n",
            "pute the probability of obtaining the output sequence\n",
            "given the reference sequence, P(xout,i|xref,edited ).\n",
            "Precisely, the output network g(Z) takes as input\n",
            "the final representation Z∈RT×2dand performs an\n",
            "affine transformation followed by softmax operation\n",
            "to compute the probability of conversion of every tar-\n",
            "get base (i.e. base A or C depending on the chosen\n",
            "base editor) as it is shown below:\n",
            "ˆyit=σ(Wzit+bt) (7)\n",
            "where W∈R2×2d,bt∈R2andσis softmax function.\n",
            "ˆyitrepresents the probability of editing occurring at\n",
            "thet-th position in the i-th outcome sequence. The\n",
            "un-normalized probability for the whole i-th output\n",
            "sequence xout,igiven its reference sequence is com-\n",
            "puted by ˆ yi=QT\n",
            "t=1ˆyi,t, which is then normalizedacross all the outcomes to make it valid probability\n",
            "distribution (Eq. 8). Therefore, the approximated\n",
            "probability for obtaining i-th edited (non-wild type)\n",
            "outcome sequence is given by:\n",
            "ˆP(xout,i|xref,edited ) =ˆyiPM\n",
            "i=1ˆyi(8)\n",
            "Objective Function We used the Kull-\n",
            "back–Leibler (KL) divergence on the model’s\n",
            "estimated distribution over all outcome sequences\n",
            "for a given reference sequence xi\n",
            "refand the actual\n",
            "distribution:\n",
            "Di\n",
            "KL(P(Xout|xi\n",
            "ref,edited )||ˆP(Xout|xi\n",
            "ref,edited )) (9)\n",
            "=MiX\n",
            "j=1P(xout,j|xi\n",
            "ref,edited ) logP(xout,j|xi\n",
            "ref,edited )\n",
            "ˆP(xout,j|xi\n",
            "ref,edited )\n",
            "Lastly, the objective function for the whole training\n",
            "set is defined by the average loss across all the refer-\n",
            "ence sequences as follows:\n",
            "Lproportion (θ2;D) = (10)\n",
            "NX\n",
            "i=1Di\n",
            "KL(P(Xout|xi\n",
            "ref,edited )||ˆP(Xout|xi\n",
            "ref,edited )\n",
            "5.4. Model architecture\n",
            "5.4.1. Single-task learning\n",
            "In this paper, we refer to single-task learning as a\n",
            "setting where we train one separate model for each\n",
            "of the libraries. The terminology is used to contrast\n",
            "with multi-task learning where we train one unified\n",
            "model for all the editors/datasets. For the single-\n",
            "task learning, we used the two-stage model ( Figure\n",
            "2) with protospacer and PAM as the reference se-\n",
            "quence representation. In this section, we provide\n",
            "an in-depth introduction to the two-stage model ar-\n",
            "chitecture, which is comprised of two distinct sub-\n",
            "models: the overall efficiency model and the propor-\n",
            "tion model.\n",
            "Overall Efficiency Model The Overall Efficiency\n",
            "Model concentrates exclusively on the target se-\n",
            "quence, overlooking the specific edit outcomes. Its\n",
            "main objective is to predict the probability of the\n",
            "target sequence undergoing modification, regardless\n",
            "of the nature of the edits. Hence, the model exclu-\n",
            "sively processes the input target sequence, which in\n",
            "our scenario is the concatenation of the protospacer\n",
            "8Multi-task learning for Base Editor Outcome Prediction\n",
            "and PAM, yielding the probability of the target se-\n",
            "quence undergoing modification (yielding non-wild\n",
            "type outcomes). To achieve this, we propose to use a\n",
            "Convolutional Neural Network (CNN) on the one-hot\n",
            "encoding of the target sequence. More specifically,\n",
            "we use three layers of 1D-CNN (kernel size: 2, stride:\n",
            "2) with filter sizes of 32, 64, and 128, respectively.\n",
            "Following the CNN layers, we apply a feed-forward\n",
            "network with ReLU activation, featuring a hidden\n",
            "layer dimension of 64. The output of this network\n",
            "is a two-dimensional vector, which is subsequently\n",
            "transformed into probabilities through the use of the\n",
            "Softmax function.\n",
            "Proportion Model Different from the absolute ef-\n",
            "ficiency model, the proportion model focuses on pre-\n",
            "dicting the probability of different types of edited out-\n",
            "comes for the target sequence. Therefore, it takes\n",
            "both the target sequence as well as one of its corre-\n",
            "sponding outcome sequences and outputs the proba-\n",
            "bility of observing such an outcome. To implement\n",
            "this model, we use two encoder networks and one\n",
            "prediction network. The architectures of the two en-\n",
            "coding networks, one for the target sequence and the\n",
            "other for the outcome sequence are identical, as il-\n",
            "lustrated in Figure 2. Consequently, we will only de-\n",
            "scribe in detail one of these networks here for clarity.\n",
            "The target/reference sequence encoder network com-\n",
            "prises two essential components: an embedding block\n",
            "and an encoder block.\n",
            "Embedding Layer The embedding block embeds\n",
            "both the nucleotides and their corresponding position\n",
            "(in the protospacer) from the one-hot encoded repre-\n",
            "sentation to a dense vector representation. Given a\n",
            "protospacer sequence extended with its correspond-\n",
            "ing PAM site: xref= [x1, x2, . . . , x T]∈RT,xtrepre-\n",
            "sents the nucleotide at position t. In our case, T=24.\n",
            "We use O= [o1,o2, . . . ,oT]∈RK×Tas its one-hot\n",
            "encoded representation. Here K= 4 as we have only\n",
            "four distinct nucleotides.\n",
            "An embedding matrix Weis used to map each ot∈\n",
            "Rkto a fixed-length vector representation:\n",
            "et=Weot (11)\n",
            "where We∈Rd2×K,et∈Rde, and deis the embed-\n",
            "ding dimension we chose.\n",
            "Similarly, each nucleotide’s position in the sequence\n",
            "xrefis represented by one-hot encoding with dictio-\n",
            "nary size T. En embedding matrix Wp′∈Rde×Tis\n",
            "applied to project the p4to a dense vector represen-tation:\n",
            "p′\n",
            "t=Wp′pt (12)\n",
            "where Wp′∈Rd2×T,pt∈Rde. Both embeddings et\n",
            "andp′\n",
            "tare summed to get a unified representation for\n",
            "every element xtin the reference sequence xref.\n",
            "ut=et+p′\n",
            "t∀t= 1,2, . . . T (13)\n",
            "This results in the embedded representation U=\n",
            "[u1,u2, . . . ,uT] of the reference sequence.\n",
            "Encoder Block To learn a good representation\n",
            "that takes into account the relationships between the\n",
            "nucleotides in the reference sequence, we use a multi-\n",
            "head self-attention to encode the embedded repre-\n",
            "sentation. Multi-head Attention is a module that\n",
            "employs multiple single-head self-attention in paral-\n",
            "lel (i.e. simultaneously) to process each input vector\n",
            "ut. The outputs from every single-head layer are then\n",
            "concatenated and transformed by an affine transfor-\n",
            "mation to generate a fixed-length vector.\n",
            "The single-head self-attention approach (Vaswani\n",
            "et al., 2017) learns three different linear projections\n",
            "of the input vector using three separate matrices: (1)\n",
            "a queries matrix Wquery , (2) keys matrix Wkey, and\n",
            "(3) values matrix Wvalue. Each input utinUis\n",
            "mapped using these matrices to compute three new\n",
            "vectors:\n",
            "qt=Wqueryut (14)\n",
            "k4=Wkeyut (15)\n",
            "vt=Wvalueut (16)\n",
            "(17)\n",
            "where Wquery ,Wkey,Wvalue∈Rd×de,qt,kt,vt∈\n",
            "Rdare query, key and value vectors, and dis the di-\n",
            "mension of the those projected vectors. In the second\n",
            "step, attention scores are computed using the pair-\n",
            "wise similarity between the query and key vectors for\n",
            "each position tin the sequence. The similarity is de-\n",
            "fined by first computing a scaled dot product between\n",
            "the pairwise vectors and then normalizing it using the\n",
            "softmax function. At each position t, we compute at-\n",
            "tention scores αtlrepresenting the similarity between\n",
            "t-th query qtandl-th key kl.\n",
            "score (qt,kl) =qT\n",
            "tkl√\n",
            "d(18)\n",
            "αtl=exp(score (qt,kl))PT\n",
            "l=1exp(score (qt,kl))(19)\n",
            "9Multi-task learning for Base Editor Outcome Prediction\n",
            "Then a weighted sum of value vector vlusing atten-\n",
            "tionαtl,∀l∈ {1,2, . . . , T }is performed to generate\n",
            "a new vector representation et∈Rdat position t.\n",
            "et=TX\n",
            "l=1αtlvl (20)\n",
            "This process is applied to every position in the orig-\n",
            "inal embedding of the sequence, U, to obtain a se-\n",
            "quence of vectors E= [e1,e2, . . . ,eT].\n",
            "In a multi-head setting with H number of heads,\n",
            "the queries, keys, and values matrices will be indexed\n",
            "by superscript h(i.e. Wh\n",
            "query ,Wh\n",
            "key,Wh\n",
            "value ∈\n",
            "Rd×de) and applied separately to generate a new\n",
            "vector representation eh\n",
            "tfor every singe-head self-\n",
            "attention layer. The output from each single-head at-\n",
            "tention layer is contenated into one vector econcat\n",
            "t =\n",
            "concat (e1\n",
            "t,e2\n",
            "t, . . . ,eH\n",
            "t) where econcat\n",
            "t ∈RdH. Then\n",
            "it goes through an affine transformation using W∈\n",
            "Rd×dHandb∈Rdto generate the encoded represen-\n",
            "tation ˆZ= [ˆz1,ˆz2, . . . , ˆzT] of the reference sequence\n",
            "ˆzt=Weconcat\n",
            "t +b (21)\n",
            "To improve the gradient flow in layers during\n",
            "training, we also use residual connections / skip-\n",
            "connections (He et al., 2016). This is done by sum-\n",
            "ming both the newly computed output of the current\n",
            "layer with the output from the previous layer. In our\n",
            "setting, a first residual connection sums the output\n",
            "of the self-attention layer ˆztand the output of em-\n",
            "bedding block utfor each position tin the sequence.\n",
            "We also deploy layer normalization (Ba et al.,\n",
            "2016) after the self-attention layer with the goal\n",
            "of ameliorating the ”covariate-shift” problem by re-\n",
            "standardizing the computed vector representations\n",
            "(i.e. using the mean and variance across the fea-\n",
            "tures/embedding dimension d). Given a computed\n",
            "vector ˆzt, the LayerNorm function will standardize\n",
            "the input vector using the mean and variance along\n",
            "the dimension of the feature dand apply scaling and\n",
            "shifting steps.\n",
            "Eventually, this learned representation goes\n",
            "through a feedforward network with one hidden layer\n",
            "and ReLu activation function. Subsequently, a layer\n",
            "normalization is applied to the output of this feed-\n",
            "forward network to obtain the learned representa-\n",
            "tionzt∈Rd. Eventually, the encoder block trans-\n",
            "formed the embedded vector Uto the learned rep-\n",
            "resentation Z= [z1,z2, . . . ,zT]∈RT×dthat incor-\n",
            "porates the contextual information/relationships be-\n",
            "tween features through attention.The above process describes one encoding block,\n",
            "we stack N such blocks to construct our encoder\n",
            "network. As it is presented in Figure 2, in the\n",
            "proportion model, we apply two encoder networks\n",
            "with the same network architecture on the refer-\n",
            "ence sequence and corresponding outcome sequence.\n",
            "This yields two encoded representations which we\n",
            "denote by Zref∈RT×dandZout∈RT×drespec-\n",
            "tively. We then concatenated them in the feature\n",
            "dimension to generate one common representation\n",
            "Z= [z1,z2, . . . ,zT]∈RT×2d.\n",
            "Output network The output network consists of\n",
            "an affine transformation and a nonlinear function to\n",
            "transform the output to probability:\n",
            "yout,t=Woutzt (22)\n",
            "where Wout∈R2d×2,yout,t∈R2. We then apply\n",
            "a softmax function on yout,t and transform it to a\n",
            "probability that represents the probability of the nu-\n",
            "cleotide at position tgetting edited. Note that, we\n",
            "use the same length of input and outcome sequence.\n",
            "However, our input sequence also includes PAM infor-\n",
            "mation, and editing only happens in the protospacer.\n",
            "Moreover, due to the nature of the Base editor, only\n",
            "specific nucleotides, in our case Adenine (A) gets\n",
            "edited while the other nucleotides are not affected.\n",
            "Therefore, we use masking technique to only consider\n",
            "the positions that are possible to be edited and mask\n",
            "out other positions. Therefore, the PAM information\n",
            "(or the contextual information such as left/right over-\n",
            "hangs is participating by affecting the embedding of\n",
            "the nucleotides in the protospacer but is not consid-\n",
            "ered in the loss as they are not changed/edited.\n",
            "After tuning the parameters, for the Proportion\n",
            "Model, we have chosen an embedding dimension of\n",
            "124 for the embedding layer. Our model consists\n",
            "of 12 encoding blocks, with each block featuring 8\n",
            "multi-head attention mechanisms. For the output\n",
            "network, we employ a single linear layer that maps\n",
            "a 248-dimensional vector to a two-dimensional out-\n",
            "put vector. Subsequently, we transform this output\n",
            "into probabilities using the softmax function.\n",
            "5.5. Multi-task Learning\n",
            "We extended the two-stage model for accommodating\n",
            "various base editors through the implementation of a\n",
            "multi-task learning framework, eliminating the need\n",
            "for training individual models per base editor.\n",
            "To achieve this, we first augment the datasets from\n",
            "different libraries with corresponding editor labels\n",
            "10Multi-task learning for Base Editor Outcome Prediction\n",
            "and combine all the libraries to create a consolidated\n",
            "dataset. Our objective is to establish a shared archi-\n",
            "tecture comprising common layers applicable across\n",
            "all libraries, along with dedicated sub-networks tai-\n",
            "lored to each specific library.\n",
            "As illustrated in Figure 3, we begin by extending\n",
            "the overall efficiency model by incorporating the first\n",
            "two layers of a 1D-CNN as the universally shared\n",
            "layers for all libraries. Subsequently, this shared\n",
            "learned representation traverses a sub-network com-\n",
            "prising two layers of 1D-CNN and two layers of MLP\n",
            "with ReLU activation functions, uniquely customized\n",
            "for each library. Given that, as we have datasets from\n",
            "six editors, we use one common network (consisting\n",
            "of two layers of CNN) and six distinct sub-networks,\n",
            "which have identical structures.\n",
            "For the proportion model, the influence of various\n",
            "libraries/Base Editors is observable through the vari-\n",
            "ations in the outcome set corresponding to the vari-\n",
            "ous base editors when applied to the same target se-\n",
            "quence. Therefore, to extend the proportional model\n",
            "for various base editors, we maintain a shared en-\n",
            "coder network for the reference sequence across all\n",
            "libraries while constructing six distinct encoder net-\n",
            "works to encode the outcome sequences from the six\n",
            "different libraries. This approach allows us to estab-\n",
            "lish a consistent representation for the reference se-\n",
            "quences across all libraries, while simultaneously ac-\n",
            "commodating the distinctions among the libraries by\n",
            "employing separate encoder networks to encode the\n",
            "outcome sequences for each one. Consequently, our\n",
            "multi-task learning proportional model comprises one\n",
            "shared reference sequence encoder network, six in-\n",
            "dividual outcome sequence networks, and six corre-\n",
            "sponding output networks.\n",
            "5.6. Optimization\n",
            "For optimization, we use Adam optimizer (Kingma\n",
            "and Ba, 2014) with a learning rate scheduler. We ini-\n",
            "tialize the base learning rate at 3 e−4and set the max-\n",
            "imum learning rate to five times the base rate. Ad-\n",
            "ditionally, we incorporate dropout (Srivastava et al.,\n",
            "2014) probability of 0.2. The regularizer parameter\n",
            "λis set to 1 e−4.\n",
            "It’s worth noting that while it’s possible to train\n",
            "both the overall efficiency model and the proportion\n",
            "Model simultaneously. However, Training them to-\n",
            "gether means, we apply the loss on the learned fi-\n",
            "nal probability, ˆP(Xout|xref) (Eq. 1), with the true\n",
            "probability which represents the probability of alloutcomes. This means that for the edited (non-wild\n",
            "outcome) outcomes, the problem of true probabil-\n",
            "ity being very low compared to the wild-type out-\n",
            "comes still exists. Breaking down this final prob-\n",
            "ability into a product of two conditional probabil-\n",
            "ities was introduced to mitigate such problem as\n",
            "P(Xout|xref, edited ) focuses only on the non-wild\n",
            "type outcomes. Therefore, training them together\n",
            "could result in an outcome where the model pre-\n",
            "dominantly focuses on predicting easily predictable\n",
            "outcomes (as wild type) while neglecting those with\n",
            "lower probabilities as we hypothesize in the One-stage\n",
            "model. Moreover, the two models do not share any\n",
            "common layer except the outcome of the two net-\n",
            "works gets multiplied to generate the final probability\n",
            "distribution. Therefore, there is no real requirement\n",
            "by model design to train them together. To avoid\n",
            "falling back to the single-step model, we train the\n",
            "Absolute Efficiency Model and the Proportion Model\n",
            "separately. This approach explicitly matches the\n",
            "probabilities P(C|xref) and P(Xout|xref,edited), pre-\n",
            "venting the model from overlooking low-probability\n",
            "outcomes.\n",
            "In terms of training specifics, we set the mini-batch\n",
            "size to 100 and the maximum number of epochs to\n",
            "300 for the Absolute Efficiency Model. For the Pro-\n",
            "portion Model, we choose a mini-batch size of 400\n",
            "and an epoch count of 150. In both models, we use\n",
            "Spearman correlation on the validation set as our\n",
            "performance metric to monitor and select the best-\n",
            "performing model.\n",
            "5.7. Performance measures\n",
            "Considering the distinctive nature of our data genera-\n",
            "tion process, we chose to employ Pearson and Spear-\n",
            "man correlations as our performance metrics, mea-\n",
            "suring the alignment between actual and predicted\n",
            "probability scores. Owing to inherent variability dur-\n",
            "ing screening, repeated experiments under identical\n",
            "conditions yield slightly divergent outcomes. For the\n",
            "biologist, the emphasis is on correlating these simi-\n",
            "lar results, rendering metrics such as mean square or\n",
            "mean absolute error less pertinent. Our primary con-\n",
            "cern isn’t exact prediction precision, but rather the\n",
            "level of correlation achieved between predictions and\n",
            "actual data.\n",
            "5.8. CRISPR related terminology\n",
            "The CRISPR-Cas9 system is a revolutionary gene-\n",
            "editing technology that allows scientists to precisely\n",
            "11Multi-task learning for Base Editor Outcome Prediction\n",
            "modify DNA within living organisms. Initially, it was\n",
            "described as an adaptive immune system of bacte-\n",
            "ria and archea to eliminate invading foreign DNA\n",
            "and/or RNA. The system uses unique sequences of\n",
            "RNA called single guide RNA (sgRNA) that are rec-\n",
            "ognized and bound by Cas9, an enzyme with a nucle-\n",
            "ase domain. The Cas9 protein carries out the initial\n",
            "steps of recognition and binding by scanning genomic\n",
            "DNA to locate a particular sequence called a PAM\n",
            "(protospacer adjacent motif). Upon PAM recogni-\n",
            "tion, the part of the sgRNA (termed spacer) com-\n",
            "plementary to the target DNA (termed protospacer)\n",
            "opens the DNA double helix and binds to the target\n",
            "site. This leads to a conformation change within the\n",
            "Cas9, bringing its nuclease domain in close proximity\n",
            "to the target DNA and thus initiating DNA double-\n",
            "strand cleavage. After introducing a DNA double-\n",
            "strand break, the cell’s repair mechanism is triggered,\n",
            "which can lead to various outcomes. Researchers can\n",
            "exploit this repair process to either introduce specific\n",
            "changes in the DNA sequence by providing a modi-\n",
            "fied DNA template or to disrupt a target gene due\n",
            "to the imperfect repair of the DNA by the cells, thus\n",
            "introducing insertions or deletions, which can lead to\n",
            "a frame shift mutations.\n",
            "Of note, Streptococcus pyogenes Cas9 (SpCas9) ex-\n",
            "clusively operates on “NGG” (“N”, any base) PAM\n",
            "sequence. Recent efforts in protein design have re-\n",
            "sulted in laboratory-generated SpCas9 variants, such\n",
            "as SpG or SpRY, which are able to recognize differ-\n",
            "ent PAM motifs. Therefore, the editors used in our\n",
            "high-throughput screening are configured to operate\n",
            "with PAM sequences comprising four nucleotides.\n",
            "5.8.1. Base Editor (BE)\n",
            "Base editing (Komor et al., 2016; Gaudelli et al.,\n",
            "2017; Rees and Liu, 2018) is a second-generation\n",
            "genome editing approach that uses components\n",
            "from CRISPR systems together with other enzymes\n",
            "to directly install point mutations into genomic\n",
            "DNA without making double-stranded DNA breaks\n",
            "(DSBs). BEs comprise a Cas protein with a catalyti-\n",
            "cally impaired nuclease domain fused to a nucleobase\n",
            "deaminase. Similar to the Cas9 nuclease, BEs are\n",
            "directed to the target DNA by the programmable\n",
            "sgRNA and are able to directly convert substrate\n",
            "bases in a specific ’editing window’ within the pro-\n",
            "tospacer.\n",
            "Editing windowPAMsgRNAd/nCas9\n",
            "Deaminase3’5’ 3’\n",
            "5’3’\n",
            "5’\n",
            "A AT T3’\n",
            "5’ 3’5’\n",
            "G AC T3’\n",
            "5’ 3’5’\n",
            "G GC C3’\n",
            "5’ 3’5’targeted adenineCorrectly edited\n",
            "target sequence \n",
            "Editing with\n",
            "bystanderprotospacerFigure 4: Adenine base editor\n",
            "5.9. Description of Each Base Editor Used in\n",
            "the Experiment\n",
            "There are two main factors that are crucial for the\n",
            "efficiency of the base editor: binding affinities and\n",
            "deaminase activity. Binding affinities dictate how ef-\n",
            "fectively an editor is able to identify and interact with\n",
            "specific target sites on the reference sequence. Edi-\n",
            "tors with higher binding affinities tend to exhibit in-\n",
            "creased accuracy in achieving the desired base mod-\n",
            "ification. Deaminase activity is defined by the type\n",
            "of deaminase used. Additionally, the deaminase also\n",
            "defines editing window size, which refers to the span\n",
            "of nucleotides that an editor can modify around its\n",
            "target site. Editors with larger editing windows can\n",
            "potentially influence a broader range of nucleotides,\n",
            "resulting in increased flexibility in terms of target se-\n",
            "lection and outcomes.\n",
            "Here we describe six editors that we used in our\n",
            "screening experiment. We used three different Cas9\n",
            "orthologs, which show different binding affinities to\n",
            "different PAMs: SpCas9, SpG, and SpRY. SpCas9\n",
            "recognizes only a few PAMs but shows a very high\n",
            "affinity for those PAMs. SpRY shows the broadest\n",
            "PAM recognition, however a lower affinity for all of\n",
            "them. SpG recognizes more PAM than SpCas9, how-\n",
            "ever less than SpRY and shows lower affinity than\n",
            "SpCas9 but higher as SpRY. For deaminases we used\n",
            "two evolved adenine deaminases named ABEmax and\n",
            "ABE8e, where ABE8e has higher deaminase activity\n",
            "and a bigger editing window size. The combination of\n",
            "the three Cas9 orthologs and the two adenine deam-\n",
            "inases leads to a total of six adenine base editors.\n",
            "12Multi-task learning for Base Editor Outcome Prediction\n",
            "Editor #ins #refseq #outcome mean std\n",
            "SpRY-ABE8e 110141 11291 9.7 0.102 0.211\n",
            "SpCas9-ABE8e 43054 11337 4.6 0.217 0.323\n",
            "SpG-ABE8e 80873 11307 7.1 0.139 0.263\n",
            "SpRY-ABEmax 70851 11347 6.2 0.159 0.301\n",
            "SpCas9-ABEmax 39606 11302 3.5 0.285 0.417\n",
            "SpG-ABEmax 70851 11347 6.2 0.159 0.301\n",
            "Table 6: Data statistics: “#ins” refers to the number of\n",
            "reference and output sequence pairs, “#refseq”\n",
            "denotes the number of distinct reference se-\n",
            "quences, “#outcome” denotes the average num-\n",
            "ber of outcomes per reference sequence, the\n",
            "mean and std refers to the mean and standard\n",
            "deviation of the probability across all the out-\n",
            "comes.\n",
            "5.10. Data Statistics\n",
            "5.11. Experiment results\n",
            "5.11.1. Reference sequence representation\n",
            "Existing models have explored different factors that\n",
            "could affect the base editor’s efficiency, which we cat-\n",
            "egorize into three scenarios: 1) the protospacer, 2)\n",
            "the protospacer along with its PAM, and 3) an ex-\n",
            "tended range including left overhangs, protospacer,\n",
            "PAM, and right overhangs. We investigate all three\n",
            "scenarios with the one-stage model to identify the\n",
            "best features to represent the reference sequence. We\n",
            "observe that (See Table 7 in appendix), incorporat-\n",
            "ing PAM information significantly enhances perfor-\n",
            "mance, whereas the inclusion of overhangs demon-\n",
            "strates minimal impact. Besides, adding overhangs\n",
            "increases the computational complexity drastically.\n",
            "Consequently, we opt to employ protospacer and\n",
            "PAM information to represent reference sequences in\n",
            "all the subsequent model results presented below.\n",
            "5.12. Comparing with the other baselines\n",
            "To assess our model’s performance against other\n",
            "state-of-the-art models, we conducted evaluations us-\n",
            "ing the test sets provided by these models. Table 8\n",
            "displays our findings, which include three most recent\n",
            "models: BE-HIVE (Arbab et al., 2020), DeepABE\n",
            "(Song et al., 2020), and BEDICT (Marquart et al.,\n",
            "2021), along with their respective test sets labeled as\n",
            "A. et al., S. et al., and M. et al.\n",
            "The idea is to take the published trained model\n",
            "and evaluate their performance on those various test\n",
            "sets. For the three baseline models, we refer to the\n",
            "results reported in the BEDICT paper. As for ourmodel, to ensure fairness in comparison, we used our\n",
            "single-step model trained on SpG-ABEmax libraries1\n",
            "since most baselines, except DeepABE, do not incor-\n",
            "porate the PAM as input. The results correspond to\n",
            "two scenarios: 1) considering all possible outcomes,\n",
            "and 2) only considering non-wild type outcomes. The\n",
            "results for the non-wild type outcomes correspond to\n",
            "the model prediction where we only consider non-wild\n",
            "outcomes. In the case of non-wild-type outcome pre-\n",
            "diction, we mention that other models were trained\n",
            "exclusively on non-wild outcomes, with outcomes per\n",
            "sequence being renormalized. Our one-stage model,\n",
            "however, was trained on data encompassing all out-\n",
            "comes, so we report non-wild-type results with out-\n",
            "comes renormalized for a fair comparison.\n",
            "5.12.1. Multi-task learning\n",
            "To extend our two-stage model in the setting of multi-\n",
            "task learning (2.3), we explored two distinct method-\n",
            "ologies for tackling multi-task learning. The first in-\n",
            "volves a direct conversion of the distribution into a\n",
            "conditional form, conditioned upon the editor label.\n",
            "The second applies a structural transformation of the\n",
            "network enabling the model to have both shared and\n",
            "distinct layers across various libraries. We refer to\n",
            "the first as a conditional model and the second as\n",
            "multi-task learning. In order to identify a suitable\n",
            "approach, we exclusively assessed both methodolo-\n",
            "gies using the absolute efficiency model, leveraging\n",
            "its inherent simplicity. This choice stems from the\n",
            "rationale that if the conditioning factor is overlooked\n",
            "within this inherently simpler context, its impact is\n",
            "likely to be minimal when applied to the proportion\n",
            "model, which is considerably more intricate. As illus-\n",
            "trated in Table 9, the multi-task setup on the abso-\n",
            "lute efficiency model has a substantial advantage over\n",
            "the model that uses the editor label as a conditioning\n",
            "factor.\n",
            "5.13. Scatter Plots\n",
            "To gain deeper insights into the model’s performance,\n",
            "we provide scatter plots showcasing the actual and\n",
            "predicted probability values derived from the multi-\n",
            "task model on the SpRY-ABEmax library test set.\n",
            "It’s important to note that the library selection is\n",
            "entirely random, and this particular library is not\n",
            "cherry-picked; similar results are observed across all\n",
            "other libraries as well.\n",
            "1. other ABEmax libraries also yield similar results\n",
            "13Multi-task learning for Base Editor Outcome Prediction\n",
            "Protspacer Protospacer & PAM Protspacer & PAM & Overhangs\n",
            "Libraries Spearman Pearson Spearman Pearson Spearman Pearson\n",
            "SpRY-ABEmax 0 .835±0.007 0 .981±0.001 0 .854±0.006 0 .983±0.001 0 .854±0.003 0 .983±0.002\n",
            "SpCas9-ABEmax 0 .786±0.003 0 .978±0.002 0 .881±0.001 0 .989±0.0005 0 .891±0.002 0 .989±0.001\n",
            "SpG-ABEmax 0 .841±0.002 0 .985±0.0007 0 .866±0.004 0 .989±0.0003 0 .878±0.008 0 .991±0.0009\n",
            "SpRY-ABE8e 0 .776±0.019 0 .965±0.001 0 .779±0.0036 0 .968±0.002 0 .803±0.008 0 .967±0.0003\n",
            "SpCas9-ABE8e 0 .762±0.007 0 .883±0.005 0 .857±0.007 0 .945±0.0006 0 .862±0.003 0 .945±0.003\n",
            "SpG-ABE8e 0 .803±0.005 0 .963±0.002 0 .820±0.005 0 .974±0.0009 0 .819±0.006 0 .9771±0.0008\n",
            "Table 7: Pearson and Spearman correlation using one-stage Model across the three different reference sequence\n",
            "representations. In our experiment, we chose 5 neighboring nucleotides for both sides to represent the\n",
            "overhangs.\n",
            "All Outocmes Non wild-types\n",
            "Datasets A.et all S. et al M. et al A.et all S. et al M. et al\n",
            "BEDICT 0.96 0.94 0.86 0.81 0.90 0.82\n",
            "DeepABE 0.86 0.93 0.8 0.86 0.96 0.84\n",
            "BE-HIVE 0.71 0.88 0.74 0.92 0.93 0.81\n",
            "Our model 0.972 0.974 0.972 0.939 0.945 0.953\n",
            "Table 8: Model performance on the test set from the\n",
            "different published studies. Columns represent\n",
            "test sets, rows represent models used\n",
            "Conditional model Multi task learning\n",
            "Libraries Spearman Pearson Spearman Pearson\n",
            "SpRY-ABEmax 0 .677±0.004 0 .629±0.003 0 .797±0.007 0 .783±0.012\n",
            "SpCas9-ABEmax 0 .811±0.009 0 .759±0.002 0 .834±0.011 0 .901±0.002\n",
            "SpG-ABEmax 0 .811±0.009 0 .748±0.006 0 .853±0.009 0 .835±0.017\n",
            "SpRY-ABE8e 0 .548±0.012 0 .537±0.018 0 .578±0.031 0 .695±0.034\n",
            "SpCas9-ABE8e 0 .751±0.010 0 .723±0.016 0 .866±0.031 0 .862±0.014\n",
            "SpG-ABE8e 0 .788±0.005 0 .760±0.011 0 .788±0.002 0 .824±0.004\n",
            "Table 9: Performance comparison of overall efficiency\n",
            "model on two different settings: conditional\n",
            "model P(C|xref,B i) and multi-task learning ap-\n",
            "proach\n",
            "Figure 5: Performance of the Multi-Task Model\n",
            "Across All Possible Outcomes, Including\n",
            "Both Wild-Type and Non-Wild-Type Vari-\n",
            "ants\n",
            "14Multi-task learning for Base Editor Outcome Prediction\n",
            "Figure 6: Performance of the Multi-Task Model\n",
            "Across Wild Type (this corresponding to\n",
            "the model absolute efficiency model perfor-\n",
            "mance)\n",
            "Figure 7: Performance of the Multi-Task Model\n",
            "Across Non-Wild Type\n",
            "Figure 8: Performance of the Multi-Task Model\n",
            "Across Non-Wild Type (presented in the\n",
            "randomized space, i.e., P(Xout|xref,edited ,\n",
            "this corresponds to the proportional model\n",
            "performance)\n",
            "15Journal of Machine Learning for Biomedical Imaging 2023:015 vol. 2, pp. 406–446\n",
            "Special issue: Perinatal, Preterm and Paediatric Image Analysis (PIPPI) 2022\n",
            "Guest editors: Jana Hutter, Roxane Licandro, Andrew Melbourne, Esra Abaci Turk,\n",
            "Daphna Link-Sourani, Christopher MacgowanSubmitted 04/2023\n",
            "Published 10/2023\n",
            "Multi-task learning for joint weakly-supervised segmentation\n",
            "and aortic arch anomaly classification in fetal cardiac MRI\n",
            "Paula Ramirezhttps://orcid.org/0000-0002-0705-5296paula.ramirez gilliland@kcl.ac.uk\n",
            "Alena Uushttps://orcid.org/0000-0001-5796-2145alena.uus@kcl.ac.uk\n",
            "Milou P.M. van Poppelhttps://orcid.org/0000-0002-1739-4726milou.van poppel@kcl.ac.uk\n",
            "Irina Grigorescuhttps://orcid.org/0000-0002-9756-3787irina.grigorescu@kcl.ac.uk\n",
            "Johannes K. Steinweghttps://orcid.org/0000-0002-3366-0932johannes.steinweg@kcl.ac.uk\n",
            "David F.A. Lloydhttps://orcid.org/0000-0003-1759-6106david.lloyd@kcl.ac.uk\n",
            "Kuberan Pushparajahhttps://orcid.org/0000-0003-1541-1155kuberan.pushparajah@kcl.ac.uk\n",
            "Andrew P. Kinghttps://orcid.org/0000-0002-9965-7015andrew.king@kcl.ac.uk\n",
            "Maria Deprezhttps://orcid.org/0000-0002-2799-6077maria.deprez@kcl.ac.uk\n",
            "School of Biomedical Engineering and Imaging Sciences, King’s College London, London, UK.\n",
            "Abstract\n",
            "Congenital Heart Disease (CHD) is a group of cardiac malformations present already dur-\n",
            "ing fetal life, representing the prevailing category of birth defects globally. Our aim in this\n",
            "study is to aid 3D fetal vessel topology visualisation in aortic arch anomalies, a group which\n",
            "encompasses a range of conditions with significant anatomical heterogeneity. We present\n",
            "a multi-task framework for automated multi-class fetal vessel segmentation from 3D black\n",
            "blood T2w MRI and anomaly classification. Our training data consists of binary manual\n",
            "segmentation masks of the cardiac vessels’ region in individual subjects and fully-labelled\n",
            "anomaly-specific population atlases. Our framework combines deep learning label propaga-\n",
            "tion using VoxelMorph with 3D Attention U-Net segmentation and DenseNet121 anomaly\n",
            "classification. We target 11 cardiac vessels and three distinct aortic arch anomalies, in-\n",
            "cluding double aortic arch, right aortic arch, and suspected coarctation of the aorta. We\n",
            "incorporate an anomaly classifier into our segmentation pipeline, delivering a multi-task\n",
            "framework with the primary motivation of correcting topological inaccuracies of the seg-\n",
            "mentation. The hypothesis is that the multi-task approach will encourage the segmenter\n",
            "network to learn anomaly-specific features. As a secondary motivation, an automated di-\n",
            "agnosis tool may have the potential to enhance diagnostic confidence in a decision support\n",
            "setting. Our results showcase that our proposed training strategy significantly outperforms\n",
            "label propagation and a network trained exclusively on propagated labels. Our classi-\n",
            "fier outperforms a classifier trained exclusively on T2w volume images, with an average\n",
            "balanced accuracy of 0.99 (0.01) after joint training. Adding a classifier improves the\n",
            "anatomical and topological accuracy of all correctly classified double aortic arch subjects.\n",
            "Our code is available at\n",
            "https://github.com/SVRTK/MASC-multi-task-segmentation-and-classification .\n",
            "Keywords: Deep Learning, Fetal Cardiac Imaging, Congenital Heart Disease, Automated\n",
            "Diagnosis, Fetal Cardiac MRI, Aortic Arch Segmentation, Multi-Task Learning, Multi-\n",
            "Class Vessel Segmentation, Anomaly Segmentation\n",
            "©2023 Ramirez et al.. License: CC-BY 4.0\n",
            "https://doi.org/10.59275/j.melba.2023-b7bcarXiv:2311.07234v1  [eess.IV]  13 Nov 2023Multi-task learning for anomaly segmentation\n",
            "1. Introduction\n",
            "Congenital heart disease (CHD) is the leading cause of mortality related to congenital de-\n",
            "fects (Mendis et al. (2011)). Accurate CHD diagnosis before birth is essential to inform\n",
            "appropriate early postnatal management, which is known to lead to improved patient out-\n",
            "comes both in terms of mortality and long-term morbidity (Brown et al. (2006); Mazwi\n",
            "et al. (2013)).\n",
            "We present a fully automated weakly-supervised multi-task tool for multi-class fetal\n",
            "cardiac vessel segmentation and anomaly classification in 3D T2w MRI. Our intention is\n",
            "to facilitate fetal cardiac vessel visualisation for prenatal diagnostic reporting purposes,\n",
            "and provide the groundwork for automated vessel biometry and detection. We target three\n",
            "aortic arch anomalies: Right Aortic Arch (RAA), Double Aortic Arch (DAA), and suspected\n",
            "Coarctation of the Aorta (CoA).\n",
            "In current clinical practice, vessel segmentation in fetal cardiac MRI is a time-consuming\n",
            "process based on thresholding followed by manual correction, resulting in a binary mask of\n",
            "fetal heart and vessels. An expert fetal cardiac clinician generally takes 1-2 hrs to complete\n",
            "the binary mask, which is a significant hurdle to wider translation outside the research\n",
            "setting. In addition, more refined multi-label segmentation would provide clinicians with\n",
            "better visualisation of individual vessels, and facilitate automated quantitative analysis, thus\n",
            "reducing reporting time and potentially improving the prediction of outcomes. However,\n",
            "manual multi-label segmentation would bring extra clinical burden and is therefore not\n",
            "currently performed in clinical practice. This is partly due to the image quality available,\n",
            "paired with the small fetal vessel size (often only 1-2 voxels wide).\n",
            "Our proposed weakly supervised deep learning framework is able to leverage existing\n",
            "binary manual segmentations in conjunction with a small number of condition-specific multi-\n",
            "label atlases, to provide a fully automated and accurate multi-label segmentation of individ-\n",
            "ual cardiac vessels. The technique is adaptable to clinical workflows as it does not require\n",
            "any manual input at inference time. We incorporate an aortic arch anomaly classifier into\n",
            "our framework, with the intention of both providing a clinically useful diagnostic tool and\n",
            "improving segmentation performance.\n",
            "1.1 Imaging the fetal heart\n",
            "2D Fetal echocardiography is commonly used for detecting CHD before birth, as it of-\n",
            "fers clear discernment of the cardiac chambers and cardiac functional measurements using\n",
            "Doppler flow. Extracting vessel positional and topological information from 2D echocar-\n",
            "diography images alone is an extremely challenging task, requiring highly trained experts.\n",
            "Although 3D STIC (DeVore et al., 2003) is available in many clinical settings, the corruption\n",
            "by fetal motion renders this imaging modality less viable for clinical assessments.\n",
            "Recently, fetal cardiac MRI has shown the potential as an adjunct to echocardiography\n",
            "in the detection of CHD prenatally (Lloyd et al., 2019), and its use is becoming widespread\n",
            "(Dong and Zhu, 2018; Salehi et al., 2021; Dong et al., 2020). State-of-the-art motion cor-\n",
            "rection algorithms (Uus et al., 2020) address fetal CMR motion challenges, allowing the\n",
            "generation of high-quality 3D MRI of fetal cardiac vessels. T2w black blood 3D MRI offers\n",
            "excellent vascular visualisation (Lloyd et al., 2019), as the vessels appear dark in contrast\n",
            "with surrounding tissue such as the lungs. This is the principal motivation behind the choice\n",
            "407Ramirez et al.\n",
            "of imaging modality used in the present work, given the aim is segmentation of vascular\n",
            "structures in aortic arch anomalies.\n",
            "1.2 Deep learning segmentation\n",
            "In order to provide a robust assessment of the cardiovascular anatomy in CMR, anatomical\n",
            "structures require segmentation, i.e. voxelwise classification into an anatomical class. Deep\n",
            "learning has become widespread for automated segmentation (Hesamian et al., 2019) and\n",
            "adult CMR segmentation (Chen et al., 2020).\n",
            "U-Net-based architectures (Ronneberger et al., 2015; Isensee et al., 2018) have been\n",
            "successfully employed for fetal brain and thorax MRI segmentation (Salehi et al., 2018; Uus\n",
            "et al., 2021). The fetal brain is the most commonly targeted fetal organ for automated\n",
            "segmentation in recent works (Keraudren et al., 2014; Khalili et al., 2019; Ebner et al.,\n",
            "2020; Payette et al., 2020; Salehi et al., 2018), due to the growing interest in early brain\n",
            "development. In addition, as a rigid, heterogeneous organ constrained with the fetal skull\n",
            "the brain represents an attractive target for these methods, compared to the fetal heart\n",
            "which regularly deforms with the rapid fetal cardiac cycle. Most forms of CHD also present\n",
            "significant anatomical variation in terms of vessel sizes and relative positions, making the\n",
            "application of the methods particularly challenging.\n",
            "Deep learning has been extensively applied to adult CMR segmentation in CHD (Arafati\n",
            "et al., 2019). Yu et al. (2016) present a 3D fractal network for whole heart and great vessel\n",
            "segmentation, while Rezaei et al. (2018) propose a framework comprising a three-stage\n",
            "cascade of conditional GANs. Xu et al. (2019) address the anatomical variability challenge\n",
            "in CHD by employing deep neural networks to segment the myocardial blood pool and\n",
            "chambers, coupled with graph matching for vessel identification. Fetal cardiac datasets,\n",
            "however, differ significantly from those acquired in postnatal life. For example, motion\n",
            "corruption is particularly common in fetal imaging, due to gross fetal, fetal cardiac and\n",
            "maternal respiratory motion. The fetal cardiac structures are also diminutive (most vessels\n",
            "are only millimetres in diameter), and are subject to low signal-to-noise ratio (SNR) and\n",
            "resolution with no option for intravenous contrast-enhancing agents. There is also differing\n",
            "baseline contrast due to the heart being surrounded by fluid filled lungs, as well as issues\n",
            "around temporal resolution due to the rapid fetal cardiac cycle. The technical approaches\n",
            "required for fetal cardiac imaging, such as fast acquisition sequences (Patel et al., 1997;\n",
            "Semelka et al., 1996) and high quality slice-to-volume registration algorithms (Kuklisova-\n",
            "Murgasova et al., 2012; Uus et al., 2020) are therefore highly specialised for this application,\n",
            "and generate unique output data which are not comparable to postnatal imaging methods.\n",
            "A bespoke approach for the application of deep learning methods is therefore required.\n",
            "1.3 Atlas-guided segmentation\n",
            "Our training dataset consists of partially labelled subjects (binary manually segmented\n",
            "labels) and fully-labelled atlases (Fig. 1). This type of setup is not uncommon in the\n",
            "medical imaging field, with exhaustive works addressing this challenge (Peng and Wang,\n",
            "2021).\n",
            "Atlas-based label propagation has been widely used for medical image segmentation\n",
            "(Aljabar et al., 2009; Heckemann et al., 2006). It uses image registration to transfer labelling\n",
            "408Multi-task learning for anomaly segmentation\n",
            "(a) Fully-labelled atlas\n",
            " (b) Partially-labelled training subject\n",
            "Figure 1: Illustration of our dataset setup.\n",
            "information from a given atlas to individual subjects. We propose to use VoxelMorph\n",
            "(Balakrishnan et al., 2019) for propagating multi-class labels from anomaly-specific atlases\n",
            "to fetal subjects.\n",
            "Deep learning atlas-based segmentation approaches include Dinsdale et al. (2019), which\n",
            "presents a binary mask warping framework based on spatial transformer networks; Xu and\n",
            "Niethammer (2019) propose to jointly train registration and segmentation networks to tackle\n",
            "partially labelled data. Similarly, in Sinclair et al. (2022), registration and segmentation are\n",
            "performed within the same framework, with the notable addition of a population-derived\n",
            "atlas being constructed in the process. Alternative strategies include one-shot or few-shot\n",
            "segmentation techniques (Zhao et al., 2019), where synthetic labelled data is generated\n",
            "using VoxelMorph and used to deal with partially labelled datasets.\n",
            "However, label propagation strategies are always limited by registration accuracy, hence\n",
            "in this work we propose to combine the benefits of atlas-based and deep learning based\n",
            "segmentation and use the atlas-propagated labels to train a convolutional neural network\n",
            "(CNN) for the task of fetal heart segmentation.\n",
            "1.4 Anomaly classifier\n",
            "We also propose to leverage the notable topological distinction between aortic arch anoma-\n",
            "lies (Fig. 2) to explore the addition of an anomaly classifier into our framework, based\n",
            "on the pipeline presented by Puyol-Ant´ on et al. (2021) which included a race classifier in\n",
            "combination with a segmentation network. A similar approach has been taken by Mehta\n",
            "et al. (2018), who address joint segmentation and classification of breast biopsy images,\n",
            "with a U-Net-based network predicting both a standard segmentation and discriminative\n",
            "map which are combined for tissue classification.\n",
            "The benefits of joint classifier and segmenter network training have also been evidenced\n",
            "in the field of computer vision. Liao et al. (2016) propose an approach which in essence is\n",
            "the reverse of our strategy, namely, features from a classifier network are used to train a\n",
            "scene segmentation network. MultiNet (Teichmann et al. (2018)) also follows an analogous\n",
            "architecture to ours, combining a shared encoder with a task-specific decoder.\n",
            "Our primary motivation is to explore whether the addition of a classifier can improve\n",
            "segmentation performance, in particular ensuring the correct topology of the automated\n",
            "409Ramirez et al.\n",
            "segmentation outputs. As a secondary motivation, we explore the possibility automated\n",
            "diagnosis of aortic arch anomalies as a potential aid to clinical decision making.\n",
            "1.5 Contributions\n",
            "In this work, we present a weakly supervised multi-task framework for automated\n",
            "multi-class fetal cardiac vessel segmentation and anomaly classification from 3D\n",
            "reconstructions of T2w black blood MRI images. We propose a deep learning framework,\n",
            "addressing three aortic arch anomalies.\n",
            "We expand on our prior research (Ramirez Gilliland et al., 2022) by incorporation of an\n",
            "anomaly classifier which improves segmentation discernment between anomalies. We also\n",
            "include detailed ablation studies to validate individual elements of our multi-task framework,\n",
            "which now consists of training using labels propagated from the anomaly-specific atlases\n",
            "and manual labels in individual images, while simultaneously classifying the anomaly from\n",
            "the predicted segmentation. This way we can leverage valuable anomaly-specific features\n",
            "learnt from a classifier to improve our segmentation output. A key novelty is the application\n",
            "of our framework to fetal Cardiac Magnetic Resonance (CMR) data, which is to date largely\n",
            "unexplored. Our framework expands on our previous work (Uus et al., 2022b), which for\n",
            "the first time proposed an automated segmentation of fetal cardiac vessels by training a\n",
            "CNN on labels propagated from anomaly-specific atlases.\n",
            "Our clinical contribution is to aid 3D vessel topology visualisation for clinical report-\n",
            "ing and training purposes and to increase confidence in diagnostic accuracy. We propose\n",
            "a multi-class approach to enable isolated visualisation of the anomaly area and affected\n",
            "vessels. Our approach does not require a fully-labelled training set, only subject-specific\n",
            "binary labels alongside multi-class anomaly-specific atlases; and thus is adaptable to clinical\n",
            "environments. These labels are only required for training, segmentation during inference is\n",
            "carried out on fully unlabelled data, from a single network.\n",
            "2. Methods\n",
            "2.1 Dataset description\n",
            "2.1.1 Aortic arch anomalies\n",
            "Our automated segmentation tool is aimed at subjects with Right Aortic Arch (RAA)\n",
            "with aberrant left subclavian artery (ALSA), Double Aortic Arch (DAA), and suspected\n",
            "Coarctation of the Aorta (CoA). These three distinct anomalies are depicted in Fig. 2.\n",
            "Briefly, in CoA, there is a narrowing of the distal aortic arch which can require urgent\n",
            "postnatal intervention if severe. In RAA the aortic arch passes to the right of the trachea\n",
            "(as opposed to the normal position on the left) with an associated risk of airway and/or\n",
            "oesophageal compression, particularly when associated with an ALSA. In DAA, both right\n",
            "and left aortic arches are present, forming a ”vascular ring” around the trachea and oesoph-\n",
            "agus, which in turn is associated with a higher risk of postnatal symptoms requiring surgical\n",
            "intervention. Whilst each of these categories presents a distinct topological phenotype, there\n",
            "is significant heterogeneity within each group in terms of vessel size and morphology.\n",
            "410Multi-task learning for anomaly segmentation\n",
            "Our dataset is representative of patients that are referred to a specialist fetal cardiac ser-\n",
            "vice, and therefore no controls are included in this study. Nonetheless, cases with Suspicion\n",
            "of CoA have the same vessel topology as healthy controls.\n",
            "Trachea\n",
            "AortaHealthyCoarctation of the Aorta \n",
            "(CoA)\n",
            "Right Aortic Arch (RAA) with Aberrant Left \n",
            "Subclavian artery (ALSA) Double Aortic Arch \n",
            "(DAA)\n",
            "Aorta to the right \n",
            "of the tracheaAorta branches into \n",
            "two around the \n",
            "tracheaNarrowing in the \n",
            "aorta\n",
            "Trachea\n",
            "Trachea\n",
            "ALSA\n",
            "Figure 2: Aortic arch anomaly illustration (atlases by Uus et al. (2022b)).\n",
            "2.1.2 Data specifications\n",
            "We employ 3D reconstructions of T2w black blood MRI. Our dataset consists of 195 fetal\n",
            "subjects with suspected coarctation of the aorta (CoA, N=94), Right Aortic Arch (RAA)\n",
            "with ALSA (N=72) and Double Aortic Arch (DAA, N=29), 31.4 ±1.5 weeks mean ges-\n",
            "tational age (min=29 weeks, max=36 weeks). In addition to the primary diagnosis of\n",
            "suspected CoA, RAA, or DAA, 56 cases presented secondary diagnoses affecting additional\n",
            "anatomical areas, outside of the segmentation area (aside from RAA >DAA). These are\n",
            "included in Table 1.\n",
            "The datasets were acquired at Evelina London Children’s Hospital using a 1.5 Tesla In-\n",
            "genia MRI system and a T2-weighted SSFSE sequence (RT=20,000 ms, ET=50ms, FA=90◦,\n",
            "voxel size=1 .25×1.25 mm, slice thickness=2.5 mm and slice overlap=1.25 mm). All re-\n",
            "search participants provided written informed consent. The raw datasets consisted of six\n",
            "to 12 multi-slice stacks of 2D images, covering the fetal thorax in three orthogonal planes.\n",
            "411Ramirez et al.\n",
            "(a) 3D reconstructed T2w MRI images\n",
            "with binary manually segmented labels.\n",
            "(b) 3D T2w MRI aortic arch anomaly atlases with\n",
            "multi-class vessel segmentations\n",
            "Figure 3: Depiction of our two dataset groups: individual fetal subjects with binary masks\n",
            "(3a), and multi-class anomaly-specific atlases (3b)\n",
            "Table 1: Number of cases with an additional diagnosis. AVSD: atrioventricular septal defect,\n",
            "VSD: ventricular septal defect, MV: mitral valve, PAPVD: partial anomalous pulmonary\n",
            "venous drainage, LV/RV: left/right ventricle, AS: aortic valve stenosis, DV: ductus venosus,\n",
            "BAV: bicuspid aortic valve, R/LAD: right/left arterial duct, R/LAA: right/left aortic arch,\n",
            "?=suspected.\n",
            "Primary diagnosis Additional diagnosis Number of cases\n",
            "Suspected CoA(unbalanced) (A)VSD 17\n",
            "?Parachute MV 1\n",
            "Small pericardial effusion 1\n",
            "PAPVD 1\n",
            "LV<RV 1\n",
            "(aneurysmal) AS 3\n",
            "Aberrant DV 1\n",
            "Aneurysm 2\n",
            "(?)BAV 3\n",
            "RAA with ALSA(?) VSD 4\n",
            "RAD 1\n",
            "DAARAA >LAA 10\n",
            "LAD 9\n",
            "Retro-aortic innominate vein 1\n",
            "VSD 1\n",
            "We use images reconstructed both with Slice-to-Volume Registration (SVR) (Kuklisova-\n",
            "Murgasova et al., 2012; Kainz et al., 2015) and Deformable SVR (Uus et al., 2020, 2022a)\n",
            "(DSVR, higher quality, N=49) to 0.75 mm isotropic resolution, to ensure a varied dataset.\n",
            "412Multi-task learning for anomaly segmentation\n",
            "Trained clinicians manually segmented a binary vessels label for the majority of our\n",
            "subjects (N=181), Fig. 3a. We employ propagated atlas labels exclusively for the remaining\n",
            "unsegmented cases (N=14, Sec. 2.2). In order to achieve our multi-class output, we employ\n",
            "three fully-labelled atlases1(Uus et al. (2022b), see Fig. 3b), one per anomaly (RAA, DAA\n",
            "and CoA). These include 11 manually segmented vascular regions for RAA and DAA, and\n",
            "10 for CoA cases.\n",
            "We affinely register all subject images to the pertinent anomaly-specific atlas prior\n",
            "to training via MIRTK2(Schnabel et al., 2001; Rueckert et al., 1999b,a), and crop to a\n",
            "standardised cardiac vessels region after affine alignment (see Fig. 3). We split the subjects\n",
            "into a training set (N CoA=71, N RAA=54, N DAA=21), validation set (N CoA=3, N RAA=3,\n",
            "NDAA=3), and testing set (N CoA=20, N RAA=15, N DAA=5). We normalise and rescale the\n",
            "intensities to between 0 and 1, and use a weighted random sampler to correct for the class\n",
            "imbalance problem.\n",
            "2.2 Multi-task segmentation framework\n",
            "Our benchmark framework is based on Ramirez Gilliland et al. (2022), where Attention\n",
            "U-Net (Oktay et al., 2018) is trained using both manual binary labels and multi-class labels\n",
            "propagated from an atlas (Uus et al., 2022b).\n",
            "The input to the network is an MRI image, and the output is a multi-class segmentation\n",
            "(see Fig. 4). We use VoxelMorph (Balakrishnan et al., 2019) for label propagation, defining\n",
            "our atlases as moving images ( m), and fetal subject images as fixed images ( f).\n",
            "Subsequent to VoxelMorph training, we generate our propagated labels and use these\n",
            "to train a CNN for segmentation. We keep the weights of VoxelMorph label propagation\n",
            "frozen while training our segmentation network. We train our proposed network with two\n",
            "losses: (1) a multi-class loss between the propagated labels and the network output; (2) a\n",
            "binary loss between the multi-class network output joined into a binary segmentation and\n",
            "the manual binary labels.\n",
            "Finally, we attach a DenseNet121 (Huang et al., 2017) classifier to our softmax segmen-\n",
            "tation output, which predicts one out of three possible diagnoses (CoA, RAA or DAA). We\n",
            "train Attention U-Net and DenseNet121 jointly. This classifier has the option of incorpo-\n",
            "rating the image as an extra channel, to make the classification less dependent on accurate\n",
            "segmentation.\n",
            "2.3 Label propagation\n",
            "We use VoxelMorph (Balakrishnan et al., 2019) for label propagation. We warp the la-\n",
            "bels from the pertinent anomaly-specific atlas (moving images, m) into each subject space\n",
            "(fixed images f), using the prior anomaly diagnosis knowledge to select the relevant atlas.\n",
            "Note that at testing or inference time, no prior diagnosis knowledge is required, and our\n",
            "framework offers an automated diagnosis prediction.\n",
            "1. https://gin.g-node.org/SVRTK/\n",
            "2. https://github.com/BioMedIA/MIRTK\n",
            "413Ramirez et al.\n",
            "Figure 4: Our proposed segmentation framework ( Attention U-Net LP + Man + Classifier )\n",
            "is trained with both a multi-class propagated labels loss, and a binary loss employing the\n",
            "manual binary masks. We use three atlases: CoA, RAA and DAA, and train our segmen-\n",
            "tation network jointly with an anomaly classifier. No labelling information (segmentation\n",
            "or diagnosis) is required during inference.\n",
            "2.3.1 Label propagation loss functions\n",
            "Our similarity reconstruction loss function ( Lsim) is Local Normalised Cross Correlation\n",
            "loss (LNCC loss) (Balakrishnan et al., 2019). We employ bending energy BE loss (LBE),\n",
            "as described in Grigorescu et al. (2020) as a displacement field regulariser. The total\n",
            "registration loss Lregmay be expressed as\n",
            "Lreg=Lsim(f, m·ϕ) +λ1LBE(ϕ), (1)\n",
            "where λ1is a loss weight.\n",
            "414Multi-task learning for anomaly segmentation\n",
            "2.3.2 Registration network implementation details\n",
            "We employ a U-Net-based encoder-decoder architecture. We use blocks of 3D strided con-\n",
            "volutions with leaky ReLU activations. We illustrate our architecture in Fig. 5.\n",
            "Figure 5: CNN architecture used for registration. The numbers under each convolution\n",
            "representation indicate the volume spatial resolution relative to the input volume. k=kernel\n",
            "size, s=stride.\n",
            "We train a single CNN on all diagnoses, appropriately pairing each subject to its cor-\n",
            "responding atlas. We train the VoxelMorph registration network until convergence (81,653\n",
            "iterations, NVIDIA GeForce RTX 3090 GPU), using a linearly decaying learning rate ini-\n",
            "tialised at 5 ×10−4and an Adam optimiser (default βparameters, weight decay of 1 ×10−5).\n",
            "We use Project MONAI spatial and intensity data augmentation3.\n",
            "2.4 CNN segmentation\n",
            "3D Attention U-Net is our backbone segmentation architecture, due to its success in seg-\n",
            "menting multi-class structures of varying locations and sizes (Oktay et al., 2018).\n",
            "2.4.1 Segmentation loss function\n",
            "We use the soft dice and cross-entropy loss (DiceCE loss, Hatamizadeh et al. (2022)) for\n",
            "all our segmentation experiments, with both cross entropy and dice loss weighted equally.\n",
            "We additionally investigated network performance using both Generalised Dice loss\n",
            "(GDL, Sudre et al. (2017)), and GDL+Focal loss (Lin et al., 2017). However, we found\n",
            "very poor performance when using both these losses, such as unlearnt small vessels.\n",
            "Our proposed framework Attention U-Net LP + Man is trained using a combined loss\n",
            "3. https://github.com/Project-MONAI/MONAI/.\n",
            "415Ramirez et al.\n",
            "Lseg=λ3DiceCE loss(predmulti,[mlab·ϕ]) +\n",
            "λ2DiceCE loss(predjoined , flab)(2)\n",
            "where predmulti are the multi-class Attention U-Net predictions, predjoined are binary label\n",
            "predictions (multi-class output labels joined together), [ mlab·ϕ] are the propagated atlas\n",
            "labels, flabare the manual binary labels, λ2is the binary loss weight, and λ3the multi-class\n",
            "loss weight. The proposed losses are schematically presented in Fig. 4.\n",
            "2.4.2 Segmentation network implementation details\n",
            "We use a 3D Attention U-Net (Oktay et al., 2018) implemented in Project MONAI3for\n",
            "automated segmentation, with five encoder-decoder blocks (output channels 32, 64, 128,\n",
            "256 and 512), convolution and upsampling kernel size of 3, ReLU activation, dropout ratio\n",
            "of 0.5, batch normalisation, and a batch size of 12. We employ an AdamW optimiser\n",
            "with a linearly decaying learning rate, initialised at 1 ×10−3, default βparameters and\n",
            "weight decay=1 ×10−5. We use intensity and spatial augmentations from Project MONAI3,\n",
            "including random intensity shifts, Gaussian smoothing, Gaussian noise, sharpening, contrast\n",
            "adjustments and bias field.\n",
            "We train our proposed method ( Attention U-Net LP + Man , NVIDIA GeForce RTX\n",
            "3090 GPU) by increasing the binary weight ( λ2) by 0.05 every 50 epochs until convergence\n",
            "(15809 iterations), with the propagated labels weight set to a fixed value ( λ3=1). This is\n",
            "to ensure accurate vessel classification while increasing the accuracy of the whole vessels\n",
            "region of interest (ROI).\n",
            "2.5 Classification\n",
            "We train an anomaly classifier to discern between three classes: CoA, RAA and DAA. We\n",
            "employ DenseNet121 (Huang et al., 2017) as our backbone classifier architecture.\n",
            "DenseMulti is our classifier trained jointly with a segmenter network ( Attention U-Net\n",
            "LP + Man ). This is a multi-task framework, where we update the weights of both networks\n",
            "simultaneously and use the features learnt from the segmenter to train the classifier. The\n",
            "inputs to the classifier are the output softmax features from the segmenter network. Fig. 6\n",
            "illustrates our joint training framework. We aim to explore whether combined training will\n",
            "encourage the segmenter to learn the characteristic features of each anomaly.\n",
            "We additionally explored training a classifier on the latent space of the segmenter net-\n",
            "work. However, we did not find a significant segmentation improvement.\n",
            "2.5.1 Classification loss function\n",
            "We employ different optimizers for classifier and segmenter networks and perform back-\n",
            "propagation using the same total loss for both networks. This is expressed as\n",
            "Ljoint=Lseg+λ4Lclass. (3)\n",
            "Lsegis the weighted sum of DiceCE lossbetween propagated labels and predictions\n",
            "(multi-class output) and between manual binary labels and predictions (binary output),\n",
            "see Sec. 2.4.1. Lclassis the classification cross entropy loss; and λ4is a tunable loss weight.\n",
            "416Multi-task learning for anomaly segmentation\n",
            "Figure 6: DenseMulti : our classifier experiment trained on the output softmax segmen-\n",
            "tation prediction.\n",
            "2.5.2 Classification implementation details\n",
            "We pre-train our segmentation networks prior to the addition of the classifier (with both\n",
            "propagated and binary labels), as described in Sec. 2.4.2. The classifier is also pre-trained\n",
            "(on the softmax segmentation output) keeping the segmenter network weights fixed. This\n",
            "is to ensure that the classifier only learns highly representative features of each anomaly;\n",
            "and that the classifier does not degrade the segmenter network performance.\n",
            "We repeat each joint training experiment three times, employing an independently\n",
            "trained segmentation network each time (the same repeated Attention U-Net LP+Man\n",
            "experiments described in Sec. 3.1). We use the latest binary weight ( λ2) from the best\n",
            "pre-trained network (on our validation set) in each case; and tune the classifier weight ( λ4\n",
            "in Eq. 3), with the optimal weight being between 6 and 12 for DenseMulti , and 0.1 to 1 for\n",
            "DenseBin andDenseImgMulti , which is a classifier trained only on binary segmentation\n",
            "output (see Sec. 3.3). We find the weight balancing to be very important and tune this\n",
            "parameter for each training experiment (which employs an independently trained segmen-\n",
            "tation network each time) by observing segmentation loss convergence, classifier validation\n",
            "set scores and visual segmentation inspection on our validation set. We train until reaching\n",
            "the best performance on our validation set (lowest multi-class DiceCE losson validation set).\n",
            "3. Experiments\n",
            "3.1 Training label type ablation study\n",
            "As presented in Ramirez Gilliland et al. (2022) we conduct ablation studies on the inclu-\n",
            "sion of each type of labelling information, e.g. manual binary labels in individual subject\n",
            "images and multi-class labels propagated from the anomaly-specific atlases. We evaluate\n",
            "segmentation performance in the following experiments:\n",
            "•LP: Label Propagation using VoxelMorph.\n",
            "•Attention U-Net LP : Attention U-Net trained exclusively with propagated labels\n",
            "(λ2= 0 in Eq. 2).\n",
            "•Attention U-Net Man : Attention U-Net trained exclusively with manually seg-\n",
            "mented binary labels ( λ3= 0).\n",
            "•Attention U-Net LP+Man : Attention U-Net trained with both manual binary\n",
            "labels and propagated labels (Eq. 2).\n",
            "417Ramirez et al.\n",
            "Expanding on our previous work (Ramirez Gilliland et al., 2022), we repeat our seg-\n",
            "mentation experiments three times, to account for network stochasticity.\n",
            "3.2 Weak supervision\n",
            "We study the impact a varying amount of manually generated labels have on segmentation\n",
            "network performance. In this ablation study, we solely examine the weak binary manual la-\n",
            "bels as they are meticulously created by expert fetal cardiac clinicians, while the propagated\n",
            "labels are automatically derived.\n",
            "Six segmentation experiments are compared, with varying percentages of manual binary\n",
            "labels (Man) included: 0% Man (no manual binary labels in training, i.e. Attention U-Net\n",
            "LP), 5% Man, 25% Man, 50% Man, 75% Man, and 100% Man (i.e. our proposed Atten-\n",
            "tion U-Net LP + Man ). These percentages are computed for each anomaly (out of their\n",
            "respective total number of training cases). In all these experiments the propagated labels\n",
            "are fully used. The same implementation details as described in Sec. 2.4.2 are employed.\n",
            "3.3 Classification experiments\n",
            "We conduct four main experiments to evaluate our aortic arch anomaly classifiers.\n",
            "DenseMulti is a DenseNet121 trained from the softmax output of the multi-class\n",
            "segmenter network ( Attention U-Net LP + Man , see Fig. 6).\n",
            "DenseBin is a DenseNet121 trained from the softmax output of our binary segmenter\n",
            "network ( Attention U-Net Man ).\n",
            "DenseImage , is a DenseNet121 trained exclusively on the volume images as input.\n",
            "DenseImgMulti , is a DenseNet121 trained on the multi-class softmax output of At-\n",
            "tention U-Net LP + Man , concatenated with the input T2w image.\n",
            "We pre-train DenseBin ,DenseMulti andDenseImgMulti using the frozen weights\n",
            "of each respective segmenter network ( Attention U-Net Man , and Attention U-Net LP +\n",
            "Man). We report these results under each classifier name followed by Separate . We\n",
            "then train our pre-trained classifier and segmenter networks jointly, which is our multi-task\n",
            "approach, and report results under the title Joint .\n",
            "We repeat each experiment three times, employing our three independently pre-trained\n",
            "segmenter network rounds.\n",
            "3.4 Evaluation metrics\n",
            "3.4.1 Quantitative analysis\n",
            "We manually generated multi-class ground truth (GT) labels for our test set (N=40) via\n",
            "ITK-SNAP (Yushkevich et al., 2006). We report similarity metrics including multi-class\n",
            "Dice scores, recall, precision, average surface distance (ASD), and 95th percentile of the\n",
            "Hausdorff Distance (HD95). We repeat our segmentation experiments three times and\n",
            "average results. We also include a short analysis studying the impact of image quality on\n",
            "performance.\n",
            "418Multi-task learning for anomaly segmentation\n",
            "3.4.2 Qualitative analysis\n",
            "There are inherent limitations to conducting a quantitative analysis exclusively, particularly\n",
            "given the quality of our data and the task at hand. We find the topological correctness of\n",
            "the anomaly area, our key segmentation objective, to not be reflected in our quantitative\n",
            "metrics described above. We thereby devise a qualitative analysis strategy, which involves\n",
            "manual inspection and scoring of the anomaly area of each test set subject.\n",
            "We assess the topological correctness of the aortic arch (anomaly area), as described\n",
            "in Table 2, where a score of 1 (best) represents an aortic arch with topologically correct\n",
            "anomaly delineation, and a score of 3 (worst) indicates topologically incorrect delineation.\n",
            "Types of topological errors for segmentations with a score of 3 include split aortic arch,\n",
            "indiscernible aortic arch (merged), an unsegmented or partially segmented right arch in a\n",
            "DAA case (split arch), a segmented left arch for RAA cases (double arch segmented), and a\n",
            "segmented right arch for CoA cases (double arch segmented). The term oversegmentation\n",
            "refers to erroneously labelling background voxels as an anatomical structure, while rarely\n",
            "labelling structure as background. This typically results in erroneously thick vessels.\n",
            "Table 2: Description of our anomaly area topological assessment. HN refers to head and\n",
            "neck vessels, and AD arterial duct.\n",
            "Aortic arch score\n",
            "1 Topologically correct aortic arch\n",
            "2 Oversegmented aortic arch merging into HN vessels or AD.\n",
            "3 Topologically incorrect aortic arch\n",
            "4. Results\n",
            "4.1 Training label type ablation study\n",
            "Here we present metrics comparing our proposed segmentation framework which learns\n",
            "from both binary individual labels and multi-label atlases, against the inclusion of just\n",
            "one type of labelling (see Sec. 3.1). Differences to Ramirez Gilliland et al. (2022) are due\n",
            "to adaptations made to the atlas in pulmonary arteries and exclusion of pulmonary veins\n",
            "(following discussions with clinicians), and stochasticity due to network retraining. All\n",
            "metrics displayed are on unseen test set, comparing to our manually generated GT.\n",
            "4.1.1 Whole vessel ROI\n",
            "Here we present metrics for the whole vessels’ ROI in violin plots in Fig. 7. In the case of\n",
            "multi-label segmentation, the vessels were joined into a single region prior to the metrics\n",
            "calculations.\n",
            "Attention U-Net Man , which is the network trained on binary labels, displays the\n",
            "highest mean vessels’ ROI dice (0 .83±0.03). However, further analysis reveals a much\n",
            "higher average recall (0 .87±0.05) than precision (0 .79±0.07), which points to significant\n",
            "oversegmentation. This is clearly visible in Fig. 8. We can observe that oversegmentation\n",
            "results in the merging of the neighbouring vessels, obstructing the visualisation of the fetal\n",
            "cardiac vessels’ anatomy.\n",
            "419Ramirez et al.\n",
            "Conversely, the proposed network Attention U-Net LP + Man has a more balanced\n",
            "performance with a mean Dice = 0 .79±0.03, a mean precision = 0 .80±0.05, and a mean\n",
            "recall = 0 .78±0.05. The resulting segmentations (Fig. 8) offer a clear depiction of individual\n",
            "vessels, which is required for our clinical application.\n",
            "Attention U-Net Man also has the lowest mean ASD of 0 .51±0.091 mm, which increases\n",
            "to 0.55±0.08 for the proposed Attention U-Net LP + Man . However, Attention U-Net Man\n",
            "presents outliers with ASD even greater than LP, whereas Attention U-Net LP + Man is\n",
            "much more consistent across cases.\n",
            "The network trained on propagated labels only Attention U-Net LP has the lowest\n",
            "performance in all the metrics.\n",
            "Recall (ROI)\n",
            "0.390.520.650.780.91\n",
            "Figure 7: Quantitative ROI metrics for our labelling ablation experiments. The horizontal\n",
            "lines represent the means.\n",
            "4.1.2 Individual fetal cardiac vessels\n",
            "Table 3 displays metrics comparing our multi-class experiments. We compare our full ap-\n",
            "proach ( Attention U-Net LP + Man ) to Attention U-Net trained exclusively on propagated\n",
            "labels ( Attention U-Net LP ) and VoxelMorph label propagation ( LP). These results in-\n",
            "dicate that adding the binary labels to the training of the segmentation network offers a\n",
            "significant improvement for Dice and ASD metrics for all vessels. Further, Attention U-Net\n",
            "trained on propagated labels improves on label propagation alone.\n",
            "4.1.3 Visual inspection\n",
            "Nevertheless, we find these quantitative metrics to not be fully descriptive of the anatomical\n",
            "correctness of the segmentation, a decisive feature when segmenting anomalies. Following\n",
            "visual inspection, we can conclude two main advantages of our multi-class approach: At-\n",
            "tention U-Net LP + Man is more consistent in both small vessel detection and\n",
            "topological correctness of the anomaly area compared to Attention U-Net Man .\n",
            "The latter is quantified in Sec. 4.4. Fig. 8 includes a depiction of these two prominent issues\n",
            "420Multi-task learning for anomaly segmentation\n",
            "Table 3: Similarity metrics between our multi-class experiment predictions and our manually\n",
            "generated GT. We highlight the best scores in bold. HN indicates the head and neck vessels\n",
            "(LSA, LCCA, BCA, RSA). The three bottom rows report metrics averaged over all vessels.\n",
            "We highlight with an * the vessels with a p-value <0.05 (Mann-Whitney U test) (non-\n",
            "parametric) compared to Attention U-Net LP + Man .\n",
            "LP Attention U-Net LPAttention U-Net\n",
            "LP + Man\n",
            "Vessel Dice ASD Dice ASD Dice ASD\n",
            "SVC 0.65 (0.15)* 0.88 (0.60)* 0.70 (0.06)* 0.70 (0.20)* 0.76 (0.06) 0.56 (0.16)\n",
            "LPA 0.58 (0.15)* 0.88 (0.93)* 0.63 (0.08)* 0.72 (0.30)* 0.65 (0.08) 0.68 (0.29)\n",
            "RPA 0.55 (0.12)* 0.80 (0.44)* 0.61 (0.07)* 0.71 (0.23) 0.63 (0.07) 0.67 (0.21)\n",
            "Aortic arch 0.58 (0.13)* 0.87 (0.46)* 0.62 (0.06)* 0.79 (0.16)* 0.76 (0.04) 0.54 (0.08)\n",
            "AD 0.70 (0.16)* 0.66 (0.57)* 0.76 (0.07)* 0.49 (0.11)* 0.78 (0.07) 0.44 (0.12)\n",
            "DAO 0.77 (0.11)* 0.63 (0.40)* 0.81 (0.04)* 0.52 (0.20)* 0.84 (0.04) 0.43 (0.18)\n",
            "MPA 0.74 (0.11)* 0.71 (0.25)* 0.76 (0.06)* 0.68 (0.16)* 0.81 (0.05) 0.55 (0.13)\n",
            "HN 0.31 (0.21)* 1.50 (1.27)* 0.38 (0.20) 1.24 (0.94)* 0.40 (0.19) 1.20 (0.93)\n",
            "Avg. Dice 0.54 (0.24) 0.59 (0.20) 0.63 (0.21)\n",
            "Avg. HD95 3.14 (1.07) 2.71 (1.81) 2.50 (1.87)\n",
            "Avg. ASD 2.26 (0.97) 0.85 (0.63) 0.77 (0.63)\n",
            "observed across Attention U-Net Man experiments: undetected small vessels, and merging\n",
            "of the aorta into surrounding vessels.\n",
            "Aorta merged into MPA \n",
            "and ADUndetected vesselAttention U-Net LP + Man Attention U-Net Man GT\n",
            "Figure 8: Depiction of aorta topological inaccuracies and undetected small vessels commonly\n",
            "occurring in Attention U-Net Man .\n",
            "4.1.4 Visualisation of the latent representation\n",
            "We inspect the anomaly-specific discernment of our labelling ablation networks by visual-\n",
            "ising t-SNE reduced latent space features (Fig. 9). With this, we can easily examine the\n",
            "effect multi-class and binary labels have on anomaly distinction for our networks.\n",
            "We can conclude that our multi-class network Attention U-Net LP + Man offers highly\n",
            "clustered groups for anomalies even in the bottleneck features, contrarily to our binary\n",
            "network ( Attention U-Net Man ) which especially struggles to discern between RAA and\n",
            "421Ramirez et al.\n",
            "Figure 9: t-SNE bottleneck reduced features for one training round of our combined label\n",
            "framework (multi-class, Attention U-Net LP + Man ) and our binary network ( Attention\n",
            "U-Net Man ).\n",
            "DAA cases, as supported by our qualitative analysis (Sec. 4.4). We include further t-SNE\n",
            "plots comparing our various experiments in Appendix B.2.\n",
            "4.1.5 Weak supervision ablation\n",
            "Fig. 10 includes Dice and ASD scores for the vessels’ ROI and aortic arch label, for an\n",
            "increasing number of training cases with manually generated binary labels. Our findings\n",
            "indicate that including binary labels in training results in a statistically significant improve-\n",
            "ment in segmentation performance (p-value <0.05, Mann-Whitney U test) compared to\n",
            "not including any binary labels (0%, Attention U-Net LP ) when 25% or more of the manual\n",
            "labels were included. For 5% of included manual labels, the improvement in Dice and ASD\n",
            "is only significant for the aortic arch region, not the whole ROI.\n",
            "4.2 Classification of anomalies\n",
            "4.2.1 Classification performance\n",
            "Table 4 presents accuracy and balanced accuracy scores (average recall obtained on each\n",
            "class) for each classifier experiment, as well as recall scores for each individual anomaly. In\n",
            "addition to comparing different types of features for classification (multi-label segmentation,\n",
            "binary segmentation and original image), we also compare separate and joint training of\n",
            "segmenter and classifier networks. The confusion matrices are provided in Appendix B.\n",
            "We observe that while joint training of the classifier with binary segmentation improves\n",
            "the classification results, this is not the case for multi-class segmentation. For multi-class\n",
            "segmentation, the differences in classification performance in jointly and separately trained\n",
            "frameworks are caused by a small number of subjects, with three misclassifications (two\n",
            "individual subjects) over the three training rounds in the joint framework (one misclassifi-\n",
            "cation per round), and two misclassifications in a single subject in the separate framework.\n",
            "There is therefore no clear improvement with joint training in the multi-label case, however,\n",
            "422Multi-task learning for anomaly segmentation\n",
            "(a) ASD scores (lower = better).\n",
            " (b) Dice scores (higher = better).\n",
            "Figure 10: Test set metrics for the vessels’ ROI, and aortic arch predicted label comparing\n",
            "the impact of adding binary manual labels (Man) during training. Error bars represent the\n",
            "95% confidence interval, computed via bootstrapping (non-parametric uncertainty repre-\n",
            "sentation).\n",
            "the classification performance is high, with an accuracy of 0.97 for DenseMulti and 0.99\n",
            "forDenseImgMulti .\n",
            "In general, CoA cases are always correctly classified, while the misclassifications happen\n",
            "between RAA and DAA cases.\n",
            "Overall, the anomaly classification from multi-label segmentation clearly outperforms\n",
            "the classification from binary segmentations or directly from the images. However, when\n",
            "incorporating image information concatenated with the output segmentation, the classifier\n",
            "achieves the highest performance. This image data inclusion allows the network to focus on\n",
            "global features and becomes less reliant on topologically accurate segmentation predictions\n",
            "compared to DenseMulti . Consequently, DenseImgMulti ’s high accuracy is not truly\n",
            "reflective of segmentation performance, as correct classification can still occur even with\n",
            "important topological errors in the anomaly area.\n",
            "We display our DenseMulti class probabilities in Fig. 11 for each anomaly (one vs.\n",
            "rest). These include the results of our three training rounds. We observe a higher number\n",
            "of cases with a lower softmax probability for RAA and DAA groups, meaning that our\n",
            "classifier is less confident when distinguishing between RAA and DAA and the rest of our\n",
            "groups, however very confident for CoA predictions. This is understandable, given the\n",
            "anatomical similarities between RAA and DAA subjects (Fig. 2).\n",
            "4.2.2 Analysis of misclassified cases\n",
            "Here we provide further detail on our DenseMulti andDenseImgMulti misclassifications.\n",
            "We obtain one misclassified case per round with DenseMulti , with one case repeated across\n",
            "two of our training rounds. This same case is the one misclassification for DenseImgMulti .\n",
            "This case is an RAA subject erroneously classified as DAA due to the double arch being\n",
            "wrongly segmented (Fig. 12). This particular case presented lower visibility in the anomaly\n",
            "423Ramirez et al.\n",
            "Table 4: Accuracy and recall scores for our classifier experiments, averaged over all three\n",
            "training rounds. Separate = prior to joint segmenter and classifier training, Joint = after\n",
            "joint training. Bl. Acc. = balanced accuracy.\n",
            "Accuracy Bl. Acc. Recall CoA Recall RAA Recall DAA\n",
            "DenseMulti\n",
            "Separate 0.98 (0.01) 0.95 (0.04) 1.0 (0.0) 1.0 (0.0) 0.87 (0.11)\n",
            "Joint 0.97 (0.0) 0.98 (0.0) 1.0 (0.0) 0.93 (0.0) 1.0 (0.0)\n",
            "DenseBin\n",
            "Separate 0.88 (0.01) 0.90 (0.01) 0.95 (0.0) 0.75 (0.04) 1.0 (0.0)\n",
            "Joint 0.92 (0.02) 0.91 (0.05) 0.97 (0.03) 0.89 (0.10) 0.81 (0.17)\n",
            "DenseImgMulti\n",
            "Separate 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0)\n",
            "Joint 0.99 (0.01) 0.99 (0.01) 1 (0.0) 0.98 (0.04) 1.0 (0.0)\n",
            "DenseImage 0.82 (0.07) 0.79 (0.09) 0.98 (0.03) 0.64 (0.01) 0.62 (0.30)\n",
            "0.00\n",
            "0.25\n",
            "0.50\n",
            "0.75\n",
            "1.00\n",
            "P(x = CoA)\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60CountCoA (N=20) × 3\n",
            "Class: CoA\n",
            "Rest\n",
            "0.00\n",
            "0.25\n",
            "0.50\n",
            "0.75\n",
            "1.00\n",
            "P(x = RAA)\n",
            "0\n",
            "20\n",
            "40\n",
            "60CountRAA (N=15) × 3\n",
            "Class: RAA\n",
            "Rest\n",
            "0.00\n",
            "0.25\n",
            "0.50\n",
            "0.75\n",
            "1.00\n",
            "P(x = DAA)\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100CountDAA (N=5) × 3\n",
            "Class: DAA\n",
            "Rest\n",
            "Figure 11: Count of probability distributions for all three rounds of DenseMulti classifier\n",
            "experiment on our test set.\n",
            "area, with more than one independently trained network partially segmenting (erroneously)\n",
            "the left arch. We highlight the blurriness in this region causing the oversegmentation. This\n",
            "particular case already presented topological issues prior to adding the classifier (Fig. 12,\n",
            "left). The addition of the classifier clearly tries to erroneously correct this initial error, by\n",
            "fully segmenting the double arch, i.e. by pushing the segmentation prediction closer to one\n",
            "of the aortic arch anomalies. Importantly, this case presents topological issues for one of\n",
            "our joint DenseImgMulti experiments but was correctly classified. These findings suggest\n",
            "DenseImgMulti to be a more reliable classifier when the segmentation is faulty, as it\n",
            "incorporates image information. Consequently, it cannot be utilized as a quality control\n",
            "tool to identify erroneous segmentations.\n",
            "Our second misclassification is another RAA case mislabelled as DAA due to poor image\n",
            "quality and misalignment to the atlas (Fig. 13). Our segmentation here is accurate, however,\n",
            "the aortic arch and AD are highly elevated compared to the atlas (Fig. 13). This may have\n",
            "424Multi-task learning for anomaly segmentation\n",
            "Before classifier addition\n",
            "Figure 12: Arrow signals the double arch erroneously segmented in an RAA subject, re-\n",
            "sulting in misclassification. We highlight the blurrier region where this error occurs in the\n",
            "image with a rectangle.\n",
            "been misclassified after joint training due to slight overfitting to the training data. However,\n",
            "it is only one case (out of 120 in total).\n",
            "Elevated\n",
            " vessels (GT)Misclassified RAA case RAA atlas\n",
            "Figure 13: Mislabelled RAA as DAA in one of our rounds due to low image quality and\n",
            "misalignments to the atlas, with a highly elevated aorta and AD compared to the atlas.\n",
            "4.2.3 Segmentation improvements due to anomaly classifier\n",
            "Regarding segmentation network gains, we find meaningful improvements in our multi-class\n",
            "framework, particularly regarding the completeness of double arch segmentation (Figs. 15\n",
            "and 14). Although our DAA test set comprises only five cases, we repeat this experiment\n",
            "three times, with independently trained segmenter networks, and find that in all correctly\n",
            "classified DAA subjects where the right arch was originally partially segmented\n",
            "or unsegmented, adding a classifier (DenseMulti and DenseImgMulti ) resolved\n",
            "this. The double arch vessel thickness is also improved. This constitutes eight subjects\n",
            "with a notable improvement over the three rounds . We report topological improve-\n",
            "ments in our overall qualitative analysis, Sec. 4.4, Fig. 19.\n",
            "425Ramirez et al.\n",
            "Fig. 14 depicts a clear example of improved segmentation performance via our multi-task\n",
            "framework, consequently correcting the predicted anomaly class.\n",
            "GT Before joint training: \n",
            "Attention U-Net LP + ManAfter joint training: \n",
            "Attention U-Net LP + Man + Classifier\n",
            "Predicted diagnosis: RAA Predicted diagnosis: DAA True diagnosis: DAASplit and partially \n",
            "segmented double \n",
            "archFully \n",
            "segmented \n",
            "double arch\n",
            "Figure 14: Test case where we find a significant topological improvement of the double arch,\n",
            "resulting in a corrected anomaly prediction.\n",
            "Attention U-Net LP + Man \n",
            "without classifier Attention U-Net LP + Man \n",
            "with classifier \n",
            "Figure 15: Arrow signals the double arch in a DAA subject, a large portion unsegmented\n",
            "prior addition of the classifier network. The GT is overlaid without filling, i.e. just the\n",
            "contours, while the predictions are filled.\n",
            "We find marginal improvements in aorta metrics for DAA cases (see Tab. 5). The fact\n",
            "that these improvements are marginal is likely due to the small surface area the anomaly\n",
            "location comprises, despite being key for robust segmentation.\n",
            "Table 5: Marginal improvements in Dice and ASD metrics of the aorta, observed after our\n",
            "joint training. These contain our averaged results for all three training rounds. We highlight\n",
            "in bold our best metrics (higher=better for dice, lower=better for ASD).\n",
            "Attention U-Net LP + Man Attention U-Net LP + Man + Classifier\n",
            "Aorta Dice Aorta ASD Aorta Dice Aorta ASD\n",
            "CoA 0.74 (0.04) 0.82 (0.15) 0.74 (0.04) 0.81 (0.14)\n",
            "RAA 0.77 (0.02) 0.83 (0.10) 0.77 (0.03) 0.85 (0.11)\n",
            "DAA 0.76 (0.02) 0.91 (0.09) 0.77 (0.02) 0.87 (0.07)\n",
            "426Multi-task learning for anomaly segmentation\n",
            "Regarding our binary framework ( Attention U-Net Man ), we find that adding a classifier\n",
            "(DenseBin ) affects the segmentation output with both improvements observed in double\n",
            "arch segmentation (Fig. 16) and degradation in certain cases such as heightened vessel\n",
            "merging.\n",
            "GT Attention U-Net Man Attention U-Net Man + Classifier \n",
            "Double arch Unsegmented Fully \n",
            "segmented\n",
            "Figure 16: Improvements observed in double arch segmentation after adding a classifier\n",
            "(DenseBin ) to our binary framework.\n",
            "We postulate that this difference in performance is due to both (a) the anomaly clas-\n",
            "sifier being presented with a more challenging task (harder to identify aorta and anomaly\n",
            "area from a single vessel), and (b) labelling variations due to inter-observer variability,\n",
            "contrasting with our propagated atlas labels.\n",
            "4.3 Image quality analysis\n",
            "In order to study the impact image quality has on our predictions, a concise analysis of\n",
            "image quality against quantitative network performance is included here.\n",
            "Two factors are considered: the reconstruction process (SVR Kuklisova-Murgasova et al.\n",
            "(2012); Kainz et al. (2015) or DSVR Uus et al. (2020)), and a manual image quality as-\n",
            "sessment. The latter consists of manually revising and scoring the test set images from\n",
            "1-3 (best to worst), based on image noise, visibility, and vessel sharpness, with 1 being the\n",
            "highest quality, 2 being average quality, and 3 being poor quality and visibility.\n",
            "A note of consideration is that our test set is majoritarily formed of SVR reconstructions\n",
            "(lower quality, N=102 across all three repeated training rounds), with only 18 DSVR cases.\n",
            "Fig. 17b includes precision and sensitivity scores of the vessels’ ROI comparing perfor-\n",
            "mance for our manual image quality assessment, with Fig. 17a depicting Dice scores for the\n",
            "vessels’ ROI and aortic arch region. We generally observe a higher number of lower-scoring\n",
            "outliers in lower-quality images (score = 3), however, we find differences between groups to\n",
            "be statistically non-significant.\n",
            "Regarding performance on image reconstruction type (Fig. 18), we find a statistically\n",
            "significant difference (p-value = 3 .97×10−4for a Mann-Whitney U test) in performance\n",
            "in HD95 of the vessels’ ROI (Fig. 18b), with DSVR cases presenting lower scores (better\n",
            "performance).\n",
            "Concerning classifier performance, we find all misclassifications from DenseMulti and\n",
            "DenseImgMulti to be exclusively SVR reconstructions.\n",
            "427Ramirez et al.\n",
            "(a) Dice scores.\n",
            " (b) Precision and sensitivity.\n",
            "Figure 17: Test set metrics comparing the impact image quality (1 →3 = best →worst) has\n",
            "on network performance.\n",
            "(a) Dice scores.\n",
            "4.55\n",
            "3.90\n",
            "3.25\n",
            "2.60\n",
            "1.95\n",
            "1.30\n",
            "0.65\n",
            " (b) HD95 (lower=better) scores.\n",
            "Figure 18: Test set metrics comparing performance for DSVR and SVR reconstructions.\n",
            "4.4 Qualitative results\n",
            "We find that the quantitative metrics presented in previous sections do not always capture\n",
            "the topological and anatomical correctness of the anomaly area, which is key for our clinical\n",
            "application. Accordingly, we present a qualitative topological anomaly area analysis for our\n",
            "segmentation predictions (Sec. 3.4 Tab. 2). Fig. 19 compares our main ablation studies for\n",
            "classifier inclusion and labelling information for all diagnoses, with our full framework being\n",
            "Attention U-Net LP + Man + Classifier . We include the performance on DAA test\n",
            "set exclusively in Fig. 19 left. See Appendix B.1 for similar CoA and RAA plots.\n",
            "Figs. 19 clearly demonstrate the added value of using multi-class propagated labels, as\n",
            "Attention U-Net LP + Man increases the number of topologically correct segmentations\n",
            "over Attention U-Net Man by 68%.\n",
            "428Multi-task learning for anomaly segmentation\n",
            "Figure 19: Qualitative analysis results on topology correctness of the anomaly area (aortic\n",
            "arch) on our test set for all diagnoses (left), and for DAA cases only (right). Each training\n",
            "experiment was repeated three times, and we include all of our results here (i.e. we have\n",
            "three sets of results for each of our 40 test set cases, one for each round, so the total number\n",
            "of values for each technique is 120)\n",
            ".\n",
            "The top performing experiment regarding the topological correctness of the\n",
            "anomaly area is Attention U-Net LP + Man + Classifier (DenseMulti) , our\n",
            "final framework. We find important improvements by adding a classifier to our multi-class\n",
            "framework, particularly regarding cases with a score of 3. These improvements are predom-\n",
            "inantly DAA cases which initially present a split or unsegmented double arch, corrected by\n",
            "adding a classifier (see Fig. 19 (right) and Sec. 4.2.3).\n",
            "Overall, we still observe some cases with a score of 2, and one with a score of 3 (which\n",
            "includes misclassified subjects) in our full framework ( Attention U-Net LP + Man + Clas-\n",
            "sifier ). We inspect these cases and find that they are the same three subjects with poor\n",
            "segmentation performance across the three experiment training rounds, presenting both\n",
            "poor image quality and misalignments to the atlases. See Appendix B.1 for extended anal-\n",
            "ysis and visualisation of these cases.\n",
            "An important disadvantage of the binary segmentation network is vessel merging, in\n",
            "extreme cases producing an indiscernible aorta (Fig. 20). Interestingly, we find the addition\n",
            "of a classifier ( DenseBin ) to our binary framework ( Attention U-Net Man + Classifier )\n",
            "to generally degrade performance by heightening this aortic merging problem. Nonetheless,\n",
            "we do observe improvement in certain correctly classed DAA cases (see Sec. 4.2.3), albeit\n",
            "not being as consistent and robust as our multi-class improvement.\n",
            "429Ramirez et al.\n",
            "GT Attention U-Net Man\n",
            "Merged aortic arch\n",
            "Figure 20: Extreme test set case with aortic merging issue observed.\n",
            "5. Discussion\n",
            "We present a multi-task deep learning framework for multi-label fetal cardiac vessel seg-\n",
            "mentation and anomaly classification in T2w 3D fetal MRI.\n",
            "Fetal CMR has only recently demonstrated its potential (Lloyd et al., 2019), due to\n",
            "the highly specific challenges this data presents. Given this, the field standards regarding\n",
            "automated segmentation performance have yet to be set. Notably, our dataset requires\n",
            "fast acquisition protocols and motion correction and reconstruction algorithms (Uus et al.,\n",
            "2020), due to the inherent motion corruption present in fetal imaging (fetal motion, rapid\n",
            "fetal heartbeat, maternal motion), as well as poor resolution and contrast.\n",
            "Additionally, there is a range of anatomical variability within each anomaly, as well as\n",
            "differing vessel sizes, from the descending aorta which is easily discernible to small head\n",
            "and neck vessels which are sometimes absent even in the GT due to low visibility.\n",
            "Our specific dataset presents a range of image qualities. We manually score our test set\n",
            "images (based on vessel visibility, artefacts and noise), to provide further explainability into\n",
            "potential failed cases. Our test set comprises 12 high quality subjects, 16 average quality,\n",
            "and 12 low quality images. Therefore, a high segmentation scoring performance is extremely\n",
            "challenging, particularly for small vessels in low quality images, or for cases with important\n",
            "anatomical variability.\n",
            "5.1 Segmentation training label type ablation\n",
            "Our labelling experiments highlight two important points: (1) adding manual labels sub-\n",
            "stantially increases network performance, and (2) multi-class information is crucial for small\n",
            "vessel detection, as well as anomaly area segmentation. We find significantly increased sim-\n",
            "ilarity metrics after including the binary manual labels, with a similar qualitative topology\n",
            "score.\n",
            "We find important anatomical inconsistencies in our binary segmentations, including\n",
            "merged vessels, contrary to our proposed multi-class networks. We postulate that this\n",
            "is due to our multi-class network learning each label individually, therefore ensuring that\n",
            "each vessel is present and distinguishable in the final network. The consistency in our\n",
            "430Multi-task learning for anomaly segmentation\n",
            "propagated atlas labels may also contribute towards this, which contrasts with the inter-\n",
            "observer variability present in the binary manual labels.\n",
            "The oversegmentation we observed both qualitatively and quantitatively from our binary\n",
            "network is detrimental to clinical usability, as this causes vessel merging which creates\n",
            "indistinguishable vessel topology.\n",
            "An important point from our analysis is that we find commonly used quantitative metrics\n",
            "to not be fully descriptive of segmentation performance regarding topological and anatom-\n",
            "ical correctness, hence our qualitative evaluation. Therefore there is a need for future work\n",
            "to explore more fitting topological metrics (Hu et al., 2019). The inclusion of topology-\n",
            "aware losses (Clough et al., 2020; Byrne et al., 2021; Clough et al., 2019; Hu et al., 2019) is\n",
            "also an important aspect for future work to interrogate, given the importance of generating\n",
            "topologically correct predictions for our task.\n",
            "Noteworthy, the segmentation task in the present paper could have been tackled fol-\n",
            "lowing alternative training strategies. A potentially simple solution could be to leverage\n",
            "propagated labels to train a splitting network to convert binary labels to multiple labels.\n",
            "Appendix A contains two additional experiments, that utilise such splitting network. In\n",
            "the first experiment, the splitting network is applied to binary labels predicted by Attention\n",
            "U-Net Man , while in the second experiment, the splitting network converts manual binary\n",
            "labels to multi-label segmentations to provide training data for a simple supervised Atten-\n",
            "tion U-Net. We found these strategies to generally lead to more topological errors in vessel\n",
            "segmentations than for our proposed framework, see Fig. 24. In particular, we observed\n",
            "more mislabelled vessels in regions with poor visibility due to imaging artefacts.\n",
            "5.2 Weak supervision\n",
            "The weak supervision ablation study demonstrates that, although our manually generated\n",
            "labels are not fully labelled (as the vessels are represented as a single entity, as opposed to\n",
            "multi-class labels), their addition to the segmenter training framework improves segmenta-\n",
            "tion performance. Even the addition of manually generated labels to only 5% of the training\n",
            "data (for each diagnosis) supposes a statistically significant improvement for the aortic arch\n",
            "label.\n",
            "The highest performance is achieved via our proposed segmentation framework: Atten-\n",
            "tion U-Net LP + Man .\n",
            "5.3 Classification of anomalies\n",
            "We find one explainable mislabelled test case per training round for our DenseMulti\n",
            "classifier (multi-class), and one misclassification overall for DenseImgMulti , with notable\n",
            "improvements in the anomaly area segmentation after joint network training. The addition\n",
            "of a classifier to our multi-class segmentation pipeline consistently improves the topological\n",
            "correctness of all correctly classified DAA cases with segmentation issues, as reflected by\n",
            "our qualitative analysis. It is important to note that the stability of our multi-task training\n",
            "framework depends on accurate hyperparameter tuning of the loss weight balancing in each\n",
            "experiment.\n",
            "Prior knowledge of the anomaly will always exist in a clinical situation, and therefore\n",
            "misclassifications in DenseMulti aid the identification of cases with faulty segmentation\n",
            "431Ramirez et al.\n",
            "predictions. DenseImgMulti , although being the most accurate classifier, would there-\n",
            "fore be less relevant to this application, given its inability to detect topologically incorrect\n",
            "segmentation predictions. It is therefore more suitable as a diagnostic support tool.\n",
            "An inherent limitation of our approach is the fact that the classifier learns distinc-\n",
            "tive anomaly-specific features, which restricts the flexibility of the segmentation topology.\n",
            "Therefore, this framework may not generalise well to data which deviate strongly from the\n",
            "training data regarding image quality or anatomical variations. The generalisability of our\n",
            "framework to out-of-distribution cases is therefore a valuable avenue for future work to\n",
            "explore.\n",
            "The worst performing cases in our test set comprise lower quality SVR reconstructions,\n",
            "that only correct rigid motion, and therefore do not account for deformations in fetal move-\n",
            "ment, which may result in blurring of the reconstructed 3D images. This was combined\n",
            "with noticable anatomical deviation from the atlases. We find one extreme case where the\n",
            "addition of a classifier accentuated an initial segmentation issue, by fully segmenting the\n",
            "left arch in an RAA case (Fig. 12). This is an extreme case where we observe the limitation\n",
            "of somewhat topologically constraining our segmentation to a set of learnt distinct features:\n",
            "if there is a low quality case, with poor visibility, the addition of the classifier will push\n",
            "the segmentation towards pertaining to one specific anomaly. Recent advancements in re-\n",
            "construction techniques (Uus et al., 2020) may ameliorate this problem, as we find that all\n",
            "of our high quality DSVR reconstructions achieve high accuracy in both segmentation and\n",
            "anomaly classification.\n",
            "Highly consistent multi-class labels are a key contributor towards our success. We\n",
            "observe a performance degradation in our binary framework ( Attention U-Net Man + Clas-\n",
            "sifier ), likely due to the more challenging anomaly classifier task: the aorta is not classed as\n",
            "an individual vessel, and the anomaly area occupies a relatively small region in the whole\n",
            "vessels’ ROI label. Additionally, the consistency divergence between manually segmented\n",
            "binary labels (inter-observer variability, shortened vessel length for lower quality images)\n",
            "and propagated atlas labels, as well as the lower-quality segmentation predictions of Atten-\n",
            "tion U-Net Man (vessel merging and undetected head and neck vessels) contribute towards\n",
            "making our classifier ( DenseBin ) training data more varied. This impedes the classifier\n",
            "from learning representative aortic arch anomaly features.\n",
            "Although our work is developed to suit a very specific dataset situation (DSVR and SVR\n",
            "fetal CMR reconstructions, anomaly-specific atlases, and partially labelled training set), our\n",
            "joint classification and segmentation framework may be applicable to many environments\n",
            "with a dual task, or for segmentation improvements in a network trained on distinctly varied\n",
            "data. We note also that our task has highly specific challenges, and therefore success in our\n",
            "application is promising for generalisation to less challenging tasks (e.g. fewer label classes,\n",
            "larger segmentation area, less diverging anomalies). Additionally, our work addresses a\n",
            "partially labelled dataset, largely common in the medical imaging field.\n",
            "6. Conclusion\n",
            "We present a multi-task approach for joint fetal cardiac vessel segmentation and aortic arch\n",
            "anomaly classification from T2w 3D MRI. We combine deep learning label propagation from\n",
            "anomaly-specific atlases with Attention U-Net segmentation and DenseNet121 classification.\n",
            "432Multi-task learning for anomaly segmentation\n",
            "We demonstrate a potential application of our segmentation tool for automated diagnosis,\n",
            "by jointly training an anomaly classifier on our output segmentations. Our multi-task\n",
            "approach improves the anomaly area segmentation performance, providing both multi-class\n",
            "segmentations and aortic arch anomaly classification. Our strategy is simple and innovative,\n",
            "and our work has strong clinical applicability, being highly novel regarding our dataset. Our\n",
            "clinical contribution is both to aid vessel visualisation for clinical reporting purposes and to\n",
            "aid diagnostic confidence and efficiency. Our automated multi-task tool may also be useful\n",
            "in training non-expert clinicians, which is challenging due to the highly specific cardiac\n",
            "anomalies, low dataset visibility, and anatomical variability between cases.\n",
            "Acknowledgments\n",
            "We would like to acknowledge funding from the EPSRC Centre for Doctoral Training in\n",
            "Smart Medical Imaging (EP/S022104/1).\n",
            "We thank everyone who was involved in the acquisition and examination of the datasets\n",
            "and all participating mothers. This work was supported by the Rosetrees Trust [A2725],\n",
            "the Wellcome/EPSRC Centre for Medical Engineering at King’s College London [WT\n",
            "203148/Z/16/Z], the Wellcome Trust and EPSRC IEH award [102431] for the iFIND project,\n",
            "the NIHR Clinical Research Facility (CRF) at Guy’s and St Thomas’ and by the National\n",
            "Institute for Health Research Biomedical Research Centre based at Guy’s and St Thomas’\n",
            "NHS Foundation Trust and King’s College London. The views expressed are those of the\n",
            "authors and not necessarily those of the NHS, the NIHR or the Department of Health.\n",
            "Ethical Standards\n",
            "All fetal MRI datasets used in this work were processed subject to informed consent of the\n",
            "participants [REC: 07/H0707/105; REC: 14/LO/1806].\n",
            "The work follows appropriate ethical standards in conducting research and writing the\n",
            "manuscript, following all applicable laws and regulations regarding treatment of animals or\n",
            "human subjects.\n",
            "Conflicts of Interest\n",
            "We declare we don’t have conflicts of interest.\n",
            "Data availability\n",
            "The individual fetal MRI datasets used for this study are not publicly available due to\n",
            "ethics regulations.\n",
            "433Ramirez et al.\n",
            "References\n",
            "Paul Aljabar, Rolf A Heckemann, Alexander Hammers, Joseph V Hajnal, and Daniel Rueck-\n",
            "ert. Multi-atlas based segmentation of brain images: atlas selection and its effect on\n",
            "accuracy. Neuroimage , 46(3):726–738, 2009.\n",
            "Arghavan Arafati, Peng Hu, J Paul Finn, Carsten Rickers, Andrew L Cheng, Hamid Ja-\n",
            "farkhani, and Arash Kheradvar. Artificial intelligence in pediatric and adult congenital\n",
            "cardiac mri: an unmet clinical need. Cardiovascular diagnosis and therapy , 9(Suppl 2):\n",
            "S310, 2019.\n",
            "Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Guttag, and Adrian V Dalca.\n",
            "Voxelmorph: a learning framework for deformable medical image registration. IEEE\n",
            "transactions on medical imaging , 38(8):1788–1800, 2019.\n",
            "Kate L Brown, Deborah A Ridout, Aparna Hoskote, Lynda Verhulst, Marco Ricci, and\n",
            "Catherine Bull. Delayed diagnosis of congenital heart disease worsens preoperative con-\n",
            "dition and outcome of surgery in neonates. Heart , 92(9):1298–1302, 2006.\n",
            "Nick Byrne, James R Clough, Giovanni Montana, and Andrew P King. A persistent\n",
            "homology-based topological loss function for multi-class cnn segmentation of cardiac mri.\n",
            "InStatistical Atlases and Computational Models of the Heart. M&Ms and EMIDEC Chal-\n",
            "lenges: 11th International Workshop, STACOM 2020, Held in Conjunction with MICCAI\n",
            "2020, Lima, Peru, October 4, 2020, Revised Selected Papers 11 , pages 3–13. Springer,\n",
            "2021.\n",
            "Chen Chen, Chen Qin, Huaqi Qiu, Giacomo Tarroni, Jinming Duan, Wenjia Bai, and\n",
            "Daniel Rueckert. Deep learning for cardiac image segmentation: a review. Frontiers in\n",
            "Cardiovascular Medicine , 7:25, 2020.\n",
            "James R Clough, Ilkay Oksuz, Nicholas Byrne, Julia A Schnabel, and Andrew P King.\n",
            "Explicit topological priors for deep-learning based image segmentation using persistent\n",
            "homology. In Information Processing in Medical Imaging: 26th International Conference,\n",
            "IPMI 2019, Hong Kong, China, June 2–7, 2019, Proceedings 26 , pages 16–28. Springer,\n",
            "2019.\n",
            "James R Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A Zimmer, Julia A Schnabel, and\n",
            "Andrew P King. A topological loss function for deep-learning based image segmenta-\n",
            "tion using persistent homology. IEEE Transactions on Pattern Analysis and Machine\n",
            "Intelligence , 44(12):8766–8778, 2020.\n",
            "GR DeVore, P Falkensammer, MS Sklansky, and LD Platt. Spatio-temporal image correla-\n",
            "tion (stic): new technology for evaluation of the fetal heart. Ultrasound in Obstetrics and\n",
            "Gynecology: The Official Journal of the International Society of Ultrasound in Obstetrics\n",
            "and Gynecology , 22(4):380–387, 2003.\n",
            "Nicola K Dinsdale, Mark Jenkinson, and Ana IL Namburete. Spatial warping network\n",
            "for 3D segmentation of the hippocampus in mr images. In International Conference on\n",
            "434Multi-task learning for anomaly segmentation\n",
            "Medical Image Computing and Computer-Assisted Intervention , pages 284–291. Springer,\n",
            "2019.\n",
            "Su-Zhen Dong and Ming Zhu. Utility of fetal cardiac magnetic resonance imaging to assess\n",
            "fetuses with right aortic arch and right ductus arteriosus. The Journal of Maternal-Fetal\n",
            "& Neonatal Medicine , 31(12):1627–1631, 2018.\n",
            "Su-Zhen Dong, Ming Zhu, Hui Ji, Jing-Ya Ren, and Ke Liu. Fetal cardiac mri: a single\n",
            "center experience over 14-years on the potential utility as an adjunct to fetal technically\n",
            "inadequate echocardiography. Scientific Reports , 10(1):1–10, 2020.\n",
            "Michael Ebner, Guotai Wang, Wenqi Li, Michael Aertsen, Premal A Patel, Rosalind Augh-\n",
            "wane, Andrew Melbourne, Tom Doel, Steven Dymarkowski, Paolo De Coppi, et al. An\n",
            "automated framework for localization, segmentation and super-resolution reconstruction\n",
            "of fetal brain MRI. NeuroImage , 206:116324, 2020.\n",
            "Irina Grigorescu, Alena Uus, Daan Christiaens, Lucilio Cordero-Grande, Jana Hutter,\n",
            "A David Edwards, Joseph V Hajnal, Marc Modat, and Maria Deprez. Diffusion ten-\n",
            "sor driven image registration: a deep learning approach. In International Workshop on\n",
            "Biomedical Image Registration , pages 131–140. Springer, 2020.\n",
            "Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett\n",
            "Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3D medical image\n",
            "segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of\n",
            "Computer Vision , pages 574–584, 2022.\n",
            "Rolf A Heckemann, Joseph V Hajnal, Paul Aljabar, Daniel Rueckert, and Alexander Ham-\n",
            "mers. Automatic anatomical brain mri segmentation combining label propagation and\n",
            "decision fusion. NeuroImage , 33(1):115–126, 2006.\n",
            "Mohammad Hesam Hesamian, Wenjing Jia, Xiangjian He, and Paul Kennedy. Deep learning\n",
            "techniques for medical image segmentation: achievements and challenges. Journal of\n",
            "digital imaging , 32(4):582–596, 2019.\n",
            "Xiaoling Hu, Fuxin Li, Dimitris Samaras, and Chao Chen. Topology-preserving deep image\n",
            "segmentation. Advances in neural information processing systems , 32, 2019.\n",
            "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely\n",
            "connected convolutional networks. In Proceedings of the IEEE conference on computer\n",
            "vision and pattern recognition , pages 4700–4708, 2017.\n",
            "Fabian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul F Jaeger, Simon Kohl,\n",
            "Jakob Wasserthal, Gregor Koehler, Tobias Norajitra, Sebastian Wirkert, et al. nnu-net:\n",
            "Self-adapting framework for u-net-based medical image segmentation. arXiv preprint\n",
            "arXiv:1809.10486 , 2018.\n",
            "Bernhard Kainz, Markus Steinberger, Wolfgang Wein, Maria Kuklisova-Murgasova,\n",
            "Christina Malamateniou, Kevin Keraudren, Thomas Torsney-Weir, Mary Rutherford,\n",
            "Paul Aljabar, Joseph V Hajnal, et al. Fast volume reconstruction from motion corrupted\n",
            "stacks of 2d slices. IEEE transactions on medical imaging , 34(9):1901–1913, 2015.\n",
            "435Ramirez et al.\n",
            "Kevin Keraudren, Maria Kuklisova-Murgasova, Vanessa Kyriakopoulou, Christina Mala-\n",
            "mateniou, Mary A Rutherford, Bernhard Kainz, Joseph V Hajnal, and Daniel Rueckert.\n",
            "Automated fetal brain segmentation from 2d MRI slices for motion correction. NeuroIm-\n",
            "age, 101:633–643, 2014.\n",
            "Nadieh Khalili, Nikolas Lessmann, Elise Turk, N Claessens, Roel de Heus, Tessel Kolk,\n",
            "Max A Viergever, Manon JNL Benders, and Ivana Iˇ sgum. Automatic brain tissue seg-\n",
            "mentation in fetal MRI using convolutional neural networks. Magnetic resonance imaging ,\n",
            "64:77–89, 2019.\n",
            "Maria Kuklisova-Murgasova, Gerardine Quaghebeur, Mary A Rutherford, Joseph V Hajnal,\n",
            "and Julia A Schnabel. Reconstruction of fetal brain MRI with intensity matching and\n",
            "complete outlier removal. Medical image analysis , 16(8):1550–1564, 2012.\n",
            "Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, and Yong Liu. Understand scene categories\n",
            "by objects: A semantic regularized scene classifier using convolutional neural networks.\n",
            "In2016 IEEE international conference on robotics and automation (ICRA) , pages 2318–\n",
            "2325. IEEE, 2016.\n",
            "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ ar. Focal loss for\n",
            "dense object detection. In Proceedings of the IEEE international conference on computer\n",
            "vision , pages 2980–2988, 2017.\n",
            "David FA Lloyd, Kuberan Pushparajah, John M Simpson, Joshua FP Van Amerom,\n",
            "Milou PM Van Poppel, Alexander Schulz, Bernard Kainz, Maria Deprez, Maelene Lo-\n",
            "hezic, Joanna Allsop, et al. Three-dimensional visualisation of the fetal heart using pre-\n",
            "natal MRI with motion-corrected slice-volume registration: a prospective, single-centre\n",
            "cohort study. The Lancet , 393(10181):1619–1627, 2019.\n",
            "Mjaye L Mazwi, David W Brown, Audrey C Marshall, Frank A Pigula, Peter C Laussen,\n",
            "Angelo Polito, David Wypij, and John M Costello. Unplanned reinterventions are associ-\n",
            "ated with postoperative mortality in neonates with critical congenital heart disease. The\n",
            "Journal of thoracic and cardiovascular surgery , 145(3):671–677, 2013.\n",
            "Sachin Mehta, Ezgi Mercan, Jamen Bartlett, Donald Weaver, Joann G Elmore, and Linda\n",
            "Shapiro. Y-net: joint segmentation and classification for diagnosis of breast biopsy im-\n",
            "ages. In International Conference on Medical Image Computing and Computer-Assisted\n",
            "Intervention , pages 893–901. Springer, 2018.\n",
            "Shanthi Mendis, Pekka Puska, Bo Norrving, World Health Organization, et al. Global atlas\n",
            "on cardiovascular disease prevention and control . World Health Organization, 2011.\n",
            "Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari\n",
            "Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. At-\n",
            "tention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999 ,\n",
            "2018.\n",
            "Mahesh R Patel, Roman A Klufas, Ronald A Alberico, and Robert R Edelman. Half-fourier\n",
            "acquisition single-shot turbo spin-echo (haste) mr: comparison with fast spin-echo mr in\n",
            "diseases of the brain. American journal of neuroradiology , 18(9):1635–1640, 1997.\n",
            "436Multi-task learning for anomaly segmentation\n",
            "Kelly Payette, Raimund Kottke, and Andras Jakab. Efficient multi-class fetal brain segmen-\n",
            "tation in high resolution MRI reconstructions with noisy labels. In Medical Ultrasound,\n",
            "and Preterm, Perinatal and Paediatric Image Analysis , pages 295–304. Springer, 2020.\n",
            "Jialin Peng and Ye Wang. Medical image segmentation with limited supervision: a review\n",
            "of deep network models. IEEE Access , 9:36827–36851, 2021.\n",
            "Esther Puyol-Ant´ on, Bram Ruijsink, Stefan K Piechnik, Stefan Neubauer, Steffen E Pe-\n",
            "tersen, Reza Razavi, and Andrew P King. Fairness in cardiac mr image analysis: an\n",
            "investigation of bias due to data imbalance in deep learning based segmentation. In\n",
            "International Conference on Medical Image Computing and Computer-Assisted Interven-\n",
            "tion, pages 413–423. Springer, 2021.\n",
            "Paula Ramirez Gilliland, Alena Uus, Milou PM van Poppel, Irina Grigorescu, Johannes K\n",
            "Steinweg, David FA Lloyd, Kuberan Pushparajah, Andrew P King, and Maria Deprez.\n",
            "Automated multi-class fetal cardiac vessel segmentation in aortic arch anomalies using t2-\n",
            "weighted 3d fetal mri. In International Workshop on Preterm, Perinatal and Paediatric\n",
            "Image Analysis , pages 82–93. Springer, 2022.\n",
            "Mina Rezaei, Haojin Yang, and Christoph Meinel. Whole heart and great vessel segmen-\n",
            "tation with context-aware of generative adversarial networks. In Bildverarbeitung f¨ ur die\n",
            "Medizin 2018 , pages 353–358. Springer, 2018.\n",
            "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\n",
            "biomedical image segmentation. In International Conference on Medical image computing\n",
            "and computer-assisted intervention , pages 234–241. Springer, 2015.\n",
            "Daniel Rueckert, Luke I Sonoda, Erica RE Denton, S Rankin, Carmel Hayes, Martin O\n",
            "Leach, Derek LG Hill, and David John Hawkes. Comparison and evaluation of rigid and\n",
            "nonrigid registration of breast mr images. In Medical Imaging 1999: Image Processing ,\n",
            "volume 3661, pages 78–88. International Society for Optics and Photonics, 1999a.\n",
            "Daniel Rueckert, Luke I Sonoda, Carmel Hayes, Derek LG Hill, Martin O Leach, and David J\n",
            "Hawkes. Nonrigid registration using free-form deformations: application to breast mr\n",
            "images. IEEE transactions on medical imaging , 18(8):712–721, 1999b.\n",
            "Daniel Salehi, Katrin Fricke, Misha Bhat, H˚ akan Arheden, Petru Liuba, and Erik Hedstr¨ om.\n",
            "Utility of fetal cardiovascular magnetic resonance for prenatal diagnosis of complex con-\n",
            "genital heart defects. JAMA network open , 4(3):e213538–e213538, 2021.\n",
            "Seyed Sadegh Mohseni Salehi, Seyed Raein Hashemi, Clemente Velasco-Annis, Abdelhakim\n",
            "Ouaalam, Judy A Estroff, Deniz Erdogmus, Simon K Warfield, and Ali Gholipour. Real-\n",
            "time automatic fetal brain extraction in fetal MRI by deep learning. In 2018 IEEE\n",
            "15th International Symposium on Biomedical Imaging (ISBI 2018) , pages 720–724. IEEE,\n",
            "2018.\n",
            "Julia A Schnabel, Daniel Rueckert, Marcel Quist, Jane M Blackall, Andy D Castellano-\n",
            "Smith, Thomas Hartkens, Graeme P Penney, Walter A Hall, Haiying Liu, Charles L\n",
            "437Ramirez et al.\n",
            "Truwit, et al. A generic framework for non-rigid registration based on non-uniform multi-\n",
            "level free-form deformations. In International Conference on Medical Image Computing\n",
            "and Computer-Assisted Intervention , pages 573–581. Springer, 2001.\n",
            "Richard C Semelka, Nikolaos L Kelekis, David Thomasson, Mark A Brown, and Gerhard A\n",
            "Laub. Haste mr imaging: description of technique and preliminary results in the abdomen.\n",
            "Journal of Magnetic Resonance Imaging , 6(4):698–699, 1996.\n",
            "Matthew Sinclair, Andreas Schuh, Karl Hahn, Kersten Petersen, Ying Bai, James Batten,\n",
            "Michiel Schaap, and Ben Glocker. Atlas-istn: Joint segmentation, registration and atlas\n",
            "construction with image-and-spatial transformer networks. Medical Image Analysis , 78:\n",
            "102383, 2022.\n",
            "Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M Jorge Cardoso.\n",
            "Generalised dice overlap as a deep learning loss function for highly unbalanced segmen-\n",
            "tations. In Deep learning in medical image analysis and multimodal learning for clinical\n",
            "decision support , pages 240–248. Springer, 2017.\n",
            "Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, and Raquel Urtasun.\n",
            "Multinet: Real-time joint semantic reasoning for autonomous driving. In 2018 IEEE\n",
            "intelligent vehicles symposium (IV) , pages 1013–1020. IEEE, 2018.\n",
            "Alena Uus, Tong Zhang, Laurence H Jackson, Thomas A Roberts, Mary A Rutherford,\n",
            "Joseph V Hajnal, and Maria Deprez. Deformable slice-to-volume registration for motion\n",
            "correction of fetal body and placenta mri. IEEE transactions on medical imaging , 39(9):\n",
            "2750–2759, 2020.\n",
            "Alena Uus, Irina Grigorescu, Milou van Poppel, Emer Hughes, Johannes Steinweg, Thomas\n",
            "Roberts, David Lloyd, Kuberan Pushparajah, and Maria Deprez. 3D unet with gan\n",
            "discriminator for robust localisation of the fetal brain and trunk in MRI with partial\n",
            "coverage of the fetal body. bioRxiv , 2021.\n",
            "Alena U. Uus, Irina Grigorescu, Milou P.M. van Poppel, Johannes K. Steinweg, Thomas A.\n",
            "Roberts, Mary A. Rutherford, Joseph V. Hajnal, David F.A. Lloyd, Kuberan Pushpara-\n",
            "jah, and Maria Deprez. Automated 3d reconstruction of the fetal thorax in the standard\n",
            "atlas space from motion-corrupted mri stacks for 21–36 weeks ga range. Medical Image\n",
            "Analysis , 80:102484, 2022a. ISSN 1361-8415. . URL https://www.sciencedirect.com/\n",
            "science/article/pii/S1361841522001311 .\n",
            "Alena U Uus, Milou PM van Poppel, Johannes K Steinweg, Irina Grigorescu, Paula\n",
            "Ramirez Gilliland, Thomas A Roberts, Alexia Egloff Collado, Mary A Rutherford,\n",
            "Joseph V Hajnal, David FA Lloyd, et al. 3d black blood cardiovascular magnetic reso-\n",
            "nance atlases of congenital aortic arch anomalies and the normal fetal heart: application\n",
            "to automated multi-label segmentation. Journal of Cardiovascular Magnetic Resonance ,\n",
            "24(1):1–13, 2022b.\n",
            "Xiaowei Xu, Tianchen Wang, Yiyu Shi, Haiyun Yuan, Qianjun Jia, Meiping Huang, and\n",
            "Jian Zhuang. Whole heart and great vessel segmentation in congenital heart disease using\n",
            "438Multi-task learning for anomaly segmentation\n",
            "deep neural networks and graph matching. In International Conference on Medical Image\n",
            "Computing and Computer-Assisted Intervention , pages 477–485. Springer, 2019.\n",
            "Zhenlin Xu and Marc Niethammer. Deepatlas: Joint semi-supervised learning of image\n",
            "registration and segmentation. In International Conference on Medical Image Computing\n",
            "and Computer-Assisted Intervention , pages 420–429. Springer, 2019.\n",
            "Lequan Yu, Xin Yang, Jing Qin, and Pheng-Ann Heng. 3D fractalnet: dense volumetric\n",
            "segmentation for cardiovascular MRI volumes. In Reconstruction, segmentation, and\n",
            "analysis of medical images , pages 103–110. Springer, 2016.\n",
            "Paul A. Yushkevich, Joseph Piven, Heather Cody Hazlett, Rachel Gimpel Smith, Sean\n",
            "Ho, James C. Gee, and Guido Gerig. User-guided 3D active contour segmentation of\n",
            "anatomical structures: Significantly improved efficiency and reliability. Neuroimage , 31\n",
            "(3):1116–1128, 2006.\n",
            "Amy Zhao, Guha Balakrishnan, Fredo Durand, John V Guttag, and Adrian V Dalca. Data\n",
            "augmentation using learned transformations for one-shot medical image segmentation.\n",
            "InProceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\n",
            "pages 8543–8553, 2019.\n",
            "439Ramirez et al.\n",
            "Appendix A. Alternative segmentation strategies\n",
            "Here we present two additional approaches to our proposed segmentation strategy. These\n",
            "represent the alternative way of combining our dataset labels (multi-class propagated labels\n",
            "and manually generated binary labels). The strategy behind these new approaches is to\n",
            "learn to split binary labels into multi-class labels. We examine the following:\n",
            "1.Attention U-Net Man + Split : Attention U-Net that splits the predicted CNN\n",
            "binary labels from Attention U-Net Man into multi-class labels.\n",
            "2.Attention U-Net Split : two-step strategy, that involves first splitting the manu-\n",
            "ally segmented binary labels into multi-class (with a CNN), followed by training an\n",
            "additional Attention U-Net using these split labels.\n",
            "Both these approaches rely on a trained splitting network. This splitting network is\n",
            "trained on joined propagated labels as input (binary) and learns to split these into multi-\n",
            "class, thereby learning to preserve the shape of the input segmentation.\n",
            "ForAttention U-Net Man + Split , we use Attention U-Net Man to generate the binary\n",
            "labels at inference time, see Fig. 21.\n",
            "T2w 3D input image\n",
            "Attention U-Net Man Attention U-Net \n",
            "Man + SplitBinary predictionMulti-class prediction \n",
            "(split vessels)\n",
            "Figure 21: Inference time predictions for Attention U-Net Man + Split , which does not\n",
            "require any labels. A binary prediction is generated for the input image, which is then split.\n",
            "In accordance with our other segmentation experiments, we repeat each experiment\n",
            "three times, to account for network stochasticity.\n",
            "A.1 Quantitative Results\n",
            "An important training disadvantage of these two approaches is that they require twice the\n",
            "computational time compared to our proposed method ( Attention U-Net LP + Man ), given\n",
            "that two CNNs are required instead of just one.\n",
            "Attention U-Net Man + LP andAttention U-Net Man + Split are compared in Fig. 22\n",
            "for the aortic arch (key anomaly area). These results display more extreme results for\n",
            "Attention U-Net Man + Split . A Mann-Whitney U test reveals a statistically significant\n",
            "difference (p-value=8 .23×10−9) between Attention U-Net LP + Man andAttention U-Net\n",
            "Man + Split , however not between Attention U-Net Split andAttention U-Net LP + Man .\n",
            "A.2 Qualitative Results\n",
            "The inherent topological issues observed in the binary network ( Attention U-Net Man ) are\n",
            "propagated to its subsequent segmentation splitting variant, Attention U-Net Man + Split .\n",
            "This causes vessel misclassification, as displayed in two independent test set cases in Fig. 23.\n",
            "Undetected small vessels are also signalled.\n",
            "440Multi-task learning for anomaly segmentation\n",
            "(a) ASD scores (lower = better).\n",
            " (b) Dice scores (higher = better).\n",
            "Figure 22: Quantitative scores for the aortic arch (anomaly area) on test set comparing our\n",
            "proposed labelling type training combination ( Attention U-Net LP + Man ) toAttention\n",
            "U-Net Man + Split . We repeat each experiment three times and include all results here.\n",
            "Attention U-Net Man + Split takes the output of Attention U-Net Man as input and\n",
            "performs vessel splitting, learning to preserve the shape of the input segmentation. This\n",
            "leads to the perpetuation of the existing topological problems in the predicted segmentation.\n",
            "Moreover, when vessel merging occurs, the splitting network further hampers performance\n",
            "by misclassifying merged vessels. This vessel misclassification issue is not as persistent in\n",
            "Attention U-Net Split , the network which is trained on split manual labels, however still\n",
            "occurs on lower quality cases, particularly cases presenting challenging discernment between\n",
            "aorta and MPA.\n",
            "We quantify this by conducting a qualitative assessment of the segmentation topology\n",
            "area (see Section 3.4.2 for definition). We include scores for all vessels and for the anomaly\n",
            "area in Fig. 24. This showcases the superior performance of our proposed method, followed\n",
            "byAttention U-Net Split .\n",
            "Appendix B. Anomaly classifier\n",
            "Figs. 25a, 25b, 25c, 25e and 25d contain our classifier test set confusion matrices for our\n",
            "three training rounds.\n",
            "B.1 Qualitative analysis extended\n",
            "Here we extend our investigation into the lower-scoring subjects of our final framework\n",
            "(Attention U-Net LP + Man + Classifier withDenseMulti classifier), as assessed by our\n",
            "qualitative analysis (Sec. 4.4). We include our topology scores separated for CoA and RAA\n",
            "in Figs. 26.\n",
            "In total, we find ten nine subjects with a topology score of 2 for the anomaly area, and\n",
            "one with a score of 3. These subjects comprise our two misclassified RAA cases (due to a\n",
            "partially and fully segmented double arch), as well as three CoA cases repeated across all\n",
            "441Ramirez et al.\n",
            "Figure 23: Vessel misclassification issue observed in Attention U-Net Man + Split , with\n",
            "comparison to our proposed labelling combination Attention U-Net LP + Man and manually\n",
            "segmented GT. Predictions are shown for two independent test set cases. Orange stars\n",
            "indicate missing vessels in the splitting network, and white arrows (T2w images) indicate\n",
            "the misclassification area.\n",
            "three rounds. The latter cases include oversegmentation of the aorta into head and neck\n",
            "vessels, and merging into AD. We inspect the images and find lower quality and alignment\n",
            "compared to the atlas. Therefore we conclude that these subjects are outliers due to lower\n",
            "image quality, which is supported by the fact that three independently trained networks\n",
            "present segmentation issues on these subjects. Fig. 27 showcases the three CoA cases with a\n",
            "score of 2, alongside a high-quality correctly segmented case, and our CoA atlas to highlight\n",
            "the image quality differences.\n",
            "B.2 Latent space representations\n",
            "Fig. 28 contains our t-SNE reduced latent space visualisations before and after adding a\n",
            "classifier to our segmentation frameworks, for one of our training rounds. We display both\n",
            "our binary segmentation networks ( Attention U-Net Man ), and our proposed multi-class\n",
            "approach ( Attention U-Net LP + Man ). We observe slightly further clustering after joint\n",
            "training with the anomaly classifier. However, this is not as pronounced as training using\n",
            "multi-class labels as opposed to binary manually segmented labels.\n",
            "442Multi-task learning for anomaly segmentation\n",
            "Figure 24: Qualitative analysis results on topology correctness of the anomaly area (aortic\n",
            "arch) on our test set for all diagnoses (left), and for DAA cases only (right). Each training\n",
            "experiment was repeated three times, and we include all of our results here (i.e. we have\n",
            "three results for each of our 40 test set cases, one for each round, so the total number of\n",
            "cases is 120)\n",
            ".\n",
            "443Ramirez et al.\n",
            "CoA RAA DAA\n",
            "Predicted labelCoA\n",
            "RAA\n",
            "DAATrue label60 0 0\n",
            "0 42 3\n",
            "0 0 15\n",
            "0102030405060\n",
            "(a)DenseMulti confusion matrix, three training\n",
            "rounds (1 misclassification per round).\n",
            "CoA RAA DAA\n",
            "Predicted labelCoA\n",
            "RAA\n",
            "DAATrue label59 0 1\n",
            "0 29 16\n",
            "0 4 11\n",
            "01020304050(b)DenseImage confusion matrix, three train-\n",
            "ing rounds.\n",
            "CoA RAA DAA\n",
            "Predicted labelCoA\n",
            "RAA\n",
            "DAATrue label58 1 1\n",
            "0 40 5\n",
            "0 2 13\n",
            "01020304050\n",
            "(c)DenseBin confusion matrix, three training\n",
            "rounds.\n",
            "CoA RAA DAA\n",
            "Predicted labelCoA\n",
            "RAA\n",
            "DAATrue label57 0 3\n",
            "0 34 11\n",
            "0 0 15\n",
            "01020304050(d)DenseBin before joint training confusion\n",
            "matrix, three training rounds.\n",
            "CoA RAA DAA\n",
            "Predicted labelCoA\n",
            "RAA\n",
            "DAATrue label60 0 0\n",
            "0 45 0\n",
            "0 2 13\n",
            "0102030405060\n",
            "(e)DenseMulti before joint training confusion\n",
            "matrix, three training rounds.\n",
            "444Multi-task learning for anomaly segmentation\n",
            "Figure 26: CoA (left) and RAA (right) test set topology scores.\n",
            "Figure 27: CoA test set cases with a topology score of 2 in our final framework, alongside\n",
            "a high-quality example case and our CoA atlas.\n",
            "445Ramirez et al.\n",
            "Figure 28: t-SNE visualisation of our network bottleneck features before (top row) and\n",
            "after (bottom row) the addition of an anomaly classifier. This is an example of one of our\n",
            "training rounds.\n",
            "446JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\n",
            "Decoupling and Interacting Multi-Task Learning\n",
            "Network for Joint Speech and Accent Recognition\n",
            "Qijie Shao, Pengcheng Guo, Jinghao Yan, Pengfei Hu, Lei Xie, Senior Member, IEEE\n",
            "Abstract —Accents, as variations from standard pronunciation,\n",
            "pose significant challenges for speech recognition systems. Al-\n",
            "though joint automatic speech recognition (ASR) and accent\n",
            "recognition (AR) training has been proven effective in handling\n",
            "multi-accent scenarios, current multi-task ASR-AR approaches\n",
            "overlook the granularity differences between tasks. Fine-grained\n",
            "units capture pronunciation-related accent characteristics, while\n",
            "coarse-grained units are better for learning linguistic information.\n",
            "Moreover, an explicit interaction of two tasks can also provide\n",
            "complementary information and improve the performance of\n",
            "each other, but it is rarely used by existing approaches. In this\n",
            "paper, we propose a novel Decoupling and Interacting Multi-task\n",
            "Network (DIMNet) for joint speech and accent recognition, which\n",
            "is comprised of a connectionist temporal classification (CTC)\n",
            "branch, an AR branch, an ASR branch, and a bottom feature en-\n",
            "coder. Specifically, AR and ASR are first decoupled by separated\n",
            "branches and two-granular modeling units to learn task-specific\n",
            "representations. The AR branch is from our previously proposed\n",
            "linguistic-acoustic bimodal AR model and the ASR branch is\n",
            "an encoder-decoder based Conformer model. Then, for the task\n",
            "interaction, the CTC branch provides aligned text for the AR\n",
            "task, while accent embeddings extracted from our AR model are\n",
            "incorporated into the ASR branch’s encoder and decoder. Finally,\n",
            "during ASR inference, a cross-granular rescoring method is\n",
            "introduced to fuse the complementary information from the CTC\n",
            "and attention decoder after the decoupling. Our experiments on\n",
            "English and Chinese datasets demonstrate the effectiveness of\n",
            "the proposed model, which achieves 21.45%/28.53% AR accuracy\n",
            "relative improvement and 32.33%/14.55% ASR error rate relative\n",
            "reduction over a published standard baseline, respectively.\n",
            "Index Terms —ASR-AR multi-task learning, LASAS, two-\n",
            "granularity modeling units\n",
            "I. I NTRODUCTION\n",
            "ACCENTS refer to the variations in standard pronunciation\n",
            "that are influenced by factors such as the speaker’s educa-\n",
            "tion level, region, or native language [1, 2]. For instance, when\n",
            "English is spoken with a Mandarin accent, it is considered a\n",
            "foreign accent, whereas Cantonese-influenced Mandarin is cat-\n",
            "egorized as a regional accent. Despite the significant progress\n",
            "made in end-to-end automatic speech recognition (E2E ASR)\n",
            "in recent years, accents remain a significant challenge to user\n",
            "equality in speech recognition, leading to a decline in the\n",
            "performance of ASR models trained on standard pronunciation\n",
            "Corresponding author: Lei Xie.\n",
            "Qijie Shao, Pengcheng Guo, and Lei Xie are with the Audio, Speech\n",
            "and Language Processing Group (ASLP), School of Computer Sci-\n",
            "ence, Northwestern Polytechnical University, Xi’an 710072, China. Email:\n",
            "qjshao@npu-aslp.org (Qijie Shao), pcguo@nwpu-aslp.org (Pengcheng Guo),\n",
            "lxie@nwpu.edu.cn (Lei Xie).\n",
            "Jinghao Yan and Pengfei Hu are with Tencent Research, Bei-\n",
            "jing 100193, China. Email: jinghaoyan@tencent.com (Jinghao Yan),\n",
            "alanpfhu@tencent.com (Pengfei Hu).data [3–6]. As a result, there has been a growing interest in\n",
            "multi-accent ASR.\n",
            "The multi-task ASR-AR framework has become a widely-\n",
            "used solution for overcoming the challenges posed by multi-\n",
            "accent [7–9]. This framework typically consists of a shared\n",
            "encoder and two branches dedicated to ASR and AR tasks,\n",
            "respectively. The shared encoder is responsible for extracting\n",
            "acoustic features from the input speech for both branches, and\n",
            "backpropagation from these two branches enables the shared\n",
            "encoder to learn how to extract both linguistic and accent\n",
            "representations. Although it has been shown effective in joint\n",
            "modeling ASR and AR [8], the granularity difference between\n",
            "tasks suggests that simultaneously extracting features via a\n",
            "shared encoder may not be an optimal strategy. Additionally,\n",
            "the ASR and AR branches currently exhibit limited interaction,\n",
            "impeding the full utilization of information from the opposing\n",
            "branch [10, 11].\n",
            "Granularity difference between ASR and AR: E2E ASR\n",
            "is a linguistic-related task, in which coarse-grained units like\n",
            "byte-pair encodings (BPEs) [12] or characters are often used\n",
            "for better representing linguistic information, such as spelling\n",
            "and words. In contrast, the AR task is pronunciation-related\n",
            "and requires capturing small acoustic variations like pitch,\n",
            "intonation, and stress, where fine-grained modeling units such\n",
            "as phonemes or syllables are more suitable [13]. Therefore,\n",
            "incorporating two-granularity modeling units into the multi-\n",
            "task ASR-AR is more appropriate. Nevertheless, the sequence\n",
            "lengths of the two-granularity units are inconsistent, making\n",
            "simultaneous encoding by a single shared encoder challenging.\n",
            "Improving AR with ASR: Earlier AR models directly\n",
            "extracted low-level features such as frequency and timbre [14–\n",
            "16], which could lead to overfitting on speaker and chan-\n",
            "nel characteristics [17, 18]. Recent studies have shown that\n",
            "leveraging linguistic information from ASR can effectively\n",
            "mitigate the overfitting issue in AR tasks [4, 7, 8, 19].\n",
            "Initializing the AR encoder with a pre-trained ASR encoder [4]\n",
            "or jointly training a multi-task ASR-AR network [8, 20] are\n",
            "the commonly used approaches, both of which have demon-\n",
            "strated their effectiveness on various datasets. Different from\n",
            "implicitly integrating linguistic information into AR models,\n",
            "in our previous study [21], we proposed a novel linguistic-\n",
            "acoustic similarity-based accent shift (LASAS) AR model.\n",
            "To estimate the accent shift of an accented speech utterance,\n",
            "we first map the frame-level aligned text to multiple accent-\n",
            "associated anchor spaces and then leverage the similarities\n",
            "between the acoustic embedding and those anchors as an\n",
            "accent shift. Compared with pure acoustic embedding, the\n",
            "learned accent shift takes full advantage of both linguisticarXiv:2311.07062v2  [cs.SD]  17 Nov 2023JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\n",
            "and acoustic information, which can effectively improve AR\n",
            "performance.\n",
            "Improving ASR with AR: Likewise, knowing the accent\n",
            "embedded in the speech utterance is also beneficial to speech\n",
            "recognition. Plenty of studies have explored using comple-\n",
            "mentary accent information to improve ASR performance on\n",
            "accent speech [7, 22]. In the multi-task ASR-AR framework,\n",
            "accent embeddings from the AR branch can be leveraged\n",
            "to enhance accent information in the ASR branch. However,\n",
            "the effectiveness of different types of accent embeddings on\n",
            "ASR performance can vary significantly. The hidden states of\n",
            "the DNN-based accent classifier [3] provide rich and stable\n",
            "utterance-level accent information, while the AR posterior\n",
            "probabilities are more straightforward and interpretable. On\n",
            "the other hand, the accent shifts in the LASAS [21] method\n",
            "capture accent variations and provide more detailed informa-\n",
            "tion. Moreover, incorporating accent embeddings into either\n",
            "the encoder [3] or decoder [22] of the ASR model allows\n",
            "for the model to adapt to variations in pronunciation or\n",
            "linguistics, leading to varying effects on ASR performance.\n",
            "Thus, comprehensive studies to investigate the interpretability\n",
            "and ASR performance of each approach are essential.\n",
            "The objective of this study is to overcome the challenge\n",
            "ofunit-granularity differences and promote full interaction\n",
            "between the ASR and AR branches in a multi-task setting. To\n",
            "achieve this, we present a Decoupling and Interacting Multi-\n",
            "task Network (DIMNet) for joint speech and accent recog-\n",
            "nition, which includes a connectionist temporal classification\n",
            "(CTC) [23] branch, an AR branch, an ASR branch, and a\n",
            "bottom feature encoder. Specifically, AR and ASR are first\n",
            "decoupled by separated branches and two-granular modeling\n",
            "units to learn task-specific representations. The AR branch is\n",
            "from our previously proposed LASAS [21] AR model and\n",
            "the ASR branch is an encoder-decoder-based Conformer [24]\n",
            "model. Then, for the task interaction, the CTC branch is\n",
            "optimized with the same modeling units as the AR branch\n",
            "to provide linguistic features for the AR task, while latent\n",
            "accent embeddings extracted from our AR model are used\n",
            "to improve the ASR branch. We also conduct comprehen-\n",
            "sive studies to explore the choice of accent embeddings and\n",
            "the fusion strategies for the ASR branch. Finally, a cross-\n",
            "granular rescoring method is introduced to effectively fuse\n",
            "the probabilities from CTC and attention decoder during ASR\n",
            "inference. Our experiments on English and Chinese datasets\n",
            "demonstrate the effectiveness of the proposed model, which\n",
            "achieves 21.45%/28.53% AR accuracy relative improvement\n",
            "and32.33%/14.55% ASR error rate relative reduction over a\n",
            "published standard baseline, respectively.\n",
            "II. R ELATED WORKS\n",
            "In this section, we present a brief summary of multi-accent\n",
            "ASR-AR frameworks and the applications of two-granularity\n",
            "modeling units.\n",
            "A. Multi-Accent ASR-AR\n",
            "Current approaches for multi-accent ASR-AR can be clas-\n",
            "sified into three categories: cascade [3, 18], multi-task [8, 25–28], and single-task [19, 29]. For the cascade ASR-AR frame-\n",
            "work, ASR and AR models are trained separately and used in\n",
            "a sequential manner. Typically, an AR model is first trained\n",
            "to extract accent features from the input speech, which is\n",
            "then utilized to assist the ASR model. Deng et al. [18]\n",
            "proposed a cascade ASR-AR based on pre-trained wav2vec\n",
            "2.0 [30], achieving state-of-the-art (SOTA) performance on\n",
            "the AESRC dataset [4] due to the powerful acoustic modeling\n",
            "capabilities of wav2vec 2.0. In [3, 31], Gong and Qian et\n",
            "al.also obtained competitive results using a cascade ASR-\n",
            "AR scheme. Their AR component has a phonetic posteri-\n",
            "orgrams (PPG) extractor and a time delay neural network\n",
            "(TDNN) based classifier [32, 33], while the ASR compo-\n",
            "nent incorporates accent information into the encoder through\n",
            "adapter layers in a CTC/attention framework. Although each\n",
            "model could be optimized with a large amount of in-domain\n",
            "data, the cascade strategy also introduces inevitable error\n",
            "propagation and increased computation complexity. For the\n",
            "multi-task structure, a shared encoder is generally utilized\n",
            "to simultaneously extract accent and linguistic information.\n",
            "In [25], Zhang et al. regarded ASR as an auxiliary task\n",
            "for AR. By extracting phoneme-level accent variations, their\n",
            "method effectively improves AR performance, which provides\n",
            "evidence of ASR’s helpfulness in AR tasks. In [8], Zhang et\n",
            "al.incorporated an AR branch into a CTC/attention ASR and\n",
            "used a shared encoder to simultaneously learn accent and\n",
            "linguistic representations, leading to improved ASR adaptation\n",
            "to accents. Finally, single-task ASR-AR splices accent and\n",
            "text labels together, using a unified encoder-decoder structure\n",
            "to predict two kinds of labels simultaneously. Following this\n",
            "direction, Gao et al. [19] proposed a single-task scheme for\n",
            "accent prediction, which extends the output token list by\n",
            "inserting accent labels into the text transcripts and yields good\n",
            "results without modifying the E2E model structure.\n",
            "In contrast to existing multi-task ASR-AR approaches, our\n",
            "proposed DIMNet incorporates decoupling and interacting,\n",
            "resulting in improved transfer and fusion of linguistic and\n",
            "acoustic information. To achieve this, we employ distinct\n",
            "branches and two granular modeling units to decouple the\n",
            "AR and ASR tasks, allowing them to focus on pronunciation-\n",
            "related and semantic-related aspects, respectively. Addition-\n",
            "ally, we introduce interacting between these branches through\n",
            "LASAS AR and encoder-decoder accent embedding fusion\n",
            "within DIMNet.\n",
            "B. Two-granularity Unit Modeling\n",
            "Research on the use of two-granularity modeling units\n",
            "in ASR-AR frameworks is relatively scarce. One notable\n",
            "approach was proposed by Rao et al. [34], which in-\n",
            "volved a multi-accent ASR that utilized phoneme-grapheme\n",
            "two-granularity modeling units. This model generated both\n",
            "phoneme and grapheme (a-z) outputs with multiple CTC\n",
            "decoders added to the encoder intermediate layers and a final\n",
            "CTC decoder stacked after the encoder. The number of CTCs\n",
            "in the intermediate layer was consistent with the accent types.\n",
            "However, unlike typical two-granularity schemes, the final\n",
            "outputs of this model were graphemes, which are finer-grainedJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\n",
            "than the phoneme-based middle outputs. Two-granularity mod-\n",
            "eling units have also been used in pure ASR studies. For\n",
            "example, Chan et al. [35] introduced syllable/character units\n",
            "to an early attention-based ASR model with two decoders, but\n",
            "only the character decoder was used for inference. Other stud-\n",
            "ies [36–40] employed cascade audio-to-phoneme (A2P) and\n",
            "phoneme-to-word (P2W) schemes. Their experiments demon-\n",
            "strated that, compared with the acoustic embedding of the\n",
            "encoder, the pure-text phoneme inputs for P2W lack sufficient\n",
            "acoustic information. Thus, this two-stage independent struc-\n",
            "ture leads to error accumulation, which requires a large amount\n",
            "of P2W data to mitigate. To address this problem, Zhang et\n",
            "al.[41] fused a P2W model and an attention decoder together\n",
            "in autoregressive decoding, which fully utilized the phoneme\n",
            "text and acoustic embedding. Alternatively, Yang et al. [42]\n",
            "directly utilized a Transformer decoder [43] to complete the\n",
            "P2W transcription. This method is elegant and concise but\n",
            "only supports two-granularity modeling units with the same\n",
            "sequence lengths, such as Chinese syllables and characters.\n",
            "Unlike the aforementioned methods that directly apply fine-\n",
            "grained units to the ASR task, our DIMNet incorporates these\n",
            "units into the AR task, resulting in significant enhancements.\n",
            "Additionally, DIMNet avoids the inclusion of cascade A2P\n",
            "and P2W structure, effectively mitigating the accumulation of\n",
            "errors.\n",
            "C. Decoupling and Interacting Multi-Task Learning\n",
            "The efficacy of decoupling and interacting multi-task learn-\n",
            "ing has been evident in various domains, including speech and\n",
            "speaker recognition [44], object detection [11], and sentiment\n",
            "analysis [10]. Tang et al. [44] introduced a collaborative\n",
            "joint training approach for speech and speaker recognition,\n",
            "where the output of one task is backpropagated to the other\n",
            "task, resulting in enhanced performance on both speech and\n",
            "speaker recognition tasks compared to single-task systems.\n",
            "Furthermore, similar decoupling and interacting methodolo-\n",
            "gies have been employed to extract information at different\n",
            "scales. Pang et al. [11] proposed aggregate interaction mod-\n",
            "ules to integrate features from adjacent levels, enabling the\n",
            "extraction of multi-scale image features. And He et al. [10]\n",
            "presented an interactive multi-task learning network capable\n",
            "of jointly learning token-level and document-level sentiment\n",
            "information.\n",
            "In the AR-ASR tasks, a major challenge in the interaction\n",
            "process is the limited ability of AR to directly leverage linguis-\n",
            "tic information from ASR. Our DIMNet tackles this challenge\n",
            "by incorporating the LASAS AR model, which utilizes ASR\n",
            "aligned text as one of its inputs. This distinguishes DIMNet\n",
            "from other approaches in decoupling and interacting multi-task\n",
            "learning methods.\n",
            "III. M ETHOD\n",
            "Fig. 1 overviews the architecture of our proposed DIMNet,\n",
            "which uses a CTC/attention ASR [45] model as the backbone.\n",
            "In addition, we incorporate a LASAS AR [21] model and a\n",
            "triple-encoder structure, resulting in three branches within the\n",
            "DIMNet. The accent and attention branches are dedicated tothe AR and ASR tasks, respectively, while the CTC branch\n",
            "provides complementary information to assist the other two\n",
            "branches. We will illustrate these components of the DIMNet\n",
            "in the following subsections.\n",
            "A. Decoupling of AR and ASR\n",
            "Decoupling AR and ASR tasks can improve their respec-\n",
            "tive performance by allowing for independent modeling of\n",
            "accent and linguistic information. Furthermore, a decoupled\n",
            "framework can enhance the clarity and interpretability of the\n",
            "interaction between the two tasks.\n",
            "For decoupling the DIMNet to allow each task to focus on\n",
            "different levels of information, we introduce a two-granularity\n",
            "modeling approach. Specifically, we use fine-grained units for\n",
            "the CTC branch to obtain aligned text as the inputs of the\n",
            "accent branch, and use coarse-grained units for the attention\n",
            "branch instead. The fine-grained units discussed here need\n",
            "to be pronunciation-related, such as the ARPAbet phoneme\n",
            "set [46] for English and Pinyin syllables for Chinese. On the\n",
            "contrary, for coarse-grained units, it is important for them\n",
            "to be semantic-related. In this paper, we use BPE and Char\n",
            "as coarse-grained units for English and Chinese, respectively.\n",
            "The use of pronunciation-related fine-grained units allows the\n",
            "accent characteristics to be captured effectively in the AR\n",
            "task. On the other hand, semantic-related coarse-grained units\n",
            "are helpful in providing contextual linguistic information for\n",
            "the ASR task. Hence, the two-granularity decoupling helps\n",
            "enhance the performance of both tasks simultaneously.\n",
            "As mentioned in Section I, sequence length inconsistency\n",
            "is a significant challenge in two-granularity modeling. To\n",
            "address this issue, we introduce the triple-encoder structure\n",
            "in the DIMNet. In this approach, a shared encoder is used\n",
            "to learn pronunciation-related shallow acoustic information,\n",
            "while two lightweight encoders are used for the CTC and\n",
            "attention branches to extract linguistic information in different\n",
            "granularity. The outputs of the triple encoders can be denoted\n",
            "as: \n",
            "x𝑠𝑒=Encoder𝑠ℎ𝑎𝑟𝑒𝑑(x𝑖𝑛),\n",
            "x𝑐𝑒=Encoder𝑐𝑡𝑐(x𝑠𝑒),\n",
            "x𝑎𝑒=Encoder𝑎𝑡𝑡(Concat(x𝑠𝑒,emb)),(1)\n",
            "where x𝑖𝑛is the acoustic features of the input speech, which\n",
            "can be MFCC or Fbank, and x𝑠𝑒denotes the last layer’s\n",
            "output of the shared encoder. The emb is an accent embedding\n",
            "extracted from the accent branch, which will be introduced in\n",
            "Section III-C.\n",
            "By stacking additional encoders after the shared encoder,\n",
            "our approach can alleviate the sequence-length inconsistency\n",
            "of the two-granularity unit modeling. And then, the compu-\n",
            "tation of the losses for the CTC and attention decoders are\n",
            "denoted as:\n",
            "L𝑐𝑡𝑐=CTC(x𝑐𝑒,y𝑓), (2)\n",
            "\u001a𝑃(y𝑐|x𝑖𝑛)=Decoder𝑎𝑡𝑡(Concat(x𝑎𝑒,emb),y𝑐),\n",
            "L𝑎𝑡𝑡=CrossEntropy(𝑃(y𝑐|x𝑖𝑛),y𝑐),(3)\n",
            "where y𝑓andy𝑐are the transcription labels with fine-grained\n",
            "and coarse-grained units, respectively. The y𝑓could be trans-\n",
            "lated to y𝑐with a lexicon. 𝑃(y𝑐|x𝑖𝑛)is the attention posterior\n",
            "probabilities of the given labels y𝑐.JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\n",
            "Fig. 1: The framework overview of DIMNet.\n",
            "The total loss of our multi-task ASR-AR consists of the\n",
            "ASR lossL𝑎𝑡𝑡, the CTC lossL𝑐𝑡𝑐, and the AR loss L𝑎𝑟,\n",
            "which can be formulated as:\n",
            "L𝐴𝑆𝑅−𝐴𝑅=L𝑎𝑡𝑡+𝜆1L𝑐𝑡𝑐+𝜆2L𝑎𝑟, (4)\n",
            "where𝜆1and𝜆2are tunable hyperparameters. The details of\n",
            "L𝑎𝑟will be introduced in Section III-B.\n",
            "B. Improving AR with ASR\n",
            "The traditional AR models only utilize acoustic features as\n",
            "input. When being integrated into a multi-task ASR-AR, their\n",
            "ability to fully interact with the ASR task is limited, which\n",
            "can hinder their performance. In our previous study [21],\n",
            "we proposed an AR model named LASAS that uses aligned\n",
            "text and acoustic features as input. This model explicitly and\n",
            "fully utilizes linguistic information, resulting in a significant\n",
            "improvement in AR performance. To enhance the ASR’s\n",
            "assistance to AR, in this study, we first introduce LASAS into\n",
            "the multi-task framework by feeding it with the aligned text\n",
            "output from the CTC decoder and acoustic features output\n",
            "from the shared encoder. In addition, we further improve\n",
            "LASAS to make it adaptable to the multi-task ASR-AR.\n",
            "The left part of Fig. 2 shows the structure of the LASAS\n",
            "AR model, which consists of a LASAS block, a Transformer\n",
            "encoder [43], and several linear layers. The details of the\n",
            "LASAS block are depicted in the right part of Fig. 2, where the\n",
            "acoustic embedding x𝑎and frame-level aligned text x𝑡serve\n",
            "as inputs to LASAS, provided by the shared encoder and CTC\n",
            "decoder, respectively. The computation of x𝑎andx𝑡can be\n",
            "denoted as:\u001a\n",
            "x𝑎=Concat(x𝑖\n",
            "𝑠𝑒,x𝑗\n",
            "𝑠𝑒,x𝑘\n",
            "𝑠𝑒)),\n",
            "x𝑡=Regular(GreedySearch𝑐𝑡𝑐(x𝑐𝑒)),(5)\n",
            "where x𝑖\n",
            "𝑠𝑒,x𝑗\n",
            "𝑠𝑒, and x𝑘\n",
            "𝑠𝑒denote the outputs of the shared\n",
            "encoder’s layers at the 1/3, 2/3, and 3/3 positions, respectively.\n",
            "SimilarText Dim \n",
            "Reduc  VecU\n",
            "Accent \n",
            "ShiftU \n",
            "Acoustic \n",
            "Map Mat\n",
            "   \n",
            "Text Dim \n",
            "Reduc  \n",
            "Mat\n",
            "Text Dim \n",
            "Reduc  \n",
            "Mat\n",
            "Concat\n",
            "Text \n",
            "Map Mat\n",
            "Text \n",
            "Map Mat\n",
            "ConcatU U U U \n",
            "Acoustic  \n",
            "Map VecAcoustic  \n",
            "Map VecText Map \n",
            "VecText Map \n",
            "VecNN\n",
            "ax\n",
            "tx\n",
            "bmx\n",
            "LASAS \n",
            "Block\n",
            "Transformer\n",
            "Enc\n",
            "Linear\n",
            "LayersU\n",
            "ax\n",
            "LASAS\n",
            "BlockLASAS\n",
            "AR Model\n",
            "ary\n",
            "txFig. 2: Details of LASAS block.\n",
            "For instance, if the shared encoder comprises 9 layers, then\n",
            "x𝑖\n",
            "𝑠𝑒,x𝑗\n",
            "𝑠𝑒, and x𝑘\n",
            "𝑠𝑒correspond to the outputs of the 3-rd,\n",
            "6-th, and 9-th layers, respectively. The greedy search used\n",
            "here retains both the blanks and repeated tokens of CTC,\n",
            "which differs from the conventional scheme. And Regular\n",
            "means to replace blanks with subsequent predicted token IDs,\n",
            "constructing a frame-level aligned text with only repeated\n",
            "token IDs. It is worth noting that we use greedy search instead\n",
            "of prefix beam search to predict only one aligned text for\n",
            "the accent branch in both the training and inference stages.\n",
            "If prefix beam search is used during the training process,\n",
            "the time consumption will skyrocket to an unacceptable level.\n",
            "Moreover, through experimental observations, we find that the\n",
            "AR performance of the model does not exhibit significant\n",
            "improvement (only a relative 0.01%) when using greedyJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\n",
            "search during training and switching to prefix beam search\n",
            "during inference. This lack of improvement can be attributed to\n",
            "the mismatch between the training process and the utilization\n",
            "of prefix beam search, despite its ability to generate text with\n",
            "a lower error rate.\n",
            "The right part of Fig. 2 illustrates the details of the LASAS\n",
            "block, which begins by mapping an aligned text vector x𝑡\n",
            "to multiple Euclidean spaces as anchors v𝑡. These anchors\n",
            "are related to the speech content and aligned with the speech\n",
            "at a frame level. Next, we map an acoustic embedding x𝑎\n",
            "to the same dimension as the text anchor v𝑡, denoted as\n",
            "v𝑎. Using a scaled dot-product frame by frame, we obtain\n",
            "a similarity value 𝑠𝑖between an anchor v𝑖\n",
            "𝑡and a mapped\n",
            "acoustic embedding v𝑖\n",
            "𝑎. Finally, we concatenate a set of\n",
            "similarities to form s, which reflects the shift directions and\n",
            "similarity degrees of different accents. The computation of the\n",
            "accent shift scan be denoted as:\n",
            " \n",
            "v𝑖\n",
            "𝑡=x𝑡·W𝑖\n",
            "𝑡,\n",
            "v𝑖\n",
            "𝑎=x𝑎·W𝑖\n",
            "𝑎,\n",
            "𝑠𝑖=DotProd(v𝑖\n",
            "𝑎,v𝑖\n",
            "𝑡)/√𝑑𝑘,\n",
            "s=Concat(𝑠1, 𝑠2,..., 𝑠𝑁),(6)\n",
            "where𝑖∈[1,𝑁]and𝑁is the number of mapping spaces. The\n",
            "W𝑖\n",
            "𝑡∈R𝐷2×𝐶andW𝑖\n",
            "𝑎∈R𝐷1×𝐶are mapping matrix. The 𝐷1,\n",
            "𝐷2, and𝐶are the dimensions of features.\n",
            "Since the accent shift sis a relative representation, a textual\n",
            "reference is necessary for the subsequent classifier. In order to\n",
            "represent pure textual information, we establish a dimension\n",
            "reduction matrix W𝑡𝑑to reduce the dimension of the input\n",
            "text OneHot vector. By concatenating the accent shift x𝑏𝑚and\n",
            "the dimension-reduced text v𝑡𝑑, we can obtain a linguistic-\n",
            "acoustic bimodal representation x𝑏𝑚, which can then be used\n",
            "as input for the subsequent classifier. Given the aligned text\n",
            "vector x𝑡and accent shift s, the bimodal representation x𝑏𝑚\n",
            "can be calculated as:\n",
            "\u001av𝑡𝑑=x𝑡·W𝑡𝑑,\n",
            "x𝑏𝑚=Concat(s,v𝑡𝑑),(7)\n",
            "where W𝑡𝑑∈R𝐷2×(𝐶−𝑁),x𝑏𝑚∈R𝑇×𝐶, and𝑇is the time\n",
            "steps of frames.\n",
            "Finally, the classifier is composed of a lightweight Trans-\n",
            "former encoder and multiple linear layers. The encoder is\n",
            "used to extract context-sensitive accent information. After\n",
            "passing through the classifier, we obtain either a frame-level\n",
            "or utterance-level accent prediction. Utterance-level accent\n",
            "prediction tends to achieve better performance on AR tasks,\n",
            "while frame-level accent is more suitable as an embedding to\n",
            "assist ASR tasks. We will analyze and discuss the choice of the\n",
            "level in Section V. Computation of the final accent prediction\n",
            "can be denoted as:\n",
            "\u001a𝑃(y𝑎𝑟|x𝑖𝑛)=Softmax(Linear(Transformer(x𝑏𝑚))),\n",
            "ˆy𝑎𝑟=arg max(𝑃(y𝑎𝑟|x𝑖𝑛)),(8)\n",
            "where𝑃(y𝑎𝑟|x𝑖𝑛)represents the posterior probabilities for\n",
            "accent, and ˆy𝑎𝑟refers to the predicted accent outcome from\n",
            "the accent branch.\n",
            "To improve the adaptation of the original LASAS AR\n",
            "model [21] to the multi-task ASR-AR, we detach the alignedtext and accent embedding, as depicted in Figure 1. This\n",
            "decouples the accent branch and allows for separate opti-\n",
            "mization of the CTC/attention and accent branches during\n",
            "back-propagation without mutual interference. The decoupling\n",
            "reduces learning difficulty and improves the performance of\n",
            "both the accent and attention branches. The loss of the accent\n",
            "branch is denoted as:\n",
            "L𝑎𝑟=CrossEntropy(𝑃(y𝑖\n",
            "𝑎𝑟|x𝑖\n",
            "𝑖𝑛),y𝑖\n",
            "𝑎𝑟), (9)\n",
            "where y𝑖\n",
            "𝑎𝑟is an accent label.\n",
            "C. Improving ASR with AR\n",
            "In a multi-accent ASR-AR, high-quality accent information\n",
            "can guide the ASR task to learn accent-specific pronunciation\n",
            "and expression. Fully utilizing accent information could signif-\n",
            "icantly improve the performance of the ASR task. Therefore,\n",
            "it is worthwhile to study better approaches for utilizing accent\n",
            "information.\n",
            "In a multi-task ASR-AR, accent embeddings can be chosen\n",
            "from the hidden embedding before the last linear layer x𝑑𝑛𝑛,\n",
            "the posterior probabilities of classification x𝑝𝑝, or the accent\n",
            "shifts s. These three choices can be denoted as:\n",
            " \n",
            "emb𝑑𝑛𝑛=x𝑑𝑛𝑛,\n",
            "emb𝑝𝑝=UpProject(x𝑝𝑝),\n",
            "emb𝑠𝑖𝑚=UpProject(s),(10)\n",
            "where UpProject represents a dimension expanding of the\n",
            "embeddings by a linear layer. Since the dimension of x𝑑𝑛𝑛\n",
            "is significantly larger than that of x𝑝𝑝ands, for a fair\n",
            "comparison, we use the up-project operation. The emb𝑑𝑛𝑛\n",
            "provides rich and stable accent information, the emb𝑝𝑝is\n",
            "intuitive and concise, and using the emb𝑠𝑖𝑚can provide text-\n",
            "related frame-level accent viriations. In this paper, we compare\n",
            "these three accent embeddings in experiments to determine\n",
            "which one is better.\n",
            "Moreover, we also investigate the effectiveness of different\n",
            "fusion strategies between accent and the ASR task. In our\n",
            "triple-encoder scheme, accent information can be fused both\n",
            "implicitly and explicitly. Implicit fusion occurs when the\n",
            "shared encoder learns to extract accent information through\n",
            "the back-propagation of the accent branch, even without the\n",
            "help of an accent embedding. Explicit fusion occurs when an\n",
            "accent embedding is integrated into the attention branch for\n",
            "the ASR task. In the multi-task ASR-AR, implicit fusion is\n",
            "inevitable due to the existence of the accent branch, while\n",
            "explicit fusion is optional. We classify accent fusion into four\n",
            "schemes based on the use of an accent embedding:\n",
            "•AF𝑖: Only implicit accent fusion is used in ASR.\n",
            "\u001ax𝑎𝑒=Encoder𝑎𝑡𝑡(x𝑠𝑒),\n",
            "𝑃(y𝑐|x𝑖𝑛)=Decoder𝑎𝑡𝑡(x𝑎𝑒,y𝑐).(11)\n",
            "•AF𝑖𝑒: Both implicit and explicit (to encoder) accent\n",
            "fusions are used in ASR.\n",
            "\u001ax𝑎𝑒=Encoder𝑎𝑡𝑡(Concat(x𝑠𝑒,emb)),\n",
            "𝑃(y𝑐|x𝑖𝑛)=Decoder𝑎𝑡𝑡(x𝑎𝑒,y𝑐),(12)JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\n",
            "•AF𝑖𝑑: Both implicit and explicit (to decoder) accent\n",
            "fusions are used in ASR.\u001ax𝑎𝑒=Encoder𝑎𝑡𝑡(x𝑠𝑒),\n",
            "𝑃(y𝑐|x𝑖𝑛)=Decoder𝑎𝑡𝑡(Concat(x𝑎𝑒,emb),y𝑐),\n",
            "(13)\n",
            "•AF𝑖𝑒𝑑: Both implicit and explicit (to encoder and de-\n",
            "coder) accent fusions are used in ASR.\n",
            "\u001ax𝑎𝑒=Encoder𝑎𝑡𝑡(Concat(x𝑠𝑒,emb)),\n",
            "𝑃(y𝑐|x𝑖𝑛)=Decoder𝑎𝑡𝑡(Concat(x𝑎𝑒,emb),y𝑐),\n",
            "(14)\n",
            "In the aforementioned schemes, the effectiveness of AF𝑖,\n",
            "AF𝑖𝑒, and AF𝑖𝑑has been proven in different studies [8,\n",
            "18, 22]. To the best of our knowledge, AF𝑖𝑒𝑑has not been\n",
            "extensively explored. However, considering the encoder and\n",
            "decoder respectively focus on linguistic and acoustic infor-\n",
            "mation, the AF𝑖𝑒𝑑incorporation has the potential to achieve\n",
            "optimal performance. Thanks to the triple-encoder structure,\n",
            "we can easily apply scheme AF𝑖𝑒𝑑to the attention branch.\n",
            "D. Two-granularity Rescoring\n",
            "After the decoupling, within our DIMNet, the CTC de-\n",
            "coder focuses on pronunciation while the attention decoder\n",
            "emphasizes linguistic information. By leveraging the com-\n",
            "plementarity between these two types of information, a two-\n",
            "granularity rescoring approach is expected to achieve a better\n",
            "ASR performance compared to single-granularity rescoring.\n",
            "However, the current mainstream CTC rescoring [47, 48] and\n",
            "attention rescoring [45] necessitate matching sequence lengths\n",
            "between the modeling units of both decoders, making them\n",
            "unsuitable for direct application in two-granularity scenarios.\n",
            "In order to solve this problem, we develop a two-granularity\n",
            "rescoring method based on the CTC rescoring technique [47]\n",
            "to merge scores from both the CTC and attention decoder.\n",
            "Fig. 3: Details of two-granularity rescoring.\n",
            "As shown in Fig. 3, the two-granularity rescoring method\n",
            "involves two-pass decoding. In the first pass, 𝑁hypothe-\n",
            "sesˆy𝑐and their corresponding posterior probability scores\n",
            "𝑃𝑎𝑡𝑡(ˆy𝑐|x𝑖𝑛)are obtained through autoregressive beam search\n",
            "decoding. The modeling units of these hypotheses are coarse-\n",
            "grained, which can be mapped into fine-grained units through\n",
            "a lexicon, without the need for training a translation model.\n",
            "This process can be represented as:\n",
            "\u001aˆy𝑐, 𝑃𝑎𝑡𝑡(ˆy𝑐|x𝑖𝑛)=BeamSearch 𝑎𝑡𝑡(Concat(x𝑎𝑒,emb)),\n",
            "ˆy𝑖\n",
            "𝑓=Lexicon(ˆy𝑖\n",
            "𝑐),\n",
            "(15)\n",
            "where𝑖∈[1,𝑁]and𝑁represents the beam size used for beam\n",
            "search decoding.Given the fine-grained hypotheses ˆy𝑓, we can use the CTC\n",
            "forward algorithm to calculate the scores in the second pass.\n",
            "According to [23], the CTC forward algorithm computes the\n",
            "negative logarithm of the conditional probabilities of the CTC\n",
            "encoder outputs x𝑐𝑒and a given text. If the given text is the\n",
            "transcription label y𝑓, the CTC loss is computed. However, if\n",
            "the given text is the hypothesis ˆy𝑓, the conditional probabilities\n",
            "ofx𝑐𝑒and ˆy𝑓can be calculated and used for rescoring.\n",
            "Specifically, the conditional probabilities log𝑃𝑐𝑡𝑐(ˆy𝑓|x𝑐𝑒)can\n",
            "be computed as:\n",
            " \n",
            "CTC(x𝑐𝑒,ˆy𝑓)=−log𝑃𝑐𝑡𝑐(ˆy𝑓|x𝑐𝑒),\n",
            "⇓\n",
            "log𝑃𝑐𝑡𝑐(ˆy𝑓|x𝑐𝑒)=−CTC(x𝑐𝑒,ˆy𝑓).(16)\n",
            "Finally, the ASR prediction ˆy𝑡𝑟𝑠using the two-granularity\n",
            "rescoring method is obtained by:\n",
            "ˆy𝑡𝑟𝑠=arg max(𝑤1log𝑃𝑎𝑡𝑡(ˆy𝑐|x𝑖𝑛)+\n",
            "𝑤2log𝑃𝑐𝑡𝑐(ˆy𝑓|x𝑐𝑒) +𝑤3log𝑃𝑙𝑚(ˆy𝑐)),(17)\n",
            "where𝑤1,𝑤2, and𝑤3are tunable parameters. Here, 𝑃𝑙𝑚(ˆy𝑐)\n",
            "is optional language model scores.\n",
            "IV. E XPERIMENTS SETUP\n",
            "A. Dataset\n",
            "We conduct extensive experiments on publicly-available En-\n",
            "glish and Chinese datasets to evaluate the proposed DIMNet.\n",
            "For the English experiments, we use the AESRC dataset [4],\n",
            "while for the Chinese experiments, we use the KeSpeech\n",
            "dataset [49]. Details of the two datasets are shown in Table I.\n",
            "Besides, following the setup of [4, 8, 13], the additional Lib-\n",
            "rispeech [50] dataset is also used for the AESRC experiments.\n",
            "TABLE I: Details of the English AESRC and the Chinese\n",
            "KeSpeech accent datasets.\n",
            "DatasetAccent\n",
            "NumTotal Duration\n",
            "(Hours)Sampling Rate\n",
            "(kHz)Style\n",
            "AESRC 8 160 16 Reading\n",
            "KeSpeech 9 1542 16 Reading\n",
            "In this study, we use BPE-phoneme as the two-granularity\n",
            "modeling units for English, and char-syllable for Chinese. This\n",
            "choice is based on the unique characteristics of each language\n",
            "and the effectiveness of these units in capturing phonetic and\n",
            "semantic features. To convert the coarse-grained units to fine-\n",
            "grained units, we utilize CMUdict1and Pypinyin2lexicons,\n",
            "respectively. Refer to Table II for detailed information.\n",
            "TABLE II: Two-granularity modeling units details.\n",
            "LanguageCoarse-grained\n",
            "Units NumFine-grained\n",
            "Units NumLexicon\n",
            "English 5002 (BPE) 40 (Phoneme) CMUdict1\n",
            "Chinese 5687 (Char) 419 (Syllable) Pypinyin2\n",
            "1Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict\n",
            "2Available at https://pypi.org/project/pypinyin/JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\n",
            "B. Model Configurations\n",
            "Our baseline model is based on a triple-branch structure,\n",
            "which utilizes the hidden embedding before the last linear\n",
            "layer in the accent branch to generate accent embeddings. The\n",
            "encoders are based on Conformer [24], while the attention de-\n",
            "coder is Transformer [43]. The shared encoder, CTC encoder,\n",
            "and attention encoder are comprised of 9, 3, and 3 Conformer\n",
            "blocks, respectively, while the attention decoder consists of\n",
            "6 blocks. The Conformer blocks have 2048 inner dimensions\n",
            "for feed-forward networks (FFN), 256 model dimensions, 4\n",
            "attention heads, and utilize a CNN kernel size of 15. During\n",
            "training, the loss weights assigned to the CTC, AR, and ASR\n",
            "branches are 0.3, 0.4, and 0.3, respectively. We specifically\n",
            "lightly amplify the AR loss weight due to its relatively smaller\n",
            "absolute loss value. In addition, for the LASAS AR model, we\n",
            "set the mapping spaces to 8, and the rest parameters are the\n",
            "same as [21]. Our experiments include SpecAugment, model\n",
            "average, and a 2-layer-Transformer based language model\n",
            "(LM).\n",
            "In English experiments, our model is first trained for 70\n",
            "epochs on the mixture of the AESRC and LibriSpeech datasets\n",
            "and then fine-tuned 50 epochs on the AESRC alone. In the\n",
            "first stage, we update the CTC and attention branches using\n",
            "both the AESRC and LibriSpeech datasets, while only the\n",
            "AESRC dataset is used to update the accent branch. In the\n",
            "Chinese experiments, we train the model for 100 epochs\n",
            "without finetuning. Similarly, we use all the available data to\n",
            "update the CTC and attention branches, while only the accent\n",
            "data is used to update the accent branch. It is worth noting\n",
            "that the accent data referred to here is denoted as Phase 1\n",
            "in [49], which includes a subset of Mandarin. To account for\n",
            "the large differences in data amount for different accents in\n",
            "the KeSpeech dataset, we use unbalanced weights for the CE\n",
            "loss in the AR task, which is set as the ratio of the number\n",
            "of accent utterances with respect to the Mandarin subset.\n",
            "V. E XPERIMENTS RESULTS\n",
            "This section presents the experimental results of our pro-\n",
            "posed DIMNet. We first introduce the results of baselines\n",
            "and our ablation and comparison experiments on the AESRC\n",
            "dataset to demonstrate the effectiveness of each module.\n",
            "Next, we compare our approach to the general A2P+P2W\n",
            "cascade modeling schemes of two-granularity units. Finally,\n",
            "we compare the performance of DIMNet to previous studies\n",
            "on both the AESRC and KeSpeech datasets.\n",
            "A. Effectiveness of DIMNet\n",
            "In Table III, 𝐵1−𝐵4represent CTC/attention ASR and\n",
            "multi-task ASR-AR baselines, while 𝐷1represents our pro-\n",
            "posed DIMNet. Specifically, 𝐵1is a classic CTC/attention\n",
            "ASR model using only coarse-grained BPE units, and 𝐵2is\n",
            "derived from 𝐵1by replacing the BPE units with phonemes\n",
            "in the CTC branch. Both 𝐵1and𝐵2have no AR branch.\n",
            "𝐵3is a widely-used multi-task ASR-AR model as described\n",
            "in study [8], which comprises a shared encoder and three\n",
            "branches: CTC decoder, AR, and attention decoder. The AR\n",
            "branch incorporates pooling and linear layers. In addition, toensure a fair comparison, we incorporate the accent embedding\n",
            "fusion into the attention decoder with the detach operation. In\n",
            "𝐵4, we substitute the shared encoder of 𝐵3with the triple\n",
            "encoder structure utilized in DIMNet. Essentially, 𝐵4can be\n",
            "viewed as replacing the LASAS AR of DIMNet with a basic\n",
            "AR model.\n",
            "The comparison between 𝐵1and𝐵2highlights the effect\n",
            "of two-granularity units. The direct introduction of phonemes\n",
            "in a CTC/attention framework can only lead to a slight\n",
            "improvement in ASR performance. After integrating an AR\n",
            "branch, the ASR performance of 𝐵3enhanced compared to\n",
            "𝐵2, which is a widely verified phenomenon. Comparing 𝐵4\n",
            "with𝐵3, the introduction of the triple encoder structure yields\n",
            "a mere average relative impact of 0.15% on accent accuracy,\n",
            "while it improves the ASR task by an average relative 1.02%.\n",
            "Ultimately, when comparing 𝐷1with𝐵1−𝐵4, it becomes\n",
            "evident that DIMNet exhibits clear advantages in both AR and\n",
            "ASR, underscoring the benefits of DIMNet beyond modeling\n",
            "units and multi-task learning.\n",
            "B. Decoupling of AR and ASR\n",
            "In𝐷2and𝐷3, we examine the influence of modeling units\n",
            "on DIMNet. Specifically, we employ grapheme-BPE instead\n",
            "of phoneme-BPE units in 𝐷2, while utilizing BPE-BPE units\n",
            "in𝐷3. Comparing 𝐷1and𝐷2, it becomes apparent that not all\n",
            "fine-grained units are compatible with DIMNet. Graphemes,\n",
            "in comparison to phonemes, exhibit semantic relevance but\n",
            "lack adequate pronunciation information. As a result, they are\n",
            "unable to effectively enhance the performance of AR tasks,\n",
            "consequently indirectly diminishing ASR performance. When\n",
            "comparing𝐷1and𝐷3, it is clear that utilizing phonemes-\n",
            "BPE two-granularity units leads to enhanced performance for\n",
            "DIMNet in terms of both accent accuracy (ACC) and word\n",
            "error rate (WER). However, if coarse-grained BPE units are\n",
            "employed in both the CTC and attention branches, we observe\n",
            "a significant decrease in results. Specifically, there is a relative\n",
            "drop of 5.29% and3.98% in the AR task, as well as a relative\n",
            "drop of 3.42% and2.23% in the ASR task. This result clearly\n",
            "demonstrates that decoupling the modeling units is a crucial\n",
            "factor in improving both AR and ASR performance, even\n",
            "when utilizing the same interactive structure. However, the\n",
            "use of a two-granularity modeling unit presents challenges\n",
            "for the ASR task without the triple-encoder structure. To\n",
            "demonstrate this, we consider the setup of 𝐷4, where we\n",
            "remove the CTC encoder and attention encoder in Fig. 1,\n",
            "keeping only a 12-layer shared encoder, and directly put the\n",
            "accent embedding to the attention decoder. Comparing 𝐷4and\n",
            "𝐷1, we observe that while 𝐷4achieves a similar AR accuracy\n",
            "to𝐷1, its ASR performance significantly degrades by 5.77%\n",
            "and5.74% respectively. This finding suggests that the triple-\n",
            "encoder structure effectively decouples the modeling process\n",
            "of different units, enabling the CTC and attention branches to\n",
            "output units with different granularity.\n",
            "Generally, accent utterances not only contain accent words\n",
            "but also include standard pronounced common words. There-\n",
            "fore, we conduct further analysis to investigate whether per-\n",
            "formance improvement of the ASR and AR is evident inJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\n",
            "TABLE III: Ablation and contrast experiments of our proposed DIMNet on the AESRC dataset. See SectionIII-C for the definitions of\n",
            "accent embedding ( emb) and accent fusion ( AF). In the third column of the table, G, P, and B represent grapheme, phoneme, and BPE,\n",
            "respectively.\n",
            "ID ModelCTC/ATT\n",
            "UnitsTriple\n",
            "EncodersLASASAccent\n",
            "EmbeddingAccent\n",
            "FusionRescoringAR\n",
            "ACC (%)ASR\n",
            "WER (%)\n",
            "Dev Test Dev Test\n",
            "B1 B/B ✗ ✗ ✗ - - 5.96 6.91\n",
            "B2 P/B ✗ ✗ ✗ - - 5.96 6.88\n",
            "B3 P/B ✗ emb 𝑑𝑛𝑛 AF𝑖𝑑 79.12 74.02 5.83 6.76\n",
            "B4CTC/ATT\n",
            "AR-ASR\n",
            "BaselineP/B ✓✗\n",
            "emb 𝑑𝑛𝑛 AF𝑖𝑑✗\n",
            "79.08 73.84 5.78 6.68\n",
            "D1DIMNet\n",
            "BaslineP/B ✓ ✓ emb 𝑑𝑛𝑛 AF𝑖𝑒𝑑 ✗ 86.47 78.82 5.55 6.27\n",
            "D2Decoupling\n",
            "AblationG/B ✓\n",
            "✓ emb 𝑑𝑛𝑛 AF𝑖𝑒𝑑 ✗81.52 73.58 5.54 6.38\n",
            "D3 B/B ✓ 81.9 75.68 5.74 6.41\n",
            "D4 P/B ✗ 86.15 79.78 5.87 6.63\n",
            "D5 w/o Text Input 76.54 71.28 5.88 6.61\n",
            "D6 w/o Detach 80.47 74.67 5.55 6.32\n",
            "D7AR\n",
            "AblationP/B ✓\n",
            "w/o Frame Levelemb 𝑑𝑛𝑛 AF𝑖𝑒𝑑 ✗\n",
            "85.7 80.15 5.55 6.3\n",
            "D8 ✗ AF𝑖 85.04 80.15 5.71 6.49\n",
            "D9 emb 𝑝𝑝 AF𝑖𝑒𝑑 85.25 79.4 5.52 6.32\n",
            "D10 emb 𝑠𝑖𝑚 AF𝑖𝑒𝑑 84.88 79.42 5.55 6.3\n",
            "D11 emb 𝑑𝑛𝑛 AF𝑖𝑒 85.11 79.4 5.47 6.36\n",
            "D12ASR\n",
            "AblationP/B ✓ ✓\n",
            "emb 𝑑𝑛𝑛 AF𝑖𝑑✗\n",
            "85.89 80.2 5.64 6.4\n",
            "D13Two-Gran\n",
            "RescoringP/B ✓ ✓ emb 𝑑𝑛𝑛 AF𝑖𝑒𝑑 ✓ 86.47 78.82 5.41 6.13\n",
            "accent words. Table IV illustrates that the use of fine-grained\n",
            "phoneme units can improve the accuracy of all accents in\n",
            "the AR task. This suggests that fine-grained phonemes are\n",
            "more effective for AR tasks than coarse-grained BPE, and\n",
            "this advantage applies to all accents. We assume that the\n",
            "phonemes with the highest PER in the CTC branch represent\n",
            "accent pronunciation, and list them in Table IV. As shown\n",
            "in the table, these high-PER phonemes align with our gen-\n",
            "eral knowledge and previous findings in accent linguistic re-\n",
            "search [51–53]. This finding suggests that the phonemes which\n",
            "cause difficulties for the DIMNet indeed contain accents, and\n",
            "that our proposed model effectively captures accent-specific\n",
            "pronunciation-related information. Furthermore, we count the\n",
            "WERs of the attention branch for words that contain the top 5\n",
            "PER phonemes which are listed in Table IV and compare them\n",
            "to the average WERs of each accent in Fig. 4. As depicted\n",
            "in Fig. 4, the WERs of these accent words are higher than\n",
            "the average WERs, suggesting that they are more difficult\n",
            "to recognize in the ASR task, and hence contribute to the\n",
            "increase in the average WER. However, the introduction of\n",
            "two-granularity units results in a decrease in the WER of\n",
            "difficult words in each accent, compared to the case without\n",
            "it. This finding clearly demonstrates the effectiveness of our\n",
            "proposed scheme in improving the recognition of difficult\n",
            "words in different accents.\n",
            "C. Improving AR with ASR\n",
            "To investigate the impact of ASR on AR, we conduct the\n",
            "𝐷5experiment presented in Table III. In this experiment, we\n",
            "remove the text input of the LASAS-based accent branch in\n",
            "the𝐷1model and replace it with the outputs of the shared\n",
            "encoder. This design allowed us to eliminate the fusion of\n",
            "linguistic information from the ASR to the AR while keepingTABLE IV: Impact of fine-grained units on AR accuracy, as well as\n",
            "the fine-grained units with the highest PER. TGM in the table\n",
            "refers to two-granularity modeling. Only the results of the AESRC\n",
            "test set are shown.\n",
            "AccentAR ACC (%)Phonemes of Top 5 PER§\n",
            "w/ TGM†w/o TGM‡\n",
            "CHN 80.68 79.98 [ZH], [EH], [AO], [TH], [EY]\n",
            "IND 93.30 91.05 [OY], [ZH], [SH], [JH], [TH]\n",
            "JPN 72.30 68.90 [ZH], [L], [OW], [R], [AO]\n",
            "KR 83.15 79.39 [ZH], [OW], [AE], [AO], [UH]\n",
            "PT 80.76 75.92 [ZH], [UH], [AE], [AO], [EH]\n",
            "RU 74.49 70.39 [OY], [AE], [JH], [OW], [UH]\n",
            "UK 94.32 93.49 [NG], [AO], [OW], [ER], [AA]\n",
            "US 58.36 53.64 [UH], [AE], [AA], [ZH], [OW]\n",
            "†: This model is 𝐷1in the Table III.\n",
            "‡: This model is 𝐷3in the Table III.\n",
            "§: Correspondence between ARPABET and IPA phoneme sets:\n",
            "https://en.wikipedia.org/wiki/ARPABET\n",
            "the total parameters of the DIMNet model unchanged. After\n",
            "comparing the performance of 𝐷1and𝐷5, it is apparent that\n",
            "the exclusion of linguistic information from the CTC branch\n",
            "has a substantial impact on the accuracy of the accent branch,\n",
            "resulting in a decrease in the AR performance. This outcome\n",
            "clearly demonstrates the significance of the linguistic informa-\n",
            "tion from the CTC branch in enhancing the performance of\n",
            "AR tasks.\n",
            "Furthermore, we evaluate the modifications made to the\n",
            "original LASAS AR model [21] for its adaptation to the\n",
            "multi-task framework. Comparison of 𝐷6and𝐷1reveals that\n",
            "the detach operation of linguistic inputs and accent outputs\n",
            "in Fig. 1 of the accent branch has a significant positiveJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\n",
            "Fig. 4: WER for the word which contains the top 5 PER phonemes.\n",
            "impact on the performance of AR. In both the dev and test\n",
            "sets, the relative improvements are up to 6.94% and5.27%,\n",
            "respectively. The detach operation enables the AR task to focus\n",
            "solely on accent-specific information, thereby improving the\n",
            "effectiveness of accent branch optimization. Additionally, the\n",
            "detach operation reduces interference from the AR task to\n",
            "the ASR task, thereby improving ASR performance to some\n",
            "extent. These results demonstrate the significance of the detach\n",
            "operation of the accent branch in multi-task ASR-AR. By\n",
            "comparing𝐷7and𝐷1, we can analyze the impact of frame-\n",
            "level and utterance-level CE loss on accent prediction. The AR\n",
            "accuracy of𝐷7is slightly lower than that of 𝐷1in the dev set,\n",
            "but the opposite is observed in the test set. This suggests that\n",
            "the utterance-level loss used in 𝐷7can enhance the model’s\n",
            "generalization and mitigate overfitting. However, the frame-\n",
            "level accent information used in 𝐷1achieves a slightly better\n",
            "performance in the ASR task. We believe this is because the\n",
            "frame-level accent information can help correct fine-grained\n",
            "errors caused by accents in the ASR task. In our experience,\n",
            "accent speech utterances often consist of a small portion of\n",
            "words that exhibit accent characteristics, while the remaining\n",
            "words are pronounced in a standard manner. In other words,\n",
            "the accent words in a sentence pose a challenge for the ASR\n",
            "task but are easier to recognize for the AR task. Therefore,\n",
            "although utterance-level pooling operation indeed improves\n",
            "the overall accuracy in the AR task, it may not capture\n",
            "the distinguishing details of the accent words as effectively\n",
            "as without pooling. Hence, the selection of frame-level or\n",
            "utterance-level CE loss depends on the practical application\n",
            "and trade-off between the AR and ASR tasks.\n",
            "D. Improving ASR with AR\n",
            "We further investigate the benefits of incorporating AR\n",
            "information into the ASR task, similar to Section V-C. We\n",
            "first consider the 𝐷8experiment in Table III, where no accent\n",
            "embedding is fused to the attention branch. Comparing 𝐵1and\n",
            "𝐷8, we find that although 𝐷8lacks explicit accent embedding,\n",
            "it can still implicitly extract accent information through the\n",
            "shared encoder. This leads to significant improvements in the\n",
            "ASR task compared to 𝐵1. Specifically, the relative improve-\n",
            "ment in implicit accent fusion on the ASR dev and test sets is\n",
            "approximate 4.19% and6.08%, respectively. However, when\n",
            "comparing𝐷8with𝐷1, we observe that explicit fusion of the\n",
            "accent embedding to the ASR branch leads to better results,\n",
            "which underscores the value of AR-to-ASR interaction.Next, the impact of different accent embeddings is analyzed.\n",
            "ASR performance can be used to evaluate the quality of\n",
            "accent embedding since it serves the ASR task. The results\n",
            "in Table III show that emb𝑑𝑛𝑛 performs the best, while\n",
            "emb𝑠𝑖𝑚performs moderately well, and emb𝑝𝑝performs the\n",
            "worst. emb𝑑𝑛𝑛contains utterance-level accent classification\n",
            "information that is relatively stable, which is easier for an\n",
            "ASR model to recognize. On the other hand, emb𝑝𝑝loses a\n",
            "significant amount of acoustic information valuable to the ASR\n",
            "model due to compression by the final DNN layer. In addition,\n",
            "although emb𝑠𝑖𝑚andemb𝑝𝑝have the same dimensions in\n",
            "our experiments, emb𝑠𝑖𝑚is better. This finding suggests that\n",
            "accent shifts emb𝑠𝑖𝑚also have utility in improving ASR\n",
            "performance. However, since emb𝑠𝑖𝑚operate at the frame\n",
            "level, they are inherently more complex and variable, which\n",
            "presents a challenge for ASR models to effectively leverage\n",
            "this information.\n",
            "In experiments 𝐷11and𝐷12, we compare two additional\n",
            "accent fusion schemes AF𝑖𝑒andAF𝑖𝑑, both of which explicitly\n",
            "integrate an accent embedding into the attention branch of\n",
            "the ASR task. Comparing the performance of 𝐷1,𝐷11, and\n",
            "𝐷12, we find that using an encoder to process an accent\n",
            "embedding is more effective than using a decoder, but the\n",
            "best performance is achieved when both are used. The effec-\n",
            "tiveness of using an encoder to fuse accent embeddings is\n",
            "widely acknowledged, as the encoder can focus on acoustic\n",
            "information and be adjusted based on accent embedding.\n",
            "This enables the generation of a context representation that\n",
            "is easier for the decoder to understand. Although there are\n",
            "limited studies on fusing accent embeddings to a decoder,\n",
            "our experiments suggest that it is also an effective approach,\n",
            "as the decoder focuses more on linguistic information, and\n",
            "accent embeddings can help correct accent-specific words.\n",
            "The complementary roles of the encoder and decoder in\n",
            "utilizing accent embeddings explain why AF𝑖𝑒𝑑outperforms\n",
            "bothAF𝑖𝑒andAF𝑖𝑑. Therefore, integrating accent embeddings\n",
            "into both the encoder and decoder is essential to achieve better\n",
            "performance. Thanks to the triple-encoder structure, this can\n",
            "be easily accomplished in the attention branch.\n",
            "E. Two-granularity Rescoring\n",
            "In Table V, we evaluate the effectiveness of two-granularity\n",
            "rescoring. As shown in the table, attention rescoring yields\n",
            "an average relative reduction in WER of 1.63% within the\n",
            "CTC/attention framework. To mitigate the framework’s im-\n",
            "pact, we also assessed the effects of CTC rescoring on the\n",
            "DIMNet with single-granularity BPE units. The average WER\n",
            "decrease observed in this case is similar to that of attention\n",
            "rescoring, approximately relative 1.41%. In comparison to\n",
            "these two classic rescoring techniques, our proposed two-\n",
            "granularity rescoring achieves an average relative WER de-\n",
            "crease of 2.38%. In the DIMNet model, the CTC and atten-\n",
            "tion branches operate independently, and each branch has a\n",
            "different and complementary focus in terms of information\n",
            "granularity. Two-granularity rescoring effectively integrates\n",
            "the prediction scores of the two-granularity units, thereby\n",
            "complementing the limitations of the decoupling operationJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\n",
            "TABLE V: Effectiveness of two-granularity rescoring. In this table,\n",
            "ARS, CRS and TRS represent attention rescoring, CTC rescoring,\n",
            "and two-granularity rescoring decoding, respectively.\n",
            "ID ModelDecode Pass WER (%)\n",
            "1-st 2-nd Dev Test\n",
            "B1CTC/ATTATT - 5.96 6.91\n",
            "B1+ARS CTC ATT 5.86 6.80\n",
            "D3DIMNet w/o TGMATT - 5.74 6.41\n",
            "D3+CRS ATT CTC 5.65 6.33\n",
            "D1DIMNetATT - 5.55 6.27\n",
            "D1+TRS†ATT CTC 5.41 6.13\n",
            "†: This model is 𝐷13in the Table III.\n",
            "and leading to further improvements in the DIMNet model’s\n",
            "performance.\n",
            "F . A2P+P2W vs. A2P+A2W for Two-granularity Units\n",
            "As mentioned in Section II, a common approach for two-\n",
            "granularity modeling in ASR involves phoneme recognition\n",
            "and translation of phonemes into words or BPEs (A2P+P2W).\n",
            "However, this approach faces the challenges of error accumu-\n",
            "lation in the P2W stage. In contrast, the DIMNet directly rec-\n",
            "ognizes phonemes and BPEs from audio features (A2P+A2W).\n",
            "In this section, we compare the two approaches.\n",
            "TABLE VI: Comparison of A2P+P2W and A2P+A2W approaches\n",
            "to modeling two-granularity units. Only the AESRC test set results\n",
            "are shown.\n",
            "ModelCTC Dec\n",
            "PER (%)Att Dec\n",
            "WER (%)\n",
            "A2P + A2W†4.41 6.27\n",
            "A2P + Soft P2W 4.59 6.4\n",
            "A2P + Hard P2W 4.61 7.54\n",
            "†: This model is 𝐷1in the Table III.\n",
            "Table VI presents the results of two models that translate\n",
            "soft embeddings and hard OneHot vectors of phonemes into\n",
            "BPE, both based on the DIMNet. In both models, we main-\n",
            "tain the triple-encoder structure, but modify the input of the\n",
            "attention encoder to CTC phoneme information instead of the\n",
            "shared encoder outputs, while keeping the accent embedding\n",
            "concatenation unchanged. We aim for the attention branch to\n",
            "act as a P2W model in both schemes. To achieve this, we de-\n",
            "tach the outputs of the CTC branch as well as the accent branch\n",
            "to ensure that the attention branch solely focuses on translating\n",
            "phonemes into BPE. The results show that the change in\n",
            "PER for the CTC decoder is minimal because detaching the\n",
            "CTC phonemes makes the CTC branch relatively independent,\n",
            "ensuring a fair comparison of the P2W process. As shown in\n",
            "Table VI, Firstly, the WER of A2P+Soft P2W and A2P+Hard\n",
            "P2W is inferior to that of A2P+A2W, indicating the superiority\n",
            "of the DIMNet. Secondly, when the input phoneme sequence’s\n",
            "PER is equivalent, the WER of using soft embeddings is\n",
            "relative 17.81% higher than that of using OneHot vectors. This\n",
            "is because the CTC encoder outputs contain richer linguistic\n",
            "and acoustic information, while regular phonemes only contain\n",
            "linguistic information. However, in most two-granularity unitASR, hard OneHot vectors of phonemes are used as inputs,\n",
            "which limits the performance of P2W. Moreover, the use of\n",
            "completely correct phonemes during training and hypothesis\n",
            "phonemes during inference in P2W can cause a mismatch and\n",
            "error accumulation. In contrast, our triple-encoder scheme in-\n",
            "dependently models phonemes and BPEs by the CTC encoder\n",
            "and attention encoder, respectively, which can help mitigate\n",
            "error accumulation.\n",
            "G. Comparison with Previous Studies\n",
            "In Table VII, we present a comparison of our DIMNet\n",
            "with several other typical approaches on the AESRC dataset.\n",
            "The second row shows an ASR-AR cascade scheme [3] that\n",
            "achieves top-level performance on this dataset. Comparing\n",
            "it with our DIMNet, we can see that their model achieves\n",
            "a better result in the AR task, which is mainly due to\n",
            "extensive data augmentation. Without the data augmentation,\n",
            "their AR accuracy on the dev set is 84.51%, which is slightly\n",
            "lower than that of the DIMNet. Apart from the second row,\n",
            "the DIMNet significantly outperforms other schemes in the\n",
            "AR task. In particular, the DIMNet surpasses our previous\n",
            "LASAS AR model [21], which demonstrates the value of our\n",
            "improvements in the original LASAS. These results indicate\n",
            "that the DIMNet is highly competitive in AR tasks. In the ASR\n",
            "task, the first row is a CTC/attention ASR, while the second\n",
            "to fourth rows are typical multi-task ASR-AR introduced in\n",
            "Section II. Except for the first row, the rest of the models\n",
            "do not use language models. In a comparable situation, our\n",
            "DIMNet’s ASR performance surpasses the above schemes.\n",
            "This indicates that the DIMNet also has significant advantages\n",
            "in the ASR task. Moreover, the last row shows that after\n",
            "adding the LM, the performance of the DIMNet can be further\n",
            "improved. By comparing the DIMNet in the last row and\n",
            "the CTC/attention-based baseline in the first row, we obtain\n",
            "relative improvements of 21.45% and32.33% on the test sets\n",
            "of AR and ASR tasks, respectively. This fully demonstrates\n",
            "that our scheme is effective in English.\n",
            "TABLE VII: Comparison of different approaches on the AESRC\n",
            "dataset.\n",
            "ModelAR Task\n",
            "ACC (%)ASR Task\n",
            "WER (%)\n",
            "Dev Test Dev Test\n",
            "AESRC Baseline [4] 76.1 64.9 6.92 8.29\n",
            "ASR-AR Cascade [3] 91.13 83.63 5.53 6.56\n",
            "STJR [8]: ASR-AR Single-task 77 72.2 5.8 6.6\n",
            "MTJR [8]: ASR-AR Multi-task 82.4 75.2 6.2 7.1\n",
            "LASAS [21]: Only AR task 84.88 77.42 - -\n",
            "DIMNet w/ TRS†86.47 78.82 5.41 6.13\n",
            "DIMNet w/ TRS+LM 86.47 78.82 5.03 5.61\n",
            "†: This model is 𝐷13in the Table III.\n",
            "Table VIII presents the experimental results on the Ke-\n",
            "Speech dataset [49]. In the ASR task, the first row repre-\n",
            "sents an official baseline model [49] trained using the Es-\n",
            "pnet [47] tool, incorporating an LM. The second and thirdJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\n",
            "rows correspond to baselines that we trained ourselves using\n",
            "the Wenet [45] tools. All three rows are to CTC/attention\n",
            "frameworks. For the AR task, the KeSpeech baseline [49]\n",
            "is a ResNet34 [54] model. We train the CTC/attention and\n",
            "DIMNet models under a comparable conditions. As shown in\n",
            "the table, the DIMNet outperforms both baselines significantly\n",
            "on both AR and ASR tasks. Specifically, the DIMNet achieves\n",
            "a relative improvement of 28.53% over the KeSpeech baseline\n",
            "on the AR task and a relative improvement 14.55% on the\n",
            "ASR task, demonstrating the effectiveness of the DIMNet in\n",
            "Chinese. Notably, unlike phoneme/BPE units, syllable/char\n",
            "units have the same time steps, indicating the robustness of\n",
            "the DIMNet to the time steps of coarse and fine-grained units,\n",
            "which makes it applicable to other languages as well.\n",
            "TABLE VIII: Comparison of different approaches on the KeSpeech\n",
            "dataset. The ASR CER in the first line is obtained by the utterance\n",
            "number weighted average of the results in [49]. And the AR ACC\n",
            "is the average of 6 accents.\n",
            "ModelAR Task\n",
            "ACC (%)ASR Task\n",
            "CER (%)\n",
            "Dev Test Dev Test\n",
            "KeSpeech Baseline [49] - 61.13 - 10.38\n",
            "CTC/ATT w/ ARS - - 6.05 9.54\n",
            "CTC/ATT w/ ARS + LM - - 5.95 9.39\n",
            "DIMNet w/ TRS 80.06 78.57 5.90 9.40\n",
            "DIMNet w/ TRS + LM 80.06 78.57 5.71 8.87\n",
            "VI. C ONCLUSIONS\n",
            "In this paper, we propose the DIMNet, a multi-task frame-\n",
            "work for joint ASR-AR tasks. Our approach first decouples the\n",
            "AR and ASR tasks using a triple-encoder structure that can\n",
            "model two-granularity units in each task. Then we enhance\n",
            "the interaction between the two tasks by introducing and\n",
            "improving the LASAS AR model and studying the selection\n",
            "and fusion of accent embeddings. Finally, we develop a\n",
            "two-granularity rescoring scheme that effectively combines\n",
            "two-granularity scores to further enhance ASR performance.\n",
            "Experimental results demonstrate that our scheme achieves\n",
            "relative improvements in AR accuracy of 21.45% and28.53%,\n",
            "as well as relative reductions in ASR error rate of 32.33% and\n",
            "14.55% on test sets of the AESRC and KeSpeech datasets,\n",
            "respectively, compared to the E2E baselines. Looking forward,\n",
            "we aim to further reduce the computational complexity of the\n",
            "DIMNet and extend its application to multilingual ASR tasks.\n",
            "REFERENCES\n",
            "[1] R. Lippi-Green, English with an accent: Language, ideol-\n",
            "ogy, and discrimination in the United States . Routledge,\n",
            "2012.\n",
            "[2] C. Huang, T. Chen, and E. Chang, “Accent issues in large\n",
            "vocabulary continuous speech recognition,” International\n",
            "Journal of Speech Technology , vol. 7, no. 2, pp. 141–153,\n",
            "2004.[3] Y . Qian, X. Gong, and H. Huang, “Layer-wise fast\n",
            "adaptation for end-to-end multi-accent speech recogni-\n",
            "tion,” IEEE/ACM Transactions on Audio, Speech, and\n",
            "Language Processing (TASLP) , vol. 30, pp. 2842–2853,\n",
            "2022.\n",
            "[4] X. Shi, F. Yu, Y . Lu, Y . Liang, Q. Feng, D. Wang,\n",
            "Y . Qian, and L. Xie, “The accented English speech recog-\n",
            "nition challenge 2020: Open datasets, tracks, baselines,\n",
            "results and methods,” in IEEE International Conference\n",
            "on Acoustics, Speech and Signal Processing (ICASSP) ,\n",
            "2021, pp. 6918–6922.\n",
            "[5] Y . Yang, H. Shi, Y . Lin, M. Ge, L. Wang, Q. Hou, and\n",
            "J. Dang, “Adaptive attention network with domain ad-\n",
            "versarial training for multi-accent speech recognition,” in\n",
            "International Symposium on Chinese Spoken Language\n",
            "Processing (ISCSLP) , 2022, pp. 6–10.\n",
            "[6] K. Deng and P. C. Woodland, “Adaptable end-to-end asr\n",
            "models using replaceable internal lms and residual soft-\n",
            "max,” in IEEE International Conference on Acoustics,\n",
            "Speech and Signal Processing (ICASSP) , 2023, pp. 1–5.\n",
            "[7] A. Jain, M. Upreti, and P. Jyothi, “Improved accented\n",
            "speech recognition using accent embeddings and multi-\n",
            "task learning,” in Proceedings of the Annual Conference\n",
            "of the International Speech Communication Association\n",
            "(INTERSPEECH) , 2018, pp. 2454–2458.\n",
            "[8] J. Zhang, Y . Peng, P. Van Tung, H. Xu, H. Huang, and\n",
            "E. S. Chng, “E2E-based multi-task learning approach\n",
            "to joint speech and accent recognition,” in Proceedings\n",
            "of the Annual Conference of the International Speech\n",
            "Communication Association (INTERSPEECH) , 2021, pp.\n",
            "876–880.\n",
            "[9] S. Toshniwal, T. N. Sainath, R. J. Weiss, B. Li,\n",
            "P. Moreno, E. Weinstein, and K. Rao, “Multilingual\n",
            "speech recognition with a single end-to-end model,” in\n",
            "IEEE International Conference on Acoustics, Speech and\n",
            "Signal Processing (ICASSP) , 2018, pp. 4904–4908.\n",
            "[10] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, “An\n",
            "interactive multi-task learning network for end-to-end\n",
            "aspect-based sentiment analysis,” in Proceedings of the\n",
            "Annual Meeting of the Association for Computational\n",
            "Linguistics (ACL) , 2019, pp. 504–515.\n",
            "[11] Y . Pang, X. Zhao, L. Zhang, and H. Lu, “Multi-scale\n",
            "interactive network for salient object detection,” in Pro-\n",
            "ceedings of the IEEE/CVF conference on computer vision\n",
            "and pattern recognition (CVPR) , 2020, pp. 9413–9422.\n",
            "[12] R. Sennrich, B. Haddow, and A. Birch, “Neural machine\n",
            "translation of rare words with subword units,” in Pro-\n",
            "ceedings of the Annual Meeting of the Association for\n",
            "Computational Linguistics (ACL) , 2016, pp. 1715–1725.\n",
            "[13] H. Huang, X. Xiang, Y . Yang, R. Ma, and Y . Qian,\n",
            "“AISpeech-SJTU accent identification system for the\n",
            "accented English speech recognition challenge,” in IEEE\n",
            "International Conference on Acoustics, Speech and Sig-\n",
            "nal Processing (ICASSP) , 2021, pp. 6254–6258.\n",
            "[14] M. Najafian, A. DeMarco, S. Cox, and M. Russell,\n",
            "“Unsupervised model selection for recognition of re-\n",
            "gional accented speech,” in Proceedings of the Annual\n",
            "Conference of the International Speech CommunicationJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\n",
            "Association (INTERSPEECH) , 2014.\n",
            "[15] A. Hanani and R. Naser, “Spoken Arabic dialect recog-\n",
            "nition using X-vectors,” Natural Language Engineering ,\n",
            "vol. 26, no. 6, pp. 691–700, 2020.\n",
            "[16] M. A. T. Turan, E. Vincent, and D. Jouvet, “Achiev-\n",
            "ing multi-accent ASR via unsupervised acoustic model\n",
            "adaptation,” in Proceedings of the Annual Conference\n",
            "of the International Speech Communication Association\n",
            "(INTERSPEECH) , 2020, pp. 1286–1290.\n",
            "[17] S. A. Chowdhury, A. M. Ali, S. Shon, and J. R. Glass,\n",
            "“What does an end-to-end dialect identification model\n",
            "learn about non-dialectal information?” in Proceedings\n",
            "of the Annual Conference of the International Speech\n",
            "Communication Association (INTERSPEECH) , 2020, pp.\n",
            "462–466.\n",
            "[18] K. Deng, S. Cao, and L. Ma, “Improving accent identi-\n",
            "fication and accented speech recognition under a frame-\n",
            "work of self-supervised learning,” in Proceedings of the\n",
            "Annual Conference of the International Speech Commu-\n",
            "nication Association (INTERSPEECH) , 2021, pp. 881–\n",
            "885.\n",
            "[19] Q. Gao, H. Wu, Y . Sun, and Y . Duan, “An end-to-\n",
            "end speech accent recognition method based on hybrid\n",
            "CTC/attention Transformer ASR,” in IEEE International\n",
            "Conference on Acoustics, Speech and Signal Processing\n",
            "(ICASSP) , 2021, pp. 7253–7257.\n",
            "[20] S. Sun, C.-F. Yeh, M.-Y . Hwang, M. Ostendorf, and\n",
            "L. Xie, “Domain adversarial training for accented\n",
            "speech recognition,” in IEEE International Conference\n",
            "on Acoustics, Speech and Signal Processing (ICASSP) ,\n",
            "2018, pp. 4854–4858.\n",
            "[21] Q. Shao, J. Yan, J. Kang, P. Guo, X. Shi, P. Hu, and\n",
            "L. Xie, “Linguistic-acoustic similarity based accent shift\n",
            "for accent recognition,” in Proceedings of the Annual\n",
            "Conference of the International Speech Communication\n",
            "Association (INTERSPEECH) , 2022, pp. 3719–3723.\n",
            "[22] R. Imaizumi, R. Masumura, S. Shiota, and H. Kiya,\n",
            "“Dialect-aware modeling for end-to-end Japanese di-\n",
            "alect speech recognition,” in Asia-Pacific Signal and\n",
            "Information Processing Association Annual Summit and\n",
            "Conference (APSIPA ASC) , 2020, pp. 297–301.\n",
            "[23] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber,\n",
            "“Connectionist temporal classification: Labelling unseg-\n",
            "mented sequence data with recurrent neural networks,” in\n",
            "Proceedings of the International Conference on Machine\n",
            "Learning (ICML) , 2006, pp. 369–376.\n",
            "[24] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang,\n",
            "J. Yu, W. Han, S. Wang, Z. Zhang, Y . Wu et al. , “Con-\n",
            "former: Convolution-augmented Transformer for speech\n",
            "recognition,” in Proceedings of the Annual Conference\n",
            "of the International Speech Communication Association\n",
            "(INTERSPEECH) , 2020, pp. 5036–5040.\n",
            "[25] Z. Zhang, Y . Wang, and J. Yang, “Accent recognition\n",
            "with hybrid phonetic features,” Sensors , vol. 21, no. 18,\n",
            "p. 6258, 2021.\n",
            "[26] H. Hu, X. Yang, Z. Raeesy, J. Guo, G. Keskin, H. Ar-\n",
            "sikere, A. Rastrow, A. Stolcke, and R. Maas, “Redat:\n",
            "Accent-invariant representation for end-to-end ASR bydomain adversarial training with relabeling,” in IEEE In-\n",
            "ternational Conference on Acoustics, Speech and Signal\n",
            "Processing (ICASSP) , 2021, pp. 6408–6412.\n",
            "[27] T. Viglino, P. Motlicek, and M. Cernak, “End-to-end ac-\n",
            "cented speech recognition,” in Proceedings of the Annual\n",
            "Conference of the International Speech Communication\n",
            "Association (INTERSPEECH) , 2019, pp. 2140–2144.\n",
            "[28] X. Yang, K. Audhkhasi, A. Rosenberg, S. Thomas,\n",
            "B. Ramabhadran, and M. Hasegawa-Johnson, “Joint\n",
            "modeling of accents and acoustics for multi-accent\n",
            "speech recognition,” in IEEE International Conference\n",
            "on Acoustics, Speech and Signal Processing (ICASSP) ,\n",
            "2018, pp. 1–5.\n",
            "[29] A. Yadavalli, G. Mirishkar, and A. K. Vuppala, “Multi-\n",
            "task end-to-end model for Telugu dialect and speech\n",
            "recognition,” in Proceedings of the Annual Conference\n",
            "of the International Speech Communication Association\n",
            "(INTERSPEECH) , 2022, pp. 1387–1391.\n",
            "[30] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli,\n",
            "“Wav2Vec 2.0: A framework for self-supervised learning\n",
            "of speech representations,” Advances in Neural Infor-\n",
            "mation Processing Systems (NIPS) , vol. 33, pp. 12 449–\n",
            "12 460, 2020.\n",
            "[31] X. Gong, Y . Lu, Z. Zhou, and Y . Qian, “Layer-wise fast\n",
            "adaptation for end-to-end multi-accent speech recogni-\n",
            "tion,” in Proceedings of the Annual Conference of the\n",
            "International Speech Communication Association (IN-\n",
            "TERSPEECH) , 2021, pp. 4501–4505.\n",
            "[32] B. Desplanques, J. Thienpondt, and K. Demuynck,\n",
            "“ECAPA-TDNN: Emphasized channel attention, propa-\n",
            "gation and aggregation in tdnn based speaker verifica-\n",
            "tion,” in Proceedings of the Annual Conference of the\n",
            "International Speech Communication Association (IN-\n",
            "TERSPEECH) , 2020, pp. 3830–3834.\n",
            "[33] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and\n",
            "K. J. Lang, “Phoneme recognition using time-delay neu-\n",
            "ral networks,” IEEE Transactions on Acoustics, Speech,\n",
            "and Signal Processing , vol. 37, no. 3, pp. 328–339, 1989.\n",
            "[34] K. Rao and H. Sak, “Multi-accent speech recognition\n",
            "with hierarchical grapheme based models,” in IEEE In-\n",
            "ternational Conference on Acoustics, Speech and Signal\n",
            "Processing (ICASSP) , 2017, pp. 4815–4819.\n",
            "[35] W. Chan and I. R. Lane, “On online attention-based\n",
            "speech recognition and joint Mandarin character-pinyin\n",
            "training,” in Proceedings of the Annual Conference of the\n",
            "International Speech Communication Association (IN-\n",
            "TERSPEECH) , 2016, pp. 3404–3408.\n",
            "[36] S. Zhou, L. Dong, S. Xu, and B. Xu, “A comparison of\n",
            "modeling units in sequence-to-sequence speech recog-\n",
            "nition with the Transformer on Mandarin Chinese,” in\n",
            "International Conference on Neural Information Process-\n",
            "ing (ICONIP) , 2018, pp. 210–220.\n",
            "[37] Z. Chen, Q. Liu, H. Li, and K. Yu, “On modular\n",
            "training of neural acoustics-to-word model for LVCSR,”\n",
            "inIEEE International Conference on Acoustics, Speech\n",
            "and Signal Processing (ICASSP) , 2018, pp. 4754–4758.\n",
            "[38] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based\n",
            "sequence-to-sequence speech recognition with the Trans-JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\n",
            "former in Mandarin Chinese,” in Proceedings of the\n",
            "Annual Conference of the International Speech Commu-\n",
            "nication Association (INTERSPEECH) , 2018, pp. 791–\n",
            "795.\n",
            "[39] J. Yuan, X. Cai, D. Gao, R. Zheng, L. Huang, and\n",
            "K. Church, “Decoupling recognition and transcription in\n",
            "Mandarin ASR,” in IEEE Automatic Speech Recognition\n",
            "and Understanding Workshop (ASRU) , 2021, pp. 1019–\n",
            "1025.\n",
            "[40] X. Wang, Z. Yao, X. Shi, and L. Xie, “Cascade RNN-\n",
            "transducer: Syllable based streaming on-device Man-\n",
            "darin speech recognition with a syllable-to-character con-\n",
            "verter,” in IEEE Spoken Language Technology Workshop\n",
            "(SLT) , 2021, pp. 15–21.\n",
            "[41] S. Zhang, J. Yi, Z. Tian, Y . Bai, J. Tao et al. , “Decou-\n",
            "pling pronunciation and language for end-to-end code-\n",
            "switching automatic speech recognition,” in IEEE In-\n",
            "ternational Conference on Acoustics, Speech and Signal\n",
            "Processing (ICASSP) , 2021, pp. 6249–6253.\n",
            "[42] Y . Yang, B. Du, and Y . Li, “Multi-level modeling\n",
            "units for end-to-end Mandarin speech recognition,” arXiv\n",
            "preprint arXiv:2205.11998 , 2022.\n",
            "[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\n",
            "L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n",
            "“Attention is all you need,” Advances in Neural Infor-\n",
            "mation Processing Systems (NIPS) , vol. 30, 2017.\n",
            "[44] Z. Tang, L. Li, D. Wang, and R. Vipperla, “Collab-\n",
            "orative joint training with multitask recurrent model\n",
            "for speech and speaker recognition,” IEEE/ACM Trans-\n",
            "actions on Audio, Speech, and Language Processing\n",
            "(TASLP) , vol. 25, pp. 493–504, 2016.\n",
            "[45] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang,\n",
            "Z. Peng, X. Chen, L. Xie, and X. Lei, “Wenet: Pro-\n",
            "duction oriented streaming and non-streaming end-to-end\n",
            "speech recognition toolkit,” in Proceedings of the Annual\n",
            "Conference of the International Speech Communication\n",
            "Association (INTERSPEECH) , 2021, pp. 2093–2097.\n",
            "[46] A. Klautau, “Arpabet and the timit alphabet,” an archived\n",
            "file. https://web.archive.org/web/20160603180727/http:\n",
            "//www.laps.ufpa.br/aldebaro/papers/ak_arpabet01.pdf\n",
            "(Accessed Mar. 12, 2020) , 2001.\n",
            "[47] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and\n",
            "T. Hayashi, “Hybrid CTC/attention architecture for end-\n",
            "to-end speech recognition,” IEEE Journal of Selected\n",
            "Topics in Signal Processing , vol. 11, no. 8, pp. 1240–\n",
            "1253, 2017.\n",
            "[48] J. Li, Y . Wu, Y . Gaur, C. Wang, R. Zhao, and\n",
            "S. Liu, “On the comparison of popular end-to-end mod-\n",
            "els for large scale speech recognition,” arXiv preprint\n",
            "arXiv:2005.14327 , 2020.\n",
            "[49] Z. Tang, D. Wang, Y . Xu, J. Sun, X. Lei, S. Zhao,\n",
            "C. Wen, X. Tan, C. Xie, S. Zhou et al. , “KeSpeech: An\n",
            "open source speech dataset of Mandarin and its eight\n",
            "subdialects,” 2021.\n",
            "[50] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur,\n",
            "“Librispeech: An ASR corpus based on public do-\n",
            "main audio books,” in IEEE International Conference\n",
            "on Acoustics, Speech and Signal Processing (ICASSP) ,2015, pp. 5206–5210.\n",
            "[51] C. Yarra, R. Aggarwal, A. Rajpal, and P. K. Ghosh,\n",
            "“Indic TIMIT and Indic English lexicon: A speech\n",
            "database of Indian speakers using TIMIT stimuli and a\n",
            "lexicon from their mispronunciations,” in Conference of\n",
            "the Oriental COCOSDA International Committee for the\n",
            "Co-ordination and Standardisation of Speech Databases\n",
            "and Assessment Techniques (O-COCOSDA) , 2019, pp.\n",
            "1–6.\n",
            "[52] K. Igarashi and I. Wilson, “Improving Japanese English\n",
            "pronunciation with speech recognition and feed-back\n",
            "system,” in SHS Web of Conferences , 2020, p. 02003.\n",
            "[53] F. Han, “Pronunciation problems of Chinese learners of\n",
            "English,” ORTESOL Journal , vol. 30, pp. 26–30, 2013.\n",
            "[54] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\n",
            "learning for image recognition,” in Proceedings of the\n",
            "IEEE/CVF Conference on Computer Vision and Pattern\n",
            "Recognition (CVPR) , 2016, pp. 770–778.Joint Sensing and Semantic Communications with\n",
            "Multi-Task Deep Learning\n",
            "Yalin E. Sagduyu1, Tugba Erpek1, Aylin Yener2, and Sennur Ulukus3\n",
            "1Virginia Tech, Arlington, V A, USA\n",
            "2The Ohio State University, Columbus, OH, USA\n",
            "3University of Maryland, College Park, MD, USA\n",
            "Abstract —This paper explores the integration of deep learn-\n",
            "ing techniques for joint sensing and communications, with an\n",
            "extension to semantic communications. The integrated system\n",
            "comprises a transmitter and receiver operating over a wireless\n",
            "channel, subject to noise and fading effects. The transmitter\n",
            "employs a deep neural network, namely an encoder, for joint\n",
            "operations of source coding, channel coding, and modulation,\n",
            "while the receiver utilizes another deep neural network, namely a\n",
            "decoder, for joint operations of demodulation, channel decoding,\n",
            "and source decoding to reconstruct the data samples. The trans-\n",
            "mitted signal serves a dual purpose, supporting communication\n",
            "with the receiver and enabling sensing. When a target is present,\n",
            "the reflected signal is received, and another deep neural network\n",
            "decoder is utilized for sensing. This decoder is responsible for\n",
            "detecting the target’s presence and determining its range. All\n",
            "these deep neural networks, including one encoder and two\n",
            "decoders, undergo joint training through multi-task learning,\n",
            "considering data and channel characteristics. This paper extends\n",
            "to incorporate semantic communications by introducing an ad-\n",
            "ditional deep neural network, another decoder at the receiver,\n",
            "operating as a task classifier. This decoder evaluates the fidelity of\n",
            "label classification for received signals, enhancing the integration\n",
            "of semantics within the communication process. The study\n",
            "presents results based on using the CIFAR-10 as the input data\n",
            "and accounting for channel effects like Additive White Gaussian\n",
            "Noise (A WGN) and Rayleigh fading. The results underscore the\n",
            "effectiveness of multi-task deep learning in achieving high-fidelity\n",
            "joint sensing and semantic communications.\n",
            "Index Terms —Joint sensing and communications, integrated\n",
            "sensing and communications, semantic communications, deep\n",
            "learning, multi-task learning.\n",
            "I. I NTRODUCTION\n",
            "Joint sensing and communications (also referred to as inte-\n",
            "grated sensing and communications ) has been envisioned as\n",
            "a pivotal concept for next-generation communication systems\n",
            "such as 6G and Wi-Fi 7 by unifying information gathering\n",
            "(sensing) and information exchange (communication) within\n",
            "a common framework [1]–[3]. This integration is instrumental\n",
            "for real-time decision-making, efficient resource utilization,\n",
            "enhancing adaptability in challenging environments, and en-\n",
            "abling context-aware data sharing in a variety of wireless\n",
            "networks, such as Internet of Things [4] and vehicle-to-\n",
            "infrastructure networks [5], [6]. To that end, communication\n",
            "signals can be dual-purposed for sensing, enabling tasks like\n",
            "object detection, tracking, and environmental monitoring. One\n",
            "example of leveraging wireless signals for sensing is WiFi\n",
            "sensing, where WiFi signals that are emitted as part of theirstandard operation are utilized for sensing such that their\n",
            "reflected signals that bounce off objects are collected to detect\n",
            "objects, movements, and activities in an environment. Unlike\n",
            "traditional WiFi, where the primary goal is data communi-\n",
            "cation, WiFi sensing repurposes the WiFi signals to serve\n",
            "as a sensing medium. This approach also distinguishes itself\n",
            "from conventional sensing methods like radar, which involve\n",
            "the transmission of specialized radio waves and subsequent\n",
            "detection of reflected signals, as opposed to utilizing existing\n",
            "communication signals.\n",
            "While sensing and communication functionalities have been\n",
            "separated in conventional designs and achieved by various sig-\n",
            "nal processing modules, they can be integrated by exploiting\n",
            "the broadcast nature of wireless signals. Fig. 1 illustrates the\n",
            "three cases of individual communications, individual sensing\n",
            "and joint sensing and communications. In joint sensing and\n",
            "communications , wireless signals can be designed purposely to\n",
            "serve both communication and sensing functionalities jointly\n",
            "to enable applications that require real-time environmental\n",
            "data such as augmented reality, virtual reality or autonomous\n",
            "vehicles. This joint design can benefit from the use of deep\n",
            "neural networks (DNNs) by harnessing the power of deep\n",
            "learning to learn and adapt to wireless signal characteristics.\n",
            "Pairs of DNNs have been used in form of autoencoders to\n",
            "model transmitter and receiver functionalities of communica-\n",
            "tions [7] and can be extended to perform joint sensing and\n",
            "communications (encompassing joint operations of channel\n",
            "coding and modulation at the transmitter and corresponding\n",
            "operations at the receiver) [8], [9].\n",
            "In communication systems, DNNs can effectively represent\n",
            "(i) transmitter blocks including source coding in addition to\n",
            "channel coding and modulation and (ii) receiver blocks of de-\n",
            "modulation and channel decoding followed by source decoding\n",
            "to reconstruct data samples such as images beyond bit/symbol\n",
            "representations. These DNNs can be trained either separately\n",
            "or better jointly to optimize the fidelity of communications\n",
            "such as minimizing the reconstruction loss. On the other hand,\n",
            "sensing functionality can also benefit from using DNNs for the\n",
            "generation of sensing waveform as well as detection of targets\n",
            "with the ultimate goal of maximizing the sensing accuracy. In\n",
            "this paper, we combine the use of DNNs for joint sensing and\n",
            "communications in a multi-task learning framework and show\n",
            "that they can efficiently learn and adapt to input data (such asarXiv:2311.05017v1  [cs.NI]  8 Nov 2023Fig. 1: From individual sensing and communications to joint sensing and\n",
            "communications.\n",
            "images), channel conditions, and target characteristics.\n",
            "In this framework, an encoder DNN is used at the transmit-\n",
            "ter that takes data samples such as images as the input and\n",
            "generates wireless signals to be transmitted over a wireless\n",
            "channel by jointly performing source coding, channel coding,\n",
            "and modulation operations. The output of the encoder provides\n",
            "a latent representation that provides smaller dimension than the\n",
            "input sample, thereby reducing the number of channel uses for\n",
            "more efficient resource utilization. At the receiver, a decoder\n",
            "DNN operates on the received signals to reconstruct data sam-\n",
            "ples by jointly performing demodulation, channel decoding,\n",
            "and source decoding operations. These two DNNs can be\n",
            "jointly trained for end-to-end optimization of transmitter and\n",
            "receiver functionalities by adapting to input data and channel\n",
            "characteristics.\n",
            "To integrate sensing with communications, we assume the\n",
            "encoder’s output signal is also used for sensing purposes. In\n",
            "cases where a target is present, the signal reflects from the\n",
            "target, and the transmitter captures the reflected signal. To\n",
            "identify the presence and range of the target, the transmitter\n",
            "employs a third DNN, namely a decoder, for sensing. This\n",
            "decoder of the transmitter can be jointly trained with the\n",
            "encoder of the transmitter and the other decoder at the receiver\n",
            "to facilitate joint sensing and communications by accounting\n",
            "for input data, channel, and potential target characteristics.\n",
            "For that purpose, a multi-task learning approach is employed\n",
            "where the training is performed for the combined loss for two\n",
            "tasks, namely reconstruction of input samples at the receiver\n",
            "and sensing (target detection and target position classification)\n",
            "at the transmitter.\n",
            "While conventional communications primarily focuses on\n",
            "the fidelity of information transfer by minimizing some form\n",
            "of reconstruction loss, there is a growing interest in preserving\n",
            "meaning during the information transfer . To that end, seman-\n",
            "tic communications has emerged as a novel communication\n",
            "paradigm that goes beyond the mere exchange of data or\n",
            "information and aims to convey the meaning, intent, and\n",
            "context behind the information being shared [10]–[12]. Inother words, it focuses on understanding and interpreting the\n",
            "significance of the transmitted messages, rather than just the\n",
            "raw data itself. Semantic communications can be formulated\n",
            "by training DNNs at the transmitter-receiver pair for data\n",
            "reconstruction while preserving semantic information [13],\n",
            "[14].\n",
            "In this paper, we integrate semantic communications with\n",
            "joint sensing and communications . A fourth DNN, namely\n",
            "another decoder at the receiver, is used to validate the meaning\n",
            "conveyed to the receiver by performing a deep learning task\n",
            "(such as classification of transferred images to their labels) on\n",
            "received signals. This DNN is jointly trained with the other\n",
            "DNNs for joint sensing and communications in the multi-task\n",
            "learning framework where the task of sensing is added. Note\n",
            "that this joint sensing and semantic communications can be\n",
            "converted to joint sensing and task-oriented communications\n",
            "by removing the data reconstruction objective and replacing\n",
            "semantic information validation with a general task classifier\n",
            "such as used in deep learning-based task (or goal)-oriented\n",
            "communications [15],\n",
            "For performance evaluation, images from CIFAR-10 dataset\n",
            "are used for the input data samples. Both Additive White\n",
            "Gaussian Noise (AWGN) and Rayleigh fading channels are\n",
            "considered. Results are collected for the reconstruction loss,\n",
            "sensing accuracy (both for target detection and identification\n",
            "of its range) as well as the task classification accuracy that\n",
            "quantifies how the meaning of information is preserved at the\n",
            "receiver. These measures are obtained by varying the sensing\n",
            "signal-to-noise ratio (SNR), namely the SNR for the signal\n",
            "reflected by the target and received at the transmitter), the\n",
            "communication SNR, namely the SNR for the signal received\n",
            "at the receiver, the number of potential target ranges, and the\n",
            "size of encoder input (namely a representative of the number\n",
            "of channel uses or compression level). Results show that multi-\n",
            "task learning with joint training of DNNs provides an effective\n",
            "optimization framework for joint sensing and communications\n",
            "as well as its extension with semantic communications, and\n",
            "achieves high fidelity in terms of reconstructing data samples,\n",
            "preserving the meaning of information transfer and sensing\n",
            "targets.\n",
            "The rest of the paper is organized as follows. Sec. II\n",
            "describes multi-task learning for joint sensing and communica-\n",
            "tions and its extension to semantic communications. Sec. III\n",
            "evaluates the performance. Sec. IV concludes the paper by\n",
            "highlighting the main points covered and discussing future\n",
            "research directions.\n",
            "II. J OINT SENSING AND COMMUNICATIONS VIA\n",
            "MULTI -TASK DEEPLEARNING\n",
            "Fig. 2 shows the system model for joint sensing and\n",
            "communications as an extension of system models for con-\n",
            "ventional communications and conventional sensing. DNNs\n",
            "are employed at the transmitter and the receiver. These DNNs\n",
            "are jointly trained via multi-task learning with taking the\n",
            "channel characteristics into account to provide the unifiedFig. 2: System model for joint sensing and (semantic) communications in\n",
            "comparison with conventional (individual) sensing and communications.\n",
            "functionalities of sensing and communications along with the\n",
            "extension to semantic communications.\n",
            "Encoder and decoders for joint sensing and semantic com-\n",
            "munications. The system model shown in Fig. 2 encompasses\n",
            "several key components, including an encoder at the transmit-\n",
            "ter, a decoder at the receiver for data reconstruction (Decoder\n",
            "1), a decoder at the transmitter for sensing of targets (Decoder\n",
            "2), and a decoder at the receiver for validating semantic\n",
            "information to enable semantic communications (Decoder 3).\n",
            "•Encoder at the transmitter : The encoder is a DNN\n",
            "responsible for processing input data, which includes\n",
            "samples like images. It performs source coding, channel\n",
            "coding, and modulation operations on the input data to\n",
            "generate wireless signals for transmission. The output of\n",
            "the encoder is a latent-space representation of the input\n",
            "data samples and serves a dual purpose: it facilitates\n",
            "data communication and is used for sensing purposes.\n",
            "Overall, the encoder output has a smaller dimension than\n",
            "the input samples, thereby relying on a small number\n",
            "of channel uses. By jointly optimizing the roles conven-\n",
            "tionally played by source coding, channel coding and\n",
            "modulation, it ensures efficient resource utilization and\n",
            "minimizes transmission and semantic errors.\n",
            "•Decoder for information reconstruction at the receiver :\n",
            "Decoder 1 is a DNN situated at the receiver and re-\n",
            "sponsible for reconstructing data samples (inputs to the\n",
            "encoder at the transmitter). Decoder 1 jointly performs\n",
            "the operations of demodulating received signals, decodingchannel-encoded data, and reconstructing the original\n",
            "data samples with source decoding. Decoder 1 can be\n",
            "trained by minimizing the reconstruction loss such as the\n",
            "mean squared error (MSE) as the loss function.\n",
            "•Decoder for sensing at the transmitter : Decoder 2, po-\n",
            "sitioned at the transmitter, takes on the role of sensing.\n",
            "When a target is present in the environment, the signal\n",
            "transmitted by the encoder reflects off the target and\n",
            "returns to the transmitter as in radio detection and ranging\n",
            "(radar) systems. Decoder 2 is responsible for detecting the\n",
            "presence of the target (a binary classification problem)\n",
            "as well as classifying its range within the environment\n",
            "(a multi-label classification problem) in the presence of\n",
            "channel and clutter effects. Decoder 2 can be trained\n",
            "by minimizing the categorical cross-entropy as the loss\n",
            "function.\n",
            "•Decoder for semantic task classification at the receiver :\n",
            "Decoder 3 is another DNN situated at the receiver, serving\n",
            "the crucial role of validating the semantic content of the\n",
            "received data. It performs deep learning tasks that go\n",
            "beyond simple data reconstruction, such as classifying\n",
            "received signals into the labels of the corresponding data\n",
            "samples. By validating the semantic content, it ensures\n",
            "that the meaning behind the transmitted information is\n",
            "preserved and reliably interpreted. Decoder 3 can be\n",
            "trained by minimizing the categorical cross-entropy as\n",
            "the loss function for the underlying task classifier (such\n",
            "as identifying labels or the subsets of labels for the\n",
            "corresponding data samples that are given as input to the\n",
            "encoder at the transmitter).\n",
            "The encoder and decoder architectures are parameterized\n",
            "by the encoder output size. DNN architectures are selected by\n",
            "starting with a small number of layers and small layer sizes,\n",
            "and gradually increasing them until the best performance is\n",
            "achieved. The resulting DNN architectures are shown in Fig. 3,\n",
            "when the encoder output size is selected as 20. A convolutional\n",
            "neural network (CNN) is trained for the encoder and feedfor-\n",
            "ward neural networks (FNNs) are trained for the decoders.\n",
            "CNN consists of convolution layers (with the appropriate\n",
            "kernels and filters), maxpooling layers (for downsampling)\n",
            "and dropout layers (to prevent overfitting). ReLU activation\n",
            "is used at the hidden layers, linear activation is used at\n",
            "the encoder output (to generate signal for transmission over\n",
            "the wireless channel) and Decoder 1 outputs (to reconstruct\n",
            "input samples), and Softmax activation is used at the Decoder\n",
            "2 output (for sensing decision) and Decoder 3 output (for\n",
            "semantic validation).\n",
            "Joint training with multi-task learning. Multi-task learning\n",
            "is the foundational technique that allows us to jointly train the\n",
            "encoder and the three decoders. In this approach, the goal\n",
            "is to optimize multiple related tasks simultaneously, rather\n",
            "than training each component in isolation. The tasks within\n",
            "our system model encompass data reconstruction, sensing, and\n",
            "semantic validation. We take the channel effects into account\n",
            "while optimizing the encoder and decoders.\n",
            "To perform joint training, we define a combined loss func-Fig. 3: DNN architectures for the transmitter and receiver.\n",
            "tion that encapsulates the objectives of each of the tasks. A\n",
            "weight is assigned to each task’s loss, indicating its relative\n",
            "importance. Then we consider the weighted sum of losses as\n",
            "the combined loss. This combined loss function ensures that\n",
            "the encoder and decoders work in concert to achieve their\n",
            "respective goals while sharing knowledge and optimizing their\n",
            "overall performance.\n",
            "Numerical results are obtained using Python, and the models\n",
            "are trained in Keras with the TensorFlow backend. Adam is\n",
            "used as the optimizer.\n",
            "Input data. The CIFAR-10 dataset is used for input samples\n",
            "of the encoder at the transmitter. CIFAR-10 is a collection\n",
            "of color images from 10 classes, namely ‘Airplane’, ‘Auto-\n",
            "mobile’, ‘Bird’, ‘Cat’, ‘Deer’, ‘Dog’, ‘Frog’, ‘Horse’, ‘Ship’,\n",
            "‘Truck’. The size of each data sample (image) is 32×32×3.\n",
            "Each data sample consists of red, green, blue (RGB) pixels. A\n",
            "pixel in each RGB component takes value between 0and255.\n",
            "We normalize these pixel values to the range between 0 and\n",
            "1 before feeding them to the encoder DNN at the transmitter.\n",
            "We consider 50,000 training samples and 10,000 test samples\n",
            "for numerical results.\n",
            "Channel effects. The transmitter transmits the output of its\n",
            "encoder over a wireless channel. We consider two types of\n",
            "channels.\n",
            "•AWGN channel, where the signal received at the receiver\n",
            "is the transmitted signal plus the white Gaussian noise.\n",
            "•Rayleigh fading channel, where the transmitted signal is\n",
            "subject to Rayleigh fading and white Gaussian noise is\n",
            "added to the received signal.\n",
            "In both channel cases, joint training of the encoder and\n",
            "decoder sets learns channel effects. To do so, the overall\n",
            "model (including an encoder and three decoders) is augmented\n",
            "with Gaussian noise layer and Rayleigh fading channel layer\n",
            "with the appropriate SNR added between the encoder and\n",
            "each decoder. Note that the channels between the transmitterand the receiver, and the channels between the transmitter\n",
            "and the target undergo different channel effects by accounting\n",
            "for noise, fading, phase shift and frequency shift effects. For\n",
            "training, a channel realization is randomly drawn from the\n",
            "given channel distribution for each data sample. During the\n",
            "test time, it is assumed that the channel distribution remains\n",
            "the same even though individual realizations may vary and\n",
            "may not be known in advance to the already trained DNN\n",
            "models.\n",
            "For sensing task, the overall channel is a concatenation of\n",
            "channel from the transmitter to the target and from the target\n",
            "back to the transmitter. In addition to noise and fading effects,\n",
            "the received signal back at the transmitter may also include\n",
            "frequency shift (due to the relative speed of potential target)\n",
            "and phase shift (due to the reflection off the target) compared\n",
            "to the transmitted signal depending on the mobility of the\n",
            "target. All these effects are considered during training and the\n",
            "corresponding realizations are input to the test-time operation.\n",
            "Performance measures. We measure the performance for\n",
            "the three underlying tasks of communications, sensing, and\n",
            "semantic information validation in the case of semantic com-\n",
            "munications.\n",
            "•Reconstruction loss : the loss of recovering input samples\n",
            "at the output of Decoder 1 at the receiver (the MSE\n",
            "is used as the reconstruction loss when the input and\n",
            "reconstructed samples are compared to each other).\n",
            "•Sensing accuracy : average accuracy of signal classifi-\n",
            "cation task for sensing performed by Decoder 2 at the\n",
            "transmitter (signal classification task can either detect\n",
            "only whether the target is present or not, or further\n",
            "incorporate decisions on potential ranges of the target).\n",
            "•Semantic accuracy : the average accuracy of the classi-\n",
            "fication task performed by Decoder 3 at the receiver to\n",
            "validate whether the semantic information is preserved\n",
            "during the information transfer, i.e., whether the under-\n",
            "lying classification task can correctly identify the output\n",
            "labels of data samples, e.g., from 10 classes of CIFAR-10\n",
            "dataset, or which label subset of interest they belong to,\n",
            "e.g., the image contains an animal.\n",
            "System parameters. The system model involves several tun-\n",
            "able parameters. We will vary these parameters to measure the\n",
            "performance.\n",
            "•Multi-task weights : weights used to aggregate the losses\n",
            "of different tasks in the combined loss function of multi-\n",
            "task learning, namely sensing and communications in\n",
            "addition to the extension to semantic communications.\n",
            "•Communication SNR : SNR of signals received as the\n",
            "input of decoders at the receiver (Decoder 1 and Decoder\n",
            "3).\n",
            "•Sensing SNR : SNR of signals reflected by a potential tar-\n",
            "get and received as input to the decoder at the transmitter\n",
            "(Decoder 2).\n",
            "•Encoder output size : number of symbols transmitted as\n",
            "output of the encoder at the transmitter (represents the\n",
            "compression of the encoder with respect to the size ofinput samples and determines the input size of all three\n",
            "decoders).\n",
            "•Number of potential target ranges : number of potential\n",
            "ranges for the positions where a potential target may be\n",
            "located (this number determines the number of labels in\n",
            "the classification problem for sensing since the sensing\n",
            "objective is to identify the presence and if so, the range\n",
            "of a target).\n",
            "Baselines. We compare the performance of joint sensing and\n",
            "communications with two baselines of individual sensing and\n",
            "individual communications.\n",
            "•Individual sensing : The encoder and the decoder DNNs\n",
            "are trained for the sole purpose of sensing such that the\n",
            "sensing loss function is the only one assigned a non-zero\n",
            "weight in the multi-task learning formulation.\n",
            "•Individual communications : The encoder and the decoder\n",
            "DNNs are trained for the sole purpose of communications\n",
            "such that the reconstruction loss function is the only one\n",
            "assigned a non-zero weight in the multi-task learning\n",
            "formulation.\n",
            "Fig. 4: Joint sensing and communications performance vs. weight for sensing\n",
            "task.\n",
            "III. P ERFORMANCE OF JOINT SENSING AND\n",
            "COMMUNICATIONS\n",
            "We consider the following default values of system param-\n",
            "eters for performance evaluation. Multi-task learning weights\n",
            "are 1/2 for the two-task problem of sensing and communica-\n",
            "tions, or 1/3 for the three-task problem of sensing, communi-\n",
            "cations, and semantic information validation. Communication\n",
            "SNR is 3dB. Sensing SNR is 3dB. Size of encoder output is\n",
            "20. Number of potential target ranges is 1. When we obtain\n",
            "the performance results, we vary system parameters one at a\n",
            "time by setting the rest of parameters to default values.\n",
            "Joint vs. individual sensing and communications. We start\n",
            "with the two-task problem of joint sensing and communica-\n",
            "tions. First, we vary the weight of sensing loss for multi-task\n",
            "learning to highlight the value of multi-task learning over\n",
            "single-task learning of sensing or communications. Sensingaccuracy and reconstruction loss are shown in Fig. 4 for both\n",
            "AWGN and Rayleigh channels. When the sensing weight is\n",
            "zero, we end up with the baseline of individual communi-\n",
            "cations such that reconstruction is highly successful (recon-\n",
            "struction loss is low) but sensing is not successful (sensing\n",
            "accuracy is very low). On the other hand, when this sensing\n",
            "weight is 1, we end up with the baseline of individual sensing\n",
            "such that sensing is highly successful (sensing accuracy is\n",
            "high) but reconstruction is not successful (reconstruction loss\n",
            "is very high). When we select the sensing weight in between\n",
            "(such as 1/2), we can achieve a desirable performance for both\n",
            "sensing and reconstruction such that the sensing accuracy is\n",
            "high while the reconstruction loss remains low.\n",
            "Results in Fig. 4 show the value of multi-task learning\n",
            "for joint sensing and communication in contrast to individual\n",
            "optimization of either sensing or communications. Note that\n",
            "the sensing and communication waveforms can be individu-\n",
            "ally optimized, as well. However, this conventional approach\n",
            "would completely separate the sensing and communication\n",
            "functionalities, and therefore require separate transmissions of\n",
            "individual sensing and communication signals resulting in poor\n",
            "channel utilization, waste of energy resources and additional\n",
            "delay to complete sensing and communication tasks separately.\n",
            "Effects of system parameters on joint sensing and com-\n",
            "munications . Next, we vary the rest of system parameters\n",
            "described in the previous section and show the performance in\n",
            "Fig. 5. First, we evaluate the sensing accuracy and reconstruc-\n",
            "tion loss when we vary the communication SNR to measure its\n",
            "effect on reconstruction performance and separately vary the\n",
            "sensing SNR to measure its effect on sensing performance.\n",
            "Results in Fig. 5 show that the performance quickly improves\n",
            "with the SNR for both AWGN and Rayleigh channels. In all\n",
            "cases, the learning performance is better under the AWGN\n",
            "channel compared to the more difficult case of Rayleigh\n",
            "channel, as we will observe also for other performance results.\n",
            "Next, we the performance as a function of encoder output\n",
            "size at the transmitter. The smaller this size becomes, the more\n",
            "efficiently the underlying resources such as channel access and\n",
            "transmit energy are utilized potentially with less delay. Results\n",
            "in Fig. 5 show that the performance quickly improves for both\n",
            "AWGN and Rayleigh channels when we start increasing the\n",
            "encoder output size. For example, high sensing accuracy and\n",
            "low reconstruction loss are achieved when the encoder output\n",
            "size is selected as 20, which corresponds to 0.65% compres-\n",
            "sion, namely the dimension is reduced from 3072 (namely, 32\n",
            "×32×3) for input samples to 20 in latent space. This way,\n",
            "joint sensing and communications can simultaneously sustain\n",
            "high performance as well as high resource efficiency with the\n",
            "right choice of DNNs.\n",
            "Next, we extend the sensing problem from identifying\n",
            "whether a target is present or not, to classifying the range\n",
            "(proximity) of the target (including the absence of the target).\n",
            "In this context, the sensing SNR (with default value of 3dB)\n",
            "corresponds to the closest potential position, whereas the SNRs\n",
            "for other ranges are made incrementally smaller. Results in\n",
            "Fig. 5 show the performance as a function of the numberFig. 5: Joint sensing and communications performance as a function of system parameters (SNR, encoder output size, and number of potential target ranges).\n",
            "of potential target ranges. As we add more potential ranges\n",
            "for the target, the number of labels increases along with the\n",
            "difficulty of the underlying multi-label classification problem\n",
            "such that the performance starts dropping (especially observed\n",
            "for sensing accuracy under Rayleigh channel).\n",
            "Joint sensing and semantic communications. Next, we add\n",
            "semantic communications to the multi-task learning frame-\n",
            "work of joint sensing and communications, and jointly op-\n",
            "timize the objectives of three tasks, namely sensing, informa-\n",
            "tion reconstruction, and semantic information validation. For\n",
            "the CIFAR-10 data input, we can check different levels of\n",
            "semantic information preserved in the transferred information\n",
            "such as classification labels themselves or their membership\n",
            "in different sets of labels. For numerical results, we consider\n",
            "semantic validation in terms of checking whether the trans-\n",
            "ferred image belongs to a certain set of labels of interest,\n",
            "specifically, whether the image includes an animal (i.e., the\n",
            "label is ‘Bird’, ‘Cat’, ‘Deer’, ‘Dog’, ‘Frog’, or ‘Horse’), or\n",
            "not. We measure the performance by varying the SNR for\n",
            "both communications and sensing while keeping the rest of\n",
            "system parameters at their default values. Results reported in\n",
            "Fig. 6 show that both sensing accuracy and task accuracy (for\n",
            "semantic information validation) can quickly increase with\n",
            "the SNR for both AWGN and Rayleigh channels while the\n",
            "reconstruction loss is further reduced. The implication is that\n",
            "it is possible with high success to transfer information over the\n",
            "wireless channel while preserving the semantic information\n",
            "and sense the target in the meantime by using the same\n",
            "transmitted signal for communications and sensing.\n",
            "IV. C ONCLUSION AND FUTURE RESEARCH DIRECTIONS\n",
            "We have presented a multi-task learning framework of\n",
            "deep learning techniques for joint sensing and communica-\n",
            "tions, while introducing the addition of semantic communi-\n",
            "cation capabilities. The key components of the system are\n",
            "the dual-purpose encoder for sensing and communications,\n",
            "and decoders for data reconstruction, sensing and semantic\n",
            "information validation decoder. By harnessing the power of\n",
            "multi-task learning, these components collectively contribute\n",
            "to optimizing target sensing and data transfer while preserving\n",
            "semantic information, and offer a versatile, resource-efficient,\n",
            "and high-fidelity solution, capable of addressing the demands\n",
            "of real-time decision-making, context-aware data sharing, and\n",
            "Fig. 6: Joint sensing and semantic communication performance vs. SNR (when\n",
            "sensing SNR is the same as communication SNR, both measured in dB).\n",
            "efficient resource utilization. Our performance evaluation us-\n",
            "ing the image data and under varying channel conditions,\n",
            "underscore the practicality and real-world viability of this\n",
            "innovative system by representing a significant leap in the\n",
            "evolution of next-generation wireless systems.\n",
            "There are various future research directions that can build\n",
            "upon the multi-task learning framework of joint sensing and\n",
            "semantic communications. In addition to images, other data\n",
            "modalities such as text can be considered as inputs. The\n",
            "communication scenario can be extended from point-to-point\n",
            "communication to other network settings such as broadcasting\n",
            "to multiple receivers. To that end, multiple semantic tasks can\n",
            "be also incorporated for different receivers. In terms of sensing\n",
            "task, additional objectives such as target direction finding and\n",
            "target tracking can be added to multi-task learning. Finally,\n",
            "tasks can be enriched by adding security objectives such as\n",
            "anti-jamming and covert communications.\n",
            "REFERENCES\n",
            "[1] U. Demirhan and A. Alkhateeb, “Integrated sensing and commu-\n",
            "nication for 6G: Ten key machine learning roles,” arXiv preprint\n",
            "arXiv:2208.02157 , 2022.\n",
            "[2] F. Liu, Y . Cui, C. Masouros, J. Xu, T. X. Han, Y . C. Eldar, and S. Buzzi,\n",
            "“Integrated sensing and communications: Toward dual-functional wire-\n",
            "less networks for 6G and beyond,” IEEE Journal on Selected Areas in\n",
            "Communications , vol. 40, no. 6, pp. 1728–1767, 2022.[3] Y . Xiong, F. Liu, Y . Cui, W. Yuan, T. X. Han, and G. Caire, “On the\n",
            "fundamental tradeoff of integrated sensing and communications under\n",
            "gaussian channels,” IEEE Transactions on Information Theory , vol. 69,\n",
            "no. 9, pp. 5723–5751, 2023.\n",
            "[4] Y . Cui, F. Liu, X. Jing, and J. Mu, “Integrating sensing and communi-\n",
            "cations for ubiquitous iot: Applications, trends, and challenges,” IEEE\n",
            "Network , vol. 35, no. 5, pp. 158–167, 2021.\n",
            "[5] Z. Du, F. Liu, W. Yuan, C. Masouros, Z. Zhang, S. Xia, and G. Caire,\n",
            "“Integrated sensing and communications for V2I networks: Dynamic\n",
            "predictive beamforming for extended vehicle targets,” IEEE Transactions\n",
            "on Wireless Communications , 2022.\n",
            "[6] C. Liu, W. Yuan, S. Li, X. Liu, H. Li, D. W. K. Ng, and Y . Li,\n",
            "“Learning-based predictive beamforming for integrated sensing and\n",
            "communication in vehicular networks,” IEEE Journal on Selected Areas\n",
            "in Communications , vol. 40, no. 8, pp. 2317–2334, 2022.\n",
            "[7] T. Erpek, T. J. O’Shea, Y . E. Sagduyu, Y . Shi, and T. C. Clancy, “Deep\n",
            "learning for wireless communications,” Development and Analysis of\n",
            "Deep Learning Architectures , pp. 223–266, 2020.\n",
            "[8] J. M. Mateos-Ramos, J. Song, Y . Wu, C. H ¨ager, M. F. Keskin, V . Ya-\n",
            "jnanarayana, and H. Wymeersch, “End-to-end learning for integrated\n",
            "sensing and communication,” in IEEE International Conference on\n",
            "Communications , 2022, pp. 1942–1947.\n",
            "[9] C. Muth and L. Schmalen, “Autoencoder-based joint communication\n",
            "and sensing of multiple targets,” in 26th International ITG Workshop\n",
            "on Smart Antennas and 13th Conference on Systems, Communications,\n",
            "and Coding , 2023, pp. 1–6.\n",
            "[10] B. Guler and A. Yener, “Semantic index assignment,” in IEEE In-\n",
            "ternational Conference on Pervasive Computing and Communication\n",
            "(PERCOM) Workshops , 2014.\n",
            "[11] D. G ¨und¨uz, Z. Qin, I. E. Aguerri, H. S. Dhillon, Z. Yang, A. Yener, K. K.\n",
            "Wong, and C.-B. Chae, “Beyond transmitting bits: Context, semantics,\n",
            "and task-oriented communications,” IEEE Journal on Selected Areas in\n",
            "Communications , vol. 41, no. 1, pp. 5–41, 2022.\n",
            "[12] C. Chaccour, W. Saad, M. Debbah, Z. Han, and H. V . Poor, “Less data,\n",
            "more knowledge: Building next generation semantic communication\n",
            "networks,” arXiv preprint arXiv:2211.14343 , 2022.\n",
            "[13] H. Xie, Z. Qin, G. Y . Li, and B.-H. Juang, “Deep learning enabled\n",
            "semantic communication systems,” IEEE Transactions on Signal Pro-\n",
            "cessing , vol. 69, pp. 2663–2675, 2021.\n",
            "[14] Y . E. Sagduyu, T. Erpek, S. Ulukus, and A. Yener, “Is semantic\n",
            "communications secure? a tale of multi-domain adversarial attacks,”\n",
            "arXiv preprint arXiv:2212.10438 , 2022.\n",
            "[15] Y . E. Sagduyu, S. Ulukus, and A. Yener, “Task-oriented communications\n",
            "for nextG: End-to-end deep learning and ai security aspects,” IEEE\n",
            "Wireless Communications , vol. 30, no. 3, pp. 52–60, 2023.Challenging Common Assumptions in Multi-task Learning\n",
            "Cathrin Elich‡,1,2,3, Lukas Kirchdorfer‡,1,4, Jan M. K ¨ohler∗,1, Lukas Schott∗,1\n",
            "1Bosch Center for Artificial Intelligence,2MPI for Intelligent Systems, T ¨ubingen,\n",
            "3Max Planck ETH Center for Learning Systems,4Uni Mannheim\n",
            "cathrin.elich@tuebingen.mpg.de, jan.koehler@bosch.com, lukas.schott@bosch.com\n",
            "‡Work done during an internship at Bosch.∗Joint senior authors.\n",
            "Abstract\n",
            "While multi-task learning (MTL) has gained significant\n",
            "attention in recent years, its underlying mechanisms remain\n",
            "poorly understood. Recent methods did not yield consis-\n",
            "tent performance improvements over single task learning\n",
            "(STL) baselines, underscoring the importance of gaining\n",
            "more profound insights about challenges specific to MTL.\n",
            "In our study, we challenge common assumptions in MTL in\n",
            "the context of STL: First, the choice of optimizer has only\n",
            "been mildly investigated in MTL. We show the pivotal role\n",
            "of common STL tools such as the Adam optimizer in MTL.\n",
            "We deduce the effectiveness of Adam to its partial loss-scale\n",
            "invariance. Second, the notion of gradient conflicts has of-\n",
            "ten been phrased as a specific problem in MTL. We delve\n",
            "into the role of gradient conflicts in MTL and compare it to\n",
            "STL. For angular gradient alignment we find no evidence\n",
            "that this is a unique problem in MTL. We emphasize dif-\n",
            "ferences in gradient magnitude as the main distinguishing\n",
            "factor. Lastly, we compare the transferability of features\n",
            "learned through MTL and STL on common image corrup-\n",
            "tions, and find no conclusive evidence that MTL leads to\n",
            "superior transferability. Overall, we find surprising similar-\n",
            "ities between STL and MTL suggesting to consider methods\n",
            "from both fields in a broader context.\n",
            "1. Introduction\n",
            "Multi-task learning (MTL) is gaining significance in the\n",
            "deep learning literature and in industry applications. Es-\n",
            "pecially, tasks like autonomous driving and robotics neces-\n",
            "sitate real-time execution of neural networks while obey-\n",
            "ing constraints of limited computational resources. Conse-\n",
            "quently, there is a demand for neural networks capable of\n",
            "simultaneously inferring multiple tasks [18, 25].\n",
            "In a seminal study, Caruana [3] highlights both advan-\n",
            "tages and challenges in MTL. On the one hand, certain\n",
            "tasks can exhibit a symbiotic relationship, resulting in a\n",
            "mutual performance enhancement when trained together. Itwas further suggested that features learned in a MTL sce-\n",
            "nario rely less on incidental correlations and demonstrate\n",
            "improved transferability. On the other hand, conflicts be-\n",
            "tween tasks can arise and decrease the performance when\n",
            "trained jointly, also known as negative transfer .\n",
            "Several approaches have been suggested to mitigate the\n",
            "issue of negative transfer among tasks during network train-\n",
            "ing. Our study focuses on two main branches in the litera-\n",
            "ture: First, gradient magnitude methods which incorporate\n",
            "weights to scale task-specific losses to achieve an adequate\n",
            "balance between tasks. Second, gradient alignment meth-\n",
            "ods which aim to resolve conflicts in gradient vectors that\n",
            "may arise between tasks within a shared network backbone.\n",
            "The effectiveness of the proposed MTL methods remains\n",
            "uncertain in the literature. Upon comparing various studies,\n",
            "it becomes evident that there is no definitive approach that\n",
            "consistently performs well across different settings [44].\n",
            "This observation has been reinforced in more recent stud-\n",
            "ies where competitive performance was achieved through\n",
            "unitary scaling in combination with common regularization\n",
            "methods [24] or tuned task weighting [45].\n",
            "The current understanding of MTL is still limited and\n",
            "lacks a deeper comprehension of its underlying mecha-\n",
            "nisms. To address this gap, our study aims to challenge\n",
            "commonly held assumptions, such as the notion of gradi-\n",
            "ent alignment, gradient magnitudes, and transferability of\n",
            "features. Our contributions are:\n",
            "• We evaluate the empirical effectiveness of the Adam [21]\n",
            "optimizer in MTL which we show to perform favorably\n",
            "in various experiments in comparison to SGD + momen-\n",
            "tum. We trace this back to Adam addressing differences\n",
            "in gradient magnitudes by tracking first and second mo-\n",
            "ment estimates of gradients.\n",
            "• We theoretically attribute the performance of methods\n",
            "in MTL to their invariance w.r.t. to different loss scal-\n",
            "ings. We show this invariance of uncertainty weighting\n",
            "(UW) under mild conditions. Similarly, we demonstrate\n",
            "Adam’s a partial loss-scale invariance in MTL.\n",
            "1arXiv:2311.04698v2  [cs.LG]  10 Nov 2023• In contrast to implicit assumptions from previous studies\n",
            "[6,19,28,49], we present empirical evidence that conflicts\n",
            "arising from gradient alignment are not exclusive to MTL\n",
            "and can even be more pronounced in single-task learning.\n",
            "• Corroborating the methods proposed to balance magni-\n",
            "tude related issues across tasks [20, 29, 31, 45], we con-\n",
            "firm that gradient magnitudes pose a challenge in MTL\n",
            "compared to single task learning.\n",
            "• We examine the presumption of increased robustness on\n",
            "corrupted data as a result of MTL [23, 34]. We find no\n",
            "evidence that an increased number of tasks would con-\n",
            "sistently result in improved transferability.\n",
            "Overall, we provide a vast set of experiments and theoreti-\n",
            "cal insights which challenge common assumptions and con-\n",
            "tribute to a more comprehensive understanding of MTL in\n",
            "computer vision to guide future research.\n",
            "2. Related Work\n",
            "Recent work in multi-task learning (MTL) can be\n",
            "roughly divided into three different fields: Network archi-\n",
            "tectures focus on the question of how features should be\n",
            "shared across tasks [31, 33, 35, 46]. Multi-task optimization\n",
            "(MTO) aims to resolve imbalances and conflicts of tasks\n",
            "during MTL. Task affinities examine a grouping of tasks that\n",
            "should be learned together to benefit from the joint train-\n",
            "ing [11,30,43]. A general overview of recent works in MTL\n",
            "can be found in [39,44]. Our work focuses on MTO, which\n",
            "we will review more thoroughly in the following.\n",
            "Gradient magnitude methods prevent the dominance\n",
            "of individual tasks by balancing them with task-specific\n",
            "weights. One line of works are loss-weighting methods.\n",
            "Here, weights are determined before any (task-wise) gradi-\n",
            "ent computation and are used for a weighted aggregation of\n",
            "the tasks’ losses. These methods consider either the task un-\n",
            "certainty ( UW) [20], rate of change of the loss ( DWA ) [31],\n",
            "the tasks’ difficulty ( DTP ) [13], or randomly chosen task\n",
            "weights ( RLW ) [26]. In line with these, the geometric mean\n",
            "of task losses has been used to handle the different conver-\n",
            "gence rates of the tasks [7]. An advantage of theses methods\n",
            "is their computational efficiency as the gradient needs to be\n",
            "computed only once for the aggregated loss. Alternatively,\n",
            "other methods consider the task-specific gradients directly,\n",
            "e.g., by normalizing them ( GradNorm ) [5] or computing\n",
            "scaling factors which produce an aggregated gradient with\n",
            "equal projection onto each task’s gradient ( IMTL ) [29].\n",
            "Furthermore, there are several adaptions for the multiple-\n",
            "gradient descent algorithm ( MGDA ) [9], e.g. for applying\n",
            "it efficiently in deep learning setups [40] or by introducing\n",
            "a stochastic gradient correction [10]. Recently, task-wise\n",
            "gradient weights have been estimated by treating MTL as a\n",
            "bargaining problem ( Nash-MTL ) [37], or considering a sta-\n",
            "bility criterion ( Aligned-MTL ) [41]. Crucially, in the con-\n",
            "text of this study, all gradient magnitude methods considerscalar weightings of task-wise gradients within the back-\n",
            "bone and/or heads. They do not modify the alignment of\n",
            "task-specific gradient vectors.\n",
            "Gradient alignment methods perform more profound\n",
            "vector manipulations on the task-wise gradients w.r.t. to\n",
            "the network weights of a shared backbone before aggre-\n",
            "gating them. The underlying assumption indicates conflict-\n",
            "ing gradients as a major problem in MTL. To address this,\n",
            "GradDrop [6] randomly drops gradient components in the\n",
            "case of opposing signs. PCGrad [49] proposes to circum-\n",
            "vent problems of conflicting gradients by projecting them\n",
            "onto each other’s normal plane. Following this idea, Liu\n",
            "et al. [28] propose CAGrad to converge to a minimum of\n",
            "the average loss instead of any point on the Pareto front.\n",
            "RotoGrad [19] rotates gradients at the intersection of the\n",
            "heads and backbone to improve their alignment. Shi et\n",
            "al. [42] propose to alter the network architecture based on\n",
            "the occurrence of layer-wise gradient conflicts. Lastly, [38]\n",
            "use separate optimizers per task.\n",
            "Recent studies question the effectiveness of\n",
            "optimization-based methods in MTL. Xin et al. [45]\n",
            "execute an extensive hyperparameter search to show\n",
            "that simple scalar task-weighting performs equivalent or\n",
            "superior to many aforementioned multi-task optimiza-\n",
            "tion methods. Their hyperparameter search not only\n",
            "include the task-weights, but also common deep learning\n",
            "parameters such as the learning rate and regularization.\n",
            "Concurrently, [24] empirically show that fixed task-weights\n",
            "combined with regularization and stabilization techniques\n",
            "yield to equivalent performance compared to sophisticated\n",
            "multi-task optimization methods. We extend both critical\n",
            "studies to provide a more nuanced perspective. In particu-\n",
            "lar, we theoretically and empirically demonstrate that the\n",
            "choice of optimizer is crucial and could potentially help\n",
            "to explain discrepancies found in prior studies (4.1). We\n",
            "further specifically distinguish between gradient alignment\n",
            "and gradient magnitude methods (4.2).\n",
            "3. Problem Statement\n",
            "Multi-task learning addresses the problem of learning\n",
            "a set of Ttasks simultaneously. We consider supervised\n",
            "learning setups, use a shared backbone architecture, and\n",
            "learn all tasks together. Formally, given input data X, the\n",
            "goal is to learn a function fθ(x)which maps a point x∈ X\n",
            "to each task label ytwitht= 1, .., T . The trainable parame-\n",
            "tersθ={ϕ, ψ 1:T}consist of shared parameters ϕandtask-\n",
            "specific parameters ψt. Training a task tis associated with\n",
            "the loss Lt(fθ(x);θ), e.g., a regression or classification\n",
            "loss. We denote respective gradients on the shared and task-\n",
            "specific parameters with gϕ\n",
            "t=∇ϕLt, and gψ\n",
            "t=∇ψLt.\n",
            "When training on multiple tasks, the shared parameters ϕ\n",
            "needs to be updated w.r.t. all task-wise gradients gϕ\n",
            "twhich\n",
            "requires an appropriate aggregation. A simple solution is to\n",
            "2uniformly sum up the task losses L=P\n",
            "tLtwhich is re-\n",
            "ferred to as Equal Weighting (EW). However, as tasks might\n",
            "be competing against each other, this can result in negative\n",
            "transfer and thus sub-optimal solutions. One way to deal\n",
            "with this difficulty is to adapt the magnitude of task-specific\n",
            "gradients. This can be achieved by weighting tasks dur-\n",
            "ing training, e.g., by scaling different losses L=P\n",
            "tαtLt,\n",
            "where oftenP\n",
            "tαt= 1. Note, the αtcan change during\n",
            "training. Furthermore, the weighing can also be performed\n",
            "on gradient level to distinguish between shared and task-\n",
            "specific gradients. We refer to those approaches as gradi-\n",
            "ent magnitude methods. Interestingly, the relationship be-\n",
            "tween loss weights, network-updates and learning rate also\n",
            "depends on the optimizer. We show a derivation for SGD\n",
            "and Adam in Appendix A1.2. Additionally to adapting the\n",
            "gradient magnitude, one can directly adapt the alignment\n",
            "of task-wise gradient vectors within the shared backbone\n",
            "˜gϕ=h(g1ϕ, ...,gTϕ).\n",
            "In practice, an optimum for θthat yields best perfor-\n",
            "mance on all tasks often does not exist. Instead, improving\n",
            "performance on some task often yields a performance de-\n",
            "crease in another task. To still enable a comparison across\n",
            "network instances in MTL, an instance θ∗is called to be\n",
            "Pareto optimal , if there is no other θ′such that Lt(θ′)≤\n",
            "Lt(θ∗)∀twith strict inequality in at least one task. The\n",
            "Pareto front consists of the Pareto optimal solutions.\n",
            "4. Experiments and results\n",
            "In this section we perform several experiments to gain\n",
            "a more profound understanding of multi-task learning in\n",
            "computer vision by questioning common assumptions. We\n",
            "compare the impact of Adam and SGD in multi-task learn-\n",
            "ing in section 4.1, examine the process of gradient similarity\n",
            "in different settings in section 4.2, and evaluate the gen-\n",
            "eralization performance on corrupted data in section 4.3.\n",
            "Throughout this evaluation, we repeatedly make use of\n",
            "common setups, which we will specify as follows and in\n",
            "more detail in appendix A3.\n",
            "Datasets For our experiment, we consider three different\n",
            "datasets that are commonly used for evaluating multi-task\n",
            "learning in computer vision:\n",
            "CityScapes [8] contains images of urban street scenes. In\n",
            "line with previous work, we consider the tasks of semantic\n",
            "segmentation (7 classes) and depth estimation. NYUv2 [36]\n",
            "is an indoor dataset for scene understanding which was\n",
            "recorded over 464 different scenes across three different\n",
            "cities. Besides semantic segmentation (13-class) and depth\n",
            "estimation, it also contains the task of surface normal pre-\n",
            "diction. CelebA [32] consists of 200K face images which\n",
            "are labeled with 40 binary attributes.\n",
            "Networks We use network architectures with hard-\n",
            "parameter sharing, which consist of a shared backbone\n",
            "and task-specific heads. For the dense prediction taskson CityScapes and NYUv2, we compare SegNet [1] and\n",
            "DeepLabV3+ [4]. While both networks contain a similar\n",
            "number of shared parameters, DeepLabV3+ has relatively\n",
            "more of task-specific parameters. Experiments on CelebA\n",
            "are performed on a ResNet-18 backbone [14] with an addi-\n",
            "tional single linear layer for each head.\n",
            "Training For each method, we follow the loss or gradi-\n",
            "ent aggregation as described in the related work, e.g., for\n",
            "equal weighting all task-specific losses are simply summed\n",
            "up to compute the joint network gradients. We also tune\n",
            "the learning rate for each approach. We use the validation\n",
            "set performance of the ∆mmetric as early stopping crite-\n",
            "ria. The ∆mmetric [33] measures the average relative task\n",
            "performance drop of a method mcompared to the single-\n",
            "task baseline busing the same backbone and is computed as\n",
            "∆m=1\n",
            "TPT\n",
            "t=1(−1)lt(Mm,t−Mb,t)/Mb,twhere lt= 1if\n",
            "a higher value means better for measure M·,tof some task\n",
            "metric t, and 0 otherwise.\n",
            "4.1. The reasonable effectiveness of Adam in multi-\n",
            "task learning\n",
            "We investigate the impact of Adam with respect to\n",
            "stochastic gradient descent with momentum (SGD+mom)\n",
            "in conjunction with common multi-task learning (MTL)\n",
            "methods. We identify the choice of optimizer as an im-\n",
            "portant confounder in the experimental setup. Previous\n",
            "works reported mixed performances on dedicated multi-\n",
            "task weighting methods compared to a plain scalar weight-\n",
            "ing. For instance, Adam [21] was successfully used in\n",
            "studies to show that random/constant weighting of tasks’\n",
            "losses performs competitive compared to MTO methods\n",
            "[24, 26, 45]. In contrast, many methods proposing adap-\n",
            "tive, task-specific weighing methods [20, 29] use the SGD\n",
            "optimizer with momentum (see Table A1 for an overview).\n",
            "Intuitively, gradient magnitude differences between tasks\n",
            "could require an adequate balancing which, in the linear\n",
            "case, could also be seen as a task-specific learning rate\n",
            "(derivation in Appendix A1.2). Overall, Adam’s capability\n",
            "of estimating the learning rate per parameter based on the\n",
            "first and second moment estimates of the gradients might\n",
            "help to mitigate claimed differences in gradient magnitude.\n",
            "4.1.1 Toy Task Experiment\n",
            "To get a first impression of the impact of the optimizer and\n",
            "common hyperparameters such as the the learning rate, we\n",
            "investigate the impact of Adam and plain gradient descent\n",
            "(GD) in a simple toy task.\n",
            "Approach We repeat the experiment of Liu et al. [28] us-\n",
            "ing the original implementation but further tested different\n",
            "learning rates and optimizers. They motivate their gradi-\n",
            "ent alignment method CAGrad with a simple toy optimiza-\n",
            "tion problem in which their method reliably converges to the\n",
            "3GD 1.0 0.05 0.001EW\n",
            " CAGrad\n",
            "Adam 1.0 0.05 0.001EW\n",
            " CAGrad\n",
            "Figure 1. Toy Task Experiment from CAGrad [28] for different\n",
            "learning rates and optimizers. Consistent with results from [45],\n",
            "we observe that the choice of the learning rate is crucial even for\n",
            "this toy optimization problem. Moreover, it becomes apparent,\n",
            "that selecting Adam over simple gradient decent (GD) yields supe-\n",
            "rior results. The contour lines depict the 2D loss landscape and the\n",
            "optimization trajectories are colored from red to yellow for 100k\n",
            "iteration steps from three different starting points (seeds). Liu et\n",
            "al. [28] used Adam and lr=0.001. Trajectories for further learn-\n",
            "ing rates and methods can be found in Figure A5.\n",
            "minimum of the average loss, while other MTO approaches\n",
            "would either get stuck (e.g., EW) or only convert to any\n",
            "point on the Pareto front (e.g., PCGrad [49], MGDA [40]).\n",
            "Result and conclusion We find that for higher learn-\n",
            "ing rates with the Adam optimizer, even the simple equal\n",
            "weighting (EW) method reaches the global optimum (cf.\n",
            "Figure 1, e.g., EW with Adam and lr=0.05) and often con-\n",
            "verges faster than dedicated multi-task optimization meth-\n",
            "ods such as CAGrad (Table 1). Note, original results were\n",
            "shown for learning rate 0.001 using Adam (blue shaded col-\n",
            "umn) and were, therefore, in favor of CAGrad. Further-\n",
            "more, the choice of optimizer appears to be more impor-\n",
            "tant on the success of the outcome of the experiments than\n",
            "the choice of multi-task optimization method, as Adam con-\n",
            "verges considerably faster and more reliably than GD.\n",
            "4.1.2 Experiments on CityScapes and NYUv2\n",
            "We further extensively test the effectiveness of Adam and its\n",
            "role as a confounder in common MTL datasets for various\n",
            "multi-task optimization methods.\n",
            "Approach We compare Adam and SGD+mom in combi-\n",
            "nation with any MTO method from equal weighting (EW),\n",
            "uncertainty weighting (UW) [20], random loss weightinglearning rate\n",
            "method 10.0 1.0 0.1 0.01 0.001*GDEW - - - - -\n",
            "PCGrad - - - - -\n",
            "CAGrad 644 213 8,069 20,418 -AdamEW 26 22 709 9,015 -\n",
            "PCGrad 25 56 34,175 - -\n",
            "CAGrad 27 32 802 11,239 57,700\n",
            "*LR used for results in [28] with Adam\n",
            "Table 1. Number of iterations after which all seeds in the toy\n",
            "task experiment from CAGrad [28] reach the global minimum .\n",
            "We report the maximum iteration number over all three seeds for\n",
            "each MTO method, learning rate, and optimizer combination. In\n",
            "several setups, EW+Adam shows the fastest convergence to the\n",
            "global minimum. If not all seeds converged to the global mini-\n",
            "mum within 100k iteration steps, we denote it as ’-’. As reported\n",
            "in previous work, we found that PCGrad converges only to some\n",
            "point on the Pareto Front. The best andsecond best run for each\n",
            "learning rate over all MTO methods are indicated via font type.\n",
            "Results for additional learning rates are reported in Table A7.\n",
            "0.550.60.650.7SemSeg/mIoU\n",
            "0.71976\n",
            "0.500170.840.860.880.90.92SemSeg/pixA cc\n",
            "0.925057\n",
            "0.8399−0.07−0.06−0.05−0.04−0.03−0.02Depth/AbsEr r [neg]\n",
            "−0.013142\n",
            "−0.075823−120−100−80−60−40Depth/R elErr [neg]\n",
            "−31.428\n",
            "−126.925\n",
            "Figure 2. Parallel coordinate plot over all experiments run on\n",
            "CityScapes and SegNet. We distinguish between experiments us-\n",
            "ing SGD+mom and Adam optimizer. Experiments that reached\n",
            "Pareto front performance are drawn with higher saturation. We\n",
            "observe that Adam clearly outperforms the usage of SGD+mom.\n",
            "Further results can be found in the Figure A4.\n",
            "(RLW) [26], PCGrad [49], CAGrad [28], and IMTL [29]\n",
            "for which we used the implementation from [27]. We dis-\n",
            "tinguish between any combination of dataset {CityScapes\n",
            "[8], NYUv2 [36] }and network architecture {SegNet [1],\n",
            "DeepLabv3 [4] }. We run experiments for ten different ini-\n",
            "tial learning rates from [0.5,0.1,0.05, ...,0.00001] . More\n",
            "details are described in Appendix A3.2. As we observe that\n",
            "some setups on Cityscapes yield unsuitable results, we ex-\n",
            "clude runs with mIoU < 0.1on the segmentation task. For\n",
            "both datasets, we further remove diverged models with NaN\n",
            "output, e.g., due to inadequate learning rates.\n",
            "As different models and parameter setups typically show\n",
            "preference towards different tasks and metrics, we are in-\n",
            "terested in those models which are Pareto optimal (PO). We\n",
            "report the number of models whose performance lie on the\n",
            "overall Pareto front for either Adam or SGD+mom.\n",
            "Results We observe over all experimental setups that\n",
            "Adam performs favorably over SGD+mom (Table 2). This\n",
            "4Adam SGD+mom.\n",
            "#Exp. (valid) PO PO PO PO\n",
            "[Adam / SGD] (full) w.r.t. SGD (full) w.r.t. Adam\n",
            "CityScapes SegNet [151 / 149] 12 109 0 0\n",
            "CityScapes DeepLabV3 [177 / 178] 15 108 0 0\n",
            "NYUv2 SegNet [180 / 169] 9 20 2 2\n",
            "NYUv2 DeepLabV3 [177 / 179] 16 31 7 7\n",
            "Table 2. Comparison of number of Pareto optimal (PO) exper-\n",
            "iments using either Adam or SGD+momentum as optimizer.\n",
            "Models trained with Adam are consistently more often on the\n",
            "Pareto front compared to those trained with SGD+mom. The num-\n",
            "ber of Adam-based runs that are not dominated by any SGD-based\n",
            "run (PO w.r.t. SGD) is even higher, which does not hold the other\n",
            "way around. We further list the number of Pareto optimal runs\n",
            "corresponding to each MTO method in Table A2.\n",
            "especially holds true for experiments on CityScapes where\n",
            "the Pareto front for both network architectures only consists\n",
            "of Adam-based models. Moreover, an even larger num-\n",
            "ber of Adam-based models is not dominated by any model\n",
            "trained with SGD+mom (PO w.r.t. SGD). For NYUv2,\n",
            "Adam still performs stronger but SGD+mom. also occa-\n",
            "sionally delivers a PO result.\n",
            "For the individual metrics, the predominance of Adam\n",
            "on Cityscapes is further visualized in a parallel coordinate\n",
            "plot in Figure 2. Bold lines indicate the overall Pareto opti-\n",
            "mal experiments (PO full).\n",
            "In Appendix A4 we further report best ∆mresults for ev-\n",
            "ery MTO method in combination with Adam or SGD+mom\n",
            "(Tables A3 to A6). Again, Adam boosts the overall per-\n",
            "formance across methods. Noteworthy, no multi-task op-\n",
            "timization method seems to be Pareto dominant over plain\n",
            "EW with Adam which supports claims questioning the ef-\n",
            "fectiveness of specific MTO methods [24,45]. Nonetheless,\n",
            "looking at the ∆m-metric and individual metric, we see that\n",
            "sometimes with a small relative performance drop on one\n",
            "metric, significant gains on another metric can be achieved\n",
            "(e.g., Cityscapes+Semseg and depth for UW vs EW).\n",
            "Conclusion Overall, we conclude that not only a well-\n",
            "tuned learning rate but also the optimizer is crucial for MTL\n",
            "performance. In a fair and extensive experimental compar-\n",
            "ison, we were able to show that Adam shows superior per-\n",
            "formance in MTL setup compared to SGD+mom.\n",
            "4.1.3 The reasonable effectiveness of Adam in the con-\n",
            "text of uncertainty weighting\n",
            "We attribute our observed effectiveness of Adam in MTL\n",
            "to its partial loss-scale invariance [21] which we show the-\n",
            "oretically and empirically by a handcrafted loss-scaling ex-\n",
            "periment. This invariance can also be shown under mild\n",
            "assumptions for uncertainty weighting (UW) [20] which is\n",
            "the most prevalent loss weighting method in the literature.\n",
            "The loss-scale invariance of UW can be shown by as-suming an optimal solution for the σvalues similar to [22].\n",
            "This assumption is mild as this is a 1-dimensional convex\n",
            "optimization problem for each σ. The invariance can be\n",
            "demonstrated by inserting the analytical solution starting\n",
            "from UW. For example, assuming a Laplacian distribution\n",
            "for simplicity (this can be shown for other distributions as\n",
            "well), we have\n",
            "min\n",
            "σt1\n",
            "σtLt+ log σt⇒σt=Lt (1)\n",
            "The left hand side shows the typical form of UW, as shown\n",
            "for a Gaussian in [20, eq.(5)] Here, Ltis a task-specific loss\n",
            "andσtis a scalar parameter that is usually learned. Plugging\n",
            "back the solution of the optimal solution into the UW we get\n",
            "L=X\n",
            "tLt\n",
            "sg[Lt]+c, (2)\n",
            "where sgis the stop-gradient operator and cis a constant\n",
            "that can be omitted during optimization. Given this equa-\n",
            "tion, we can directly, see the invariance w.r.t. loss-scalings.\n",
            "For instance, with L1→α1L1andL2→α2L2, the deriva-\n",
            "tive of the total loss Lremains unchanged. As this invari-\n",
            "ance can be shown on the loss-level, it holds for all gradi-\n",
            "ent updates w.r.t. the head and backbone. Intuitively, this\n",
            "explains why UW is invariant to loss scalings such as mea-\n",
            "suring depth in centimeters, meters or inches. For further\n",
            "details, we refer to Appendix A1.\n",
            "Similarly for Adam, we can prove a scale invariance of\n",
            "losses in MTL that holds for the parameters of network\n",
            "heads. As before, we assume a hydra-like network archi-\n",
            "tecture with a shared backbone and task-specific heads. We\n",
            "start with the parameter-update rule from Adam and scale\n",
            "the corresponding losses Lt→αtLt. When only consider-\n",
            "ing the parameters of the corresponding heads ψt, the scal-\n",
            "ingsαtcancel out\n",
            "ψt,i=ψt,i−1−γq\n",
            "α2\n",
            "tˆv′\n",
            "tαtˆm′\n",
            "t. (3)\n",
            "Thus for the network heads, we see the same effect as for\n",
            "UW that different scalings do not impact the network up-\n",
            "date. However, this does not hold for the backbone. The\n",
            "full derivation is shown in Appendix A1. We confirm em-\n",
            "pirically in a handcrafted loss-scaling experiment in Ap-\n",
            "pendix A2 and Figures A1 and A2 that SGD does not offer\n",
            "any scaling invariance, whereas Adam involves the invari-\n",
            "ance property for the heads. The optimal UW demonstrates\n",
            "a scaling invariance for the heads and the backbone.\n",
            "We would like to note that the derivation for Adam is\n",
            "only valid for constant αt, e.g., measuring depth in different\n",
            "units. In case of dynamic loss weights that are not constant\n",
            "(e.g., UW), the weights do not cancel out fully due to the ac-\n",
            "cumulation of gradient histories within Adam. Nonetheless,\n",
            "this has profound implications for loss weighting methods\n",
            "that are used in conjunction with Adam. For instance, when\n",
            "5turning off the history within Adam and having a fixed\n",
            "backbone (by setting β1,2= 0), all loss weighting meth-\n",
            "ods, such as UW, random loss weighting and others, be-\n",
            "come equivalent to equal weighting (also for the backbone).\n",
            "Conclusion During MTL network training, we derive\n",
            "and measure a full loss-scale invariance for an optimal UW\n",
            "and a partial invariance for Adam. This partial invariance\n",
            "does not hold for SGD+momentum and could explain the\n",
            "effectiveness of Adam in MTL. Thus, when comparing dif-\n",
            "ferent loss weighting methods, it is crucial to be aware of\n",
            "the influence of the optimizer.\n",
            "To investigate the real-world presence of differences of\n",
            "gradient magnitudes and the gradient alignment within the\n",
            "network backbone, we provide empirical investigations in\n",
            "the subsequent section.\n",
            "4.2. Investigating gradient conflicts in multi-task\n",
            "learning and single-task learning\n",
            "Given our prior experiments showing the effectiveness\n",
            "of a standard (=single-task) learning tool such as the Adam\n",
            "optimizer over dedicated multi-task optimization tools, we\n",
            "challenge notion of multi-task learning (MTL) as it is com-\n",
            "monly interpreted in the literature.\n",
            "Motivation The common interpretation of MTL is fairly\n",
            "vague. In computer vision, tasks are often defined to be seg-\n",
            "mentation and depth (CityScapes) or recognizing multiple\n",
            "attributes (CelebA). This might actually not be fully distin-\n",
            "guished from regular single-task learning. We argue that in\n",
            "an extreme case, even recognizing a single cat in multiple\n",
            "images could be considered MTL. The cat could be hiding\n",
            "behind a plant and only revealing its eyes, requiring a neu-\n",
            "ral network to recognize the cat solely based on the eyes.\n",
            "In other images, the cat might hiding under the couch only\n",
            "revealing its paws, or mad and curled up into a furry ball\n",
            "because we took so many pictures. This would require a\n",
            "paw or fur classifier. Overall, a neural network is required\n",
            "to solve multiple tasks to reliably recognize our cat.\n",
            "Motivated by this example, we would like to quantify\n",
            "the distinction between STL and MTL in common datasets\n",
            "from a perspective of the multi-task optimization literature,\n",
            "which inspects gradient conflicts in neural networks. In par-\n",
            "ticular, we challenge the view point of conflicts between\n",
            "different gradients being a specific problem in MTL [49].\n",
            "While several works follow the idea of overcoming gradi-\n",
            "ent conflicts in MTL [19,28,42], the appearance of gradient\n",
            "conflicts has only been mildly investigated so far.\n",
            "Prerequisite To quantify differences between STL and\n",
            "MTL, we compare gradients between tasks tand samples\n",
            "xi. We can compare the alignment of two gradients g,g′\n",
            "on the shared parameters, e.g., of task aand task b, with the\n",
            "cosine similarity\n",
            "Scos(g,g′) = cos( ϕ) =g·g′\n",
            "∥g∥∥g′∥. (4)\n",
            "CityScapes\n",
            "SegNet\n",
            "0 100 200−1−0.500.51\n",
            "0 100 20000.51\n",
            "0 100 20000.51\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epochGrad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts) DeepLabV3\n",
            "0 100 200−1−0.500.51\n",
            "0 100 20000.51\n",
            "0 100 20000.51\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epochGrad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)\n",
            "Loading [MathJax]/extensions/MathMenu.jsNYUv2\n",
            "SegNet\n",
            "0 50 100 150−1−0.500.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 15000.51\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epochGrad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)\n",
            "Loading [MathJax]/extensions/MathMenu.js DeepLabV3\n",
            "0 50 100 150−1−0.500.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 15000.51\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epochGrad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)\n",
            "Loading [MathJax]/extensions/MathMenu.jsCelebA\n",
            "ResNet50\n",
            "0 20 40 60 80−1−0.500.51\n",
            "0 20 40 60 8000.51\n",
            "0 20 40 60 8000.51\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epochGrad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)\n",
            "Figure 3. Comparison of gradient similarities and conflicts\n",
            "between datasets and network architectures over training epochs.\n",
            "For each dataset and network combination, we report (from left\n",
            "to right) gradient cosine similarity, gradient magnitude similar-\n",
            "ity, and the ratio of conflicting gradient parameters with respect\n",
            "to gradient pairs corresponding to either different samples (STL\n",
            "setting) or different tasks (MTL setting). We report mean (solid\n",
            "line), standard deviation (shaded area), upper ( 97.5%) and lower\n",
            "(2.5%) percentile (dotted line) within an epoch. Overall, the di-\n",
            "rection conflicts are similar (first / last column), whereas the mag-\n",
            "nitude differences are more pronounced in MTL (middle column).\n",
            "Thus, two gradients are in conflict, if their cosine similarity\n",
            "is smaller than zero [49]. In particular, Scosis1/−1if gra-\n",
            "dients point in the same/opposite direction and 0in case of\n",
            "orthogonal directions. The gradient magnitude similarity\n",
            "Smag(g,g′) =2∥g∥2· ∥g′∥2\n",
            "∥g∥2\n",
            "2+∥g′∥2\n",
            "2(5)\n",
            "as defined in [49] yields values close to 1 for gradients of\n",
            "similar magnitude, or close to 0 for large discrepancies in\n",
            "magnitude. High dissimilarity in both gradient direction\n",
            "and magnitude is presumed to be a common MTL problem.\n",
            "Experimental setup During the training on aforemen-\n",
            "tioned datasets, we examine gradient similarity across two\n",
            "different setups: (1) between gradients of different tasks\n",
            "with respect to a single sample (typical MTL setup), e.g.,\n",
            "6g=∇ϕL0(fθ(xi))andg′=∇ϕL1(fθ(xi)); and (2) be-\n",
            "tween gradients corresponding to the same task but different\n",
            "samples within a batch (typical gradient behavior in STL),\n",
            "e.g.,g=∇ϕLt(fθ(x0))andg′=∇ϕLt(fθ(x1)). For\n",
            "both setups, we compute the previously named measures,\n",
            "gradient cosine similarity and gradient magnitude similarity\n",
            "as well as the ratio of conflicting gradient parameters. We\n",
            "are aware that our comparison between samples and tasks\n",
            "is not direct. Nonetheless, it serves as a coarse indicator to\n",
            "estimate their impact during network training. Implementa-\n",
            "tion details can be found in appendix A3.\n",
            "Results We show the evolvement of different gradient\n",
            "similarity measures over epochs in Figure 3. Surprisingly,\n",
            "when comparing STL (red line) and MTL (blue line), we\n",
            "find no consistent evidence for gradient alignment conflicts\n",
            "(left column) to be an exclusive problem of MTL. For in-\n",
            "stance, for Cityscapes, the variation of gradient alignment\n",
            "is fully encapsulated within the spread we observe in STL.\n",
            "For CelebA, the converse seems to be mostly the case.\n",
            "Furthermore, the choice of network architecture and distri-\n",
            "bution of task-specific and shared parameters (SegNet vs.\n",
            "DeepLabV3) can have a large influence on the spread of the\n",
            "cosine-similarity. Both architectures have roughly a similar\n",
            "number of shared-parameters. However, DeepLabV3 has\n",
            "a higher number of task-specific parameters which seems\n",
            "to reduce the variance in conflicts for both MTL and STL\n",
            "(variances decrease on row one vs. two and row three vs.\n",
            "four). In line with these observations, we found a similar\n",
            "number of conflicting gradient parameters (last column) for\n",
            "both STL and MTL among all experiments.\n",
            "For gradient magnitude similarities (middle column), we\n",
            "observe a clearer pattern. The similarity in magnitudes are\n",
            "continuously (in the mean) less pronounced in MTL com-\n",
            "pared to STL (blue line is below red one in all settings).\n",
            "Interestingly, the relative difference between the two setups\n",
            "remains similar over training which justifies the choice of\n",
            "fixed scalar task weightings as done in [45]. Further mea-\n",
            "sures can be found in Figures A6 to A9.\n",
            "Conclusion Overall, we conclude that the difficulty of\n",
            "MTL as opposed to STL is predominantly due to dif-\n",
            "ferences in gradient magnitudes. Although the problem\n",
            "of conflicting gradients has been typically associated with\n",
            "MTL [19, 28, 49], we found that gradient conflicts can ac-\n",
            "tually be even more pronounced in STL. Thus, gradient-\n",
            "alignment methods should be considered in a wider context\n",
            "in deep learning. Furthermore, this finding rises the ques-\n",
            "tion, whether common, well developed methods from STL\n",
            "already address gradient conflicts in deep learning.\n",
            "4.3. Robustness of multi-task representations on\n",
            "corrupted data\n",
            "In the last part of our analysis, we investigate whether\n",
            "features learned for multiple tasks generalize better to cor-rupted data compared to those learned for single tasks only.\n",
            "Motivation In his seminal paper [3], Caruana gives pre-\n",
            "liminary evidence that MTL provides stronger features and\n",
            "avoids spurious correlations (referred to better attribute se-\n",
            "lection in [3]). More recently, spurious correlations have\n",
            "often been directly connected with robustness [12, 17]. Re-\n",
            "sults from current literature on the robustness of MTL fea-\n",
            "tures are mixed. While MTL is stated to increase the\n",
            "adversarial- and noise-robustness over STL [23,34,47], oth-\n",
            "ers argue features selected by MTL could be more likely\n",
            "to be non-causal and, therefore, less robust. [2, 16]. Here,\n",
            "we further challenge the assumption whether MTL features\n",
            "lead to better robustness. We would like to nuance that we\n",
            "do not challenge the transferability of representations, e.g.,\n",
            "to new tasks, but solely focus on the claim that the MTL\n",
            "trained features are more robust w.r.t. different inputs.\n",
            "Approach For our experiment, we use models trained on\n",
            "clean data and test these on corrupted data. We select the\n",
            "model with the best performing hyperparameter configura-\n",
            "tion from previous experiments and compare the test perfor-\n",
            "mance of models trained in an multi-task setup to those that\n",
            "learned a single-task only. Note that none of the used mod-\n",
            "els were explicitly trained to handle data corruption. For\n",
            "testing, we use of the different perturbation modes proposed\n",
            "by Hendrycks et al. [15] which include different variants of\n",
            "noise, blur, and weather conditions and apply five levels of\n",
            "severity. We create a corrupted version of the test data for\n",
            "both CityScapes and NYUv2 for all proposed corruptions\n",
            "and perturbation levels (details in Appendix A3).\n",
            "To quantify the robustness of single- and multi-task mod-\n",
            "els, we first compute the individual task metrics M(e.g.,\n",
            "mIoU) per task tfor a STL and MTL network. Next, we\n",
            "compute the relative performance when each model is faced\n",
            "with corrupted data. Lastly, we calculate the difference of\n",
            "relative performances of the MTL compared to the STL\n",
            "model. In detail, over all corruption modes Cand levels\n",
            "of severity Swe have\n",
            "δt=1\n",
            "|C| · |S|X\n",
            "c∈CX\n",
            "s∈S(−1)p(t)δt,c,s (6)\n",
            "with δt,c,s=MMTL,corrupted\n",
            "t,c,s\n",
            "MMTL,clean\n",
            "t−MSTL,corrupted\n",
            "t,c,s\n",
            "MSTL,clean\n",
            "t\n",
            "where p(t) = 1 if a higher value on task tcorresponds to\n",
            "better performance and p(t) = 0 otherwise. This metric\n",
            "yields a negative value δt<0if the MTL model was able\n",
            "to handle data corruption better. If the STL model is more\n",
            "robust, it holds that δt>0.\n",
            "Results Figure 4 shows δtfor different corruption types\n",
            "for a SegNet with EW on NYUv2 averaged over five cor-\n",
            "ruption levels and three random runs. On the semantic seg-\n",
            "mentation and depth task, the STL models show a lower\n",
            "decrease in performance on the corrupted data than MTL\n",
            "(δt>0more often; shaded in red), indicating that the\n",
            "70.4\n",
            "0.2\n",
            "0.00.20.4Sem.Seg.(mIoU)\n",
            "(pixAcc)\n",
            "0.4\n",
            "0.2\n",
            "0.00.20.4Depth(AbsErr)\n",
            "(RelErr)\n",
            "Gaussian NoiseShot Noise\n",
            "Impulse NoiseDefocus BlurGlass BlurMotion BlurZoom BlurSnow FrostFog\n",
            "BrightnessContrastElasticPixelateJPEG0.4\n",
            "0.2\n",
            "0.00.20.4Normal(<11.25)\n",
            "(<22.5)\n",
            "(<30.0)\n",
            "(Mean)\n",
            "(Median)\n",
            "Figure 4. Comparison of generalization performance to cor-\n",
            "rupted data for MTL and STL on NYUv2 with SegNet and\n",
            "EW. We show the difference over relative performance decrease\n",
            "over all corruption modes averaged over five levels of severity and\n",
            "three runs. We color blocks in case either STL or MTL is able to\n",
            "handle the respective corruption better for all metrics of one task.\n",
            "STL is more robust compared to MTL models for the semantic\n",
            "and depth task In contrast, the MTL model was able to deal better\n",
            "with a couple of corruption modes for the normal task. Results for\n",
            "other setups can be found in the supplementary material.\n",
            "Sem.Seg. Depth\n",
            "MTO δmIoU δpixAcc δAbsErr δRelErr\n",
            "EW 0.0226 0.0298 0.1713 0.1869\n",
            "UW 0.0149 0.0200 0.1691 0.1652\n",
            "RLW 0.0236 0.0243 0.1250 0.1266\n",
            "IMTL 0.0144 0.0174 0.2434 0.2721\n",
            "PCGrad 0.0206 0.0156 0.1665 0.1634\n",
            "CAGrad 0.0053 0.0147 0.2006 0.2282\n",
            "Normal Mean\n",
            "δMean δMedian δ<11.25δ<22.5δ<30.0\n",
            "EW -0.0750 -0.1390 0.0378 0.0167 0.0129 0.0293\n",
            "UW -0.0461 -0.0895 0.0200 0.0168 0.0165 0.0319\n",
            "RLW -0.0942 -0.1717 0.0342 0.0152 0.0110 0.0104\n",
            "IMTL -0.0142 -0.0356 0.0342 0.0249 0.0217 0.0643\n",
            "PCGrad -0.0594 -0.1131 0.0364 0.0208 0.0171 0.0298\n",
            "CAGrad -0.0018 -0.0138 0.0402 0.0284 0.0240 0.0584\n",
            "Table 3. Out-Of Distribution generalization on corrupted\n",
            "NYUv2 [36] for different MTO methods on SegNet. We re-\n",
            "port difference between relative performance decrease for STL\n",
            "and MTL averaged over all modes of corruption and severity (cf.\n",
            "Equation (6)). Mean of three runs is reported.\n",
            "features learned for these respective tasks can better gen-\n",
            "eralize to corrupted data. In contrast, the MTL modelshows slightly better relative performance on the normal\n",
            "task ( δt<0more often for some of the normal metrics).\n",
            "To summarize, we see a slight indication that the depth and\n",
            "segmentation task might be more robust for STL than MTL.\n",
            "The results of other multi-task optimization methods (cf.\n",
            "Table 3) are similar to EW. Only IMTL and CAGrad show\n",
            "some stronger δtaverage for the depth task. Though, results\n",
            "on NYUv2 with DeepLabV3 (Table A9) and for Cityscapes\n",
            "(Table A8) could not work out a consistent pattern, as some-\n",
            "times MTL is more robust and sometimes STL.\n",
            "Conclusion We cannot confirm the outcome of [23] as\n",
            "we do not see any indication that the segmentation task is\n",
            "generally more robust in the MTL setting.\n",
            "Controversial to the claim of [34], our evaluation shows\n",
            "that none of the MTL approaches, even IMTL, PCGrad or\n",
            "CAGrad which adjust the gradients, yields consistent values\n",
            "ofδt<0which would have shown an advantage of certain\n",
            "MTO methods over STL.\n",
            "Instead, it depends on the task, the type of corruption, the\n",
            "network, the dataset and chosen hyperparameters whether\n",
            "MTL or STL is superior towards corrupted data. Whether\n",
            "there is a general pattern, we leave to further research.\n",
            "5. Conclusion and outlook\n",
            "This study aims to enhance our understanding of multi-\n",
            "task learning (MTL) in computer vision, providing valuable\n",
            "insights for future research as well as guidance for imple-\n",
            "mentations of real-world applications.\n",
            "We show that common optimization methods from sin-\n",
            "gle task learning (STL) like the Adam optimizer are effec-\n",
            "tive in MTL problems. We explain this with Adam’s partial\n",
            "loss-scale invariance. Next, we compare gradient statistics\n",
            "during training for STL and MTL. While gradient magni-\n",
            "tudes are a specific problem in MTL, we find the variability\n",
            "in gradient alignment to be similar in STL and MTL.\n",
            "Thus, we encourage a more unified viewpoint in which\n",
            "specific multi-task optimization methods are also consid-\n",
            "ered in single-task problems and vice versa. Furthermore,\n",
            "current methods often require exhaustive hyperparameter\n",
            "searches to perform well on multiple tasks due to the un-\n",
            "known/ non-linear behavior of the Pareto front and opti-\n",
            "mization landscape [45]. Hence, methods alleviating this\n",
            "extensive search and finding more robust methods are a\n",
            "promising direction. Lastly, our understanding of task (and\n",
            "sample) specific capacity allocation within a network and\n",
            "how best to tune it to custom requirements, is still not thor-\n",
            "oughly understood. Often task-weights are increased to as-\n",
            "sign more importance to a task which is in contrast to tuning\n",
            "the learning rate per task where sometimes a smaller learn-\n",
            "ing rate can be beneficial. Therefore, we require further\n",
            "investigations and disentanglement of these two concepts.\n",
            "For the ongoing debate regarding the robustness of fea-\n",
            "tures from multi-task approaches, we find empirical evi-\n",
            "8dence that multi-task features can actually be more brittle\n",
            "than single-task features. Here, future work could try to\n",
            "disentangle the different current perspectives from causal\n",
            "features [16], adversarial/noise robustness [23, 34, 47] and\n",
            "our results on common corruption robustness.\n",
            "Acknowledgments\n",
            "We thank Claudia Blaiotta, Martin Rapp, Frank R.\n",
            "Schmidt, Leonhard Hennicke, and Bastian Bischoff for their\n",
            "feedback and valuable discussions. Cathrin Elich thanks\n",
            "her supervisors, J ¨org St ¨uckler and Marc Pollefeys, for en-\n",
            "abling the opportunity to pursue an internship during her\n",
            "Ph.D. studies.\n",
            "The Bosch Group is carbon neutral. Administration,\n",
            "manufacturing and research activities do no longer leave a\n",
            "carbon footprint. This also includes GPU clusters on which\n",
            "the experiments have been performed.\n",
            "References\n",
            "[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.\n",
            "SegNet: A Deep Convolutional Encoder-Decoder Architec-\n",
            "ture for Image Segmentation. IEEE Transactions on Pattern\n",
            "Analysis and Machine Intelligence , 2017. 3, 4, 10, 11\n",
            "[2] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition\n",
            "in terra incognita. In Proceedings of the European confer-\n",
            "ence on computer vision (ECCV) , pages 456–473, 2018. 7\n",
            "[3] Rich Caruana. Multitask learning. Machine learning , 28:41–\n",
            "75, 1997. 1, 7\n",
            "[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\n",
            "Schroff, and Hartwig Adam. Encoder-decoder with atrous\n",
            "separable convolution for semantic image segmentation. In\n",
            "Computer Vision – ECCV 2018 , 2018. 3, 4, 7, 10, 11\n",
            "[5] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An-\n",
            "drew Rabinovich. Gradnorm: Gradient normalization for\n",
            "adaptive loss balancing in deep multitask networks. In Jen-\n",
            "nifer G. Dy and Andreas Krause, editors, Proc. of ICML ,\n",
            "volume 80 of Proceedings of Machine Learning Research ,\n",
            "pages 793–802. PMLR, 2018. 2\n",
            "[6] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong,\n",
            "Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov.\n",
            "Just pick a sign: Optimizing deep multitask models with\n",
            "gradient sign dropout. In Hugo Larochelle, Marc’Aurelio\n",
            "Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-\n",
            "Tien Lin, editors, Advances in Neural Information Process-\n",
            "ing Systems 33: Annual Conference on Neural Information\n",
            "Processing Systems 2020, NeurIPS 2020, December 6-12,\n",
            "2020, virtual , 2020. 2\n",
            "[7] Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani, and\n",
            "Samir A Rawashdeh. Multinet++: Multi-stream feature ag-\n",
            "gregation and geometric loss strategy for multi-task learning.\n",
            "InProceedings of the IEEE/CVF Conference on Computer\n",
            "Vision and Pattern Recognition (CVPR) Workshops , 2019. 2\n",
            "[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\n",
            "Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\n",
            "Franke, Stefan Roth, and Bernt Schiele. The cityscapesdataset for semantic urban scene understanding. In 2016\n",
            "IEEE Conference on Computer Vision and Pattern Recog-\n",
            "nition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016 ,\n",
            "pages 3213–3223. IEEE Computer Society, 2016. 3, 4, 7, 10,\n",
            "15\n",
            "[9] Jean-Antoine D ´esid´eri. Multiple-gradient descent algorithm\n",
            "(mgda) for multiobjective optimization. Comptes Rendus\n",
            "Mathematique , 350:313–318, 2012. 2\n",
            "[10] Heshan Devaka Fernando, Han Shen, Miao Liu, Subhajit\n",
            "Chaudhury, Keerthiram Murugesan, and Tianyi Chen. Miti-\n",
            "gating gradient bias in multi-objective learning: A provably\n",
            "convergent approach. In The Eleventh International Con-\n",
            "ference on Learning Representations, ICLR 2023, Kigali,\n",
            "Rwanda, May 1-5, 2023 . OpenReview.net, 2023. 2\n",
            "[11] Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan\n",
            "Anil, and Chelsea Finn. Efficiently identifying task group-\n",
            "ings for multi-task learning. In Marc’Aurelio Ranzato,\n",
            "Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jen-\n",
            "nifer Wortman Vaughan, editors, Advances in Neural Infor-\n",
            "mation Processing Systems 34: Annual Conference on Neu-\n",
            "ral Information Processing Systems 2021, NeurIPS 2021,\n",
            "December 6-14, 2021, virtual , pages 27503–27516, 2021.\n",
            "2\n",
            "[12] Robert Geirhos, J ¨orn-Henrik Jacobsen, Claudio Michaelis,\n",
            "Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe-\n",
            "lix A Wichmann. Shortcut learning in deep neural networks.\n",
            "Nature Machine Intelligence , 2(11):665–673, 2020. 7\n",
            "[13] Michelle Guo, Albert Haque, De-An Huang, Serena Yeung,\n",
            "and Li Fei-Fei. Dynamic task prioritization for multitask\n",
            "learning. In Proceedings of the European Conference on\n",
            "Computer Vision (ECCV) , 2018. 2\n",
            "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
            "Deep residual learning for image recognition. In 2016 IEEE\n",
            "Conference on Computer Vision and Pattern Recognition,\n",
            "CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016 , pages\n",
            "770–778. IEEE Computer Society, 2016. 3, 7\n",
            "[15] Dan Hendrycks and Thomas G. Dietterich. Benchmarking\n",
            "neural network robustness to common corruptions and per-\n",
            "turbations. In Proc. of ICLR . OpenReview.net, 2019. 7\n",
            "[16] Ziniu Hu, Zhe Zhao, Xinyang Yi, Tiansheng Yao, Lichan\n",
            "Hong, Yizhou Sun, and Ed Chi. Improving multi-task gen-\n",
            "eralization via regularizing spurious correlation. Advances\n",
            "in Neural Information Processing Systems , 35:11450–11466,\n",
            "2022. 7, 9\n",
            "[17] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan\n",
            "Engstrom, Brandon Tran, and Aleksander Madry. Adversar-\n",
            "ial examples are not bugs, they are features. In Hanna M.\n",
            "Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\n",
            "d’Alch ´e-Buc, Emily B. Fox, and Roman Garnett, editors,\n",
            "Advances in Neural Information Processing Systems 32: An-\n",
            "nual Conference on Neural Information Processing Systems\n",
            "2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\n",
            "Canada , pages 125–136, 2019. 7\n",
            "[18] Keishi Ishihara, Anssi Kanervisto, Jun Miura, and Ville Hau-\n",
            "tam¨aki. Multi-task learning with attention for end-to-end au-\n",
            "tonomous driving. In IEEE Conference on Computer Vision\n",
            "and Pattern Recognition Workshops, CVPR Workshops 2021,\n",
            "9virtual, June 19-25, 2021 , pages 2902–2911. Computer Vi-\n",
            "sion Foundation / IEEE, 2021. 1\n",
            "[19] Adri ´an Javaloy and Isabel Valera. Rotograd: Gradient ho-\n",
            "mogenization in multitask learning. In Proc. of ICLR . Open-\n",
            "Review.net, 2022. 2, 6, 7\n",
            "[20] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\n",
            "learning using uncertainty to weigh losses for scene geome-\n",
            "try and semantics. In 2018 IEEE Conference on Computer\n",
            "Vision and Pattern Recognition, CVPR 2018, Salt Lake City,\n",
            "UT, USA, June 18-22, 2018 , pages 7482–7491. IEEE Com-\n",
            "puter Society, 2018. 2, 3, 4, 5, 1, 7\n",
            "[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for\n",
            "stochastic optimization. In Yoshua Bengio and Yann LeCun,\n",
            "editors, Proc. of ICLR , 2015. 1, 3, 5, 2\n",
            "[22] Lukas Kirchdorfer, Cathrin Elich, Simon Kutsche, Heiner\n",
            "Stuckenschmidt, Lukas Schott, and K ¨ohler. Analytical\n",
            "uncertainty-based loss weighting in multi-task learning. un-\n",
            "published , 2023. 5, 1\n",
            "[23] Marvin Klingner, Andreas Bar, and Tim Fingscheidt. Im-\n",
            "proved noise and attack robustness for semantic segmenta-\n",
            "tion by using multi-task training with self-supervised depth\n",
            "estimation. In Proceedings of the IEEE/CVF Conference on\n",
            "Computer Vision and Pattern Recognition Workshops , pages\n",
            "320–321, 2020. 2, 7, 8, 9\n",
            "[24] Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon\n",
            "Whiteson, and M. Pawan Kumar. In Defense of the Unitary\n",
            "Scalarization for Deep Multi-Task Learning. In Neural In-\n",
            "formation Processing Systems , 2022. 1, 2, 3, 5\n",
            "[25] Dong-Gyu Lee. Fast drivable areas estimation with multi-\n",
            "task learning for real-time autonomous driving assistant. Ap-\n",
            "plied Sciences , 11(22):10713, 2021. 1\n",
            "[26] Baijiong Lin, Feiyang YE, Yu Zhang, and Ivor Tsang. Rea-\n",
            "sonable Effectiveness of Random Weighting: A Litmus Test\n",
            "for Multi-Task Learning. Transactions on Machine Learning\n",
            "Research , 2022. 2, 3, 4, 7\n",
            "[27] Baijiong Lin and Yu Zhang. LibMTL: A Python Library for\n",
            "Multi-Task Learning. ArXiv preprint , abs/2203.14338, 2022.\n",
            "4, 7, 8\n",
            "[28] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang\n",
            "Liu. Conflict-averse gradient descent for multi-task learn-\n",
            "ing. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\n",
            "Dauphin, Percy Liang, and Jennifer Wortman Vaughan, ed-\n",
            "itors, Advances in Neural Information Processing Systems\n",
            "34: Annual Conference on Neural Information Processing\n",
            "Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual ,\n",
            "pages 18878–18890, 2021. 2, 3, 4, 6, 7, 8, 12\n",
            "[29] Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin\n",
            "Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang.\n",
            "Towards impartial multi-task learning. In Proc. of ICLR .\n",
            "OpenReview.net, 2021. 2, 3, 4, 7\n",
            "[30] Shikun Liu, Stephen James, Andrew J Davison, and Ed-\n",
            "ward Johns. Auto-Lambda: Disentangling Dynamic Task\n",
            "Relationships. Transactions on Machine Learning Research ,\n",
            "2022. 2\n",
            "[31] Shikun Liu, Edward Johns, and Andrew J. Davison. End-to-\n",
            "end multi-task learning with attention. In IEEE Conference\n",
            "on Computer Vision and Pattern Recognition, CVPR 2019,Long Beach, CA, USA, June 16-20, 2019 , pages 1871–1880.\n",
            "Computer Vision Foundation / IEEE, 2019. 2, 7\n",
            "[32] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\n",
            "Deep learning face attributes in the wild. In 2015 IEEE Inter-\n",
            "national Conference on Computer Vision, ICCV 2015, San-\n",
            "tiago, Chile, December 7-13, 2015 , pages 3730–3738. IEEE\n",
            "Computer Society, 2015. 3, 7\n",
            "[33] Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas\n",
            "Kokkinos. Attentive single-tasking of multiple tasks. In\n",
            "IEEE Conference on Computer Vision and Pattern Recogni-\n",
            "tion, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 ,\n",
            "pages 1851–1860. Computer Vision Foundation / IEEE,\n",
            "2019. 2, 3\n",
            "[34] Chengzhi Mao, Amogh Gupta, Vikram Nitin, Baishakhi Ray,\n",
            "Shuran Song, Junfeng Yang, and Carl V ondrick. Multitask\n",
            "learning strengthens adversarial robustness. In Computer\n",
            "Vision–ECCV 2020: 16th European Conference, Glasgow,\n",
            "UK, August 23–28, 2020, Proceedings, Part II 16 , pages\n",
            "158–174. Springer, 2020. 2, 7, 8, 9\n",
            "[35] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-\n",
            "tial Hebert. Cross-stitch networks for multi-task learning.\n",
            "In2016 IEEE Conference on Computer Vision and Pattern\n",
            "Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30,\n",
            "2016 , pages 3994–4003. IEEE Computer Society, 2016. 2\n",
            "[36] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob\n",
            "Fergus. Indoor segmentation and support inference from\n",
            "rgbd images. In ECCV , 2012. 3, 4, 8, 7, 11, 15\n",
            "[37] Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron,\n",
            "Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya. Multi-\n",
            "task learning as a bargaining game. In Kamalika Chaudhuri,\n",
            "Stefanie Jegelka, Le Song, Csaba Szepesv ´ari, Gang Niu, and\n",
            "Sivan Sabato, editors, International Conference on Machine\n",
            "Learning, ICML 2022, 17-23 July 2022, Baltimore, Mary-\n",
            "land, USA , volume 162 of Proceedings of Machine Learning\n",
            "Research , pages 16428–16446. PMLR, 2022. 2\n",
            "[38] Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet,\n",
            "and Maria A Zuluaga. Improved optimization strategies for\n",
            "deep multi-task networks. ArXiv preprint , abs/2109.11678,\n",
            "2021. 2, 4\n",
            "[39] Sebastian Ruder. An overview of multi-task learning in deep\n",
            "neural networks. ArXiv preprint , abs/1706.05098, 2017. 2\n",
            "[40] Ozan Sener and Vladlen Koltun. Multi-task learning as\n",
            "multi-objective optimization. In Samy Bengio, Hanna M.\n",
            "Wallach, Hugo Larochelle, Kristen Grauman, Nicol `o Cesa-\n",
            "Bianchi, and Roman Garnett, editors, Advances in Neu-\n",
            "ral Information Processing Systems 31: Annual Conference\n",
            "on Neural Information Processing Systems 2018, NeurIPS\n",
            "2018, December 3-8, 2018, Montr ´eal, Canada , pages 525–\n",
            "536, 2018. 2, 4\n",
            "[41] Dmitry Senushkin, Nikolay Patakin, Arseny Kuznetsov, and\n",
            "Anton Konushin. Independent component alignment for\n",
            "multi-task learning. In IEEE/CVF Conference on Computer\n",
            "Vision and Pattern Recognition, CVPR 2023, Vancouver,\n",
            "BC, Canada, June 17-24, 2023 , pages 20083–20093. IEEE,\n",
            "2023. 2\n",
            "[42] Guangyuan Shi, Qimai Li, Wenlong Zhang, Jiaxin Chen,\n",
            "and Xiao-Ming Wu. Recon: Reducing Conflicting Gradients\n",
            "10From the Root For Multi-Task Learning. In The Eleventh In-\n",
            "ternational Conference on Learning Representations , 2023.\n",
            "2, 6\n",
            "[43] Trevor Standley, Amir Roshan Zamir, Dawn Chen,\n",
            "Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese.\n",
            "Which tasks should be learned together in multi-task learn-\n",
            "ing? In Proc. of ICML , volume 119 of Proceedings of Ma-\n",
            "chine Learning Research , pages 9120–9132. PMLR, 2020.\n",
            "2\n",
            "[44] Simon Vandenhende, Stamatios Georgoulis, Wouter Van\n",
            "Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van\n",
            "Gool. Multi-task learning for dense prediction tasks: A sur-\n",
            "vey. IEEE Transactions on Pattern Analysis and Machine\n",
            "Intelligence , 2021. 1, 2\n",
            "[45] Derrick Xin, Behrooz Ghorbani, Ankush Garg, Orhan Fi-\n",
            "rat, and Justin Gilmer. Do Current Multi-Task Optimization\n",
            "Methods in Deep Learning Even Help? In Neural Informa-\n",
            "tion Processing Systems , 2022. 1, 2, 3, 4, 5, 7, 8\n",
            "[46] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe.\n",
            "Pad-net: Multi-tasks guided prediction-and-distillation net-\n",
            "work for simultaneous depth estimation and scene parsing.\n",
            "In2018 IEEE Conference on Computer Vision and Pattern\n",
            "Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-\n",
            "22, 2018 , pages 675–684. IEEE Computer Society, 2018. 2\n",
            "[47] Teresa Yeo, Oguzhan Fatih Kar, and Amir Zamir. Robust-\n",
            "ness via cross-domain ensembles. In 2021 IEEE/CVF In-\n",
            "ternational Conference on Computer Vision, ICCV 2021,\n",
            "Montreal, QC, Canada, October 10-17, 2021 , pages 12169–\n",
            "12179. IEEE, 2021. 7, 9\n",
            "[48] Fisher Yu, Vladlen Koltun, and Thomas A. Funkhouser. Di-\n",
            "lated residual networks. In 2017 IEEE Conference on Com-\n",
            "puter Vision and Pattern Recognition, CVPR 2017, Hon-\n",
            "olulu, HI, USA, July 21-26, 2017 , pages 636–644. IEEE\n",
            "Computer Society, 2017. 7\n",
            "[49] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine,\n",
            "Karol Hausman, and Chelsea Finn. Gradient surgery for\n",
            "multi-task learning. In Hugo Larochelle, Marc’Aurelio Ran-\n",
            "zato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien\n",
            "Lin, editors, Advances in Neural Information Processing\n",
            "Systems 33: Annual Conference on Neural Information Pro-\n",
            "cessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\n",
            "virtual , 2020. 2, 4, 6, 7\n",
            "[50] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\n",
            "Wang, and Jiaya Jia. Pyramid scene parsing network. In 2017\n",
            "IEEE Conference on Computer Vision and Pattern Recog-\n",
            "nition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 ,\n",
            "pages 6230–6239. IEEE Computer Society, 2017. 7\n",
            "11Challenging common assumptions in multi-task learning\n",
            "-Supplementary Material-\n",
            "A1. Theoretical insights into multi-task learning dynamics\n",
            "In this section, we aim to explain the success of the Adam optimizer [21] by relating it to uncertainty weighting [20]. We\n",
            "show partial invariances w.r.t. prior task-weights for the Adam optimizer and full invariances for the uncertainty weighting\n",
            "under mild assumptions. We further show that for SGD + momentum no invariance can be observed and the loss-weight\n",
            "can be seen as a task-specific learning rate which is not the case for the Adam optimizer. Previous literature on weighting\n",
            "methods in MTL did not explicitly show how task-weighting methods are affected by different optimizers.\n",
            "A1.1. Uncertainty weighting (UW): Full loss-scale invariance\n",
            "To demonstrate the invariance of uncertainty weighting (UW) [20], we start from the the original formula. In UW,\n",
            "the homoscedastic uncertainty1σtto weight task tis learned by gradient descent as done in [22]. However, we can also\n",
            "analytically compute the optimal uncertainty weights in each iteration instead of learning them using gradient descent. The\n",
            "minimization objective depends on the underlying loss function and likelihood. For simplicity, we show the derivation\n",
            "exemplary for the L1loss. It is straight-forward to derive the same for a Gaussian and other distributions. The objective of\n",
            "uncertainty weighting is given as\n",
            "min\n",
            "σt1\n",
            "σtLt+ log σt (7)\n",
            "withLt=|y−fW(x)|which can be derived from a log likelihood of a Laplace distribution p(y|fW(x), σ) =\n",
            "1\n",
            "2σexp(−|y−fW(x)|\n",
            "σ). Taking the derivative and solving for σtresults in an analytically optimal solution:\n",
            "∂\n",
            "∂σt1\n",
            "σtLt+ log σt=−1\n",
            "σ2\n",
            "tLt+1\n",
            "σt(8)\n",
            "−1\n",
            "σ2\n",
            "tLt+1\n",
            "σt!= 0 (9)\n",
            "σt=Lt (10)\n",
            "withσt>0. Here, we further see, that the optimization problem is convex and just one dimensional. Thus assuming an\n",
            "optimal log-sigma is a mild assumption. Plugging the optimal solution back into the original uncertainty weighting, we get\n",
            "L=X\n",
            "t1\n",
            "sg[Lt]Lt+ logp\n",
            "sg[Lt], (11)\n",
            "where we denote sgas the stopgradient operator. Since no gradient is computed of the second part of the loss, it can be\n",
            "simplified, such that\n",
            "L=X\n",
            "tLt\n",
            "sg[Lt]. (12)\n",
            "Now considering task-specific weights αt, the final equation does not change as task-specific weights cancel out:\n",
            "L=X\n",
            "tαtLt\n",
            "αtsg[Lt]\n",
            "=X\n",
            "tLt\n",
            "sg[Lt](13)\n",
            "Therefore, the optimal uncertainty weighting is invariant w.r.t. task-specific loss-scalings, as each scaling cancels out.\n",
            "A1.2. SGD: No loss-scale invariance and relationship of learning rate and task weights on a gradient level\n",
            "Unlike for UW-O, we show that the SGD update rule itself does not show any invariances and that task-weights are\n",
            "essentially task-specific learning rates. We show that task-weights and learning rate are interacting hyperparameters and thus\n",
            "cannot be viewed in isolation. Thus, it is crucially to tune the learning rate for different loss weighting methods.\n",
            "1In Kendall et al., this is termed the aleatoric homoscedastic uncertainty. However, as the task weights vary over the course of training and also with\n",
            "respect to the model capacity, it is technically not only the aleatoric uncertainty but also encapsulates further components such as model capacity and amount\n",
            "of data seen.\n",
            "1The parameter update rule in neural networks optimized with SGD is\n",
            "θi=θi−1−γ∂\n",
            "∂θi−1L, (14)\n",
            "where the network parameters in iteration iare defined as θi,γis the learning rate and L=P\n",
            "tαLtfor uniform task\n",
            "weights (EW). Therefore, in the case of EW we can rewrite Equation 14, such that\n",
            "θi=θi−1−γ∂\n",
            "∂θi−1X\n",
            "iαLt\n",
            "=θi−1−γα∂\n",
            "∂θi−1X\n",
            "iLt(15)\n",
            "We can conclude that in the case of EW and SGD, task weight and learning rate are interchangeable. For example,\n",
            "increasing the weights αby a factor of 10 has the same effect as increasing the learning rate by a factor of 10. A value >1\n",
            "forαincreases the parameter update while a value <1reduces the update.\n",
            "In the case of non-uniform task weights, the parameter update is given as follows:\n",
            "θi=θi−1−γ∂\n",
            "∂θi−1X\n",
            "iαtLt\n",
            "=θi−1−∂\n",
            "∂θi−1X\n",
            "iγαtLt(16)\n",
            "Therefore, for non-uniform task weights and SGD, we conclude that the learning rate can be included in the task-specific\n",
            "weight. This means that task weighting is nothing else than assigning task-specific learning rates. Tasks with a higher weight\n",
            "αihave a proportionally higher parameter update step while tasks with smaller weights are moving slower towards the loss\n",
            "minimum.\n",
            "Hence, task weight αacts exactly like the learning rate γ. The major difference is that task weights are task-specific while\n",
            "usually a single learning rate is applied to all network parameters. Note that this holds for SGD and SGD + momentum, but\n",
            "it does not apply to optimizers such as Adam, Adagrad, or RMSProp. In the following, we show our findings for the widely\n",
            "used Adam optimizer.\n",
            "A1.3. Adam: Partial invariance towards loss-scales\n",
            "Similarly to the invariance demonstrated for UW, we can derive a partial invariance for Adam. This invariance has already\n",
            "been demonstrated by Kingma and Ba [21], who showed that the magnitudes of the parameter updates using Adam are\n",
            "invariant to rescaling the gradients. Our novelty lies in showing this invariance property in the context of multi-task learning\n",
            "and its impact on different MTL optimizers. For Adam, we claim that the magnitude of task-specific weights only affects the\n",
            "backbone and cancels out for the heads.\n",
            "Comparing Adam to SGD, the interaction between task weights and learning rate is fundamentally different. For SGD, the\n",
            "task weights could be viewed as task-specific learning rates. However, Adam interferes with many task-weighting methods\n",
            "as shown in more detail below. Thus comparisons of loss weighting methods in the multi-task learning literature based on\n",
            "SGD and Adam cannot be directly set side by side.\n",
            "Our derivation relies on the usual MTL setting with task-specific heads and shared backbone. Here, we only look at the\n",
            "task-specific parameters ψtof task twhose loss Ltis scaled by αt, such that Lt→αtLtand assume a frozen backbone. We\n",
            "can look at the parameter update of one head independently of the other heads because the derivative of the losses w.r.t. the\n",
            "other tasks is 0:\n",
            "∂\n",
            "∂ψt,i−1Lj= 0 for t ̸= j. (17)\n",
            "The general update rule for parameters psiat time step iusing Adam is\n",
            "ψi=ψi−1−γ√ˆvi+ϵˆmi, (18)\n",
            "where mi=β1mi−1+ (1−β1)giandvi=β2vi−1+ (1−β2)g2\n",
            "i. To counteract the bias towards 0, the moments are\n",
            "corrected as ˆmi=mi\n",
            "1−βi\n",
            "1andˆvi=vi\n",
            "1−βi\n",
            "2.\n",
            "For task-specific parameters ψt, task weights αtlinearly scale the first moment mt,i\n",
            "2mt,i=β1mt,i−1+ (1−β1)gt,i\n",
            "=β1mt,i−1+ (1−β1)∂\n",
            "∂ψt,i−1αtLt\n",
            "=β1mt,i−1+ (1−β1)αt∂\n",
            "∂ψt,i−1Lt\n",
            "=β1mt,i−1+ (1−β1)αtg′\n",
            "t,i(19)\n",
            "and quadratically scale the second moment vt,i\n",
            "vt,i=β2vt,i−1+ (1−β2)g2\n",
            "t,i\n",
            "=β2vt,i−1+ (1−β2)(∂\n",
            "∂ψt,i−1αtLt)2\n",
            "=β2vt,i−1+ (1−β2)α2\n",
            "t(∂\n",
            "∂ψt,i−1Lt)2\n",
            "=β2vt,i−1+ (1−β2)α2\n",
            "tg′2\n",
            "t,i,(20)\n",
            "where g′\n",
            "t,iis the gradient of the unscaled loss Ltw.r.t. the task-specific parameters for task t. As this holds for iteration i\n",
            "and because we have mt,1=αg′\n",
            "t,1+ 0respectively vt,1=α2\n",
            "tg′2\n",
            "t,1+ 0withmt,0= 0,vt,0= 0 at the first iteration, this\n",
            "holds for any iteration step. This thus allows us to rewrite ˆmt,i=αtˆm′\n",
            "t,iandˆvt,i=α2\n",
            "tˆv′t,i.\n",
            "Plugging, this back into the update rule, the final update rule for the task-specific parameters is given as\n",
            "ψt,i−1=ψt,i−1−γ√ˆvtiˆmt,i\n",
            "=ψt,i−1−γq\n",
            "α2\n",
            "tˆv′t,iαtˆm′t,i\n",
            "=ψt,i−1−γq\n",
            "ˆv′t,iˆm′t,i,(21)\n",
            "where the loss-scaling αtcancels out. Therefore, the parameters of the task-specific heads are invariant to loss-scalings\n",
            "using Adam.\n",
            "This partial invariance is a highly desired property, as there is a fundamental trade-off between tuning the learning rate\n",
            "and manual task weights. Given Adams invariance for the head, the weighting only affects the backbone. Thus the learning\n",
            "rate can be set for the parameters of the head independent of the loss weights. With the loss weights, we can prioritize tasks\n",
            "in the backbone and therefore walk along the Pareto front as empirically shown by [45].\n",
            "The invariance, however, does not hold anymore when the backbone parameters θare updated as well. As we have\n",
            "mi=β1mi−1+ (1−β1)∂\n",
            "∂θi−1X\n",
            "tαtLt\n",
            "=β1mi−1+ (1−β1)X\n",
            "tαtg′\n",
            "t,i(22)\n",
            "and\n",
            "vi=β1vi−1+ (1−β1)(∂\n",
            "∂θi−1X\n",
            "tαtLt)2\n",
            "=β1vi−1+ (1−β1)(X\n",
            "tαtg′\n",
            "t,i)2(23)\n",
            "we conclude that the task weights atlinearly affect the first moment mi, while having a quadratic effect on the update of the\n",
            "second moment vi.\n",
            "Note that for both task-heads only as well as the backbone, we have a full invariance in case of independent optimizers,\n",
            "3e.g., one Adam optimizer per task similar to [38]. However, naive implementations scale poorly (in terms of computational\n",
            "complexity) with the number of tasks here.\n",
            "In the following experiments, we provide empirical evidence for our finding that a) Adam offers loss-scale invariance for\n",
            "the parameters of the task-specific heads, and b) Adam offers loss-scale invariance for all network parameters (backbone and\n",
            "heads) if β1,2= 0.\n",
            "A2. Empirical Confirmation of scale invariances in Adam and Optimal Uncertainty Weighting\n",
            "In the prior section, we derived theoretical results for loss-scale (partial) invariance within multi-task learning for the\n",
            "Adam optimizer and uncertainty weighting. In this section, we confirm this invariance empirically with a toy task.\n",
            "Experimental Setup We consider a two-task toy experiment in which we look at the gradient magnitudes with different\n",
            "combinations of Adam, SGD, EW, optimal uncertainty weighting (UW-O), and loss-scalings. To generate the data, we sample\n",
            "scalar input values from a uniform distribution. The outputs are just scalings of the input. As a neural network which consists\n",
            "of a shared backbone (two layers with LeakyReLU as non-linearity and 20 neurons per hidden layer) and two heads for the\n",
            "two tasks, each consisting again of two layers. Both task measure the depth but in different units. Both tasks are measured\n",
            "with an L1-loss. We provide two settings, in the first one, depth is measured on the same scale. In the second setting, one\n",
            "depth loss is scaled by 10x (e.g., measured in cm instead of deci-meters) and one other loss is scaled by 0.1 (e.g., measured in\n",
            "meters instead of deci-meters). For each setting, we test various combinations of loss weighting and optimizer combinations.\n",
            "The 8 different experiments are:\n",
            "• Equal weighting using SGD\n",
            "• Equal weighting using SGD with scalings 10∗Lsegand0.01∗Ldep\n",
            "• Equal weighting using Adam\n",
            "• Equal weighting using Adam with scalings 10∗Lsegand0.01∗Ldep\n",
            "• Optimal uncertainty weighting using SGD\n",
            "• Optimal uncertainty weighting using SGD with scalings 10∗Lsegand0.01∗Ldep\n",
            "• Equal weighting using separate Adam optimizers per task\n",
            "• Equal weighting using separate Adam optimizers per task with scalings 10∗Lsegand0.01∗Ldep\n",
            "To better control for different factors of influence, we first perform the first 6 of the listed experiments with a fixed\n",
            "backbone, i.e., we do not update the parameters in the backbone but only in the heads. Afterward, we show all 8 experiments\n",
            "trained with a network where all parameters (including the backbone) are updated. This allows us to verify if our theoretical\n",
            "derivations regarding the (partial) loss-scaling invariance of Adam and UW-O also hold in practice, and compare this to the\n",
            "SGD optimizer.\n",
            "Note that we only care about the invariance and did not tune any hyperparameters for performance.\n",
            "Results for fixed backbone Figure A1 shows the losses, the scaled losses (by loss weighting method), the gradient mag-\n",
            "nitudes as well as the gradient update magnitudes for both heads along the 100 epochs of training with a fixed backbone.\n",
            "Regarding SGD, we can observe that the equal weighting experiment differs from its scaled variant along all 8 dimensions.\n",
            "This is because SGD does not offer any loss-scaling invariance. As expected, at the beginning of the training the gradient\n",
            "update magnitude of the first depth head parameters with the scaled loss (dotted line) is by a factor of 10higher than the\n",
            "unscaled (solid line) one. The same effect applies to the gradient update magnitude of the second depth head parameters, but\n",
            "with a factor of 0.01.\n",
            "In contrast, Adam is loss-scale invariant. We can observe that the unscaled (solid line) and the scaled version (dotted\n",
            "line) have equal gradient update magnitudes in the last row. Note that practically due to an epsilon = 10−8parameter in\n",
            "the denominator and float precision a slight divergence would occur with larger number of epochs. This result confirms\n",
            "our theoretical finding in equation 21. We skip the experiment of separated Adam optimizers per task because it would be\n",
            "equivalent to this version given a fixed backbone.\n",
            "40 20 40 60 80102\n",
            "101\n",
            "100101Depth loss 1EW    + SGD mom.\n",
            "EW    + SGD mom. 10xL1 0.1xL2\n",
            "EW    + Adam\n",
            "EW    + Adam 10xL1 0.1xL2\n",
            "UW-O + SGD mom.\n",
            "UW-O + SGD mom. 10xL1 0.1xL2\n",
            "0 20 40 60 80103\n",
            "102\n",
            "101\n",
            "100Depth loss 2\n",
            "0 20 40 60 80102\n",
            "101\n",
            "100Depth loss 1  \n",
            " after weighting method\n",
            "0 20 40 60 80103\n",
            "102\n",
            "101\n",
            "100Depth loss 2  \n",
            " after weighting method\n",
            "0 20 40 60 80105\n",
            "104\n",
            "103\n",
            "102\n",
            "101\n",
            "Depth head 1 \n",
            " gradient magnitude\n",
            "0 20 40 60 80105\n",
            "104\n",
            "103\n",
            "102\n",
            "Depth head 2 \n",
            " gradient magnitude\n",
            "0 20 40 60 80105\n",
            "104\n",
            "103\n",
            "Depth head 1 \n",
            " update magnitude\n",
            "0 20 40 60 80105\n",
            "104\n",
            "103\n",
            "Depth head 2 \n",
            " update magnitudeFigure A1. Invariances within the neural network for a frozen backbone. Comparing the effect of loss-scalings in a toy experiment\n",
            "with two tasks. For each optimizer and loss weighting combination, we run two settings with a) loss L1 and loss L2 are equally weighted\n",
            "or b) L1 is scaled by 10x and L2 by 0.1. For each setting, we measure the SGD + momentum and Adam optimizer with no post weighting\n",
            "(EW) and SGD + momentum with optimimal uncertainty weighting. We show the scaled losses, gradient magnitudes, and gradient update\n",
            "magnitudes in the the two task heads and keep the backbone frozen. While SGD does not offer any loss-scaling invariance, Adam makes\n",
            "the gradient updates of the head parameters invariant to scales confirming our derivation (red lines overlap in lowest row). Equivalently,\n",
            "for UW-O we also observe the theoretically derived invariances (green lines overlap in lowest row)\n",
            "Lastly, we want to investigate the invariance properties of UW-O. We compare the scaled (dotted line) and unscaled (solid\n",
            "line) version of UW-O with the SGD optimizer. As expected, the gradients, as well as the gradient updates, match in both\n",
            "heads.\n",
            "In the following, let’s investigate whether the observed results still hold if we also consider the update of the backbone\n",
            "parameters.\n",
            "50 20 40 60 80102\n",
            "101\n",
            "100101Summed loss\n",
            "0 20 40 60 80102\n",
            "101\n",
            "100101Depth loss scaledEW    + SGD mom.\n",
            "EW    + SGD mom. 10xL1 0.1xL2\n",
            "EW    + Adam\n",
            "EW    + Adam 10xL1 0.1xL2\n",
            "UW-O + SGD mom.\n",
            "UW-O + SGD mom. 10xL1 0.1xL2\n",
            "EW    + separated Adam\n",
            "EW    + separated Adam 10xL1 0.1xL2\n",
            "0 20 40 60 80103\n",
            "102\n",
            "101\n",
            "100Depth loss 2 scaled\n",
            "0 20 40 60 80102\n",
            "101\n",
            "100Training loss\n",
            "0 20 40 60 80102\n",
            "101\n",
            "100Depth loss 1  \n",
            " after weighting method\n",
            "0 20 40 60 80103\n",
            "102\n",
            "101\n",
            "100Depth loss 2  \n",
            " after weighting method\n",
            "0 20 40 60 80103\n",
            "102\n",
            "101\n",
            "Backbone1 \n",
            " gradient magnitude\n",
            "0 20 40 60 80103\n",
            "102\n",
            "101\n",
            "Depth head 1 \n",
            " gradient magnitude\n",
            "0 20 40 60 80105\n",
            "104\n",
            "103\n",
            "102\n",
            "Depth head 2 \n",
            " gradient magnitude\n",
            "0 20 40 60 80105\n",
            "104\n",
            "103\n",
            "Backbone \n",
            " gradient magnitude\n",
            "0 20 40 60 80106\n",
            "105\n",
            "104\n",
            "103\n",
            "Depth head 1 \n",
            " gradient magnitude\n",
            "0 20 40 60 80106\n",
            "105\n",
            "104\n",
            "103\n",
            "Depth head 2 \n",
            " update magnitudeFigure A2. Invariances within the neural network for a learnable backbone. Comparing the effect of loss-scalings in a toy experiment\n",
            "with two tasks. For each optimizer and loss weighting combination, we run two settings with a) loss L1 and loss L2 are equally weighted\n",
            "or b) L1 is scaled by 10x and L2 by 0.1. For each setting, we measure the SGD + momentum and Adam optimizer with no post weighting\n",
            "(EW) and SGD + momentum with optimimal uncertainty weighting. Additionally, we implement independent Adam optimizer per task.\n",
            "We show the scaled losses, gradient magnitudes, and gradient update magnitudes in the backbone(first row) and the the two task heads\n",
            "(2nd and 3rd row). Neither Adam, nor SGD show invariances if the backbone is trained as well. UW-O is still invriant (green lines are\n",
            "overlapping). We revoke Adam’s inveriance by implementing separate optimizers per task (lowerst black lines are overlapping).\n",
            "Results for free backbone Figure A2 shows the scaled losses, the gradient magnitudes as well as the gradient update\n",
            "magnitudes in the backbone and the depth heads along the 100 epochs of training with a free backbone. Again, the loss-\n",
            "scalings affect the gradient magnitudes using SGD. This applies to both backbone and heads.\n",
            "When looking at the Adam experiments, we can observe that it is partly loss-scale invariant by looking at the first iteration\n",
            "in the heads. However, due to different updates in the backbone, the networks behave different in both settings (scaled and\n",
            "unscaled loses). Furthermore, when implementing task-specific optimizers, we can observe that not only the gradient update\n",
            "magnitudes in the task heads, but also in the backbone match between the scaled ( dotted line ) and the unscaled ( solid line )\n",
            "variant. Thus, all network parameters are invariant to loss-scalings when using separate Adam optimizers. This confirms our\n",
            "theoretical results.\n",
            "Along the lines of our theoretical findings, we can observe that UW-O offers scaling-invariance across the whole network\n",
            "as the gradients as well as the gradient updates match among the two variants in the backbone and in both heads. This\n",
            "empirical observation matches our theoretical derivation in equation 13.\n",
            "6A3. Implementation Details\n",
            "In this section, we explain the applied experiment settings used for the reported experiments in more detail. In particular,\n",
            "we describe the handling of the different datasets in appendix A3.1 and provide further information on the applied training\n",
            "procedures in appendix A3.2. Our chosen experimental setups are designed to follow previous work. In particular, we took\n",
            "inspiration from [28] and [45]. However, we found that the experimental setup would vary widely across different works in\n",
            "the field of multi-task learning as can be seen in Table A1. We decided for a uniform setup for each dataset independent of\n",
            "the choice of network and MTO.\n",
            "A3.1. Datasets\n",
            "CityScapes [8] We make use of the official split of the dataset which consists of 2975 training and 500 validation scenes.\n",
            "Similar to [45], we denote 595 random samples from the training split as validation data and report test results on the\n",
            "original validation split. We further follow the pre-processing scheme from [31] of re-scaling images to 128x256 pixels and\n",
            "use inverse depth labels. During training, we apply random scaling and cropping for data augmentation2. Following previous\n",
            "work [28] for number of epochs, and learning rate schedule, we train for 300 epochs and decrease the learning rate by a factor\n",
            "of 0.5 every 100 epochs. The batch size is set the batch size to 64, similar to [45]. We only consider a fixed weight decay of\n",
            "10−5for all datasets and experiments as we found varying this parameter had only little influence in initial experiments.\n",
            "NYUv2 [36] From the 795 official training images we use 159 for our validation split as in [27] and report test performance\n",
            "on the official 654 test images. Similar to [31], we re-size the images to 288x384 pixels. Training is run for 200 epochs with\n",
            "a batch size of 8. We apply the same data augmentation and learning rate scheduler as for CityScapes.\n",
            "CelebA [32] We re-size images to 64x64 pixels as done in [26] and consider the original split of 162,770/19,867/19,962\n",
            "for training, validation, and testing. We set the batch size to 512, train for 100 epochs, and halve the learning rates every 30\n",
            "epochs.\n",
            "Corrupted variants For Cityscapes and NYUv2 we also apply common corruptions form [15]. Here, we use the original\n",
            "code from code https://github.com/hendrycks/robustness .\n",
            "Data MTO Network Optimizer learning rate weight decay batch size #train. iterations\n",
            "CityScapes [8] UW [20] DeepLabV3 [4]\n",
            "with ResNet101 [48]SGD\n",
            "+ Nesterov updates, Mom.init.: 2.5·10−3;\n",
            "polynomial lr decay1·10−48 100k iter.\n",
            "RLW [26] DeepLabV3 [4]\n",
            "with ResNet50 [48]Adam 1·10−41·10−564\n",
            "IMTL [29] ResNet50 [48]\n",
            "+ PSPNet [50] headsSGD+Mom. init.: 0.02;\n",
            "polynomial lr decay1·10−432 200 epochs\n",
            "PCGrad [49] MTAN [31] Adam init.: 1·10−4;\n",
            "halving lr after 40k iter.- 8 80k iter\n",
            "CAGrad [28] MTAN [31] Adam init.: 1·10−4;\n",
            "halving lr every 100 epochs- 8 200 epochs\n",
            "NYUv2 [36] RLW [26] DeepLabV3 [4]\n",
            "with ResNet50 [48]Adam 1·10−41·10−58\n",
            "IMTL [29] ResNet50 [48]\n",
            "+ PSPNet [50] headsSGD+Mom. init.: 0.03 - 48 200 epochs\n",
            "PCGrad [49] MTAN [31] Adam init.: 1·10−4;\n",
            "halving lr after 40k iter.- 2 80k iter\n",
            "CAGrad [28] MTAN [31] Adam init.: 1·10−4;\n",
            "halving lr after 100 epochs- 2 200 epochs\n",
            "CelebA [32] RLW [26] ResNet17 [14]\n",
            "+ lin. classifierAdam 1·10−3- 512\n",
            "IMTL [29] ResNet17 [14]\n",
            "+ lin. classifierAdam 0.003 - 256 100 epochs\n",
            "PCGrad [49] ResNet17 [14]\n",
            "+ lin. classifierAdam init. from {10−4, ..., 5·10−2};\n",
            "halving lr every 30 epochs- 256 100 epochs\n",
            "Table A1. Original experiment setup as reported in respective papers. We note a high variation regarding the choice of network,\n",
            "optimizer, and other hyper-parameters among the different works.\n",
            "2https://github.com/Cranial-XIX/CAGrad\n",
            "7A3.2. Training\n",
            "Effectiveness of Adam in MTL. All presented results are based on performing early stopping wrt. ∆m- metric on the\n",
            "validation set. For this, we further trained single-task learning (STL) models for each experiment combination (dataset and\n",
            "network) using the respective network architecture except for the missing head(s). In particular, we trained the models using\n",
            "any learning rate from {0.01,0.005, ...,0.00005}. The training was stopped early based on the validaton loss.\n",
            "Our implementation for all experiments is based on the LibMTL library [27].\n",
            "Gradient Similarity. Our gradient similarity experiments were conducted on the best performing hyper-parameter config-\n",
            "uration for EW from the previous extensive evaluation. Over the full training, gradient similarity measures are computed\n",
            "every five iteration steps and summarized per epoch. To make the computation effort more feasible in case of settings with\n",
            "large batch size or high number of tasks, we randomly select eight samples or tasks respectively and consider corresponding\n",
            "gradients in these cases.\n",
            "A4. Additional results on comparison between Adam and SGD\n",
            "We present additional evaluation results for our comparison between optimizers for multi-task learning. Figure A4 shows\n",
            "additional parallel coordinate plots for more choices of dataset/network combinations. In Table A2, we count for each\n",
            "used MTO the number of experiment runs that are located on the Pareto front of each setup. We compare the ∆m-metric\n",
            "performance between the usage of Adam and SGD+Momentum in Figure A3. Best performing quantitative results for all\n",
            "MTOs can be found in Tables A3 to A6.\n",
            "We further show extended results on the toy task by Liu et al. [28] for more learning rates in Figure A5.\n",
            "/uni00000044/uni00000047/uni00000044/uni00000050 /uni00000056/uni0000004a/uni00000047\n",
            "/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000055/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000027/uni00000048/uni0000004f/uni00000057/uni00000044/uni00000030/uni00000026/uni0000004c/uni00000057/uni0000005c/uni00000056/uni00000046/uni00000044/uni00000053/uni00000048/uni00000056/uni00000010/uni00000036/uni00000048/uni0000004a/uni00000031/uni00000048/uni00000057\n",
            "/uni00000030/uni00000037/uni00000032\n",
            "/uni00000028/uni0000003a\n",
            "/uni00000038/uni0000003a\n",
            "/uni00000035/uni0000002f/uni0000003a\n",
            "/uni0000002c/uni00000030/uni00000037/uni0000002f\n",
            "/uni00000033/uni00000026/uni0000002a/uni00000055/uni00000044/uni00000047\n",
            "/uni00000026/uni00000024/uni0000002a/uni00000055/uni00000044/uni00000047\n",
            "/uni00000044/uni00000047/uni00000044/uni00000050 /uni00000056/uni0000004a/uni00000047\n",
            "/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000055/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000027/uni00000048/uni0000004f/uni00000057/uni00000044/uni00000030/uni00000026/uni0000004c/uni00000057/uni0000005c/uni00000056/uni00000046/uni00000044/uni00000053/uni00000048/uni00000056/uni00000010/uni00000027/uni00000048/uni00000048/uni00000053/uni0000002f/uni00000044/uni00000045\n",
            "/uni00000044/uni00000047/uni00000044/uni00000050 /uni00000056/uni0000004a/uni00000047\n",
            "/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000055/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000027/uni00000048/uni0000004f/uni00000057/uni00000044/uni00000030/uni00000031/uni0000003c/uni00000038/uni00000059/uni00000015/uni00000010/uni00000036/uni00000048/uni0000004a/uni00000031/uni00000048/uni00000057\n",
            "/uni00000044/uni00000047/uni00000044/uni00000050 /uni00000056/uni0000004a/uni00000047\n",
            "/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000055/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000027/uni00000048/uni0000004f/uni00000057/uni00000044/uni00000030/uni00000031/uni0000003c/uni00000038/uni00000059/uni00000015/uni00000010/uni00000027/uni00000048/uni00000048/uni00000053/uni0000002f/uni00000044/uni00000045\n",
            "Figure A3. Line Plot of mean of ∆m-metric for experiments run on CityScapes and NYUv2 with SegNet and DeepLabV3. We\n",
            "compare the performance of the best hyperparameter setting (lower is better) for every MTO using either Adam (left) or SGD+Momentum\n",
            "(right). Every MTO is associated with a different line color/style. A lower value indicates better performance. On Cityscapes there is a\n",
            "large difference for the ∆mscore for Adam compared to SGD+Momentum, especially for UW, IMTL, and CAGrad. Therefore, using these\n",
            "methods, the result depends more on the optimizer than on the MTO method. On the NYUv2 dataset this observation weakens. Adam still\n",
            "achieves the lowest ∆mscores across different MTO methods (except for SegNet with UW and IMTL), though, besides chosing Adam, it\n",
            "is also important to select the appropriate MTO method.\n",
            "80.550.60.650.7SemSeg/mIoU\n",
            "0.72759\n",
            "0.500170.840.860.880.90.92SemSeg/pixA cc\n",
            "0.927636\n",
            "0.834557−0.08−0.06−0.04−0.02Depth/AbsEr r [neg]\n",
            "−0.011792\n",
            "−0.096606−140−120−100−80−60Depth/R elErr [neg]\n",
            "−42.51\n",
            "−145.18(a) CityScapes, DeepLabV3\n",
            "0.30.350.4SemSeg/mIoU\n",
            "0.4111\n",
            "0.250950.560.580.60.620.640.66SemSeg/pixA cc\n",
            "0.67135\n",
            "0.54057−0.66−0.64−0.62−0.6−0.58−0.56−0.54−0.52Depth/AbsEr r [neg]\n",
            "−0.51552\n",
            "−0.66273−0.28−0.26−0.24−0.22Depth/R elErr [neg]\n",
            "−0.206274\n",
            "−0.280486−38−36−34−32−30−28−26Normal/mean [neg]\n",
            "−25.676\n",
            "−39.602−35−30−25−20Normal/median [neg]\n",
            "−19.808\n",
            "−37.6690.150.20.25Normal/<11.25\n",
            "0.28661\n",
            "0.1260.30.350.40.450.50.55Normal/<22.5\n",
            "0.55352\n",
            "0.283060.40.450.50.550.60.65Normal/<30.0\n",
            "0.67672\n",
            "0.38885\n",
            "(b) NYUv2, SegNet\n",
            "0.30.350.40.450.50.55SemSeg/mIoU\n",
            "0.571\n",
            "0.25920.550.60.650.70.75SemSeg/pixA cc\n",
            "0.7745\n",
            "0.53717−0.9−0.8−0.7−0.6−0.5−0.4Depth/AbsEr r [neg]\n",
            "−0.34986\n",
            "−0.94301−0.35−0.3−0.25−0.2−0.15Depth/R elErr [neg]\n",
            "−0.13901\n",
            "−0.35673−35−30−25Normal/mean [neg]\n",
            "−21.145\n",
            "−37.587−30−25−20−15Normal/median [neg]\n",
            "−14.467\n",
            "−33.5830.10.150.20.250.30.350.4Normal/<11.25\n",
            "0.40377\n",
            "0.094530.40.50.6Normal/<22.5\n",
            "0.66955\n",
            "0.303210.450.50.550.60.650.70.75Normal/<30.0\n",
            "0.76903\n",
            "0.44146\n",
            "(c) NYUv2, DeepLabV3\n",
            "Figure A4. Parallel coordinate plot for further results on additional combinations of CityScapes and NYUv2 with either SegNet or\n",
            "DeepLabV3. Every line in the respective plot represents one run from section 4.1 with a specific choice of hyperparameters. We distinguish\n",
            "between experiments using SGD+mom and Adam optimizer. Experiments that reached Pareto front performance are drawn with higher\n",
            "saturation.\n",
            "We note a clear dominance of Adam on the CityScapes dataset when using DeepLabV3, similar to the results in SegNet which we presented\n",
            "in the main paper. For NYUv2, we observe a similar trend albeit here we also have some experiments using SGD+Mom. on the overall\n",
            "Pareto front.\n",
            "Data Network Optimizer EW UW RLW IMTL PCGrad CAGrad Total\n",
            "CityScapes SegNet Adam 1 7 - 3 - 1 12\n",
            "CityScapes SegNet SGD+Mom. - - - - - - -\n",
            "CityScapes DeepLabV3 Adam 3 3 2 3 2 2 15\n",
            "CityScapes DeepLabV3 SGD+Mom. - - - - - - -\n",
            "NYUv2 SegNet Adam 1 3 - 3 1 1 9\n",
            "NYUv2 SegNet SGD+Mom. - - - 2 - - 2\n",
            "NYUv2 DeepLabV3 Adam 2 2 - 6 1 5 16\n",
            "NYUv2 DeepLabV3 SGD+Mom. - 1 - 2 - 4 7\n",
            "Table A2. Count of Pareto optimal experiments for each MTO method. We found no single MTO method to be superior over all\n",
            "combinations of dataset and networks. When using Adam, all methods but RLW and PCGrad in one case (CityScapes+SegNet) would\n",
            "result in at least one model yielding Pareto optimal performance. Total numbers can be compared to Table 2\n",
            "9Sem.Seg. Depth\n",
            "MTO Optimizer lr mIoU ↑ pixAcc ↑ AbsErr ↓ RelErr↓ DeltaM ↓\n",
            "STL adam 0.7122 0.9221 0.0134 29.88\n",
            "EW adam 0.005 0.6898 0.9165 0.0196 109.84 79.43 ±3.68\n",
            "EW sgd 0.1 0.6967 0.9179 0.0216 113.82 86.24 ±1.97\n",
            "UW adam 0.001 0.7052 0.9202 0.0136 35.69 5.44±2.38\n",
            "UW sgd 0.01 0.6750 0.9110 0.0219 114.67 88.39 ±1.35\n",
            "RLW adam 0.001 0.7013 0.9196 0.0197 103.61 73.91 ±7.63\n",
            "RLW sgd 0.1 0.6918 0.9156 0.0227 113.59 88.16 ±0.67\n",
            "IMTL adam 0.005 0.6963 0.9170 0.0148 45.63 16.55 ±1.52\n",
            "IMTL sgd 0.01 0.6716 0.9107 0.0230 114.38 90.21 ±0.74\n",
            "PCGrad adam 0.01 0.6770 0.9135 0.0226 103.88 80.56 ±3.70\n",
            "PCGrad sgd 0.1 0.6972 0.9176 0.0235 107.06 84.09 ±0.93\n",
            "CAGrad adam 0.001 0.7088 0.9208 0.0162 66.39 35.81 ±14.91\n",
            "CAGrad sgd 0.1 0.6896 0.9156 0.0205 115.52 85.88 ±0.31\n",
            "Table A3. Results for different MTO methods and optimizers on CityScapes [8] using a SegNet network [1]. We report the mean test\n",
            "performance over three seeds for the best learning rate w.r.t. the validation data based on the delta-M metric. The best score per metric is\n",
            "highlighted for each MTO method as well as over all methods and optimizers . While we observe that different MTO perform best over\n",
            "the distinct metrics, Models trained with Adam outperform those trained with SGD + Momentum in most cases. On the overall ∆m-metric,\n",
            "using Adam show superior performance for all MTO, in some cases even with a high margin.\n",
            "Sem.Seg. Depth\n",
            "MTO Optimizer lr mIoU ↑ pixAcc ↑ AbsErr ↓ RelErr↓ DeltaM ↓\n",
            "STL adam 0.7203 0.9253 0.0132 47.37\n",
            "EW adam 0.001 0.7247 0.9268 0.0128 47.73 -0.80 ±0.69\n",
            "EW sgd 0.05 0.7100 0.9217 0.0174 120.34 46.88 ±2.02\n",
            "UW adam 0.001 0.7224 0.9259 0.0122 44.37 -3.65±0.61\n",
            "UW sgd 0.005 0.7003 0.9187 0.0171 116.09 44.47 ±1.91\n",
            "RLW adam 0.001 0.7230 0.9263 0.0133 47.76 0.26 ±1.08\n",
            "RLW sgd 0.05 0.7070 0.9205 0.0176 123.13 48.84 ±1.79\n",
            "IMTL adam 0.001 0.7226 0.9259 0.0121 45.25 -3.38 ±0.92\n",
            "IMTL sgd 0.005 0.7027 0.9192 0.0185 122.37 50.33 ±3.30\n",
            "PCGrad adam 0.001 0.7247 0.9272 0.0130 47.24 -0.58 ±0.56\n",
            "PCGrad sgd 0.05 0.7083 0.9212 0.0173 122.52 47.90 ±3.68\n",
            "CAGrad adam 0.001 0.7245 0.9264 0.0124 45.56 -2.59 ±0.68\n",
            "CAGrad sgd 0.1 0.7096 0.9220 0.0172 120.24 46.58 ±2.21\n",
            "Table A4. Results for different MTO methods and optimizers on CityScapes [8] using a DeepLabV3+ network [4]. We report the\n",
            "mean test performance over three seeds for the best learning rate w.r.t. the validation data. The best score per metric is highlighted for each\n",
            "MTO method as well as over all methods and optimizers . While we observe that different MTO perform best over the distinct metrics,\n",
            "Models trained with Adam consistently outperform those trained with SGD + Momentum. We further note a clear superiority on the\n",
            "∆m-metric which indicates for all but RLW better improved performance compared to the STL baseline.\n",
            "10Sem.Seg. Depth Normal\n",
            "MTO Optimizer lr mIoU ↑pixAcc ↑ AbsErr ↓RelErr↓ Mean↓Median ↓<11.25↑<22.5↑<30.0↑ DeltaM ↓\n",
            "STL adam 0.3922 0.6462 0.6068 0.2579 24.74 18.49 0.3084 0.5816 0.6996\n",
            "EW adam 0.0001 0.3979 0.6496 0.5299 0.2123 29.53 25.02 0.2162 0.4542 0.5838 10.08 ±2.84\n",
            "EW sgd 0.01 0.3843 0.6438 0.5511 0.2271 30.12 25.72 0.2121 0.4427 0.5712 12.84 ±0.65\n",
            "UW adam 0.0001 0.4011 0.6517 0.5215 0.2132 27.93 22.80 0.2432 0.4943 0.6231 5.42 ±1.04\n",
            "UW sgd 0.05 0.3832 0.6421 0.5508 0.2178 27.07 21.66 0.2586 0.5162 0.6435 4.46 ±2.13\n",
            "RLW adam 0.0001 0.3898 0.6380 0.5367 0.2179 30.83 26.78 0.1961 0.4243 0.5538 14.28 ±2.38\n",
            "RLW sgd 0.05 0.3690 0.6325 0.5722 0.2328 31.21 27.17 0.1976 0.4197 0.5459 16.82 ±1.16\n",
            "IMTL adam 0.0001 0.3796 0.6436 0.5242 0.2156 26.35 20.77 0.2703 0.5342 0.6603 2.06 ±1.31\n",
            "IMTL sgd 0.05 0.3957 0.6557 0.5320 0.2154 26.16 20.38 0.2773 0.5419 0.6664 0.72 ±0.68\n",
            "PCGrad adam 0.0001 0.4057 0.6540 0.5286 0.2152 28.60 23.75 0.2307 0.4768 0.6062 7.39 ±0.86\n",
            "PCGrad sgd 0.01 0.3893 0.6441 0.5473 0.2227 29.76 25.22 0.2143 0.4506 0.5800 11.61 ±0.36\n",
            "CAGrad adam 0.0001 0.4046 0.6606 0.5273 0.2114 25.85 20.02 0.2816 0.5495 0.6735 -0.65±0.96\n",
            "CAGrad sgd 0.05 0.4003 0.6561 0.5474 0.2226 26.39 20.78 0.2682 0.5344 0.6615 2.06 ±0.65\n",
            "Table A5. Results for different MTO methods and optimizers on NYUv2 [36] using a SegNet network [1]. We report the mean test\n",
            "performance over three seeds for the best learning rate w.r.t. the validation data. The best score per metric is highlighted for each MTO\n",
            "method as well as over all methods and optimizers . Over all metrics, best performance is always achieved by a model trained with Adam.\n",
            "We note that the ∆mis more effected by the normal task due to the higher number of corresponding metrics as can be observed in the case\n",
            "of UW.\n",
            "Sem.Seg. Depth Normal\n",
            "MTO Optimizer lr mIoU ↑pixAcc ↑ AbsErr ↓RelErr↓ Mean↓Median ↓<11.25↑<22.5↑<30.0↑ DeltaM ↓\n",
            "STL adam 0.5517 0.7668 0.3650 0.1524 21.16 14.52 0.4023 0.6679 0.7679\n",
            "EW adam 0.0001 0.5521 0.7697 0.3515 0.1432 22.54 16.05 0.3659 0.6340 0.7422 2.70 ±0.12\n",
            "EW sgd 0.01 0.5558 0.7686 0.3646 0.1473 23.35 16.76 0.3520 0.6167 0.7273 5.21 ±0.18\n",
            "UW adam 0.0005 0.5319 0.7553 0.3579 0.1408 22.01 15.30 0.3825 0.6502 0.7533 1.59 ±0.14\n",
            "UW sgd 0.01 0.5570 0.7692 0.3643 0.1479 22.37 15.69 0.3742 0.6408 0.7464 2.57 ±0.56\n",
            "RLW adam 0.0001 0.5562 0.7675 0.3604 0.1495 22.37 15.87 0.3697 0.6375 0.7446 2.96 ±0.55\n",
            "RLW sgd 0.05 0.5412 0.7617 0.3678 0.1508 23.00 16.44 0.3571 0.6245 0.7338 5.15 ±0.54\n",
            "IMTL adam 0.0005 0.5324 0.7557 0.3526 0.1404 21.34 14.61 0.3990 0.6661 0.7660 -0.39±0.05\n",
            "IMTL sgd 0.05 0.5449 0.7617 0.3669 0.1499 21.63 14.98 0.3902 0.6576 0.7602 1.30 ±0.40\n",
            "PCGrad adam 0.0001 0.5583 0.7716 0.3569 0.1460 22.33 15.81 0.3711 0.6388 0.7460 2.34 ±0.48\n",
            "PCGrad sgd 0.01 0.5563 0.7685 0.3609 0.1471 23.36 16.79 0.3514 0.6167 0.7279 5.10 ±0.28\n",
            "CAGrad adam 0.0005 0.5310 0.7550 0.3568 0.1428 21.42 14.67 0.3988 0.6640 0.7639 0.10 ±0.55\n",
            "CAGrad sgd 0.05 0.5555 0.7684 0.3600 0.1437 21.77 14.99 0.3906 0.6573 0.7592 0.42 ±0.07\n",
            "Table A6. Results for different MTO methods and optimizers on NYUv2 [36] using a DeepLabV3+ network [4]. We report the mean\n",
            "test performance over three seeds for the best learning rate w.r.t. the validation data. The best score per metric is highlighted for each\n",
            "MTO method as well as over all methods and optimizers . We note a clear dominance of Adam on both the depth and normal tasks as well\n",
            "as on the ∆m-metric. Overall, best results were also achieved using Adam as optimizer for all metrics.\n",
            "11learning rate\n",
            "method 10.0 5.0 1.0 0.5 0.1 0.05 0.01 0.005 0.001*GDEW - 103 - - - - - - -\n",
            "PCGrad - - - - - - - - -\n",
            "CAGrad 644 - 213 621 8,069 5,732 20,418 34,405 -AdamEW 26 37 22 58 709 2,135 9,015 16,005 -\n",
            "PCGrad 25 4,960 56 15,741 34,175 41,438 - - -\n",
            "CAGrad 27 30 32 106 802 7,109 11,239 14,323 57,700\n",
            "*LR used for results in [28] with Adam\n",
            "Table A7. Number of iterations after which all seeds in toy task experiment from CAGrad [28] have reached the global minimum\n",
            "for different learning rates and optimizer. We show results for additional learning rates compared to the main paper. The maximum iteration\n",
            "number over all three seeds for each MTO method / learning rate / optimizer combination is reported. If not all seeds converged to the\n",
            "global minimum within 100k iteration steps, we denote it as ’-’. In several setups, EW+Adam converges fastest to the global minimum.\n",
            "Especially for small learning rates, CAGrad performs advantageous compared to EW. As reported in previous work, we found that PCGrad\n",
            "often would converge only to some point on the Pareto Front. The best andsecond best run for each learning rate over all MTO methods\n",
            "are indicated via font type. Best and second best learning rate + optimzer combination for each MTO are marked via cell background\n",
            "color.\n",
            "GD\n",
            "1.0 0.5 0.1 0.05 0.01 0.005 0.001EW\n",
            " PCGrad\n",
            " CAGrad\n",
            "Adam\n",
            "1.0 0.5 0.1 0.05 0.01 0.005 0.001EW\n",
            " PCGrad\n",
            " CAGrad\n",
            "Figure A5. Extension of Figure 1 with additional learning rates. We show the optimization trajectories for three different seeds (black\n",
            "dots). The Optimization trajectories are colored from red to yellow for 100k iteration steps. The global optimum is depicted as asterix\n",
            "(∗), the Pareto front is highlighted in gray. We note the importance of a good selection of learning rate even for simple toy examples.\n",
            "Moreover, using Adam yields overall faster convergence to the global optimum. We use the original implementation which can be found\n",
            "under https://github.com/Cranial-XIX/CAGrad .\n",
            "12A5. Additional gradient alignment results\n",
            "Besides the gradient similarity measures defined in the main paper, we provide some further insights when comparing\n",
            "pairs of gradient in either in the multi-task or single-task learning setup. In Figure A6, we differentiate between conflicting\n",
            "and supporting gradient pairs when evaluating the cosine similarity. Figure A7 shows the evaluation of the scalar product as\n",
            "an combined measure of similarity in gradient direction and magnitude. We report the number of gradient components which\n",
            "are either positive, negative or zero in Figure A8. Finally, in Figure A9, we plot the course of loss functions with respective\n",
            "to the different dataset splits.\n",
            "0 100 200−1−0.500.51\n",
            "0 100 20000.51\n",
            "0 100 20000.51\n",
            "0 100 200−1−0.500.51\n",
            "0 100 20000.51\n",
            "0 100 20000.51\n",
            "0 100 200−1−0.500.51\n",
            "0 100 20000.51\n",
            "0 100 20000.51Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epoch[overall] [pos] [neg]Grad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)\n",
            "(a) CityScapes,\n",
            "SegNet\n",
            "0 100 200−1−0.500.51\n",
            "0 100 20000.51\n",
            "0 100 20000.51\n",
            "0 100 200−1−0.500.51\n",
            "0 100 20000.51\n",
            "0 100 20000.51\n",
            "0 100 200−1−0.500.51\n",
            "0 100 20000.51\n",
            "0 100 20000.51Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epoch[overall] [pos] [neg]Grad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)(b) CityScapes,\n",
            "DeepLabV3\n",
            "0 50 100 150−1−0.500.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 150−1−0.500.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 150−1−0.500.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 15000.51Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epoch[overall] [pos] [neg]Grad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)(c) NYUv2,\n",
            "SegNet\n",
            "0 50 100 150−1−0.500.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 150−1−0.500.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 150−1−0.500.51\n",
            "0 50 100 15000.51\n",
            "0 50 100 15000.51Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epoch[overall] [pos] [neg]Grad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)(d) NYUv2,\n",
            "DeepLabV3\n",
            "0 20 40 60 80−1−0.500.51\n",
            "0 20 40 60 8000.51\n",
            "0 20 40 60 8000.51\n",
            "0 20 40 60 80−1−0.500.51\n",
            "0 20 40 60 8000.51\n",
            "0 20 40 60 8000.51\n",
            "0 20 40 60 80−1−0.500.51\n",
            "0 20 40 60 8000.51\n",
            "0 20 40 60 8000.51Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epoch epoch epoch[overall] [pos] [neg]Grad. Cosine Similarity Grad. Magnitude Similarity ratio(#Conﬂicts)(e) CelebA,\n",
            "ResNet50\n",
            "Figure A6. Differentiation between conflicting and supportive gradients. We report mean (solid line), standard deviation (shaded area),\n",
            "upper ( 97.5%) and lower ( 2.5%) percentile (dotted line) of the gradient cosine similarity between either gradients of different samples or\n",
            "different tasks within an epoch. While showing overall results over all respective gradient pairs (Top) as can be also found in Figure 3,\n",
            "we also show the course of cosine similarity for either gradients that are conflicting ([neg], bottom) or those which have cosine similarity\n",
            "greater than zero ([pos], middle).\n",
            "0 100 200−1−0.500.51\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epochScalar P roduct\n",
            "(a) CityScapes,\n",
            "SegNet\n",
            "0 100 200−0.500.5\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epochScalar P roduct(b) CityScapes,\n",
            "DeepLabV3\n",
            "0 50 100 150−50050\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epochScalar P roduct\n",
            "Loading [MathJax]/extensions/MathMenu.js(c) NYUv2,\n",
            "SegNet\n",
            "0 50 100 150−10−50510\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epochScalar P roduct(d) NYUv2,\n",
            "DeepLabV3\n",
            "0 20 40 60 80−500005000\n",
            "Diﬀ.Sample\n",
            "Diﬀ.T ask\n",
            "epochScalar P roduct(e) CelebA,\n",
            "ResNet50\n",
            "Figure A7. Scalar product between pairs of gradients. We report mean (solid line), standard deviation (shaded area), upper ( 97.5%) and\n",
            "lower ( 2.5%) percentile (dotted line) of the gradient cosine similarity between either gradients of different samples or different tasks within\n",
            "an epoch. We observe an overall decrease of the variance of the scalar product for both CityScapes setups and the NYUv2+DeepLabV3\n",
            "experiment over the training which we explain with evenly smaller overall gradients. Surprisingly, this does not apply for NYUv2 with\n",
            "SegNet or CelebA. Similar to previous results, we do not see any indication for gradients of different samples being better aligned than\n",
            "gradients of different tasks.\n",
            "130 100 20000.51# zer o gradient components\n",
            "# negative gradient components\n",
            "# positive gradient components(a) CityScapes,\n",
            "SegNet\n",
            "0 100 20000.51# zer o gradient components\n",
            "# negative gradient components\n",
            "# positive gradient components(b) CityScapes,\n",
            "DeepLabV3\n",
            "0 50 100 15000.51# zer o gradient components\n",
            "# negative gradient components\n",
            "# positive gradient components(c) NYUv2,\n",
            "SegNet\n",
            "0 50 100 15000.51# zer o gradient components\n",
            "# negative gradient components\n",
            "# positive gradient components(d) NYUv2,\n",
            "DeepLabV3\n",
            "0 20 40 60 8000.51# zer o gradient components\n",
            "# negative gradient components\n",
            "# positive gradient components(e) CelebA,\n",
            "ResNet50\n",
            "Figure A8. Count of positive/ negative/ zero components in gradients. For each gradient corresponding to one task and one sample, we\n",
            "count the number of positive and negative as well as zero-valued scalar entries. As expected, the amount of positive and negative values is\n",
            "approximately equal during the entire time of training along all experiments. For experiments on CityScapes and CelebA, we found that an\n",
            "increasing number of network components would not receive gradient updates anymore starting at some point in training. This especially\n",
            "holds for CelebA, were we assume to happen because of the ReLU activation functions used in ResNet18.\n",
            "0 50 100 150 200 25000.20.40.60.81\n",
            "train. loss\n",
            "val. loss\n",
            "test. loss\n",
            "(a) CityScapes,\n",
            "SegNet\n",
            "0 50 100 150 200 25000.20.40.60.81\n",
            "train. loss\n",
            "val. loss\n",
            "test. loss(b) CityScapes,\n",
            "DeepLabV3\n",
            "0 50 100 15000.511.522.533.54\n",
            "train. loss\n",
            "val. loss\n",
            "test. loss\n",
            "Loading [MathJax]/extensions/MathMenu.js(c) NYUv2,\n",
            "SegNet\n",
            "0 50 100 15000.511.522.533.54\n",
            "train. loss\n",
            "val. loss\n",
            "test. loss(d) NYUv2,\n",
            "DeepLabV3\n",
            "0 20 40 60 8002468101214train. loss\n",
            "val. loss\n",
            "test. loss\n",
            "Loading [MathJax]/extensions/MathMenu.js(e) CelebA,\n",
            "ResNet50\n",
            "Figure A9. Loss over experiments for gradient similarity experiment. We visualize the loss with respect to the training, validation, and\n",
            "test set during the training process of the gradient similarity experiments.\n",
            "14A6. Additional results for out-of distribution generalization\n",
            "We present results on generalization performance to corrupted data on NYUv2 with DeepLabV3 as well as on CityScapes\n",
            "with either choice of network in Figure A10 and Tables A8 and A9.\n",
            "Additional to the main part of the paper, we run the corruption robustness experiments for NYUv2 with a DeepLabV3\n",
            "network (Table A9) and for Cityscapes (Table A8, Figure A10). Across all experiments, we could not find any con-\n",
            "sistent pattern. Segnet+Cityscapes seems to vote in favour of MTL for more robustness on the depth task, though, for\n",
            "DeepLabV3+Cityscapes, STL is more robust on the same task. For the Sem.Seg task, both networks seem to be slightly more\n",
            "robust with STL.\n",
            "Interestingly, the δMean andδMedian for the Normal task of NYUv2 is consistently better for MTL, though, this does not\n",
            "hold for the other task metrics.\n",
            "Sem.Seg. Depth Mean\n",
            "Network MTO δmIoU δpixAcc δAbsErr δRelErr\n",
            "SegNet EW 0.0268 0.0513 -1.6291 -3.1684 -1.1798\n",
            "UW 0.0532 0.0542 -0.1942 -1.2537 -0.3351\n",
            "RLW 0.0377 0.0483 -1.6592 -3.1739 -1.1868\n",
            "IMTL 0.0443 0.0301 -0.8538 -1.8310 -0.6526\n",
            "PCGrad 0.0385 0.0405 -1.6255 -3.1475 -1.1735\n",
            "CAGrad 0.0211 0.0221 -0.6951 -2.3448 -0.7492\n",
            "DepLabV3 EW 0.0084 0.0079 0.7569 0.5861 0.3398\n",
            "UW 0.0051 0.0119 0.6217 0.2126 0.2128\n",
            "RLW 0.0178 0.0251 -0.0504 0.1443 0.0342\n",
            "IMTL 0.0261 0.0240 1.0149 0.5637 0.4072\n",
            "PCGrad 0.0227 0.0190 0.2163 0.4825 0.1851\n",
            "CAGrad 0.0135 0.0142 0.2889 0.4765 0.1983\n",
            "Table A8. Out-Of Distribution generalization on corrupted CityScapes [8] dataset for different networks. We report difference\n",
            "between relative performance decrease for single-task and multi-task learning averaged over all modes of corruption and all levels of\n",
            "severity (cf. Equation (6)). A value lower than zero indicates a better generalization capability of the MTL model, a positive value displays\n",
            "that the STL shows a lower decrease when evaluated on the corrupted data. Results are averaged over runs for three seeds for both multi-\n",
            "task and single-task models. We note a stronger performance of the MTL when using SegNet which is mainly due to a strong performance\n",
            "on the depth task. However, this behavior is not reinforced when using DeepLabV3, where the performance of the STL model decreases\n",
            "less over nearly all metrics and MTO methods.\n",
            "Sem.Seg. Depth Normal Mean\n",
            "Network MTO δmIoU δpixAcc δAbsErr δRelErr δMean δMedian δ<11.25δ<22.5δ<30.0\n",
            "SegNet EW 0.0226 0.0298 0.1713 0.1869 -0.0750 -0.1390 0.0378 0.0167 0.0129 0.0293\n",
            "UW 0.0149 0.0200 0.1691 0.1652 -0.0461 -0.0895 0.0200 0.0168 0.0165 0.0319\n",
            "RLW 0.0236 0.0243 0.1250 0.1266 -0.0942 -0.1717 0.0342 0.0152 0.0110 0.0104\n",
            "IMTL 0.0144 0.0174 0.2434 0.2721 -0.0142 -0.0356 0.0342 0.0249 0.0217 0.0643\n",
            "PCGrad 0.0206 0.0156 0.1665 0.1634 -0.0594 -0.1131 0.0364 0.0208 0.0171 0.0298\n",
            "CAGrad 0.0053 0.0147 0.2006 0.2282 -0.0018 -0.0138 0.0402 0.0284 0.0240 0.0584\n",
            "DeepLabV3 EW -0.0027 -0.0131 0.1118 0.0880 -0.0328 -0.0628 -0.0056 0.0046 0.0049 0.0102\n",
            "UW 0.0050 -0.0147 0.0555 0.0336 -0.0160 -0.0328 0.0140 0.0163 0.0146 0.0084\n",
            "RLW 0.0067 -0.0040 0.0553 0.0006 -0.0125 -0.0309 0.0142 0.0170 0.0155 0.0069\n",
            "IMTL 0.0022 -0.0150 0.0511 0.0325 0.0073 -0.0086 0.0022 0.0068 0.0076 0.0096\n",
            "PCGrad -0.0116 -0.0302 0.0505 0.0390 -0.0339 -0.0563 0.0040 0.0108 0.0102 -0.0019\n",
            "CAGrad 0.0064 0.0017 0.1699 0.1124 0.0156 0.0070 0.0097 0.0121 0.0117 0.0385\n",
            "Table A9. Out-Of Distribution generalization on corrupted NYUv2 [36] dataset for different multi-task optimization methods. We\n",
            "report difference between relative performance decrease for single-task and multi-task learning averaged over all modes of corruption and\n",
            "all levels of severity (cf. Equation (6)). A value lower than zero indicates a better generalization capability of the MTL model, a positive\n",
            "value displays that the STL shows a lower decrease when evaluated on the corrupted data. Results are averaged over runs for three seeds\n",
            "for both multi-task and single-task models. Interestingly we found that even for different metrics corresponding to the same task, either the\n",
            "multi-task or single-task learning model would show lower decrease in performance on the corrupted data. There is no evidence that MTO\n",
            "methods would increase generalization capabilities of the trained models.\n",
            "150.4\n",
            "0.2\n",
            "0.00.20.4Sem.Seg.(mIoU)\n",
            "(pixAcc)\n",
            "Gaussian NoiseShot Noise\n",
            "Impulse NoiseDefocus BlurGlass BlurMotion BlurZoom BlurSnow FrostFog\n",
            "BrightnessContrastElasticPixelateJPEG10\n",
            "5\n",
            "0510Depth(AbsErr)\n",
            "(RelErr)\n",
            "(a) CityScapes, SegNet\n",
            "0.4\n",
            "0.2\n",
            "0.00.20.4Sem.Seg.(mIoU)\n",
            "(pixAcc)\n",
            "Gaussian NoiseShot Noise\n",
            "Impulse NoiseDefocus BlurGlass BlurMotion BlurZoom BlurSnow FrostFog\n",
            "BrightnessContrastElasticPixelateJPEG4\n",
            "2\n",
            "024Depth(AbsErr)\n",
            "(RelErr)\n",
            " (b) CityScapes, DeepLabV3\n",
            "0.4\n",
            "0.2\n",
            "0.00.20.4Sem.Seg.(mIoU)\n",
            "(pixAcc)\n",
            "1.0\n",
            "0.5\n",
            "0.00.51.0Depth(AbsErr)\n",
            "(RelErr)\n",
            "Gaussian NoiseShot Noise\n",
            "Impulse NoiseDefocus BlurGlass BlurMotion BlurZoom BlurSnow FrostFog\n",
            "BrightnessContrastElasticPixelateJPEG0.4\n",
            "0.2\n",
            "0.00.20.4Normal(<11.25)\n",
            "(<22.5)\n",
            "(<30.0)\n",
            "(Mean)\n",
            "(Median)\n",
            "(c) NYUv2, DeepLabV3\n",
            "Figure A10. Comparison of generalization performance to out-of-distribution data for MTL and STL For every task and respective\n",
            "metrics, we show the difference over relative performance decrease over all corruption modes averaged over five levels of severity and\n",
            "three runs. EW was used to train the MTL model on uncorrupted data. We color blocks in case either STL or MTL is able to handle the\n",
            "respective corruption better for all metrics of one task. Regarding the CityScapes dataset, the performance on the depth task would benefit\n",
            "when using the SegNet network but not in case of DeepLabV3. For NYUv2+DeepLabV3, we even observe different behaviours across a\n",
            "single task for the different corruption modes. Overall, we do not see a clear evidence that multi-task learning would result in features that\n",
            "would generalize better to corrupted data.\n",
            "16H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING 1\n",
            "Data exploitation: multi-task learning of\n",
            "object detection and semantic segmentation\n",
            "on partially annotated data\n",
            "Hoàng-Ân Lê\n",
            "hoang-an.le@irisa.fr\n",
            "Minh-Tan Pham\n",
            "minh-tan.pham@irisa.frIRISA, Université Bretagne Sud,\n",
            "UMR 6074, 56000 Vannes, France\n",
            "Abstract\n",
            "Multi-task partially annotated data where each data point is annotated for only a sin-\n",
            "gle task are potentially helpful for data scarcity if a network can leverage the inter-task\n",
            "relationship. In this paper, we study the joint learning of object detection and semantic\n",
            "segmentation, the two most popular vision problems, from multi-task data with partial\n",
            "annotations. Extensive experiments are performed to evaluate each task performance and\n",
            "explore their complementarity when a multi-task network cannot optimize both tasks si-\n",
            "multaneously. We propose employing knowledge distillation to leverage joint-task op-\n",
            "timization. The experimental results show favorable results for multi-task learning and\n",
            "knowledge distillation over single-task learning and even full supervision scenario. All\n",
            "code and data splits are available at https://github.com/lhoangan/multas\n",
            "1 Introduction\n",
            "Although both object detection and semantic segmentation aim to understand the image con-\n",
            "tent, the two problems differ in spatial structure and information granularity. Object detec-\n",
            "tion performs at the object level outputting unordered list of bounding boxes with corner\n",
            "coordinates and object types while semantic segmentation provides per-pixel predictions;\n",
            "object detection distinguishes object instances while semantic segmentation recognizes each\n",
            "category as a whole and also amorphous regions such as ground, sky, sea, etc.\n",
            "Attempts have been made to jointly learn both tasks in a single model. Methods such as\n",
            "Mask R-CNN [13] overcomes the spatial structure difference by generating an object mask\n",
            "for each predicted bounding box, effectively predicting instance segmentation. On the other\n",
            "hand, the introduction of panoptic segmentation [17] can be seen as resolving the information\n",
            "granularity difference in which instance-level objects and amorphous categories are tackled\n",
            "together as a dense prediction problem. Combining both tasks under the common form of\n",
            "instance segmentation, however, leaves the original tasks unfinished: Mask R-CNN does\n",
            "not provide segmentation masks for stuff categories nor does panoptic segmentation directly\n",
            "provide bounding box coordinates.\n",
            "Multi-task learning is a research area that allows training different problems under the\n",
            "same model. The general assumption is that several tasks are inherently related to one\n",
            "© 2023. The copyright of this document resides with its authors.\n",
            "It may be distributed unchanged freely in print or electronic forms.arXiv:2311.04040v1  [cs.CV]  7 Nov 20232 H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING\n",
            "Figure 1: Class activation maps at the same feature layers of an object detection network (left) and\n",
            "semantic segmentation (right) showing incompatible feature attentions: detection only activates a (few)\n",
            "feature at the scale producing fit boxes while segmentation activates all those belong to the objects.\n",
            "another and by optimizing them together for each input image, the network could extract\n",
            "common features and pick up the salient interrelationships. Although training multiple\n",
            "tasks could potentially increase tasks coherency and, for particular setups, also allow self-\n",
            "supervision [4, 35], it is challenging as each task would require specific architecture and\n",
            "optimization criteria, and maintaining a training dataset with consistent annotations for all\n",
            "tasks proves to be expensive.\n",
            "In this paper, the joint learning of object detection and semantic segmentation is consid-\n",
            "ered, which despite their popularity as single tasks, seems to receive limited attentions in the\n",
            "literature. Due to different targets, although the two tasks are closely related, the features\n",
            "learned for each task are not readily compatible. Figure 1 shows the activation map using\n",
            "Grad-CAM [30] at the same layers of two networks with the same encoder architectures,\n",
            "trained for object detection and semantic segmentation. Semantic segmentation activates\n",
            "(nearly) all the features covering the object of interest while object detection activates only\n",
            "those at the feature scale that produces fitted bounding boxes, no matter if they belong to the\n",
            "objects. Table 1 and Sec. 4.2 show that an encoder trained for one task cannot immediately\n",
            "be used for the other tasks when only the task-specific head is finetuned.\n",
            "Diverging from the usual multi-task learning assumption that annotations are available\n",
            "in all tasks for each training example, we limit the scope of the paper to multi-task partially\n",
            "annotated data, where each image is annotated for a single task and there are no images con-\n",
            "taining both task annotations. This is interesting because (1) the network cannot optimize\n",
            "both tasks for the same input and is hindered in attempt to learn joint features and salient\n",
            "interrelationships; (2) therefore, this setting would illustrate the complementarity of the two\n",
            "tasks of interest; and (3) it is data efficient and would be an alternative method to amelio-\n",
            "rate the data scarcity problem as more data with single-task annotations could be used for\n",
            "training, allowing for expanding training ability.\n",
            "To that end, we employ a simple multi-task learning framework to study the combina-\n",
            "tion of object detection and semantic segmentation. We experiment with various setups and\n",
            "observe each task’s performance under different input conditions. By varying the datasets,\n",
            "the interaction between the two tasks can be observed which can be useful for further study.\n",
            "The simple feature-imitation knowledge distillation model is employed for cross-task opti-\n",
            "mization which is seemingly not possible for partially annotated data.\n",
            "The paper contributions are as follows. We explore the combination of object detection\n",
            "and semantic segmentation in a multi-task learning framework for partially annotated data.\n",
            "Extensive experiments are performed to evaluate both quantitatively and qualitatively the\n",
            "benefit of one task to the other. A knowledge distillation method is employed and evaluated\n",
            "for joint-task optimization.H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING 3\n",
            "2 Related work\n",
            "2.1 Multi-task learning\n",
            "Multi-task learning (MTL) trains a single model that can infer different task targets from a\n",
            "given input. One of the main assumptions is the compatibility of the features learned for each\n",
            "task and by optimizing them for each input, the network could learn the common knowledge\n",
            "that benefits and complements one another [20, 26].\n",
            "Several methods have been proposed to accommodate various tasks and network archi-\n",
            "tectures to improve shared information among the tasks [3] using attention mechanisms [23]\n",
            "and gating strategies [2], and to study cross-task relationships [26, 33]. The fully supervised\n",
            "learning strategy requires annotations available for all tasks per training example for opti-\n",
            "mization, which is costly and hinders scalability. Therefore, attempts have been made for\n",
            "semi-supervised learning [6, 15] that allows learning from unlabelled data and relaxes the\n",
            "number of annotations, yet all-task annotations per training sample are still required.\n",
            "Closely related to the problem in our paper is the work of Li et al. [19], in which each\n",
            "training data point is only required to contain an annotation for a single task, or the multi-task\n",
            "partial annotation scenario. The cross-task consistency constraint is proposed and the task-\n",
            "specific annotations are projected to the joint pairwise task-space from which supervised\n",
            "signals are provided to the training process. The method requires the dense spatial structures\n",
            "of the annotations making it inapplicable for object detection in this paper.\n",
            "2.2 Knowledge distillation\n",
            "Hinton et al. [14] has shown that a network could benefit from a larger or an ensemble of\n",
            "models, called teachers, by mimicking the predicted logits or imitate the deep features [11]\n",
            "learned by them. Depending on the purpose, knowledge distillation (KD) could be seen as\n",
            "model compression which aims to reduce model complexity with less performance sacrific-\n",
            "ing, or a self-training technique [32, 37] where a network is trained using the combination of\n",
            "available annotations and pseudo-labels provided by the teacher’s predictions. Self-training\n",
            "with uncertain teachers for object detection has recently been studied [27], where the teachers\n",
            "are trained with a small number of supervised data disjoint with the students’ training set, or\n",
            "for a different task (segmentation). Different from their paper which also involves detection-\n",
            "segmentation multi-task training but focuses only on the detection benefit, our work interests\n",
            "in both tasks’ performances and shows the multi-task advantage with TIDE [1] error analy-\n",
            "sis. Multi-task learning has seen other applications with self-training such as the extension of\n",
            "Born Again Network [10] for learning context in NLP problem [7] using a weight annealing\n",
            "strategy to update the distillation and multi-task losses. Li et al. [18] apply knowledge dis-\n",
            "tillation to solve the unbalanced loss optimization problem in multi-task learning and show\n",
            "favorable results for fully annotated semantic segmentation and depth prediction training.\n",
            "3 Method\n",
            "To study the relationship between object detection and semantic segmentation, we apply a\n",
            "simple multi-task learning framework following the encoder-decoder principle. Common to\n",
            "many approaches is a shared encoder, comprising a backbone ( e.g. the ResNet family) and a\n",
            "neck ( e.g. the FPN family), which extracts and aggregates features from input images while\n",
            "multiple decoders, or heads, provide task-specific predictions. The overview framework4 H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING\n",
            "3×3×128\n",
            "3×3×256\n",
            " 3×3×256\n",
            "3×3×256na\n",
            " 3×3×256na\n",
            "backbone\n",
            "localization loss\n",
            "classification loss\n",
            "1×1×nc+1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "4\n",
            "2\n",
            "FPN neck\n",
            "1×1×ncna\n",
            " 1×1×4na\n",
            "segmentation loss\n",
            "convolution + BN\n",
            "+ LeakyReLU(0.1)\n",
            "2\n",
            "4\n",
            "upsampling 2x, 4x\n",
            "convolution\n",
            "detection head\n",
            "segmentation head\n",
            "input image\n",
            "Figure 2: A general view of the network architecture including an encoder (backbone and neck) and\n",
            "2 heads for object detection and semantic segmentation. Object detection performs at each pyramid\n",
            "scale while semantic segmentation aggregates all scales and upsamples them to the image-size.\n",
            "is shown in Figure 2. In this work, the one-stage anchor-based object detection architec-\n",
            "tures [22, 25] are studied. The detection head (red boxes) at each pyramid scale comprises\n",
            "2 output branches with the same architecture for localization and classification losses. For\n",
            "semantic segmentation, the multi-scale pyramid features are aggregated using the architec-\n",
            "ture by Kirillov et al. [17]. The aggregation is performed by alternating between convolution\n",
            "and double up-sampling the features at each scale until one-fourth of the input size before\n",
            "element-wise adding together and finally quadruple up-sampling to the input size. The ag-\n",
            "gregated and segmentation features’ dimensions are set to 128 following the original work\n",
            "while detection heads features are 256 as output from the encoder. The detection head uses\n",
            "the Focal Loss [22] and the Balanced L1 Loss [28] for localization and classification while\n",
            "the semantic segmentation head uses the regular cross-entropy with softmax loss.\n",
            "Multi-task training. As each data point is annotated for only a single task, not all the\n",
            "losses can be optimized together. Two optimization approaches are considered, alternating\n",
            "the tasks (1) every epoch or (2) every iteration. For the former, the network is trained with\n",
            "one task for one epoch with the gradients computed from the respective task-specific head\n",
            "and leaving the other task head untouched before being trained with the other task in the\n",
            "following epoch. For the latter, a mini-batch of images with annotations for one task is\n",
            "passed through the network immediately after one with the other task. The gradients from\n",
            "each mini-batch is computed for the corresponding head and accumulated for the encoder.\n",
            "Only after mini-batches from both tasks have been fed in and gradients accumulated are the\n",
            "network parameters updated. As a result, both tasks start and end an epoch together. Thus,\n",
            "the task with fewer annotations will randomly have some images repeated in waiting for a\n",
            "new epoch. We show in Sec. 4.2 and Table 1 the performance difference of the two strategies.\n",
            "Knowledge distillation. We concatenate the features of all scale levels along the flat-\n",
            "tened spatial dimensions. The features of the student network are projected by a 1 ×1 convo-\n",
            "lution before being compared to the corresponding teachers’. The simple Mean Square Error\n",
            "(MSE) [34] is applied for feature imitation distillation. The illustration is shown in Figure 3.H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING 5\n",
            "C\n",
            "1×1×256\n",
            "C\n",
            " C\n",
            "StudentTeacher\n",
            "detectionTeacher\n",
            "segment.\n",
            "MSE MSE\n",
            "Figure 3: Visualization the knowledge distillation process in the multi-task learning for partially anno-\n",
            "tated data. Images and task-specific annotations are fed to the student and the respective teachers, with\n",
            "KD losses computed on teacher-student flattened and concatenated neck features.\n",
            "As each training image can optimize a single task, there are 3 cases for distilling the\n",
            "student features per iteration: (1) from the teacher whose task is annotated (1mse) so the\n",
            "student’s features are forced to follow the teacher’s while learning from the provided ground\n",
            "truths at the same time, (2) from the task teacher without annotations (0mse) so that the head\n",
            "is trained with one task (using ground truth) while the encoder is forced to follow the other’s\n",
            "teacher, and (3) is the combination of both (2mse).\n",
            "4 Experiments\n",
            "4.1 Setup\n",
            "Datasets. All the experiments are conducted on the Pascal VOC [9] containing 20 object\n",
            "categories with 8,218 bounding-box annotated images for training, 8,333 for validation, and\n",
            "4,952 for testing. Due to limited semantic segmentation annotations originally provided\n",
            "(1,464 and 1,449 for training and validation, respectively), the common practices use extra\n",
            "annotations provided by [12], resulting in 10,582 training images.\n",
            "To simulate the partial supervision scenario, images are randomly sampled into 2 sub-\n",
            "sets, one for detection whose semantic annotations are held back and the other for semantic\n",
            "segmentation whose bounding-box annotations are kept out, resulting in 7,558 and 7,656\n",
            "respectively (images without semantic segmentation ground truth are prioritized to the detec-\n",
            "tion subset). For validation, the originally provided validation set for semantic segmentation\n",
            "with both task annotations are used with 1,443 images (6 images with only semantic seg-\n",
            "mentation are withheld). Unless stated otherwise, the image lists are kept the same in all\n",
            "experiments.\n",
            "For out-of-domain experiments, the Cityscapes [8] dataset with 2,975 training, 500 vali-\n",
            "dation images, and 7 semantic classes is employed for semantic segmentation. We resize the\n",
            "images to 128 ×256 to speed up the training process following [19, 23].\n",
            "Network architectures. For comparison purposes, two backbone models from the ResNet\n",
            "family are employed, including ResNet50 backbone with PAFPN [24] neck (RN50+PAFPN),\n",
            "and ResNet18 backbone with FPN [21] neck (RN18+FPN). A few modifications are made6 H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING\n",
            "Training Detection\n",
            "RN18+FPN RN50+PAFPN\n",
            "Single task 42.81 50.22\n",
            "Finetuning head 31.80 36.08\n",
            "Finetuning full 43.30 50.61\n",
            "ReCAM [5]\n",
            "Multi-task (epoch) 44.51 51.62\n",
            "Multi-task (iteration) 44.78 52.10\n",
            "Multi-task (full) 46.83 53.68Segmentation\n",
            "RN18+FPN RN50+PAFPN\n",
            "64.55 72.32\n",
            "61.08 65.84\n",
            "65.21 72.01\n",
            "63.30 70.11\n",
            "66.99 72.93\n",
            "67.57 73.66\n",
            "67.47 73.08\n",
            "Table 1: Comparing single tasks and multi-task learning on partially annotated data. Multi-task are\n",
            "trained by alternating the tasks every epoch or iteration. Training both tasks (full) results are included\n",
            "for reference. Multi-task learning outperforms all other settings.\n",
            "Training Detection\n",
            "RN18+FPN RN50+PAFPN\n",
            "Single task 42.81 50.22\n",
            "38.10 43.73\n",
            "Multi-task 44.78 52.10\n",
            "40.89 47.27\n",
            "44.99 51.43Segmentation\n",
            "RN18+FPN RN50+PAFPN\n",
            "64.55 72.32\n",
            "63.02 68.96\n",
            "67.57 73.66\n",
            "65.39 73.17\n",
            "66.16 73.03\n",
            "Table 2: Single-task and multi-task performance when trained with half annotated detection and\n",
            "segmentation . Single-task is impacted more from the reduced training sizes for the respective task.\n",
            "following the implementation of [34], including removing the first max-pooling layer of\n",
            "ResNet as in ScratchDet [36], and adding the context enhancement module as in Thunder-\n",
            "Net [29]. The number of convolutional blocks in the detection head subsets is reduced from 4\n",
            "to 2 to speed up the training time. The two networks are also used for knowledge distillation\n",
            "as teacher and student, respectively, with parameter ratio of 1.61. The networks are trained\n",
            "for 30 epochs, with learning rate of 5 ×10−3.\n",
            "Evaluation and analysis. We employ, for semantic segmentation the conventional IOU\n",
            "score [16] and, for object detection, the mAP metric implemented by the Detectron2 li-\n",
            "brary [31], which follows the original VOC code but averages APs at multiple IOU thresh-\n",
            "olds in the range [.5, .95, .05]. The detection results at IOU of 50%, i.e. AP50, are used\n",
            "as inputs to the TIDE [1] framework for analyzing the error sources. TIDE breaks down\n",
            "detection errors into 6 types and estimates the isolated contribution of each to the overall\n",
            "performance, as follows: (1) Clserrors localize correctly but classify incorrectly; (2) Loc\n",
            "errors classify correctly but localize incorrectly; (3) Both errors classify and localize incor-\n",
            "rectly; (4) Dupe errors would be correct if not for a higher scoring detection; (5) Bkg errors\n",
            "detect background as foreground; (6) Miss errors are all undetected ground truths not already\n",
            "covered by Cls or Loc error.H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING 7\n",
            "Detection Segmentation Multi-task Multi-task+0mse\n",
            "Figure 4: Class activation maps on the same layers for single task object detection and semantic seg-\n",
            "mentation networks (first 2 columns) and multi-task network (last 2 columns). Failed detected objects\n",
            "are recovered by multi-task networks and further with cross-task enforcing by KD (+0mse).\n",
            "AP50↑ Cls↓Loc↓Both↓Dupe↓Bkg↓Miss↓ FP↓ FN↓\n",
            "40.89 4.23 7.16 0.70 0.37 1.45 8.38 13.12 16.58\n",
            "44.99 3.75 6.08 0.58 0.43 1.46 6.54 14.02 13.09\n",
            "∆ 4.10 -0.48 -1.08 -0.12 0.06 0.01 -1.84 0.90 -3.49\n",
            "47.27 2.69 6.81 0.56 0.37 1.07 8.57 9.55 15.12\n",
            "51.43 2.46 6.06 0.51 0.31 1.07 6.39 9.47 12.30\n",
            "∆ 4.16 -0.23 -0.75 -0.05 -0.06 0 -2.18 -0.08 -2.82\n",
            "Table 3: TIDE analysis of detection from RN18+FPN (top) and RN50+PAFPN (bottom).\n",
            "4.2 Single-task learning and multi-task learning\n",
            "In this experiment, we confirm the benefit of multi-task training for partially annotated data\n",
            "where each training example is only annotated for a single task. Possible data exploitation\n",
            "includes (1) training a single-task network with the provided annotated data; (2) training a\n",
            "single-task network by finetuning one pretrained for the other task; (3) training a single-task\n",
            "network with provided annotated data and pseudo labels generated by a semi-supervised\n",
            "learning method for the other task’s data; and (4) training a multi-task network with 2 de-\n",
            "coders. Except for (1), all other strategies involve the data from the other task in the training\n",
            "process: (2) is a standard transfer learning approach and (3) formulates as a weak-supervised\n",
            "problem. The ReCAM method [5] is applied to generate semantic mask for images from the\n",
            "detection subset . For the transfer learning approach, we also include experiments where\n",
            "the pretrained backbone and neck are kept frozen during finetuning to show the compatibility\n",
            "of the features extracted for one task to the other. The results are shown in Table 1.8 H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING\n",
            "AP50↑Cls↓Loc↓Both↓Dupe↓Bkg↓Miss↓ FP↓FN↓\n",
            "STL 42.81 4.71 6.17 0.73 0.40 1.61 6.70 16.40 12.85\n",
            "MTL 40.89 4.23 7.16 0.70 0.37 1.45 8.38 13.12 16.58\n",
            "∆ -1.92 -0.48 0.99 -0.03 -0.03 -0.16 1.68 -3.28 3.73\n",
            "STL 50.22 2.91 6.45 0.54 0.34 1.25 6.48 10.51 13.21\n",
            "MTL 47.27 2.69 6.81 0.56 0.37 1.07 8.57 9.55 15.12\n",
            "∆ -2.95 -0.22 0.36 0.02 0.03 -0.18 2.09 -0.96 1.91\n",
            "Table 4: TIDE analysis of detection from RN18+FPN (top) and RN50+PAFPN (bottom).\n",
            "It could be seen that the performance of multi-task learning even with partially annotated\n",
            "data are highest for all settings. The results when the two tasks are alternated every iteration\n",
            "seems to perform slightly better than every epoch. We also include the results when an image\n",
            "is annotated with both task, i.e. fully supervised setting with all annotations from [12] for ref-\n",
            "erence (the results are not comparable as there are 10,476 images with both task annotations).\n",
            "Even with fewer effective images, optimizing both tasks shows superior performance. Train-\n",
            "ing a semantic network with joint annotated data and pseudo-label generated by ReCAM\n",
            "method does not help even when compared to single task learning.\n",
            "Regarding transfer learning scenario, finetuning the whole network pretrained with the\n",
            "other task improves over single-task learning while finetuning with frozen encoders plunges,\n",
            "showing the incompatible features learned by one task to the other. Class activation maps\n",
            "generated by Grad-CAM [30] are shown in Figure 4 for single and multi-task networks at\n",
            "the same layer output by the encoder. The multi-task activation seems spreading out for both\n",
            "detection and segmentation and could recover a miss-detected object.\n",
            "Multi-task with fewer data We extend the study by adjusting the number of training data\n",
            "used for multi-task learning. Half number of images from one task subset are randomly\n",
            "removed while retaining those of the other. The results are shown in Table 2.\n",
            "Although reducing training data size takes a great toll on the respective single task per-\n",
            "formance, the multi-task results seem to have less impact, especially for ResNet50+PAFPN\n",
            "with semantic segmentation task. From a multi-task point of view, reducing detection set\n",
            "also affects semantic segmentation performance, especially for RN18+FPN, while reducing\n",
            "segmentation data does not seem to affect the performance of the detection counterpart.\n",
            "TIDE analysis on Table 3 shows that the most contribution to the difference between\n",
            "and is FN, especially the missing objects (Miss, ∆=-1.84, -2.18), then faulty localization\n",
            "with correct classification (Loc, ∆=-1.08, -0.75). The Classification error (Cls) seems to be\n",
            "affected at a lesser degree ( ∆=-0.48, -0.23). As the semantic segmentation masks do not\n",
            "contain precise locations of object instances, even when VOC has limited instances per im-\n",
            "age, using semantic masks helps more with classification and less with localization. This is\n",
            "confirmed in Table 4 where STL trained with full detection data is compared to MTL with\n",
            "half . Although inferior in general performance, MTL with half detection has lower classi-\n",
            "fication error (Cls, ∆=-0.48,-0.22) background confusion (Bkg, ∆=-0.16,-0.18), and generally\n",
            "FP (∆=-3.28,-0.96).\n",
            "Multi-task with different category sets To understand the interrelationships between ob-\n",
            "ject detection and semantic segmentation, we gradually deviate one task from the other andH.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING 9\n",
            "Detection (20 classes)\n",
            "RN18+FPN RN50+PAFPN\n",
            "Single task 42.81 50.22\n",
            "Multi-task 44.48 50.38Segmentation (4 classes)\n",
            "RN18+FPN RN50+PAFPN\n",
            "78.47 81.82\n",
            "79.32 81.89\n",
            "Table 5: Single-task and multi-task performance when the tasks have different label sets. The perfor-\n",
            "mance gap decreases yet still in favor of multi-task learning.\n",
            "Detection (VOC)\n",
            "RN18+FPN RN50+FPN\n",
            "Single task 38.688 44.683\n",
            "Multitask 37.531 39.910Segmentation (Cityscapes)\n",
            "RN18+FPN RN50+FPN\n",
            "71.389 72.398\n",
            "69.481 70.247\n",
            "Table 6: Single tasks and multi-task performance when the tasks are from different domains. Jointly\n",
            "learning 2 tasks from different domains does not help but hurt the performance.\n",
            "observe the performance differences. In this experiment, the category set for semantic seg-\n",
            "mentation is modified. Various VOC semantic categories are merged into an “abstract” class\n",
            "representing the group of the original labels such as the vehicle group (from aeroplane, bicy-\n",
            "cle, boat, bus, car, motorbike, and train), animal (from bird, cat, cow, dog, horse, and sheep),\n",
            "furniture (bottle, chair, dining table, potted plant, sofa, and TV monitor), and person as a\n",
            "group in itself.\n",
            "Arranging different classes into the same group arrives at a semantic segmentation task\n",
            "that aims to learn entirely different concepts from the object detection task. The results are\n",
            "shown in Table 5. Although the performance distance between multi-task and single-task\n",
            "is shortened as each task has to cope with its own concept targets, multi-task still has its\n",
            "superiority. There is a diminishing return with higher capacity architectures.\n",
            "Multi-task with out-of-domain data In this experiment, the two tasks are further pushed\n",
            "to different data domains. To that end, the semantic segmentation images are taken from\n",
            "the Cityscapes dataset [8] with 7 classes. Some of the classes are shared between the two\n",
            "datasets, such as car, human, vegetation. The results in Table 6 show that jointly learning data\n",
            "from different domains worsens the multi-task performance. It is not surprising as the data\n",
            "belong to different distributions with different semantic concept targets, the jointly learned\n",
            "features from one task is not helping but impede the other’s learned features.\n",
            "4.3 Knowledge distillation\n",
            "In this section, knowledge distillation (KD) is used to enact joint-task training for partially\n",
            "annotated data and multi-task learning. To that end, the ResNet50+PAFPN architecture is\n",
            "used as the teacher model and results of the student ResNet18+FPN are reported. The task-\n",
            "specific heads are kept the same for the two networks. The teacher-student parameter ratio is\n",
            "1.61. Unless stated otherwise, the teachers are initialized with the corresponding weights for\n",
            "single tasks from the previous experiments and stay frozen during the training of the students.\n",
            "The results are shown in Table 7. By simply forcing the student encoders to imitate the output\n",
            "of the teachers, the corresponding results are improved, confirming the benefit of knowledge\n",
            "distillation. Distilling the encoder neck features using one task’s teacher while training the10 H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING\n",
            "Training Detection Segmentation\n",
            "Single task 42.907 65.291\n",
            "+ KD 44.982 67.375\n",
            "Multi-task 45.678 67.310\n",
            "+ 1mse 45.989 69.126\n",
            "+ 0mse 47.337 70.056\n",
            "+ 2mse 47.611 69.911\n",
            "Table 7: Adding feature imitation knowledge distillation. For multi-task learning, the distilled features\n",
            "can be on the task with (1mse), or without annotations (0mse), or both (2mse). The performances are\n",
            "in favor for (0mse) and (2mse).\n",
            "AP50↑Cls↓Loc↓Both↓Dupe↓Bkg↓Miss↓ FP↓FN↓\n",
            "STK 42.81 4.71 6.17 0.73 0.40 1.61 6.70 16.40 12.85\n",
            "+MSE 44.98 4.94 5.70 0.67 0.43 1.71 5.39 17.32 11.67\n",
            "∆ 2.17 0.23 -0.47 -0.06 0.03 0.10 -1.31 0.92 -1.18\n",
            "MTL 44.78 3.70 6.72 0.73 0.40 1.61 6.13 14.90 12.43\n",
            "+MSE 47.61 3.10 6.17 0.65 0.50 1.61 5.56 13.47 11.66\n",
            "∆ 2.83 -0.60 -0.55 -0.08 0.1 0 -0.57 -1.43 -0.77\n",
            "Table 8: TIDE analysis of detection results with (+MSE) and without knowledge distillation.\n",
            "other task’s head using provided ground truths (0mse) shows favorable results over distilling\n",
            "the same task that has annotations (1mse). The results are even higher when both tasks are\n",
            "optimized simultaneously in the fully-supervised scenario in Table 1, showing the benefit of\n",
            "multi-task data exploitation and joint-task optimization using knowledge distillation.\n",
            "Table 8 shows the errors reduced by KD for both single-task learning (STL) and multi-\n",
            "task learning (MTL). It could be seen that among the first 6 errors, KD helps the most with\n",
            "Miss detection and Cls. The effects, however, are not the same for STL and MTL: STL\n",
            "benefits substantially from Miss error ( ∆=-1.31), reflecting also in FN ( ∆=-1.18) while MTL\n",
            "benefits equally on Cls, Loc, and Miss, emphasizing on FP ( ∆=-1.43). This suggests the\n",
            "more balancing performance of MTL over STL and the improvement of robustness by KD.\n",
            "5 Conclusion\n",
            "The paper studies the possibility for jointly learning object detection and semantic segmen-\n",
            "tation using partially annotated data. As there are no images with both task annotations,\n",
            "optimization is alternated between the tasks. The experiments show that by alternating ev-\n",
            "ery iteration, the networks could pick up useful information from the other task’s data and\n",
            "improve over the single-task cases. Knowledge distillation could be an alternative method\n",
            "allowing to learn interrelationship between one task and the other’s features.\n",
            "Acknowledgments\n",
            "This work was supported by the SAD 2021 ROMMEO project (ID 21007759) and the ANR\n",
            "AI chair OTTOPIA project (ANR-20-CHIA-0030).H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING 11\n",
            "References\n",
            "[1] Daniel Bolya, Sean Foley, James Hays, and Judy Hoffman. Tide: A general toolbox\n",
            "for identifying object detection errors. In European Conference on Computer Vision\n",
            "(ECCV) , 2020.\n",
            "[2] David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. Au-\n",
            "tomated Search for Resource-Efficient Branched Multi-Task Networks. In British Ma-\n",
            "chine Vision Conference (BMVC) , 2020.\n",
            "[3] David Brüggemann, Menelaos Kanakis, Anton Obukhov, Stamatios Georgoulis, and\n",
            "Luc Van Gool. Exploring Relational Context for Multi-Task Dense Prediction. In\n",
            "IEEE/CVF Proceedings of International Conference of Computer Vision (ICCV) , 2021.\n",
            "[4] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia Angelova. Depth Prediction\n",
            "without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular\n",
            "Videos. In Thirty-Third AAAI Conference on Artificial Intelligence (AAAI) , 2019.\n",
            "[5] Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng Hua, Hanwang Zhang, and\n",
            "Qianru Sun. Class Re-Activation Maps for Weakly-Supervised Semantic Segmenta-\n",
            "tion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n",
            "2022.\n",
            "[6] Zhihao Chen, Lei Zhu, Liang Wan, Song Wang, Wei Feng, and Pheng-Ann Heng. A\n",
            "Multi-Task Mean Teacher for Semi-Supervised Shadow Detection. In Proceedings of\n",
            "the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2020.\n",
            "[7] Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. BAM!\n",
            "Born-Again Multi-Task Networks for Natural Language Understanding. In ACL, 2019.\n",
            "[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Scharwächter, Markus\n",
            "Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The\n",
            "Cityscapes Dataset. volume 3, 2016.\n",
            "[9] M Everingham, L Van ∼Gool, C K I Williams, J Winn, and A Zisserman. The Pascal\n",
            "Visual Object Classes (VOC) Challenge. International Journal of Computer Vision\n",
            "(IJCV) , 88(2), 2010.\n",
            "[10] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima\n",
            "Anandkumar. Born Again Neural Networks. In Jennifer Dy and Andreas Krause, edi-\n",
            "tors, Proceedings of the 35th International Conference on Machine Learning (ICML) ,\n",
            "volume 80, 2018.\n",
            "[11] Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xinghao Chen, Chunjing Xu, and\n",
            "Chang Xu. Distilling Object Detectors via Decoupled Features. In Proceedings of the\n",
            "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.\n",
            "[12] Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, and Jitendra\n",
            "Malik. Semantic contours from inverse detectors. In IEEE/CVF Proceedings of Inter-\n",
            "national Conference of Computer Vision (ICCV) , 2011.\n",
            "[13] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In\n",
            "IEEE/CVF Proceedings of International Conference of Computer Vision (ICCV) , 2017.12 H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING\n",
            "[14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural\n",
            "Network. arXiv preprint arXiv:1503.02531 , 2015.\n",
            "[15] Abdullah-Al-Zubaer Imran, Chao Huang, Hui Tang, Wei Fan, Yuan Xiao, Dingjun Hao,\n",
            "Zhen Qian, and Demetri Terzopoulos. Partly Supervised Multi-Task Learning. In 2020\n",
            "19th IEEE International Conference on Machine Learning and Applications (ICMLA) ,\n",
            "2020.\n",
            "[16] Paul Jaccard. The distribution of the Flora in the Alpine Zone. 1. New Phytologist ,\n",
            "1912.\n",
            "[17] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollar. Panoptic Feature\n",
            "Pyramid Networks. In IEEE/CVF Proceedings of Computer Vision and Pattern Recog-\n",
            "nition (CVPR) , 2019.\n",
            "[18] Wei-Hong Li and Hakan Bilen. Knowledge Distillation for Multi-task Learning. In\n",
            "European Conference on Computer Vision workshop (ECCVw) , 2020.\n",
            "[19] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Learning Multiple Dense Prediction Tasks\n",
            "from Partially Annotated Data. In IEEE/CVF International Conference on Computer\n",
            "Vision and Pattern Recognition (CVPR) , 2022.\n",
            "[20] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Universal Representations: A Unified Look\n",
            "at Multiple Task and Domain Learning. arXiv preprint arXiv:2204.02744 , 2022.\n",
            "[21] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge\n",
            "Belongie. Feature Pyramid Networks for Object Detection. In Proceedings of the IEEE\n",
            "Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.\n",
            "[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal Loss\n",
            "for Dense Object Detection. In Proceedings of the IEEE International Conference on\n",
            "Computer Vision (ICCV) , 2017.\n",
            "[23] Shikun Liu, Edward Johns, and Andrew J Davison. End-To-End Multi-Task Learning\n",
            "With Attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
            "Pattern Recognition (CVPR) , 2019.\n",
            "[24] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path Aggregation Network\n",
            "for Instance Segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
            "and Pattern Recognition (CVPR) , 2018.\n",
            "[25] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-\n",
            "Yang Fu, and Alexander C. Berg. SSD : Single Shot MultiBox Detector. In European\n",
            "Conference on Computer Vision (ECCV) , 2016.\n",
            "[26] Yao Lu, Soren Pirk, Jan Dlabal, Anthony Brohan, Ankita Pasad, Zhao Chen, Vincent\n",
            "Casser, Anelia Angelova, and Ariel Gordon. Taskology: Utilizing Task Relations at\n",
            "Scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
            "Recognition (CVPR) , 2021.\n",
            "[27] Hoàng-Ân Lê and Minh-Tan Pham. Self-Training and Multi-Task Learning for Limited\n",
            "Data: Evaluation Study on Object Detection. In IEEE/CVF Proceedings of Interna-\n",
            "tional Conference of Computer Vision workshop (ICCVw) , 2023.H.-Â. LÊ AND M.-T. PHAM: DATA EXPLOITATION WITH MULTI-TASK LEARNING 13\n",
            "[28] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, and Dahua Lin.\n",
            "Libra R-CNN: Towards Balanced Learning for Object Detection. In Proceedings of the\n",
            "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.\n",
            "[29] Zheng Qin, Zeming Li, Zhaoning Zhang, Yiping Bao, Gang Yu, Yuxing Peng, and Jian\n",
            "Sun. ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices. In\n",
            "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) ,\n",
            "2019.\n",
            "[30] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,\n",
            "Devi Parikh, and Dhruv Batra. Grad-CAM: Visual Explanations From Deep Networks\n",
            "via Gradient-Based Localization. In Proceedings of the IEEE International Conference\n",
            "on Computer Vision (ICCV) , 2017.\n",
            "[31] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. De-\n",
            "tectron2. https://github.com/facebookresearch/detectron2 , 2019.\n",
            "[32] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-Training With\n",
            "Noisy Student Improves ImageNet Classification. In Proceedings of the IEEE/CVF\n",
            "Conference on Computer Vision and Pattern Recognition (CVPR) , 2020.\n",
            "[33] Amir R Zamir, Alexander Sax, Nikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra\n",
            "Malik, and Leonidas J Guibas. Robust Learning Through Cross-Task Consistency. In\n",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n",
            "(CVPR) , 2020.\n",
            "[34] Heng Zhang, Elisa Fromont, Sebastien Lefevre, and Bruno Avignon. PDF-Distil: in-\n",
            "cluding Prediction Disagreements in Feature-based Distillation for object detection. In\n",
            "Proceedings of the British Machine Vision Conference (BMVC) , 2021.\n",
            "[35] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised\n",
            "Learning of Depth and Ego-Motion from Video. In IEEE/CVF Proceedings of Com-\n",
            "puter Vision and Pattern Recognition (CVPR) , 2017.\n",
            "[36] Rui Zhu, Shifeng Zhang, Xiaobo Wang, Longyin Wen, Hailin Shi, Liefeng Bo, and Tao\n",
            "Mei. ScratchDet: Training Single-Shot Object Detectors From Scratch. In IEEE/CVF\n",
            "Proceedings of Computer Vision and Pattern Recognition (CVPR) , 2019.\n",
            "[37] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and\n",
            "Quoc V Le. Rethinking Pre-Training and Self-Training. In Proceedings of the 34th\n",
            "International Conference on Neural Information Processing Systems (NeurIPS) , 2020.Rethinking and Improving Multi-task Learning\n",
            "for End-to-end Speech Translation\n",
            "Yuhao Zhang1, Chen Xu1, Bei Li1, Hao Chen1,\n",
            "Tong Xiao1,2∗, Chunliang Zhang1,2and Jingbo Zhu1,2\n",
            "1School of Computer Science and Engineering,\n",
            "Northeastern University, Shenyang, China\n",
            "2NiuTrans Research, Shenyang, China\n",
            "yoohao.zhang@gmail.com, {xuchenneu,libei_neu}@outlook.com\n",
            "{xiaotong, zhangcl, zhujingbo}@mail.neu.edu.cn\n",
            "Abstract\n",
            "Significant improvements in end-to-end speech\n",
            "translation (ST) have been achieved through\n",
            "the application of multi-task learning. How-\n",
            "ever, the extent to which auxiliary tasks are\n",
            "highly consistent with the ST task, and how\n",
            "much this approach truly helps, have not been\n",
            "thoroughly studied. In this paper, we investi-\n",
            "gate the consistency between different tasks,\n",
            "considering different times and modules. We\n",
            "find that the textual encoder primarily facili-\n",
            "tates cross-modal conversion, but the presence\n",
            "of noise in speech impedes the consistency be-\n",
            "tween text and speech representations. Further-\n",
            "more, we propose an improved multi-task learn-\n",
            "ing (IMTL) approach for the ST task, which\n",
            "bridges the modal gap by mitigating the differ-\n",
            "ence in length and representation. We conduct\n",
            "experiments on the MuST-C dataset. The re-\n",
            "sults demonstrate that our method attains state-\n",
            "of-the-art results. Moreover, when additional\n",
            "data is used, we achieve the new SOTA result\n",
            "on MuST-C English to Spanish task with 20.8%\n",
            "of the training time required by the current\n",
            "SOTA method.\n",
            "1 Introduction\n",
            "End-to-end (E2E) models have made significant\n",
            "strides in the artificial intelligence realm, especially\n",
            "in speech translation (ST). These models have low\n",
            "latency and less error propagation by providing di-\n",
            "rect translations from speech inputs (Bérard et al.,\n",
            "2016; Duong et al., 2016). This approach contrasts\n",
            "with traditional pipeline models that rely on sepa-\n",
            "rate automatic speech recognition (ASR) and ma-\n",
            "chine translation (MT) systems. However, the E2E\n",
            "model’s single-model design poses new challenges\n",
            "due to its need for cross-modal and cross-lingual\n",
            "transfers (Zheng et al., 2021; Xu et al., 2021). To\n",
            "address this, recent studies have utilized multi-task\n",
            "learning (MTL), leveraging cross-modal or cross-\n",
            "lingual training objectives for pre-training or joint\n",
            "∗Corresponding author.Acoustic\n",
            "encoderTextual\n",
            "encoderDecoder\n",
            "Speech Source text Target textASR/MT/\n",
            "ST lossAuxiliary loss\n",
            "Figure 1: Multi-task training architecture for Speech\n",
            "translation. The dashed line part will be removed in\n",
            "fine-tune stage.\n",
            "training (Tang et al., 2021; Ye et al., 2021; Dong\n",
            "et al., 2021b; Han et al., 2021). This technique as-\n",
            "sures good convergence of the models and empha-\n",
            "sizes the importance of the auxiliary loss, offering\n",
            "a brand-new perspective for further advancements\n",
            "in E2E ST (Tang et al., 2022).\n",
            "However, further exploration is necessary to de-\n",
            "termine how and to what extent these auxiliary\n",
            "tasks aid the final ST model. Notably, not all aux-\n",
            "iliary tasks in the fine-tuning stage are beneficial.\n",
            "This inconsistency arises because MTL is typically\n",
            "viewed as a multi-objective optimization problem,\n",
            "often resulting in training trade-offs when objec-\n",
            "tives conflict (Désidéri, 2012). The ideal MTL\n",
            "outcome is to achieve Pareto optimality (Sener and\n",
            "Koltun, 2018), indicating solutions are superior to\n",
            "any alternatives. Since MTL does not ensure op-\n",
            "timal performance for the ST task, fine-tuning is\n",
            "crucial to overcome this shortcoming. Some stud-\n",
            "ies even underscore a critical conflict between the\n",
            "ST task and ASR and MT tasks (Xu et al., 2021;\n",
            "Tang et al., 2022), necessitating a fine-tuning strat-\n",
            "egy. So answering these questions is crucial to\n",
            "designing an optimal MTL strategy for the ST task.\n",
            "In this paper, we rethink task consistency in MTL\n",
            "and introduce a gradient-based consistency metric,\n",
            "which denotes the consistency of the gradient direc-\n",
            "tion between the ST task and other auxiliary tasks.\n",
            "Our analysis shows that 1) ASR aids the acousticarXiv:2311.03810v1  [cs.CL]  7 Nov 2023encoder and MT facilitates the textual encoder in\n",
            "audio-to-text transfer, 2) length inconsistency hin-\n",
            "ders aligning the representations of the two modal-\n",
            "ities, 3) disparity between noisy speech features\n",
            "and clean text embeddings as considerable obsta-\n",
            "cles, and 4) the timing and degree of task influence\n",
            "exhibit significant variation.\n",
            "Inspired by the aforementioned observations, we\n",
            "relax the ASR task that only uses it to help the\n",
            "acoustic encoder. We propose the Looking-Back\n",
            "Mechanism (LBM) to overcome length consistency.\n",
            "It can significantly shrink the speech length with-\n",
            "out information loss. To bridge the modality gap,\n",
            "we introduce the Local-to-Global (L2G) training\n",
            "strategy. By incorporating speech-like noise into\n",
            "the text and utilizing an L2G extractor, we enhance\n",
            "contextual information at each layer. This method\n",
            "effectively guides the interaction between audio\n",
            "and text sequences and aligns with audio process-\n",
            "ing requirements. We further propose a task-norm-\n",
            "based weight decrease method to speed up train-\n",
            "ing, adjusting task weights based on auxiliary task\n",
            "influence and timing, thus avoiding unnecessary\n",
            "training costs.\n",
            "We test our method on MuST-C 8 language data-\n",
            "sets. The results show our method can achieve\n",
            "comparable performance with SOTA work without\n",
            "fine-tuning on ST task. We then set the new SOTA\n",
            "by utilizing the knowledge distillation method (Liu\n",
            "et al., 2019). With additional data, we achieve\n",
            "comparable performance with that of using only\n",
            "12.5% ∼33.3% training cost compared with the\n",
            "SOTA work1.\n",
            "2 Task Consistency Quantification\n",
            "In this section we investigate the consistency issue\n",
            "of multi-task learning on three tasks. We randomly\n",
            "sample nsamples as analysis set D={(s,x,y)}.\n",
            "Here s,x,ydenote the speech, transcription and\n",
            "translation sequences respectively. We then build\n",
            "the ASR set DASR={(s,x)}and MT set DMT=\n",
            "{(x,y)}. By inputting the same data (s,x,y)into\n",
            "the model, we observe that different training tasks\n",
            "yield distinct parameter gradients. The losses of\n",
            "different tasks are given by:\n",
            "LST=−|y|X\n",
            "ilogp(yi|y1:i−1,s) (1)\n",
            "1Our code is available at https://github.com/xiaozhang521/\n",
            "IMTL.LASR=−|x|X\n",
            "ilogp(xi|x1:i−1,s) (2)\n",
            "LMT=−|y|X\n",
            "ilogp(yi|y1:i−1,x) (3)\n",
            "where | · |denotes length of the sequence. The\n",
            "model contains three modules: the acoustic en-\n",
            "coder (A-Enc), the textual encoder (T-Enc), and\n",
            "the decoder. The MT task shares the T-Enc and the\n",
            "decoder with the ST while the ASR shares all pa-\n",
            "rameters. Thus we can quantify the consistency of\n",
            "different tasks from the perspective of the gradient.\n",
            "We employ cosine similarity as the metric for\n",
            "gradient direction, where higher values indicate\n",
            "stronger consistency between tasks within a sin-\n",
            "gle model. To calculate the similarity, we flatten\n",
            "the gradient matrix into a vector, providing a more\n",
            "accurate assessment of task consistency, despite\n",
            "yielding lower values. We focus on evaluating the\n",
            "gradient of the feed-forward (FFN) sub-layer and\n",
            "self-attention (ATTEN) sub-layer as representative\n",
            "parameters of the entire model. Our backbone\n",
            "model is the robust ConST (Ye et al., 2022). In\n",
            "our experiments, we set n= 200 . We sample five\n",
            "times and use average values to obtain solid results.\n",
            "2.1 Consistency in Different Modules\n",
            "The consistency between the ST task and the other\n",
            "two tasks (ASR and MT) within these modules is\n",
            "shown in Figure 2. Although the ASR task shares\n",
            "all the parameters with the ST task, only the A-Enc\n",
            "exhibits high consistency with the ST task. This\n",
            "indicates that modeling speech in the A-Enc serves\n",
            "the same purpose for both ASR and ST tasks, which\n",
            "aligns with the conclusions of Anastasopoulos and\n",
            "Chiang (2018) and Bahar et al. (2019). However,\n",
            "the consistency between the two tasks decreases\n",
            "sharply after the textual modal processing in the\n",
            "T-Enc and decoder. The decoder’s divergence is\n",
            "expected due to generating texts in different lan-\n",
            "guages. The T-Enc converts the acoustic feature\n",
            "to a textual feature for both tasks, but Figure 2 re-\n",
            "veals lower consistency. It suggests a specific need\n",
            "for semantic-level representation in the ST task to\n",
            "achieve the cross-lingual goal.\n",
            "The decoder exhibits higher consistency com-\n",
            "pared to the encoder for the MT task, suggesting\n",
            "that the behavior of the ST decoder leans towards\n",
            "cross-language processing. However, the T-Enc\n",
            "still exhibits low consistency. Taking into account\n",
            "the above analysis on the ASR task, the T-Enc playsASR MT0.00.20.40.6ConsistencyA-Enc T-Enc Decoder\n",
            "(a) ATTEN sublayerASR MT0.00.20.40.6Consistency\n",
            "(b) FFN sublayer\n",
            "Figure 2: Consistency of different tasks in different\n",
            "modules.\n",
            "a unique role in MTL and does not align closely\n",
            "with either of the other two tasks (Xu et al., 2021).\n",
            "Therefore, our subsequent analysis will focus on\n",
            "the T-Enc.\n",
            "2.2 Impact of T-Enc\n",
            "We conducted a comparison of the consistency be-\n",
            "tween the ST task and the other two tasks within\n",
            "each layer of the T-Enc. Figure 3 illustrates that the\n",
            "bottom layer of the T-Enc demonstrates a stronger\n",
            "resemblance to the ASR task. The discrepancy\n",
            "arises as the feature extraction process diverges fur-\n",
            "ther between the ASR and ST tasks. This suggests\n",
            "that the cross-modal transformation of A-Enc is not\n",
            "achieved, then the T-Enc begins extracting textual\n",
            "information which is required by ST to adequately\n",
            "address the cross-lingual objective. We also no-\n",
            "ticed that the ST task gradually aligns with the MT\n",
            "task, but the consistency between them is still low.\n",
            "This raises the question of whether the ASR task\n",
            "leads to insufficient alignment between speech and\n",
            "text in the T-Enc.\n",
            "We conducted further investigations into the im-\n",
            "pact of the ASR task on the T-Enc. We introduced\n",
            "two widely used ASR losses: 1) the Connectionist\n",
            "Temporal Classification (CTC) loss (Graves et al.,\n",
            "2006) after the A-Enc, and 2) the cross-entropy\n",
            "(CE) loss after the decoder. The former updates\n",
            "only the A-Enc (Bahar et al., 2019), while the lat-\n",
            "ter updates the entire model. By exploring various\n",
            "MTL combinations, the changes in consistency be-\n",
            "tween the MT and ST tasks are depicted in Fig-\n",
            "ure 4. We discovered that as the ASR training be-\n",
            "comes more intensive, the decrease in consistency\n",
            "between the MT and ST tasks becomes more pro-\n",
            "nounced at the top layers. Although increasing the\n",
            "ASR training workload burdens the T-Enc, research\n",
            "indicates that the ASR task is crucial in helping the\n",
            "acoustic encoder model speech (Le et al., 2023).\n",
            "We find the impact of the ASR task on the T-Enc is1234560.00.10.2ConsistencyASR MT\n",
            "(a) ATTEN sublayer1234560.00.10.2Consistency\n",
            "(b) FFN sublayer\n",
            "Figure 3: Consistency of different tasks in different\n",
            "layer of T-Enc.\n",
            "limited. Thus we further investigate other factors\n",
            "that impede the consistency between the MT and\n",
            "ST tasks.\n",
            "2.3 Discrepancy between MT and ST\n",
            "We focus on two main differences between speech\n",
            "and text features: length and representation space.\n",
            "The length disparity arises from modeling granu-\n",
            "larity (frames for speech while sub-words for text)\n",
            "(Xu et al., 2023b). The representation space dis-\n",
            "crepancy is due to acoustic features extracted by\n",
            "the acoustic encoder lacking text-based information\n",
            "(Li et al., 2021; Fang et al., 2022). We implement\n",
            "the shrinking method (Liu et al., 2020; Dong et al.,\n",
            "2021a) and contrastive learning (CL) loss (Ye et al.,\n",
            "2022; Zhang et al., 2023) at the top of T-Enc re-\n",
            "spectively to investigate the two issues. we employ\n",
            "\"Length\" and \"Rep\" to represent the shrinking and\n",
            "CL methods respectively in Figure 5 and 6.\n",
            "To remove the influence of the ASR task, the\n",
            "experiments are conducted with the MT and ST\n",
            "tasks. Figure 5 illustrates the shrinking method\n",
            "effectively increases consistency in the decoder,\n",
            "as a more compact sequence is easier to extract\n",
            "information from during cross-attention. However,\n",
            "this approach also results in the loss of original\n",
            "information, leading to a significant degradation in\n",
            "the consistency between the two tasks in the T-Enc.\n",
            "On the other hand, when incorporating additional\n",
            "alignment loss, the changes in consistency within\n",
            "both modules are minimal.\n",
            "To explore why CL loss does not work, we con-\n",
            "duct an in-depth analysis from the perspective of\n",
            "information entropy (IE). Higher entropy implies\n",
            "greater outcome uncertainty. We compute the IE\n",
            "of each T-Enc’s self-attention weights, as shown in\n",
            "Figure 6. The MT task shows lower IE compared\n",
            "to the ST task, indicating reduced noise sources in\n",
            "its representation. When we shrink the length of\n",
            "the speech, the IE is noticeably reduced. This can1 2 3 4 5 60.060.080.100.12ConsistencyCE loss CTC loss\n",
            "CE + CTC w/o ASR loss\n",
            "Figure 4: Influence of textual encoder by ASR training\n",
            "strategy.\n",
            "T-Enc Decoder0.00.10.20.3ConsistencyBaseline Length Rep\n",
            "(a) ATTEN sublayerT-Enc Decoder0.00.10.20.3Consistency\n",
            "(b) FFN sublayer\n",
            "Figure 5: Consistency of different modules.\n",
            "explain why the decoder exhibits higher gradient\n",
            "consistency. But if we use the CL loss, it does not\n",
            "have a impact on the IE. The noisy speech sequence\n",
            "makes it difficult to learn more textual information.\n",
            "This finding can explain the CL on sentence-level\n",
            "representations performs worse than token-level ap-\n",
            "proaches (Ye et al., 2022). However, when the CL\n",
            "loss is introduced based on the compressed speech,\n",
            "we observe that additional information is learned in\n",
            "the middle layers. Thus we can find that shrinking\n",
            "is necessary and information gap between ST and\n",
            "MT tasks still needs to be mitigated.\n",
            "2.4 Time of Taking Effect\n",
            "We have identified the discrepancies between dif-\n",
            "ferent tasks, but the interplay of these tasks dur-\n",
            "ing the training process has not been thoroughly\n",
            "studied. Figure 7 illustrates the changes in con-\n",
            "sistency among the different modules throughout\n",
            "training. We observe that the assistance provided\n",
            "by the ASR task primarily occurs at the early stage\n",
            "and becomes less significant later on. On the other\n",
            "hand, the impact of the MT task on the ST task\n",
            "is more complex, with a gradual decrease in con-\n",
            "sistency over time, indicating slower assistance to\n",
            "the ST task. Additionally, the behaviors of the\n",
            "T-Enc and decoder differ significantly. These ob-\n",
            "servations highlight the diversity in the timing and\n",
            "effects of different tasks, underscoring the need for\n",
            "a careful strategy to optimize their training effects\n",
            "and timing.1 2 3 4 5 62.04.06.08.0Information entropyMT Length ST\n",
            "Rep Rep + length\n",
            "Figure 6: Information entropy of T-encoder attention\n",
            "weight.\n",
            "2468100.10.2ConsistencyMT task ASR task\n",
            "(a) T-Enc2468100.10.30.5Consistency\n",
            "(b) Decoder\n",
            "Figure 7: Changes of consistency along training epochs.\n",
            "3 Method\n",
            "We propose the improved MTL (called IMTL)\n",
            "method from three perspectives: 1) denoising the\n",
            "ST sequence, 2) adding noising to MT, and 3) and\n",
            "improving training efficiency. The overall training\n",
            "objective is:\n",
            "L=LST+waLASR+wmLMT+wcLCL (4)\n",
            "The CL denotes contrastive learning and we set the\n",
            "wcto 0.3. Based on the above analysis, we use the\n",
            "CTC loss as the ASR task.\n",
            "3.1 Stable Shrinking\n",
            "From the previous analysis, we find the de-\n",
            "coder benefits from the decrease in speech length.\n",
            "Though Liu et al. (2020) and Dong et al. (2021a)\n",
            "have proposed methods to figure the issue out, there\n",
            "are two main problems still need to be improved.\n",
            "Instability Once tokens are removed, the gradi-\n",
            "ent from the decoder can not guide the acoustic\n",
            "encoder. Especially when the prediction of speech\n",
            "is not accurate at the earlier training stage, this will\n",
            "cause the unstable training.\n",
            "Information loss If tokens are wrongly removed\n",
            "by method, information loss will happen. The\n",
            "blank token also contains pause information which\n",
            "can help the model understand the sentence.\n",
            "To address the aforementioned issues, we pro-\n",
            "pose the Looking-back mechanism (LBM), as de-\n",
            "picted in Figure 8. Given a sequence of speech... ...\n",
            "Nice <blank> to•Gradient truncation\n",
            "•Information loss\n",
            "(a) Unstable shrinking\n",
            "... ...+ +\n",
            "(b) Looking-back mechanism (LBM)\n",
            "Figure 8: (a) Unstable shrinking and (b) LBM. The red\n",
            "line in (b) can supply additional information. Thus it\n",
            "avoids the two problems in (a).\n",
            "features s= (s1, ..., s n), we first calculate the\n",
            "probabilities of CTC paths and extract the posi-\n",
            "tion with the highest value as the decoding result\n",
            "T= (t1, ..., t n). Since Tis generated through\n",
            "monotonic decoding, adjacent positions may con-\n",
            "tain repeated tokens in the result. For each repeated\n",
            "segment in the sequence, we select the token with\n",
            "the highest confidence within that segment to form\n",
            "a new unique result T′= (t′\n",
            "1, ..., t′\n",
            "m). Additionally,\n",
            "the corresponding new features s′= (s′\n",
            "1, ..., s′\n",
            "m)\n",
            "can be generated. Note that we do not filter out the\n",
            "blank positions to prevent error propagation. There-\n",
            "fore, the key to resolving the mentioned problems\n",
            "lies in effectively transferring all the information\n",
            "from sto the compressed features s′. The LBM\n",
            "method utilizes features from s′to retrospectively\n",
            "retrieve information from sand extract the missing\n",
            "information.\n",
            "Formally, for an arbitrary feature s′\n",
            "iins′, we\n",
            "can determine its index jinsbased on t′\n",
            "iandT.\n",
            "We set a boundary bfor looking back, ensuring\n",
            "that[j−b, j+b]contains all the repeated tokens.\n",
            "We construct the search matrix Aby including all\n",
            "features from max(1 , j−b)tomin(j+b, n), ex-\n",
            "cluding the feature at index j. We then search and\n",
            "aggregate information in Ausing the following\n",
            "formula:\n",
            "˜si= Softmax\u0000\n",
            "R(s′\n",
            "i)· R(A)T\u0001\n",
            "·A (5)\n",
            "where the Rdenotes the linear transfer, Softmax()\n",
            "normalizes the correlation between s′\n",
            "iandAto\n",
            "0∼1. We final use the fusion module to integrate...\n",
            "Acoustic\n",
            "encoder...\n",
            "Audio-like noiseT-Enc layerTransformer\n",
            "layer\n",
            "L2G extractorConsistency loss\n",
            "Share\n",
            "CL loss\n",
            "\"How are you\" \"How\" \"are\" \"you\" \"?\"\n",
            "Figure 9: Encoder of Local-to-global (L2G) training.\n",
            "One T-Enc layer consists of Transformer layer and L2G\n",
            "extractor. The light pink parts are shared.\n",
            "thes′\n",
            "iand˜si:\n",
            "sf\n",
            "i= FFN\u0000\n",
            "Norm( s′\n",
            "i+ ˜si)\u0001\n",
            "(6)\n",
            "hereNorm() denotes the normalization layer, and\n",
            "FFN() denotes a feed-forward network used to fil-\n",
            "ter redundant information. The LBM method can\n",
            "automatically learn the weights of each repeated\n",
            "frame, ensuring that the obtained sf\n",
            "idoes not in-\n",
            "troduce additional noise. Even when the length is\n",
            "significantly reduced, the LBM can preserve all the\n",
            "original information and avoid gradient truncation,\n",
            "thereby promoting stable training.\n",
            "3.2 Local-to-global Training\n",
            "We have observed an information difference in rep-\n",
            "resentation between the MT and ST tasks in the\n",
            "T-Enc. The main reason is that the speech fea-\n",
            "ture undergoes high-level abstraction by the acous-\n",
            "tic encoder, while the text embedding remains un-\n",
            "processed and devoid of any noise. This inherent\n",
            "difference causes the model to classify these two\n",
            "tasks differently, resulting in inconsistent gradi-\n",
            "ents. Wang et al. (2020b) injects some audio-like\n",
            "tokens into the MT sequence, while we propose the\n",
            "local-to-global (L2G) training strategy to bridge\n",
            "the information gap.\n",
            "We first introduce noise to the clean text embed-\n",
            "ding. Taking into account the characteristics of\n",
            "repeated information and blank positions in speech\n",
            "sequences, we randomly add blanks or duplicate\n",
            "certain tokens with a probability of 0.2 for each\n",
            "position. Our goal is to facilitate the learning of\n",
            "consistent representations for the two tasks. ToModels FT En-De En-Es En-Fr En-It En-Nl En-Pt En-Ro En-Ru Avg.\n",
            "Fairseq ST†(Wang et al., 2020a) - 22.7 27.2 32.9 22.7 27.3 28.1 21.9 15.3 24.8\n",
            "Revisit ST†(Zhang et al., 2022a) ✓ 23.0 28.0 33.5 23.5 27.1 28.2 23.0 15.6 25.2\n",
            "STEMM (Fang et al., 2022) ✓ 25.6 30.3 36.1 25.6 30.1 31.0 24.3 17.1 27.5\n",
            "ConST (Ye et al., 2022) ✓ 25.7 30.4 36.8 26.3 30.6 32.0 24.8 17.3 28.0\n",
            "M3ST (Cheng et al., 2022) ✓ 26.4 31.0 37.2 26.6 30.9 32.8 25.4 18.3 28.6\n",
            "CMOT (Zhou et al., 2023) ✓ 27.0 31.1 37.3 26.9 31.2 32.7 25.3 17.9 28.7\n",
            "CRESS (Fang and Feng, 2023) ✓ 27.2 31.9 37.8 27.3 31.6 33.0 25.9 18.7 29.2\n",
            "Baseline ✓ 25.8 30.4 36.7 26.1 30.5 32.0 24.7 17.3 28.0\n",
            "IMTL - 26.9 31.5 37.7 27.3 31.3 33.0 25.5 18.3 28.9\n",
            "IMTL-KD - 27.5 31.8 38.2 27.7 32.0 33.4 25.9 18.6 29.4\n",
            "Table 1: Performance on different data set. FT denotes the model needs fine-tuning stage. †means the work does\n",
            "not use the unlabeled speech data.\n",
            "Models FTEn-De En-Fr En-Es\n",
            "ConST (Ye et al., 2022) ✓ 28.3 38.3 32.0\n",
            "STPT (Tang et al., 2022) ✓ - 39.7 33.1\n",
            "M3ST (Cheng et al., 2022) ✓ 29.3 38.5 32.4\n",
            "CMOT (Zhou et al., 2023) ✓ 29.0 39.5 32.8\n",
            "CRESS (Fang and Feng, 2023) ✓ 29.4 40.1 33.2\n",
            "SpeechUT (Zhang et al., 2022b) ✓ 30.1 41.4 33.6\n",
            "Baseline ✓ 28.4 39.1 32.4\n",
            "IMTL -29.3 40.6 33.4\n",
            "IMTL-KD -29.7 41.1 33.9\n",
            "Table 2: Performance on different data set with addi-\n",
            "tional training data.\n",
            "achieve this, we propose the L2G feature extrac-\n",
            "tor. We aim to use the interaction window size to\n",
            "limit the positions from which information is ex-\n",
            "tracted. Convolution networks are well-suited for\n",
            "this purpose, and we implement the L2G extractor\n",
            "using:\n",
            "x=x+ Conv(Norm( x)) (7)\n",
            "where Conv() denotes the depthwise separable con-\n",
            "volution (Chollet, 2016). We add the extractor in\n",
            "front of each Transformer layer in T-Enc. This ex-\n",
            "tractor can learn relevant information from a given\n",
            "window c, which is determined by the convolution\n",
            "kernel size. Unlike the self-attention mechanism\n",
            "that learns from the entire sequence, this window\n",
            "focuses on a specific region, aiding the two tasks in\n",
            "learning the same information. However, it also in-\n",
            "troduces additional information for MT task, which\n",
            "necessitates the text’s ability to enhance its denois-\n",
            "ing capabilities. Finally, we utilize the consistency\n",
            "loss to align the representations extracted by the\n",
            "extractor and attention mechanisms.\n",
            "The study conducted by Xu et al. (2021) demon-\n",
            "strates that the MT task requires a more global\n",
            "understanding to form a semantic-level representa-\n",
            "tion, whereas the acoustic task primarily relies onlocal information. To address this, we propose an\n",
            "increasing window approach to assist the acoustic\n",
            "representation in capturing global textual informa-\n",
            "tion. Specifically, we introduce an increasing stride\n",
            "for the convolution field, where each layer’s field\n",
            "increases by d. Therefore, the kernel size of the\n",
            "i-th T-Enc layer is c+d∗i.\n",
            "3.3 MTL Based on Task Impact\n",
            "Our previous analysis reveals that the impact of\n",
            "different tasks and modules varies over time. This\n",
            "insight has inspired us to develop a new training\n",
            "strategy that gradually eliminates the auxiliary task,\n",
            "rather than relying on an additional fine-tuning\n",
            "stage. This approach simplifies and streamlines\n",
            "the entire training process. To achieve this objec-\n",
            "tive, we need to determine whether the auxiliary\n",
            "task is beneficial at each training step and assess\n",
            "its level of impact. We can examine the change in\n",
            "task consistency to address the first question. When\n",
            "the task consistency stabilizes and different tasks\n",
            "reach a balanced state, we can reduce the training\n",
            "weight assigned to the auxiliary task. However, to\n",
            "effectively decrease the weight, we must quantify\n",
            "the influence of the auxiliary task.\n",
            "In multi-task learning, the use of norms has been\n",
            "extensively studied (Argyriou et al., 2008; Maurer\n",
            "et al., 2013). Norms can evaluate the sparsity of\n",
            "a matrix and are commonly employed to enhance\n",
            "the information in network parameters, thereby im-\n",
            "proving the effectiveness of MTL. Consequently,\n",
            "gradient norms have been successfully utilized in\n",
            "computer vision (Chen et al., 2018) to balance the\n",
            "impact of different tasks. Taking inspiration from\n",
            "this, we propose a task impact metric for auxiliary\n",
            "tasks based on gradient norms. We sample kin-\n",
            "stances from the training set to create D′, which we\n",
            "then feed into the model to obtain gradients for theModels En-DeLengthEn-FrLength\n",
            "ratio(%) ratio(%)\n",
            "Baseline 25.8 100.00 36.7 100.00\n",
            "Shrinking 25.7 53.97 36.8 57.72\n",
            "+LBM 26.3 55.67 37.2 60.13\n",
            "Table 3: Ablation study on shrinking method.\n",
            "various tasks. The task impact mof auxiliary task\n",
            "ican be calculated using the following formula:\n",
            "mi=1\n",
            "kX\n",
            "j∈D′(||δj\n",
            "i||2\n",
            "||δj\n",
            "st+δj\n",
            "i||2) (8)\n",
            "where δj\n",
            "iis the ATTEN (self-attention sub-layer)\n",
            "gradient of data jfor task i,|| · || 2denotes the 2-\n",
            "norm of the matrix. The higher mshows updating\n",
            "the gradient will have a greater impact on the ST\n",
            "task. Containing the change of different tasks, we\n",
            "give the weight of the different task at t-th update\n",
            "as follows:\n",
            "wt\n",
            "i=wt−1\n",
            "i(mi)u/s(9)\n",
            "where urepresents the current training step, and s\n",
            "denotes the smoothing coefficient. The impact of\n",
            "these two hyper-parameters can be likened to tem-\n",
            "perature coefficients and we can set appropriate u\n",
            "andsvalues to ensure that changes in task weights\n",
            "correspond to changes in task consistency. Since\n",
            "the weight between T-Enc and Decoder differs, we\n",
            "select the maximum value as wfor the MT task.\n",
            "The design of wtakes into account the consistency\n",
            "and impact of different tasks, thus avoiding un-\n",
            "necessary computational resources when auxiliary\n",
            "tasks are not beneficial. Furthermore, this training\n",
            "strategy allows us to remove the other task in time\n",
            "and achieve optimal performance without the need\n",
            "for tedious fine-tuning stages.\n",
            "4 Experiments\n",
            "4.1 Data\n",
            "We conducted experiments on the multilingual\n",
            "MuST-C dataset (Di Gangi et al., 2019). The\n",
            "dataset consists of eight language pairs: English\n",
            "(En) to German (De), French (Fr), Spanish (Es), Ro-\n",
            "manian (Ro), Russian (Ru), Italian (It), Portuguese\n",
            "(Pt), and Dutch (Nl). For the En-De, En-Fr, and\n",
            "En-Es MT tasks, we collected external training\n",
            "data from WMT16, WMT14, and WMT13 respec-\n",
            "tively. As additional ASR data, we utilized the\n",
            "LibriSpeech (Panayotov et al., 2015) clean-100Models En-De En-Fr\n",
            "Baseline 25.8 36.7\n",
            "+Fixed window 26.2 37.3\n",
            "+L2G 26.4 37.5\n",
            "LBM 26.3 37.2\n",
            "+Fixed window 26.6 37.5\n",
            "+L2G 26.9 37.7\n",
            "Table 4: Ablation study on L2G training.\n",
            "dataset. The Dev set was used for validation, and\n",
            "tst-COMMON set served as the test set for all tasks.\n",
            "SentencePiece2segmentation with a vocabulary\n",
            "size of 10,000 was applied to all training datasets.\n",
            "The detail of the data is shown in Appendix A.\n",
            "4.2 Model settings\n",
            "We used the Fairseq toolkit (Ott et al., 2019; Wang\n",
            "et al., 2020a) to implement our methods. The\n",
            "Transformer-BASE configurations were cho-\n",
            "sen as the baseline settings, with approximately\n",
            "150M parameters. We reproduced the ConST\n",
            "method to establish a strong baseline (Ye et al.,\n",
            "2022). The acoustic encoder was initialized with\n",
            "the audio-only pre-trained HuBert (Hsu et al.,\n",
            "2021). In the presence of additional data, we fol-\n",
            "lowed the setup of SpeechUT (Zhang et al., 2022b),\n",
            "which utilized a hidden size of 768, 12 attention\n",
            "heads, and a 3072 FFN dimension. Each training\n",
            "batch contained 20M audio frames. We set the\n",
            "training steps to 80K. When using additional MT\n",
            "data, the data size for different tasks becomes ex-\n",
            "tremely unbalanced. Therefore, we first trained the\n",
            "MT task for 15 epochs with 8192 tokens per batch\n",
            "and then sampled 3 million sentences as MT data\n",
            "for MTL. We change the updated frequency to 4\n",
            "and the training step to 40K. The kernel size cand\n",
            "the increased stride dfor the L2G extractor was set\n",
            "to 5 and 3, respectively. The value of swas set to\n",
            "5000 for the ASR task and 10,000 for the MT task.\n",
            "The initial weights of ASR and MT tasks are 1.0.\n",
            "We updated the task weight every 5000 training\n",
            "steps and removed the task when the weight fell\n",
            "below 0.1. During inference, we average the last\n",
            "10 checkpoints for evaluation. The other decoding\n",
            "settings are the same as those in CRESS (Fang and\n",
            "Feng, 2023). We use ScareBLEU (Post, 2018) as\n",
            "the metric for ST performance. The experiments\n",
            "were conducted on eight NVIDIA GeForce RTX\n",
            "3090 GPUs.\n",
            "2https://github.com/google/sentencepiece0.001.002.003.004.005.00MT L2G\n",
            "FIXED L2G+KD\n",
            "1\n",
            "2\n",
            "3\n",
            "456\n",
            "51015202530350.20.40.60.8\n",
            "Training steps(K)WeightMT task\n",
            "ASR task\n",
            "Figure 10: IE of different methods at the T-Enc layers\n",
            "(left). Task weights along training steps (right).\n",
            "4.3 Results\n",
            "The comparison of our IMTL and other works un-\n",
            "der the circumstance of no additional data is shown\n",
            "in Table 1. We find that the work utilizing the pre-\n",
            "training and fine-tuning paradigm achieves a sig-\n",
            "nificant improvement compared to the vanilla train-\n",
            "ing strategy. M3ST even designs a two-stage fine-\n",
            "tuning method. However, few works have explored\n",
            "the extent of improvement gained by pre-training\n",
            "(Le et al., 2023), which is a high-cost method. Our\n",
            "IMTL, which dynamically decreases the weight of\n",
            "the auxiliary task and does not rely on fine-tuning,\n",
            "still achieves state-of-the-art (SOTA) performance.\n",
            "This proves that our method fixes the consistency\n",
            "during multi-task learning and further improves\n",
            "training efficiency. We have noticed that the newly\n",
            "proposed SOTA work implements teacher-forcing\n",
            "to bridge the modal gap, known as the knowledge\n",
            "distillation (KD) method. We further incorporate\n",
            "the KD method (Liu et al., 2019; Xu et al., 2021)\n",
            "into our IMTL, resulting in IMTL-KD. This demon-\n",
            "strates that our method is complementary to the KD\n",
            "method and achieves a new SOTA performance.\n",
            "We also compare our method with other works\n",
            "that utilize extra training data. The SOTA work\n",
            "SpeechUT aims to cover all speech-to-text tasks,\n",
            "thus it requires a significant amount of training\n",
            "resources (pre-training for 3 days with 32 GPUs)\n",
            "and a complicated training strategy. In contrast, our\n",
            "model achieves comparable or better performance\n",
            "with much fewer training resources (e.g., 1.5 days\n",
            "with 8 GPUs for the En-De task) and does not\n",
            "require fine-tuning. The building process is much\n",
            "simpler and more efficient.\n",
            "4.4 Effect of LBM\n",
            "We compare the effects of the shrinking (Liu et al.,\n",
            "2020; Dong et al., 2021a) and LBM methods in\n",
            "Table 3. Directly using the shrinking method doesModelsTraining time\n",
            "En-De En-Fr En-Es\n",
            "SpeechUT 96 Gd + 80k tuning steps\n",
            "IMTL 12 Gd 32 Gd 20Gd\n",
            "Table 5: A comparison of training cost with additional\n",
            "MT data. 1 Gd indicates that using one GPU training\n",
            "one day. The SpeechUT and IMTL use the V100 and\n",
            "3090 GPU respectively.\n",
            "not benefit the model, although it significantly re-\n",
            "duces the length of the sequence. However, after\n",
            "applying the LBM method, the model achieves a\n",
            "0.5 BLEU improvement while maintaining a low\n",
            "length ratio. This phenomenon demonstrates that\n",
            "shrinking alone is not stable, and the loss of infor-\n",
            "mation can lead to performance degradation. We\n",
            "find the average length of En-De audio is about two\n",
            "times the length of En-Fr audio, thus the shrinking\n",
            "effect is better.\n",
            "4.5 Effect of L2G\n",
            "We conducted an ablation study on L2G training,\n",
            "and the results are presented in Table 4. It shows\n",
            "that adding noise and constraining the field of in-\n",
            "formation interaction significantly improve the per-\n",
            "formance compared to the baseline. Furthermore,\n",
            "the method still performs well based on the LBM,\n",
            "which confirms the conclusion that compressed se-\n",
            "quences can learn additional information. When we\n",
            "apply the local-to-global strategy, the performance\n",
            "gains further improvement, which demonstrates\n",
            "that increasing the field size is more suitable for\n",
            "the goal of modal transformation.\n",
            "We also analyzed the changes in information\n",
            "entropy (IE) when applying different methods in\n",
            "Figure 10. We observed that the IE of the first\n",
            "MT layer is the highest since we add some noise\n",
            "to the embedding. Compared to the fixed method,\n",
            "the L2G method can learn more information in\n",
            "the middle layers of the model, indicating that a\n",
            "fixed size hinders the extraction of more global\n",
            "information. After employing the KD method, the\n",
            "IEs of all layers become more consistent with MT,\n",
            "except for the first noisy layer.\n",
            "4.6 Change of Task Weight\n",
            "We display the changes in task weights in Fig-\n",
            "ure 10. The weight of the ASR task decreases\n",
            "rapidly, while the weight of the MT task gradually\n",
            "decreases, slowly eliminating its impact on the ST\n",
            "task. This also aligns with the observed pattern ofgradient consistency in our analysis.\n",
            "We compare the training time in Table 5 and find\n",
            "that our method requires about 12.5% ∼33.3% of\n",
            "the training cost of SpeechUT on three MuST-C\n",
            "tasks. Additionally, our method does not require\n",
            "alignment with the fine-tuning stage on the ST task.\n",
            "This demonstrates the efficiency of our method.\n",
            "5 Related Work\n",
            "E2E ST has gained attention for its advantages over\n",
            "cascade systems in terms of reduced latency and\n",
            "error propagation (Bérard et al., 2016; Duong et al.,\n",
            "2016; Weiss et al., 2017; Xu et al., 2023a). How-\n",
            "ever, two main challenges hinder the adoption of\n",
            "E2E ST: 1) limited ST training data and 2) difficul-\n",
            "ties in modeling the modality gap. To address these\n",
            "challenges, pre-training strategies have emerged,\n",
            "including audio-only self-learning (Baevski et al.,\n",
            "2020; Hsu et al., 2021), joint audio-transcription en-\n",
            "coding (Ao et al., 2022; Zhang et al., 2022b; Chen\n",
            "et al., 2022), and combining MT and ASR data\n",
            "for pre-training (Wang et al., 2020c; Zheng et al.,\n",
            "2021). These approaches have shown significant\n",
            "improvements in ST performance.\n",
            "Pre-training methods are also combined with\n",
            "multi-stage and multi-task strategies. The multi-\n",
            "stage method involves pre-training all modules\n",
            "with auxiliary tasks, followed by integration and\n",
            "fine-tuning for the ST task (Xu et al., 2021; Li et al.,\n",
            "2021; Zhang et al., 2023). On the other hand, multi-\n",
            "task training utilizes multiple training objectives\n",
            "within a single model, eventually fine-tuning with\n",
            "the ST loss (Wang et al., 2020b; Le et al., 2020; Vy-\n",
            "dana et al., 2021; Tang et al., 2021; Ye et al., 2021).\n",
            "While most SOTA methods employ the pre-training\n",
            "and fine-tuning paradigm, few studies have inves-\n",
            "tigated the impact of other tasks on boosting the\n",
            "ST task, considering the time-consuming nature of\n",
            "pre-training. Tang et al. (2022) provided a simple\n",
            "analysis that showed gradient interference is not\n",
            "serious and the effectiveness of MTL. In this paper,\n",
            "we conduct a comprehensive experiment to explore\n",
            "the impact and time efficiency of other tasks.\n",
            "Mitigating differences in representation and ad-\n",
            "dressing variations in sequence lengths are two\n",
            "ways used to bridge the modality gap between\n",
            "text and speech. Some work proposes the use\n",
            "of adapters to reduce differences in pre-trained\n",
            "modules (Bahar et al., 2019; Li et al., 2021; Xu\n",
            "et al., 2021). Contrastive learning (Ye et al., 2022;\n",
            "Zhang et al., 2023) and knowledge distillation tech-niques are also employed to achieve this objective\n",
            "(Fang et al., 2022; Zhou et al., 2023; Fang and\n",
            "Feng, 2023). Furthermore, the mixing-up of two\n",
            "modal representations has been found to be effec-\n",
            "tive (Cheng et al., 2022). The inclusion of blank\n",
            "tokens (Wang et al., 2020b; Zhang et al., 2023) can\n",
            "improve denoising capabilities. To address length\n",
            "inconsistencies, shrinking based on ASR prediction\n",
            "or cluster methods have been utilized (Dong et al.,\n",
            "2021a; Zhang et al., 2022b).\n",
            "6 Conclusion\n",
            "Most advanced ST methods heavily rely on multi-\n",
            "task learning, but few studies focus on the rela-\n",
            "tionship between auxiliary tasks and the ST task\n",
            "itself. In this study, we design a gradient consis-\n",
            "tency metric to analyze the impact of other tasks\n",
            "on the ST task during the multi-task learning pro-\n",
            "cess. Based on our analysis, we propose improved\n",
            "methods that address three key aspects: length, rep-\n",
            "resentation, and training efficiency. Experimental\n",
            "results on the MuST-C dataset demonstrate that our\n",
            "approach achieves state-of-the-art performance and\n",
            "significantly improves training efficiency.\n",
            "Acknowledgement\n",
            "This work was supported in part by the National\n",
            "Science Foundation of China (No.62276056), the\n",
            "National Key R&D Program of China, the China\n",
            "HTRD Center Project (No.2020AAA0107904), the\n",
            "Natural Science Foundation of Liaoning Province\n",
            "of China (2022-KF-16-01), the Yunnan Provin-\n",
            "cial Major Science and Technology Special Plan\n",
            "Projects (No.202103AA080015), the Fundamental\n",
            "Research Funds for the Central Universities (Nos.\n",
            "N2216016, N2216001, and N2216002), and the\n",
            "Program of Introducing Talents of Discipline to\n",
            "Universities, Plan 111 (No.B16009). The authors\n",
            "would like to thank anonymous reviewers for their\n",
            "insightful comments.\n",
            "Limitations\n",
            "There are some limitations that our work has not\n",
            "figured out. The analysis is mainly carried out on\n",
            "the MuST-C dataset, where the training data size\n",
            "is not large. We did not apply the state-of-the-\n",
            "art knowledge distillation (KD) method to further\n",
            "improve performance. The effect of knowledge\n",
            "distillation based on IMTL has not been sufficiently\n",
            "investigated.References\n",
            "Antonios Anastasopoulos and David Chiang. 2018.\n",
            "Tied multitask learning for neural speech translation.\n",
            "InProceedings of the 2018 Conference of the North\n",
            "American Chapter of the Association for Computa-\n",
            "tional Linguistics: Human Language Technologies,\n",
            "Volume 1 (Long Papers) , pages 82–91, New Orleans,\n",
            "Louisiana. Association for Computational Linguis-\n",
            "tics.\n",
            "Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo\n",
            "Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang,\n",
            "Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei.\n",
            "2022. SpeechT5: Unified-modal encoder-decoder\n",
            "pre-training for spoken language processing. In Pro-\n",
            "ceedings of the 60th Annual Meeting of the Associa-\n",
            "tion for Computational Linguistics (Volume 1: Long\n",
            "Papers) , pages 5723–5738, Dublin, Ireland. Associa-\n",
            "tion for Computational Linguistics.\n",
            "Andreas Argyriou, Theodoros Evgeniou, and Massimil-\n",
            "iano Pontil. 2008. Convex multi-task feature learning.\n",
            "Machine learning , 73:243–272.\n",
            "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\n",
            "and Michael Auli. 2020. wav2vec 2.0: A framework\n",
            "for self-supervised learning of speech representations.\n",
            "InAdvances in Neural Information Processing Sys-\n",
            "tems, volume 33, pages 12449–12460.\n",
            "Parnia Bahar, Tobias Bieschke, and Hermann Ney. 2019.\n",
            "A comparative study on end-to-end speech to text\n",
            "translation. In 2019 IEEE Automatic Speech Recog-\n",
            "nition and Understanding Workshop (ASRU) , pages\n",
            "792–799. IEEE.\n",
            "Alexandre Bérard, Olivier Pietquin, Christophe Servan,\n",
            "and Laurent Besacier. 2016. Listen and translate: A\n",
            "proof of concept for end-to-end speech-to-text trans-\n",
            "lation. arXiv preprint arXiv:1612.01744 .\n",
            "Sanyuan Chen, Chengyi Wang, Zhengyang Chen,\n",
            "Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\n",
            "Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long\n",
            "Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,\n",
            "Michael Zeng, Xiangzhan Yu, and Furu Wei. 2022.\n",
            "Wavlm: Large-scale self-supervised pre-training for\n",
            "full stack speech processing. IEEE Journal of Se-\n",
            "lected Topics in Signal Processing , 16(6):1505–1518.\n",
            "Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and\n",
            "Andrew Rabinovich. 2018. Gradnorm: Gradient\n",
            "normalization for adaptive loss balancing in deep\n",
            "multitask networks. In International conference on\n",
            "machine learning , pages 794–803. PMLR.\n",
            "Xuxin Cheng, Qianqian Dong, Fengpeng Yue, Tom Ko,\n",
            "Mingxuan Wang, and Yuexian Zou. 2022. M3st:\n",
            "Mix at three levels for speech translation. ArXiv ,\n",
            "abs/2212.03657.\n",
            "François Chollet. 2016. Xception: Deep learning with\n",
            "depthwise separable convolutions. 2017 IEEE Con-\n",
            "ference on Computer Vision and Pattern Recognition\n",
            "(CVPR) , pages 1800–1807.Jean-Antoine Désidéri. 2012. Multiple-gradient descent\n",
            "algorithm (mgda) for multiobjective optimization.\n",
            "Comptes Rendus Mathematique , 350(5-6):313–318.\n",
            "Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli,\n",
            "Matteo Negri, and Marco Turchi. 2019. MuST-C: a\n",
            "Multilingual Speech Translation Corpus. In Proceed-\n",
            "ings of the 2019 Conference of the North American\n",
            "Chapter of the Association for Computational Lin-\n",
            "guistics: Human Language Technologies, Volume 1\n",
            "(Long and Short Papers) , pages 2012–2017, Min-\n",
            "neapolis, Minnesota. Association for Computational\n",
            "Linguistics.\n",
            "Qianqian Dong, Mingxuan Wang, Hao Zhou, Shuang\n",
            "Xu, Bo Xu, and Lei Li. 2021a. Consecutive decod-\n",
            "ing for speech-to-text translation. In The Thirty-fifth\n",
            "AAAI Conference on Artificial Intelligence, AAAI .\n",
            "Qianqian Dong, Rong Ye, Mingxuan Wang, Hao Zhou,\n",
            "Shuang Xu, Bo Xu, and Lei Li. 2021b. “listen, un-\n",
            "derstand and translate”: Triple supervision decouples\n",
            "end-to-end speech-to-text translation. In Proceedings\n",
            "of the AAAI Conference on Artificial Intelligence , vol-\n",
            "ume 35, pages 12749–12759.\n",
            "Long Duong, Antonios Anastasopoulos, David Chiang,\n",
            "Steven Bird, and Trevor Cohn. 2016. An attentional\n",
            "model for speech translation without transcription.\n",
            "InProceedings of the 2016 Conference of the North\n",
            "American Chapter of the Association for Computa-\n",
            "tional Linguistics: Human Language Technologies ,\n",
            "pages 949–959.\n",
            "Qingkai Fang and Yang Feng. 2023. Understanding\n",
            "and bridging the modality gap for speech translation.\n",
            "arXiv preprint arXiv:2305.08706 .\n",
            "Qingkai Fang, Rong Ye, Lei Li, Yang Feng, and\n",
            "Mingxuan Wang. 2022. STEMM: Self-learning with\n",
            "speech-text manifold mixup for speech translation.\n",
            "InProceedings of the 60th Annual Meeting of the\n",
            "Association for Computational Linguistics (Volume\n",
            "1: Long Papers) , pages 7050–7062, Dublin, Ireland.\n",
            "Association for Computational Linguistics.\n",
            "Alex Graves, Santiago Fernández, Faustino Gomez, and\n",
            "Jürgen Schmidhuber. 2006. Connectionist temporal\n",
            "classification: labelling unsegmented sequence data\n",
            "with recurrent neural networks. In Proceedings of the\n",
            "23rd international conference on Machine learning ,\n",
            "pages 369–376.\n",
            "Chi Han, Mingxuan Wang, Heng Ji, and Lei Li. 2021.\n",
            "Learning shared semantic space for speech-to-text\n",
            "translation. In Findings of the Association for Com-\n",
            "putational Linguistics: ACL-IJCNLP 2021 , pages\n",
            "2214–2225, Online. Association for Computational\n",
            "Linguistics.\n",
            "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\n",
            "Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-\n",
            "rahman Mohamed. 2021. Hubert: Self-supervised\n",
            "speech representation learning by masked prediction\n",
            "of hidden units. arXiv preprint arXiv:2106.07447 .Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier\n",
            "Schwab, and Laurent Besacier. 2020. Dual-decoder\n",
            "transformer for joint automatic speech recognition\n",
            "and multilingual speech translation. In Proceed-\n",
            "ings of the 28th International Conference on Com-\n",
            "putational Linguistics , pages 3520–3533, Barcelona,\n",
            "Spain (Online). International Committee on Compu-\n",
            "tational Linguistics.\n",
            "Phuong-Hang Le, Hongyu Gong, Changhan Wang, Juan\n",
            "Pino, Benjamin Lecouteux, and Didier Schwab. 2023.\n",
            "Pre-training for speech translation: Ctc meets opti-\n",
            "mal transport. In 40th International Conference on\n",
            "Machine Learning .\n",
            "Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing\n",
            "Tang, Juan Pino, Alexei Baevski, Alexis Conneau,\n",
            "and Michael Auli. 2021. Multilingual speech trans-\n",
            "lation from efficient finetuning of pretrained models.\n",
            "InProceedings of the 59th Annual Meeting of the As-\n",
            "sociation for Computational Linguistics and the 11th\n",
            "International Joint Conference on Natural Language\n",
            "Processing (Volume 1: Long Papers) , pages 827–838,\n",
            "Online. Association for Computational Linguistics.\n",
            "Yuchen Liu, Hao Xiong, Jiajun Zhang, Zhongjun He,\n",
            "Hua Wu, Haifeng Wang, and Chengqing Zong. 2019.\n",
            "End-to-End Speech Translation with Knowledge Dis-\n",
            "tillation. In Proc. Interspeech 2019 , pages 1128–\n",
            "1132.\n",
            "Yuchen Liu, Junnan Zhu, Jiajun Zhang, and Chengqing\n",
            "Zong. 2020. Bridging the modality gap for speech-\n",
            "to-text translation. arXiv preprint arXiv:2010.14920 .\n",
            "Andreas Maurer, Massi Pontil, and Bernardino Romera-\n",
            "Paredes. 2013. Sparse coding for multitask and trans-\n",
            "fer learning. In International conference on machine\n",
            "learning , pages 343–351. PMLR.\n",
            "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\n",
            "Sam Gross, Nathan Ng, David Grangier, and Michael\n",
            "Auli. 2019. fairseq: A fast, extensible toolkit for\n",
            "sequence modeling. In Proceedings of NAACL-HLT\n",
            "2019: Demonstrations .\n",
            "Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-\n",
            "jeev Khudanpur. 2015. Librispeech: An asr corpus\n",
            "based on public domain audio books. In 2015 IEEE\n",
            "International Conference on Acoustics, Speech and\n",
            "Signal Processing (ICASSP) , pages 5206–5210.\n",
            "Matt Post. 2018. A call for clarity in reporting BLEU\n",
            "scores. In Proceedings of the Third Conference on\n",
            "Machine Translation: Research Papers , pages 186–\n",
            "191, Brussels, Belgium. Association for Computa-\n",
            "tional Linguistics.\n",
            "Ozan Sener and Vladlen Koltun. 2018. Multi-task learn-\n",
            "ing as multi-objective optimization. Advances in\n",
            "neural information processing systems , 31.\n",
            "Yun Tang, Hongyu Gong, Ning Dong, Changhan Wang,\n",
            "Wei-Ning Hsu, Jiatao Gu, Alexei Baevski, Xian Li,\n",
            "Abdelrahman Mohamed, Michael Auli, and Juan\n",
            "Pino. 2022. Unified speech-text pre-training forspeech translation and recognition. In Proceedings\n",
            "of the 60th Annual Meeting of the Association for\n",
            "Computational Linguistics (Volume 1: Long Papers) ,\n",
            "pages 1488–1499, Dublin, Ireland. Association for\n",
            "Computational Linguistics.\n",
            "Yun Tang, Juan Pino, Changhan Wang, Xutai Ma, and\n",
            "Dmitriy Genzel. 2021. A general multi-task learning\n",
            "framework to leverage text data for speech to text\n",
            "tasks. In ICASSP 2021 - 2021 IEEE International\n",
            "Conference on Acoustics, Speech and Signal Process-\n",
            "ing (ICASSP) , pages 6209–6213.\n",
            "Hari Krishna Vydana, Martin Karafiát, Katerina Zmo-\n",
            "likova, Lukáš Burget, and Honza ˇCernocký. 2021.\n",
            "Jointly trained transformers models for spoken lan-\n",
            "guage translation. In ICASSP 2021 - 2021 IEEE\n",
            "International Conference on Acoustics, Speech and\n",
            "Signal Processing (ICASSP) , pages 7513–7517.\n",
            "Changhan Wang, Yun Tang, Xutai Ma, Anne Wu,\n",
            "Dmytro Okhonko, and Juan Pino. 2020a. Fairseq\n",
            "S2T: Fast speech-to-text modeling with fairseq. In\n",
            "Proceedings of the 1st Conference of the Asia-Pacific\n",
            "Chapter of the Association for Computational Lin-\n",
            "guistics and the 10th International Joint Conference\n",
            "on Natural Language Processing: System Demon-\n",
            "strations , pages 33–39, Suzhou, China. Association\n",
            "for Computational Linguistics.\n",
            "Chengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, and\n",
            "Ming Zhou. 2020b. Bridging the gap between pre-\n",
            "training and fine-tuning for end-to-end speech trans-\n",
            "lation. In Proceedings of the AAAI Conference on\n",
            "Artificial Intelligence , volume 34, pages 9161–9168.\n",
            "Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, and\n",
            "Zhenglu Yang. 2020c. Curriculum pre-training for\n",
            "end-to-end speech translation. In Proceedings of the\n",
            "58th Annual Meeting of the Association for Compu-\n",
            "tational Linguistics , pages 3728–3738, Online. Asso-\n",
            "ciation for Computational Linguistics.\n",
            "Ron J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui\n",
            "Wu, and Zhifeng Chen. 2017. Sequence-to-sequence\n",
            "models can directly translate foreign speech. arXiv\n",
            "preprint arXiv:1703.08581 .\n",
            "Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, Shen\n",
            "Huang, Qi Ju, Tong Xiao, and Jingbo Zhu. 2021.\n",
            "Stacked acoustic-and-textual encoding: Integrating\n",
            "the pre-trained models into speech translation en-\n",
            "coders. In Proceedings of the 59th Annual Meet-\n",
            "ing of the Association for Computational Linguistics\n",
            "and the 11th International Joint Conference on Natu-\n",
            "ral Language Processing (Volume 1: Long Papers) ,\n",
            "pages 2619–2630, Online. Association for Computa-\n",
            "tional Linguistics.\n",
            "Chen Xu, Rong Ye, Qianqian Dong, Chengqi Zhao,\n",
            "Tom Ko, Mingxuan Wang, Tong Xiao, and Jingbo\n",
            "Zhu. 2023a. Recent advances in direct speech-to-\n",
            "text translation. In Proceedings of the Thirty-Second\n",
            "International Joint Conference on Artificial Intel-\n",
            "ligence, IJCAI-23 , pages 6796–6804. InternationalJoint Conferences on Artificial Intelligence Organi-\n",
            "zation. Survey Track.\n",
            "Chen Xu, Yuhao Zhang, Chengbo Jiao, Xiaoqian Liu,\n",
            "Chi Hu, Xin Zeng, Tong Xiao, Anxiang Ma, Huizhen\n",
            "Wang, and JingBo Zhu. 2023b. Bridging the gran-\n",
            "ularity gap for acoustic modeling. arXiv preprint\n",
            "arXiv:2305.17356 .\n",
            "Rong Ye, Mingxuan Wang, and Lei Li. 2021. End-to-\n",
            "End Speech Translation via Cross-Modal Progressive\n",
            "Training. In Proc. Interspeech 2021 , pages 2267–\n",
            "2271.\n",
            "Rong Ye, Mingxuan Wang, and Lei Li. 2022. Cross-\n",
            "modal contrastive learning for speech translation. In\n",
            "Proceedings of the 2022 Conference of the North\n",
            "American Chapter of the Association for Computa-\n",
            "tional Linguistics: Human Language Technologies ,\n",
            "pages 5099–5113, Seattle, United States. Association\n",
            "for Computational Linguistics.\n",
            "Biao Zhang, Barry Haddow, and Rico Sennrich. 2022a.\n",
            "Revisiting end-to-end speech-to-text translation from\n",
            "scratch. In International Conference on Machine\n",
            "Learning , pages 26193–26205. PMLR.\n",
            "Yuhao Zhang, Chen Xu, Bojie Hu, Chunliang Zhang,\n",
            "Tong Xiao, and Jingbo Zhu. 2023. Improving end-to-\n",
            "end speech translation by leveraging auxiliary speech\n",
            "and text data. In Proceedings of the AAAI Conference\n",
            "on Artificial Intelligence , volume 37, pages 13984–\n",
            "13992.\n",
            "Ziqiang Zhang, Long Zhou, Junyi Ao, Shujie Liu,\n",
            "Lirong Dai, Jinyu Li, and Furu Wei. 2022b.\n",
            "SpeechUT: Bridging speech and text with hidden-\n",
            "unit for encoder-decoder based speech-text pre-\n",
            "training. In Proceedings of the 2022 Conference on\n",
            "Empirical Methods in Natural Language Processing ,\n",
            "pages 1663–1676, Abu Dhabi, United Arab Emirates.\n",
            "Association for Computational Linguistics.\n",
            "Renjie Zheng, Junkun Chen, Mingbo Ma, and Liang\n",
            "Huang. 2021. Fused acoustic and text encoding for\n",
            "multimodal bilingual pretraining and speech transla-\n",
            "tion. arXiv preprint arXiv:2102.05766 .\n",
            "Yan Zhou, Qingkai Fang, and Yang Feng. 2023. Cmot:\n",
            "Cross-modal mixup via optimal transport for speech\n",
            "translation. arXiv preprint arXiv:2305.14635 .Appendix\n",
            "A Data Details\n",
            "We conducted experiments on the multilingual\n",
            "MuST-C dataset (Di Gangi et al., 2019). The de-\n",
            "tail of the data is shown in Table 6. The detail of\n",
            "additional data is shown in Table 7.\n",
            "Language Hours(h) Sentence(K)\n",
            "En-De 408 234\n",
            "En-Es 504 270\n",
            "En-Fr 492 280\n",
            "En-It 465 258\n",
            "En-Nl 442 253\n",
            "En-Pt 385 211\n",
            "En-Ro 432 240\n",
            "En-Ru 489 207\n",
            "Table 6: Training data size of the MuST-C 8 languages.\n",
            "Dateset Language Sentence\n",
            "WMT16 En-De 3.9M\n",
            "WMT13 En-Es 14.2M\n",
            "WMT14 En-Fr 31.2M\n",
            "LibriSpeeh 100h En 28.5K\n",
            "Table 7: Training data size of additional MT and ASR\n",
            "data.\n",
            "B Contrastive Loss\n",
            "In this paragraph, we introduce the notation and\n",
            "define the loss function for contrastive training. We\n",
            "start by defining two outputs: A(s)represents the\n",
            "output of the ST encoder when given the speech\n",
            "input s, andM(x)represents the output of the pre-\n",
            "trained text encoder when given the transcription x.\n",
            "We then consider a set of training samples denoted\n",
            "as(si, xi).\n",
            "The loss function for contrastive training, de-\n",
            "noted as LCL, is defined as follows:\n",
            "LCL=−X\n",
            "(si,xi)logeπ(A(si),M(xi))/τ\n",
            "P\n",
            "xj:j̸=ieπ(A(si),M(xj))/τ\n",
            "(10)\n",
            "In this equation, π(·,·)is a function that com-\n",
            "putes the similarity between the input vectors. For\n",
            "our purposes, we choose the cosine function as4 8 12 16 20 24 285.05.56.06.5\n",
            "Training steps(K)PPLBaseline\n",
            "IMTL\n",
            "Figure 11: PPL of Dev set during training.\n",
            "π(·,·)and apply average pooling to the two se-\n",
            "quence representations. The variable τis a scaler\n",
            "that controls the sharpness of the function output,\n",
            "and in this case, we set τto 0.1.\n",
            "For each speech input si, we have its correspond-\n",
            "ing labeled transcription xi, which forms a positive\n",
            "sample (si, xi). Additionally, we utilize transcrip-\n",
            "tions other than xi(denoted as xjforj̸=i) to\n",
            "create negative samples.\n",
            "C Information Entropy\n",
            "Information entropy is a concept from informa-\n",
            "tion theory that measures the average amount of\n",
            "information contained in a set of data or the un-\n",
            "certainty associated with the data. In the context\n",
            "of information theory, entropy is calculated using\n",
            "the probabilities of different outcomes or events\n",
            "occurring within a system. The higher the entropy,\n",
            "the greater the uncertainty or lack of information\n",
            "about the outcomes. Conversely, lower entropy\n",
            "indicates a higher degree of predictability or knowl-\n",
            "edge about the outcomes. The formula is given\n",
            "by:\n",
            "H(X) =−X\n",
            "p(x)∗log2(p(x)) (11)\n",
            "where H(X)represents the entropy of a random\n",
            "variable X,P(x)is the probability of each possible\n",
            "outcome x, and the sum is taken over all possible\n",
            "outcomes.\n",
            "D Coverage Speed\n",
            "Figure 11 shows the coverage speeds of the base-\n",
            "line and our IMTL. We can find the IMTL is better\n",
            "in terms of convergence speed and effect.\n",
            "E Training Speed\n",
            "There are mainly three tasks (ASR, MT, and ST)\n",
            "during the training strategy. Our Improved Multi-\n",
            "Task Learning (IMTL) algorithm dynamically ad-\n",
            "justs the training weights assigned to the auxiliaryTraining task(s) Speed (Seconds/Epoch)\n",
            "ST, MT, ASR ∼1187\n",
            "ST, MT ∼936\n",
            "ST ∼675\n",
            "Table 8: Training data size of additional MT and ASR\n",
            "data.\n",
            "ASR and MT tasks. Specifically, any auxiliary task\n",
            "whose training weight diminishes below a thresh-\n",
            "old of 0.1 will be effectively halted to optimize the\n",
            "training process. As a bonus, subsequent training\n",
            "phases are computationally more efficient than the\n",
            "standard approach, given that both the forward and\n",
            "backward computations are integrated components\n",
            "of the overall training pipeline. Table 8 shows a\n",
            "rough estimate of the training speed of our IMTL\n",
            "approach on the MuST-C dataset with different\n",
            "training tasks.Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task\n",
            "Learning with Auxiliary Mutual Information Maximization\n",
            "Cam-Van Thi Nguyen, Ngoc-Hoa Thi Nguyen,\n",
            "Duc-Trong Le, Quang-Thuy Ha\n",
            "Faculty of Information Technology\n",
            "VNU - University of Engineering and Technology, Hanoi, Vietnam\n",
            "Abstract\n",
            "Multimodal representation learning poses sig-\n",
            "nificant challenges in capturing informative and\n",
            "distinct features from multiple modalities. Ex-\n",
            "isting methods often struggle to exploit the\n",
            "unique characteristics of each modality due to\n",
            "unified multimodal annotations. In this study,\n",
            "we propose Self-MI in the self-supervised learn-\n",
            "ing fashion, which also leverage Contrastive\n",
            "Predictive Coding (CPC) as an auxiliary tech-\n",
            "nique to maximize the Mutual Information\n",
            "(MI) between unimodal input pairs and the\n",
            "multimodal fusion result with unimodal inputs.\n",
            "Moreover, we design a label generation mod-\n",
            "ule,ULG MIfor short, that enables us to cre-\n",
            "ate meaningful and informative labels for each\n",
            "modality in a self-supervised manner. By max-\n",
            "imizing the Mutual Information, we encour-\n",
            "age better alignment between the multimodal\n",
            "fusion and the individual modalities, facilitat-\n",
            "ing improved multimodal fusion. Extensive ex-\n",
            "periments on three benchmark datasets includ-\n",
            "ing CMU-MOSI, CMU-MOSEI, and SIMS,\n",
            "demonstrate the effectiveness of Self-MI in en-\n",
            "hancing the multimodal fusion task.\n",
            "1 Introduction\n",
            "Multimodal sentiment analysis (MSA) has emerged\n",
            "as a prominent research area, garnering significant\n",
            "attention in recent years (Bagher Zadeh et al., 2018;\n",
            "Poria et al., 2023). It demonstrates remarkable en-\n",
            "hancements compared to the traditional unimodal\n",
            "analysis. Real-life multimodal data typically in-\n",
            "volves three distinct channels: visual, acoustic, and\n",
            "textual. Effective representation and information\n",
            "fusion from these diverse sources are essential com-\n",
            "ponents in MSA.\n",
            "In multimodal learning, Baltrušaitis et al. (2018)\n",
            "highlight five fundamental challenges including\n",
            "alignment, translation, representation, fusion, and\n",
            "co-learning. Among these challenges, representa-\n",
            "tion learning is a significant and challenging task.\n",
            "The goal is to obtain effective representations thatcapture both the commonalities and distinctions\n",
            "across different modalities. An effective represen-\n",
            "tation should include two key aspects: consistency\n",
            "and differentiation (Poria et al., 2020). The repre-\n",
            "sentations should be consistent across modalities\n",
            "to enable seamless integration and analysis of in-\n",
            "formation from various sources. Equally important,\n",
            "it should also capture the unique characteristics of\n",
            "each modality to facilitate a comprehensive under-\n",
            "standing of the data.\n",
            "Yu et al. (2021) point out the shortcomings of\n",
            "current methods in capturing distinctive informa-\n",
            "tion due to their reliance on unified multimodal\n",
            "annotation. However, incorporating additional uni-\n",
            "modal annotations can be burdensome in terms\n",
            "of time and human resource. As a solution for\n",
            "this challenge, Yu et al. (2021) introduce a novel\n",
            "approach named Self-MM. The main idea is to\n",
            "incorporate a measure based on the distance be-\n",
            "tween modality representations and the class cen-\n",
            "troids, which correlates positively with the model’s\n",
            "output. In order to generate unimodal labels, the\n",
            "authors implement a self-supervised learning strat-\n",
            "egy, of which the absolute distance calculation be-\n",
            "tween two modality representations in different\n",
            "spaces is intractable. Dealing with this problem,\n",
            "Han et al. (2021) propose MultiModal InfoMax\n",
            "(MMIM), which maximize the shared information\n",
            "in multimodal fusion via enhancing two types of\n",
            "mutual information: between unimodal represen-\n",
            "tations and between fusion embeddings and uni-\n",
            "modal representations.\n",
            "Building upon the insights from MMIM (Han\n",
            "et al., 2021) and addressing the limitations of Self-\n",
            "MM (Yu et al., 2021), this paper presents a novel\n",
            "approach named Self-MI that computes the simi-\n",
            "larity between modality representations using the\n",
            "maximize mutual information technique, while also\n",
            "incorporating self-supervised learning for the MSA\n",
            "task. Additionally, designing a multitask learning\n",
            "model using the hard parameter sharing strategyarXiv:2311.03785v1  [cs.CV]  7 Nov 2023allows us to gain a comprehensive understanding\n",
            "of how unimodal tasks impact multimodal tasks.\n",
            "To be clear, our contributions can be summarized\n",
            "as the following:\n",
            "•We propose an unified model called Self-MI,\n",
            "which is based on Self-supervised learning\n",
            "and Multi-task Learning with the auxiliary of\n",
            "Mutual Information (MI).\n",
            "•We design a module named Mutual Informa-\n",
            "tion Maximization for Unimodal Labels Gen-\n",
            "eration ( ULG MI) that utilizes the Contrastive\n",
            "Predictive Coding (CPC) method to estimate\n",
            "the correlation between multimodal represen-\n",
            "tations.\n",
            "•We perform extensive experiments on three\n",
            "benchmark datasets including CMU-MOSI,\n",
            "CMU-MOSEI, and CMU-MOSEI. Our model\n",
            "results in superior or comparable results com-\n",
            "pared to state-of-the-art models.\n",
            "2 Related Works\n",
            "In this section, we literately review several classes\n",
            "of previous research works related to the multi-\n",
            "modal sentiment analysis task and our proposed\n",
            "model Self-MI.\n",
            "2.1 Multimodal Sentiment Analysis\n",
            "Multimodal Sentiment Analysis (MSA) is a com-\n",
            "prehensive approach that combines verbal and non-\n",
            "verbal features to perform user sentiment analysis.\n",
            "This field focuses on extracting emotions, interpre-\n",
            "tations, and feelings by analyzing various sources\n",
            "such as language, facial expressions, speech, mu-\n",
            "sic, and movements (Kaur and Kautish, 2022). The\n",
            "field of multimodal sentiment analysis commonly\n",
            "utilizes various public datasets such as CMU-MOSI\n",
            "(Zadeh et al., 2016), CMU-MOSEI (Bagher Zadeh\n",
            "et al., 2018), CH-SIMS (Yu et al., 2020), IEMO-\n",
            "CAP (Busso et al., 2008), and several others.\n",
            "MSA research could be divided into four groups:\n",
            "1) early multimodal fusion methods like Tensor Fu-\n",
            "sion Network TFN (Zadeh et al., 2017), Low-rank\n",
            "Multimodal Fusion LMF (Liu et al., 2018), and\n",
            "Multimodal Factorization Model MFM (Tsai et al.,\n",
            "2019b), and 2) the methods that fuse multimodal-\n",
            "ity through modeling modality interaction , such as\n",
            "multimodal Transformer MulT (Tsai et al., 2019a)\n",
            "and modal-temporal attention graph MTAG (Yang\n",
            "et al., 2021) and 3) the methods focusing on the con-\n",
            "sistency and the difference of modality , in whichMISA (Hazarika et al., 2020), Self-MM (Yu et al.,\n",
            "2021), and MMIM (Han et al., 2021). Our work\n",
            "focuses on the third group of methods, in line with\n",
            "previous studies (Yu et al., 2021; Han et al., 2021).\n",
            "We also use these models as the baseline models\n",
            "in this study. We provide detailed descriptions of\n",
            "these models in the corresponding Section 4.\n",
            "2.2 Self-Supervised Multimodal Learning\n",
            "Multimodal learning, which seeks to comprehend\n",
            "and analyze information from various modalities,\n",
            "has made significant advancements in the super-\n",
            "vised regime in recent years. However, the reliance\n",
            "on paired data and costly human annotations hin-\n",
            "ders the scalability of models. On the other hand,\n",
            "with the abundance of unannotated data available in\n",
            "the wild, self-supervised learning has emerged as\n",
            "an appealing strategy to address the annotation bot-\n",
            "tleneck (Zong et al., 2023). The objective functions\n",
            "for training self-supervised multimodal algorithms\n",
            "can be categorized into three main types: instance\n",
            "discrimination, clustering, and masked prediction.\n",
            "In this study, our model belongs to the category\n",
            "of instance discrimination, specifically contrastive\n",
            "learning. By leveraging the ULGM module in Self-\n",
            "MM (Yu et al., 2021), we re-design this module\n",
            "and name it ULG MI(Mutual Information Maxi-\n",
            "mization for Unimodal Labels Generation). The\n",
            "purpose of this module is to generate uni-modal su-\n",
            "pervision values using multimodal annotations and\n",
            "modality representations. In contrast to Self-MM\n",
            "(Yu et al., 2021), we replace the Relative Distance\n",
            "Value, which evaluates the relative distance from\n",
            "the modality representation to the positive center\n",
            "and the negative center, with the computation of\n",
            "Mutual Information Maximization, utilizing the\n",
            "auxiliary of Contrastive Predictive Coding (CPC)\n",
            "(Oord et al., 2018a).\n",
            "2.3 Multi-task Learning\n",
            "Multi-task learning aims to enhance the generaliza-\n",
            "tion performance of multiple related tasks by lever-\n",
            "aging the knowledge from different tasks (Zhang\n",
            "and Yang, 2021). In contrast to single-task learning,\n",
            "multi-task learning faces two primary challenges\n",
            "during the training stage. The first challenge is how\n",
            "to effectively share network parameters, which can\n",
            "be achieved through hard-sharing or soft-sharing\n",
            "methods. The second challenge is to balance the\n",
            "learning process across different tasks. Multi-task\n",
            "learning has been widely applied in multimodal\n",
            "sentiment analysis (MSA) (Akhtar et al., 2019).In this work, we propose the incorporation of uni-\n",
            "modal subtasks to assist in the process of learning\n",
            "modality-specific representations.\n",
            "2.4 Mutual Information\n",
            "In probability theory and information theory, Mu-\n",
            "tual Information (MI) is a measure of the depen-\n",
            "dence between two random variables. It quantifies\n",
            "how much knowing the value of one variable re-\n",
            "duces uncertainty about the value of the other vari-\n",
            "able. In the context of machine learning, maximiz-\n",
            "ing mutual information can help to learn informa-\n",
            "tive representations and improve the performance\n",
            "of tasks such as classification and generation.\n",
            "I(X;Y) =Ep(x,y)[logp(x, y)\n",
            "p(x)p(y)] (1)\n",
            "Alemi et al. (2016) (Alemi et al., 2016) were the\n",
            "pioneers in integrating optimization techniques re-\n",
            "lated to mutual information into deep learning mod-\n",
            "els. Since then, numerous studies (Amjad and\n",
            "Geiger, 2019; He et al., 2020) have investigated\n",
            "and demonstrated the advantages of maximizing\n",
            "mutual information principles in various contexts.\n",
            "However, estimating mutual information directly\n",
            "in high-dimensional spaces is often considered im-\n",
            "practical or challenging.\n",
            "3 Methodology\n",
            "In this section, we present the overall architecture\n",
            "of Self-MI as well as its mathematical formulation\n",
            "for the multimodal sentiment analysis task.\n",
            "3.1 Task setup\n",
            "A MSA model takes unimodal raw sequences\n",
            "Xm∈Rlm×dmfrom the same video fragment as\n",
            "input, where lmis the sequence length, and dm\n",
            "is the dimension of the representation vector for\n",
            "modality m. Here, we consider three modalities\n",
            "denoted as m∈ {t, a, v}, representing text, visual,\n",
            "and acoustic modalities respectively. The main ob-\n",
            "jective of Self-MI is to integrate information from\n",
            "these input vectors, and create a unified represen-\n",
            "tation to accurately predict the sentiment intensity\n",
            "reflected via the truth value y.\n",
            "The overall architecture of Self-MI is illustrated\n",
            "in Figure 1. The model comprises one multimodal\n",
            "task and three separate unimodal subtasks. Be-\n",
            "tween the multimodal task and the unimodal tasks,\n",
            "we exploit the MI (Mutual Information) maximiza-\n",
            "tion with Contrastive Predictive Coding (CPC) as\n",
            "the fusion module in our model.3.2 Multimodal Representation\n",
            "The multimodal sentiment analysis task is tackled\n",
            "as a classification model, which consists of three\n",
            "key components: (1) The feature representation\n",
            "module , (2) The feature fusion module , and (3)\n",
            "The classification module .\n",
            "For text modality, the sequence is denoted as\n",
            "T={wC, w0, . . . , w i, . . . , w S}, where wCand\n",
            "wSrepresent the special tokens [CLS] and [SEP] re-\n",
            "spectively. The pre-trained 12-layers BERT model\n",
            "is utilized to extract the global text representation\n",
            "and the local text representation. Empirically, the\n",
            "first-word vector in the last layer is identified as\n",
            "the optimal representation for the entire sentence\n",
            "denoted as Xt:\n",
            "Xt=BERT (T;θbert\n",
            "t) (2)\n",
            "where Xt={xt\n",
            "C, xt\n",
            "0, . . . , ht\n",
            "i, . . . , ht\n",
            "S},Xt∈\n",
            "Rlt×dt;ltis the max length of the text sequence;\n",
            "anddtis the dimension of the text representation.\n",
            "For audio and visual modality, we use a pre-\n",
            "trained ToolKits to extract the initial vector fea-\n",
            "tures, i.e., A and V . Inspired by previous works\n",
            "(Hazarika et al., 2020; Yu et al., 2021), a single\n",
            "directional Long Short-Term Memory (sLSTM)\n",
            "model (Hochreiter and Schmidhuber, 1997) is em-\n",
            "ployed to capture temporal characteristics. The\n",
            "final step adopts the end-state hidden vectors as the\n",
            "comprehensive sequence representations:\n",
            "Xa=sLSTM (A;θlstm\n",
            "a)∈Rda(3)\n",
            "Xv=sLSTM (V;θlstm\n",
            "v)∈Rdv(4)\n",
            "where Xa={xa\n",
            "0, . . . , ha\n",
            "j, . . . , ha\n",
            "la},Xa∈\n",
            "Rla×da,Xv={xv\n",
            "0, . . . , hv\n",
            "k, . . . , hv\n",
            "lv},Xv∈\n",
            "Rlv×dv;laandlvare the sequence length of au-\n",
            "dio and vision, respectively.\n",
            "Subsequently, all uni-modal representations\n",
            "are concatenated and projected into a lower-\n",
            "dimensional space Rdm.\n",
            "Xm= [Xt, Xa, Xv] (5)\n",
            "Zm=ReLU (Wm\n",
            "l1TXm+bm\n",
            "l1) (6)\n",
            "where Wm\n",
            "l1T∈R(dt+da+dv)×dmandReLU is the\n",
            "relu activation function.\n",
            "Finally, the fusion representation Zmis utilized\n",
            "to generate the multimodal sentiment prediction as:\n",
            "ˆym=Wm\n",
            "l2TZm+bm\n",
            "l2(7)\n",
            "where Wm\n",
            "l2T∈Rdm×1.Acoustic Feature \n",
            "Extractor\n",
            "Pre-\n",
            "trained \n",
            "BERT\n",
            "Visual \n",
            "Feature Extractor𝑋𝑎\n",
            "𝑋𝑡\n",
            "𝑋𝑣A-sLSTM\n",
            ". . .\n",
            "𝑋𝑎\n",
            "𝑋𝑡\n",
            "𝑋𝑣Linear\n",
            "𝑿𝒎𝑋𝑚′Linear ො𝑦𝑚Linear\n",
            "LinearLinear𝑋𝑎′\n",
            "𝑋𝑡′\n",
            "𝑋𝑣′Linear\n",
            "LinearLinearCPC\n",
            "CPC\n",
            "CPCMLP\n",
            "MLP\n",
            "MLPො𝑦𝑎\n",
            "ො𝑦𝑣ො𝑦𝑡𝑦𝑡𝑦𝑎\n",
            "𝑦𝑣𝑦𝑚Mutual Information Estimation 𝑴𝑨𝑬𝒚𝒎,𝒚𝒔𝒔={𝒕,𝒂,𝒗}\n",
            "Audio\n",
            "VisualText\n",
            "They are way toocool to \n",
            "talk to me \n",
            ". . .\n",
            "V-sLSTMInputFigure 1: The overall architecture of our propose model Self-MI\n",
            "3.3 Unimodal Task\n",
            "The three unimodal tasks leverage the same modal-\n",
            "ity representations as the multimodal task. To\n",
            "reduce the dimensionality gap between different\n",
            "modalities, we project the representations into a\n",
            "new feature space. Then, linear regression is used\n",
            "to obtain the unimodal results as follows:\n",
            "Zs=ReLU (Ws\n",
            "l1TXs+bs\n",
            "l1) (8)\n",
            "ˆys=Ws\n",
            "l2TZs+bs\n",
            "l2(9)\n",
            "where s∈ {t, a, v}. In order to guide the uni-\n",
            "modal task’s training process, we design a Uni-\n",
            "modal Label Generation with Mutual Information\n",
            "Module ( ULG MI) to get unimodal labels. Section\n",
            "3.4 delves into the specifics of the ULG MI:\n",
            "ys=ULG MI(ym, Zm, Zs) (10)\n",
            "Ultimately, the multimodal task and three uni-\n",
            "modal tasks are jointly learned using human-\n",
            "annotated multimodal labels, i.e., m-labels, and\n",
            "auto-generated unimodal labels, i.e., u-labels. It\n",
            "should be emphasized that these unimodal tasks\n",
            "only exist during the training stage. Consequently,\n",
            "ymis used as the final output.3.4 Mutual Information Maximization for\n",
            "Unimodel Labels Generation - ULG MI\n",
            "Since there exits a generation path from XstoZm,\n",
            "we expect an oposite path to construct Xswith\n",
            "s∈ {t, a, v}. The objective is to create unimodal\n",
            "supervised data representations based on human-\n",
            "labeled multimodal data and modality-specific rep-\n",
            "resentations.\n",
            "Unimodal supervised data representations have\n",
            "a strong correlation with multimodal data labels,\n",
            "hence, the Contrastive Predictive Coding (CPC)\n",
            "method, a widely used unsupervised approach for\n",
            "high-dimensional data, is applied to estimate the\n",
            "correlation between the multimodal representation.\n",
            "(Oord et al., 2018b) use CPC to measure the mu-\n",
            "tual information between contextual information\n",
            "and future elements across a time horizon. In other\n",
            "words, CPC seeks to capture the “slow features”\n",
            "that extend over multiple time steps, ensuring their\n",
            "retention in the encoding process. CPC uses a\n",
            "contrastive loss function based on estimated con-\n",
            "trastive noise, called InfoNCE (Normalized Mutual\n",
            "Information-based Contrastive Estimation). The\n",
            "InfoNCE loss function is designed to measure\n",
            "the similarity between pairs of data samples by\n",
            "calculating the mutual information between them.\n",
            "Specifically, InfoNCE leverages the concepts of en-tropy and mutual information from information the-\n",
            "ory to compute the distance between pairs of data\n",
            "samples. Therefore, when using InfoNCE, we are\n",
            "essentially estimating the shared information and\n",
            "maximizing it using gradient descent. Inspired by\n",
            "this idea, Self-MI implements Zmto reversely pre-\n",
            "dict representation across modalities so that more\n",
            "modality-invariant information can be passed to\n",
            "Zm.\n",
            "As depicted in Figure 1, after extracting uni-\n",
            "modal representations and multimodal representa-\n",
            "tions, we seek to maximize the correlation between\n",
            "the two modality’s representations via CPC. The\n",
            "CPC score function is defined as follows:\n",
            "Gϕ(Zm) =Gϕ(Zm)\n",
            "∥Gϕ(Zm)∥2(11)\n",
            "Zs=Zs\n",
            "∥Zs∥2(12)\n",
            "We utilizes Euclidean normalization to compute\n",
            "the unit-length vectors. Specifically, Gϕrepresents\n",
            "a simple neural network with parameters ϕ.\n",
            "s(Zm, Zs) = exp( Zs(Gϕ(Zm))T) (13)\n",
            "LN(Zm, Zs) =EF[logs(Zm, Zs)PN\n",
            "js(Zm, Zs)](14)\n",
            "Here,LNrepresenting the CPC loss function be-\n",
            "tween two vectors ZmandZs, where s∈ {t, a, v},\n",
            "andNbeing the number of samples in a batch,\n",
            "the sample index is the sum of elements within the\n",
            "same batch, excluding the current element. From\n",
            "this, we can derive the following relationship:\n",
            "LCPC =Lmt\n",
            "N+Lma\n",
            "N+Lmv\n",
            "N (15)\n",
            "The CPC output will be passed through a re-\n",
            "gression multilayer perceptron (MLP) to generate\n",
            "the unimodal label ys. Along with the human-\n",
            "generated multimodal label ym, we can define the\n",
            "loss function as follows:\n",
            "Ltask=MAE (ys, ym) (16)\n",
            "The unimodal labels update policy is shown in\n",
            "Algorithm 1.Algorithm 1 ULG MI\n",
            "Input : unimodal input Xt, Xa, Xv, m-\n",
            "labels ym\n",
            "Output : u-label y(i)\n",
            "t, y(i)\n",
            "a, y(i)\n",
            "vwhere iis\n",
            "the number of training epochs\n",
            "Initialize model parameters M(θ;x)\n",
            "Initialize u-labels y(1)\n",
            "t=ym, y(1)\n",
            "a=\n",
            "ym, y(1)\n",
            "v=ym\n",
            "Initialize global representations Fg\n",
            "t=\n",
            "0, Fg\n",
            "a= 0, Fg\n",
            "v= 0, Fg\n",
            "m= 0\n",
            "forn∈[1, end]do\n",
            "formini-batch in dataLoader do\n",
            "Compute mini-batch modality repre-\n",
            "sentations Zt, Za, Zv, Zm\n",
            "Compute loss Lusing Equation 16, 17\n",
            "Compute parameters gradientvL\n",
            "vθ\n",
            "Update model parameters: θ=θ−\n",
            "nvL\n",
            "vθ\n",
            "ifn̸= 1then\n",
            "Compute CPC loss LCPC using Eq.\n",
            "14 and Eq.15.\n",
            "Compute yt, ya, yvfrom mutual\n",
            "information learning from CPC\n",
            "model + MLP.\n",
            "end if\n",
            "Update global representations Fg\n",
            "sus-\n",
            "ingZs, where s∈ {m, t, a, v }\n",
            "end for\n",
            "end for\n",
            "3.5 Optimization Objectives\n",
            "Finally, we utilize L1Loss as a fundamental opti-\n",
            "mization method. For unimodal tasks, we employ\n",
            "the discrepancy between the unimodal label uand\n",
            "the multimodal label mas the weighting factor for\n",
            "the loss function. This indicates that the system\n",
            "should pay more attention to samples with signifi-\n",
            "cant differences.\n",
            "L=1\n",
            "NNX\n",
            "i(|ˆyim−yi\n",
            "m|+{t,a,v}X\n",
            "sWi\n",
            "s·|ˆyis−yi\n",
            "s|)(17)\n",
            "where Nrepresenting the number of samples used\n",
            "for training, Wi\n",
            "s= tanh( |ys−ym|)denotes the\n",
            "weight of the ithsample supporting task s.\n",
            "4 Experimental Settings\n",
            "In this section, we present the details of extensive\n",
            "experiments, which investigate the advantages ofTable 1: Dataset statistics in CMU-MOSI, CMU-\n",
            "MOSEI, and SIMS.\n",
            "Dataset Train Valid Test Total\n",
            "MOSI 1,284 229 686 2,199\n",
            "SIMS 1,368 456 457 2,281\n",
            "Self-MI compared to state-of-the-art baselines in\n",
            "term of evaluation metrics.\n",
            "4.1 Dataset\n",
            "In this study, we utilize two publicly available mul-\n",
            "timodal sentiment analysis datasets: CMU-MOSI\n",
            "(Zadeh et al., 2016), and SIMS (Yu et al., 2020).\n",
            "Table 1 provides an overview of the basic statistics\n",
            "for each dataset.\n",
            "CMU-MOSI : The CMU-MOSI dataset (Zadeh\n",
            "et al., 2016), is widely recognized as one of the\n",
            "prominent benchmark datasets for Multimodal Sen-\n",
            "timent Analysis (MSA). Each sample in the dataset\n",
            "is annotated by human annotators with a sentiment\n",
            "score ranging from -3 (strongly negative sentiment)\n",
            "to 3 (strongly positive sentiment).\n",
            "MOSEI : The CMU-MOSEI dataset\n",
            "(Bagher Zadeh et al., 2018) provides a larger\n",
            "number of utterances, increased sample variety,\n",
            "speakers, and topics compared to CMU-MOSI.\n",
            "Similarly to MOSI, annotators in the CMU-MOSEI\n",
            "dataset label each sample with a sentiment score\n",
            "ranging from -3 (strongly negative) to 3 (strongly\n",
            "positive).\n",
            "SIMS : The SIMS dataset (Yu et al., 2020) is a\n",
            "unique benchmark dataset for Chinese Multimodal\n",
            "Sentiment Analysis that offers fine-grained anno-\n",
            "tations of modality. Each sample in the dataset is\n",
            "annotated by human annotators with a sentiment\n",
            "score ranging from -1 (strongly negative sentiment)\n",
            "to 1 (strongly positive sentiment).\n",
            "4.2 Baselines\n",
            "To thoroughly evaluate the performance of our\n",
            "model Self-MI, we conduct a comprehensive com-\n",
            "parison with state-of-the-art baselines for the MSA\n",
            "task as follows:\n",
            "•Tensor Fusion Network (Zadeh et al., 2017):\n",
            "TFN disentangles unimodal data into tensors\n",
            "through a threefold Cartesian product and\n",
            "computes the outer product of these tensors\n",
            "for fusion.\n",
            "•Low-rank Multimodal Fusion (Liu et al.,\n",
            "2018): LMF decomposes stacked high-orderTable 2: The details of Hyperparameter Setting\n",
            "Hyper-parameters CMU-MOSI MOSEI SIMS\n",
            "batch size 32 64 32\n",
            "learning rate\n",
            "textual modality (BERT)5e-4 1e-3 5e-3\n",
            "BERT embedding size 768 768 768\n",
            "text dropout 0.0 0.1 0.1\n",
            "learning rate\n",
            "audio modality0.001 0.001 0.005\n",
            "audio embedding size 16 16 16\n",
            "audio dropout 0.1 0.1 0.0\n",
            "learning rate\n",
            "visual modality1e-3 1e-3 0.005\n",
            "visual embedding size 16 64 16\n",
            "visual dropout 0.1 0.1 0.0\n",
            "fusion dropout 0.1 0.1 0.0\n",
            "sentiment score range -3 to 3 -3 to 3 -1 to 1\n",
            "tensors into multiple low-rank factors and effi-\n",
            "ciently performs fusion based on these factors.\n",
            "•Multimodal Transformer (Tsai et al.,\n",
            "2019a): MulT constructs an architecture\n",
            "with separate unimodal and crossmodal trans-\n",
            "former networks and completes the fusion pro-\n",
            "cess using attention mechanisms.\n",
            "•Modal-temporal Attention Graph (Yang\n",
            "et al., 2021): MTAG is an interpretable graph\n",
            "attention network model capable of both fu-\n",
            "sion and alignment.\n",
            "•MISA (Hazarika et al., 2020): MISA projects\n",
            "features into separate two spaces with specific\n",
            "constraints and performs fusion on these fea-\n",
            "tures.\n",
            "•Self-MM (Yu et al., 2021): Self-MM assigns\n",
            "each modality a unimodal training task with\n",
            "automatically generated labels, aiming to ad-\n",
            "just the gradient backpropagation.\n",
            "•MMIM (Han et al., 2021): MMIM improves\n",
            "Multimodal Fusion with Hierarchical Mutual\n",
            "Information Maximization.\n",
            "4.3 Experimental Settings\n",
            "The hyperparameters used in the experiments are\n",
            "described in Table 2.\n",
            "4.4 Evaluation Metrics\n",
            "Followed by (Han et al., 2021), we consider vari-\n",
            "ous evaluation metrics namely mean absolute error\n",
            "(MAE), Pearson correlation (Corr), binary classi-\n",
            "fication accuracy (Acc_2), F1 score computed for\n",
            "positive/negative and non-negative/negative classi-\n",
            "fication.Table 3: Experimental results on CMU-MOSI and CMU-MOSEI dataset. For F1 _score and Acc _2, the “/” sign\n",
            "separates the value of “negative/non-negative” or “negative/positive” performance. The _sign is no information.\n",
            "Equally important, the sign ↓indicates “smaller is better” while ↑signifies “higher is better”. The bold style stands\n",
            "for the best-performing model whilst the underline marks the second best.\n",
            "ModelCMU-MOSI MOSEI\n",
            "MAE↓Corr↑ Acc_2 ↑ F1_score ↑MAE↓Corr↑ Acc_2 ↑ F1_score ↑\n",
            "TFN 0.901 0.698 _/80.8 _/80.7 0.593 0.677 -/82.5 -/82.1\n",
            "LMF 0.917 0.695 _/82.5 _/82.4 0.623 0.700 -/82.0 -/82.1\n",
            "MFM 0.877 0.706 -/81.7 -/81.6 0.568 0.703 -/84.4 -/84.3\n",
            "MTAG 0.866 0.722 -/82.3 -/82.1 - - - -\n",
            "MulT 0.861 0.711 81.50/84.10 80.60/83.90 0.580 0.713 -/82.5 -/82.3\n",
            "MISA 0.804 0.764 80.79/82.10 80.77/82.03 0.568 0.717 82.59/84.23 82.67/83.97\n",
            "Self-MM 0.718 0.791 82.56/84.48 82.46/84.44 0.530 0.765 82.81/85.17 82.53/85.30\n",
            "MMIM 0.72 0.78 82.8/84.76 82.63/84.66 0.526 0.772 82.24/85.97 82.66/ 85.94\n",
            "Ours (Self-MI) 0.699 0.80 83.09/85.37 82.95/85.3 0.527 0.773 83.86/85.45 83.90 /85.37\n",
            "5 Results & Discussion\n",
            "In this section, we provide a detailed analysis and\n",
            "discussion on the experimental results, which are\n",
            "obtained from Self-MI and baselines, on three\n",
            "datasets: CMU-MOSI, CMU-MOSEI, and SIMS.\n",
            "5.1 Comparison With Baselines\n",
            "Table 3 shows the comparative results on CMU-\n",
            "MOSI and CMU-MOSEI datasets. Generally, Self-\n",
            "MI significantly outperforms baselines across all\n",
            "evaluation metrics on the CMU-MOSI dataset. For\n",
            "the CMU-MOSEI dataset, Self-MI outperforms the\n",
            "SOTA models on Corr, Acc_2, and F1 score, while\n",
            "being comparable in MAE. These results provide\n",
            "initial evidence of the effectiveness of our approach\n",
            "in MSA tasks.\n",
            "As the SIMS dataset consists of unaligned data,\n",
            "we compare Self-MI with TFN, LMF, and Self-MM\n",
            "methods. The experimental results are presented in\n",
            "Table 4. It is clear that Self-MI has a sustainable\n",
            "performance over all baselines.\n",
            "Table 4: Experimental results on SIMS dataset. The sign\n",
            "↓indicates “smaller is better” while ↑signifies “higher\n",
            "is better”. The bold style stands for the best-performing\n",
            "model whilst the underline marks the second best.\n",
            "Model MAE↓Corr↑Acc_2 ↑F1_score ↑\n",
            "TFN 0.428 0.605 79.86 80.15\n",
            "LMF 0.431 0.600 79.37 78.65\n",
            "Self-MM 0.415 0.608 78.21 78.2\n",
            "Ours (Self-MI) 0.402 0.6138 79.65 79.82\n",
            "5.2 Ablation Study\n",
            "5.2.1 Effectiveness of unimodal subtasks\n",
            "Clearly, Self-MI contributes significantly to the\n",
            "overall performance of the model when examiningthe effectiveness of combining various unimodal\n",
            "tasks. The experimental results under different\n",
            "ablation settings are presented in Table 5. In com-\n",
            "paring to the single-task model with the incorpo-\n",
            "ration of unimodal subtasks, we observe a notable\n",
            "improvement in performance. Interestingly, the\n",
            "combinations “M, T” and “M, T, V” achieve com-\n",
            "parable or even better results than the combination\n",
            "“M, T, A, V”. Additionally, the subtasks “T” and “A”\n",
            "seem to have a more positive impact on the overall\n",
            "performance compared to the subtask “V”. These\n",
            "findings offer valuable insights into the effective-\n",
            "ness of Self-MI in leveraging different unimodal\n",
            "tasks to enhance multimodal learning.\n",
            "5.2.2 Effectiveness of CPC loss\n",
            "To highlight the advantages of the CPC loss func-\n",
            "tions in Self-MI, we conduct a series of ablation\n",
            "experiments on the CMU-MOSI dataset. The re-\n",
            "sults are listed in Table 6, where we eliminated one\n",
            "of these loss terms ( Lma\n",
            "N,Lmv\n",
            "N,Lmt\n",
            "N) from the total\n",
            "CPC loss. We observe the similar phenomena on\n",
            "other datasets.\n",
            "From the initial CPC loss function, there exits\n",
            "a performance degradation after removing a por-\n",
            "tion of the CPC loss. Specifically, when using\n",
            "LCPC, Self-MI show a stable performance with\n",
            "a slightly higher MAE score compared to using\n",
            "onlyLmt\n",
            "NorLma\n",
            "N, with insignificant differences of\n",
            "0.0042 and 0.0075, respectively. Similarly, in the\n",
            "Acc_2 metric, when using LCPC, the performance\n",
            "was slightly lower than when using only Lmt\n",
            "Nor\n",
            "Lma\n",
            "N, with insignificant differences of 0.3 and 0.15,\n",
            "respectively. This demonstrates the effectiveness\n",
            "of maximizing mutual information as proposed in\n",
            "Self-MI model.Table 5: Perspective analysis results from multimodal data with different tasks on CMU-MOSI dataset. M, T, A, V\n",
            "are multimodal, text, audio and visual tasks, respectively. For F1 _score and Acc _2, the “/” sign separates the value\n",
            "of “negative/non-negative” or “negative/positive” performance. Equally important, the sign ↓indicates “smaller is\n",
            "better” while ↑signifies “higher is better”. The bold style stands for the best-performing model.\n",
            "Task MAE↓Corr↑ Acc_2 ↑ F1_score ↑\n",
            "M 0.7155 0.796 81.67/82.13 81.55/83.08\n",
            "M,T 0.7217 0.7911 81.34/83.23 81.32/83.27\n",
            "M,A 0.6955 0.8027 82.67/82.98 83.59/85.96\n",
            "M,V 0.6908 0.8051 83.53/85.12 83.42/85.8\n",
            "M,T,A 0.7215 0.801 83.61/85.15 84.52/86.21\n",
            "M,T,V 0.7172 0.8012 84.49/85.93 84.51/85.01\n",
            "M,V ,A 0.7121 0.7962 82.36/8384 82.35/83.88\n",
            "M,T,V ,A 0.6999 0.8014 83.09/85.37 82.95/85.3\n",
            "Table 6: Results of CPC on CMU-MOSI. Symbol t, v, a, m represent text, images, audio and video. For F1 _score\n",
            "and Acc _2, the “/” sign separates the value of “negative/non-negative” or “negative/positive” performance. Equally\n",
            "important, the sign ↓indicates “smaller is better” while ↑signifies “higher is better”. The bold style stands for the\n",
            "best-performing model.\n",
            "Loss MAE↓Corr↑ Acc_2 ↑ F1_score ↑\n",
            "w/oLmt\n",
            "N 0.6957 0.8016 83.53/85.67 83.41/85.62\n",
            "w/oLma\n",
            "N 0.6924 0.8019 83.24/85.52 83.05/85.42\n",
            "w/oLmv\n",
            "N 0.7014 0.7982 81.78/83.69 81.4/83.71\n",
            "LCPC\n",
            "Lmt\n",
            "N,Lma\n",
            "N,Lmv\n",
            "N0.6999 0.8014 83.09/85.37 82.95/85.30\n",
            "6 Conclusion\n",
            "In this paper, we introduce Self-MI, a model de-\n",
            "signed to enhance efficient multimodal represen-\n",
            "tation. Our method includes a label generation\n",
            "module based on self-supervised learning, which\n",
            "enables us to obtain independent unimodal super-\n",
            "visions. Moreover, we maximize the Mutual In-\n",
            "formation (MI) between unimodal input pairs and\n",
            "between the multimodal fusion result and unimodal\n",
            "inputs, utilizing the auxiliary of Contrastive Predic-\n",
            "tive Coding (CPC). This comprehensive approach\n",
            "allows for better representation learning and more\n",
            "effective multimodal fusion in MSA task. We thor-\n",
            "oughly evaluate the performance of our model on\n",
            "three benchmark datasets, and our comprehensive\n",
            "ablation study further validates the effectiveness\n",
            "of our proposed approach. We anticipate that our\n",
            "work will serve as a source of inspiration for ad-\n",
            "vancing representation learning and multimodal\n",
            "sentiment analysis in future research.\n",
            "7 Acknowledgement\n",
            "Cam-Van Nguyen Thi was funded by the Master,\n",
            "PhD Scholarship Programme of Vingroup Innova-\n",
            "tion Foundation (VINIF), code VINIF.2022.TS143.References\n",
            "Md Shad Akhtar, Dushyant Chauhan, Deepanway\n",
            "Ghosal, Soujanya Poria, Asif Ekbal, and Pushpak\n",
            "Bhattacharyya. 2019. Multi-task learning for multi-\n",
            "modal emotion recognition and sentiment analysis.\n",
            "InProceedings of the 2019 Conference of the North\n",
            "American Chapter of the Association for Computa-\n",
            "tional Linguistics: Human Language Technologies,\n",
            "Volume 1 (Long and Short Papers) , pages 370–379,\n",
            "Minneapolis, Minnesota. Association for Computa-\n",
            "tional Linguistics.\n",
            "Alexander A Alemi, Ian Fischer, Joshua V Dillon, and\n",
            "Kevin Murphy. 2016. Deep variational information\n",
            "bottleneck. arXiv preprint arXiv:1612.00410 .\n",
            "Rana Ali Amjad and Bernhard C Geiger. 2019. Learn-\n",
            "ing representations for neural network-based classi-\n",
            "fication using the information bottleneck principle.\n",
            "IEEE transactions on pattern analysis and machine\n",
            "intelligence , 42(9):2225–2239.\n",
            "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria,\n",
            "Erik Cambria, and Louis-Philippe Morency. 2018.\n",
            "Multimodal language analysis in the wild: CMU-\n",
            "MOSEI dataset and interpretable dynamic fusion\n",
            "graph. In Proceedings of the 56th Annual Meeting of\n",
            "the Association for Computational Linguistics (Vol-\n",
            "ume 1: Long Papers) , pages 2236–2246, Melbourne,\n",
            "Australia. Association for Computational Linguistics.\n",
            "Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe\n",
            "Morency. 2018. Multimodal machine learning: A\n",
            "survey and taxonomy. IEEE transactions on pattern\n",
            "analysis and machine intelligence , 41(2):423–443.Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\n",
            "Kazemzadeh, Emily Mower, Samuel Kim, Jean-\n",
            "nette N Chang, Sungbok Lee, and Shrikanth S\n",
            "Narayanan. 2008. Iemocap: Interactive emotional\n",
            "dyadic motion capture database. Language resources\n",
            "and evaluation , 42:335–359.\n",
            "Wei Han, Hui Chen, and Soujanya Poria. 2021. Im-\n",
            "proving multimodal fusion with hierarchical mutual\n",
            "information maximization for multimodal sentiment\n",
            "analysis. In Proceedings of the 2021 Conference on\n",
            "Empirical Methods in Natural Language Processing ,\n",
            "pages 9180–9192, Online and Punta Cana, Domini-\n",
            "can Republic. Association for Computational Lin-\n",
            "guistics.\n",
            "Devamanyu Hazarika, Roger Zimmermann, and Sou-\n",
            "janya Poria. 2020. Misa: Modality-invariant and-\n",
            "specific representations for multimodal sentiment\n",
            "analysis. In Proceedings of the 28th ACM interna-\n",
            "tional conference on multimedia , pages 1122–1131.\n",
            "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\n",
            "Ross Girshick. 2020. Momentum contrast for unsu-\n",
            "pervised visual representation learning. In Proceed-\n",
            "ings of the IEEE/CVF conference on computer vision\n",
            "and pattern recognition , pages 9729–9738.\n",
            "Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long\n",
            "short-term memory. Neural computation , 9(8):1735–\n",
            "1780.\n",
            "Ramandeep Kaur and Sandeep Kautish. 2022. Multi-\n",
            "modal sentiment analysis: A survey and comparison.\n",
            "Research Anthology on Implementing Sentiment Anal-\n",
            "ysis Across Multiple Disciplines , pages 1846–1870.\n",
            "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-\n",
            "narasimhan, Paul Pu Liang, AmirAli Bagher Zadeh,\n",
            "and Louis-Philippe Morency. 2018. Efficient low-\n",
            "rank multimodal fusion with modality-specific fac-\n",
            "tors. In Proceedings of the 56th Annual Meeting of\n",
            "the Association for Computational Linguistics (Vol-\n",
            "ume 1: Long Papers) , pages 2247–2256, Melbourne,\n",
            "Australia. Association for Computational Linguistics.\n",
            "Aaron van den Oord, Yazhe Li, and Oriol Vinyals.\n",
            "2018a. Representation learning with contrastive pre-\n",
            "dictive coding. arXiv preprint arXiv:1807.03748 .\n",
            "Aaron van den Oord, Yazhe Li, and Oriol Vinyals.\n",
            "2018b. Representation learning with contrastive pre-\n",
            "dictive coding. arXiv preprint arXiv:1807.03748 .\n",
            "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-\n",
            "jumder, and Rada Mihalcea. 2020. Beneath the tip of\n",
            "the iceberg: Current challenges and new directions in\n",
            "sentiment analysis research. IEEE Transactions on\n",
            "Affective Computing .\n",
            "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-\n",
            "jumder, and Rada Mihalcea. 2023. Beneath the tip of\n",
            "the iceberg: Current challenges and new directions\n",
            "in sentiment analysis research. IEEE Trans. Affect.\n",
            "Comput. , 14(1):108–132.Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\n",
            "J. Zico Kolter, Louis-Philippe Morency, and Rus-\n",
            "lan Salakhutdinov. 2019a. Multimodal transformer\n",
            "for unaligned multimodal language sequences. In\n",
            "Proceedings of the 57th Annual Meeting of the Asso-\n",
            "ciation for Computational Linguistics , pages 6558–\n",
            "6569, Florence, Italy. Association for Computational\n",
            "Linguistics.\n",
            "Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,\n",
            "Louis-Philippe Morency, and Ruslan Salakhutdinov.\n",
            "2019b. Learning factorized multimodal representa-\n",
            "tions. In ICLR .\n",
            "Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu,\n",
            "Azaan Rehman, Amir Zadeh, Soujanya Poria, and\n",
            "Louis-Philippe Morency. 2021. MTAG: Modal-\n",
            "temporal attention graph for unaligned human mul-\n",
            "timodal language sequences. In Proceedings of the\n",
            "2021 Conference of the North American Chapter of\n",
            "the Association for Computational Linguistics: Hu-\n",
            "man Language Technologies , pages 1009–1021, On-\n",
            "line. Association for Computational Linguistics.\n",
            "Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu,\n",
            "Yixiao Ma, Jiele Wu, Jiyun Zou, and Kaicheng\n",
            "Yang. 2020. CH-SIMS: A Chinese multimodal senti-\n",
            "ment analysis dataset with fine-grained annotation of\n",
            "modality. In Proceedings of the 58th Annual Meet-\n",
            "ing of the Association for Computational Linguistics ,\n",
            "pages 3718–3727, Online. Association for Computa-\n",
            "tional Linguistics.\n",
            "Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. 2021.\n",
            "Learning modality-specific representations with self-\n",
            "supervised multi-task learning for multimodal sen-\n",
            "timent analysis. In Proceedings of the AAAI con-\n",
            "ference on artificial intelligence , volume 35, pages\n",
            "10790–10797.\n",
            "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-\n",
            "bria, and Louis-Philippe Morency. 2017. Tensor\n",
            "fusion network for multimodal sentiment analysis.\n",
            "InProceedings of the 2017 Conference on Empiri-\n",
            "cal Methods in Natural Language Processing , pages\n",
            "1103–1114, Copenhagen, Denmark. Association for\n",
            "Computational Linguistics.\n",
            "Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\n",
            "Philippe Morency. 2016. MOSI: multimodal corpus\n",
            "of sentiment intensity and subjectivity analysis in\n",
            "online opinion videos. CoRR , abs/1606.06259.\n",
            "Yu Zhang and Qiang Yang. 2021. A survey on multi-\n",
            "task learning. IEEE Transactions on Knowledge and\n",
            "Data Engineering , 34(12):5586–5609.\n",
            "Yongshuo Zong, Oisin Mac Aodha, and Timothy\n",
            "Hospedales. 2023. Self-supervised multimodal learn-\n",
            "ing: A survey. arXiv preprint arXiv:2304.01008 .deep-REMAP: Parameterization of Stellar Spectra\n",
            "Using Regularized Multi-Task Learning\n",
            "Sankalp Gilda∗\n",
            "Machine Learning Collective\n",
            "sankalp.gilda@gmail.com\n",
            "Abstract\n",
            "Traditional spectral analysis methods are increasingly challenged by the ex-\n",
            "ploding volumes of data produced by contemporary astronomical surveys. In\n",
            "response, we develop deep- Regularized Ensemble-based Multi-task Learning\n",
            "with Asymmetric Loss for Probabilistic Inference ( deep−REMAP ), a novel\n",
            "framework that utilizes the rich synthetic spectra from the PHOENIX library\n",
            "and observational data from the MARVELS survey to accurately predict\n",
            "stellar atmospheric parameters. By harnessing advanced machine learning\n",
            "techniques, including multi-task learning and an innovative asymmetric loss\n",
            "function, deep−REMAP demonstrates superior predictive capabilities in\n",
            "determining effective temperature, surface gravity, and metallicity from\n",
            "observed spectra. Our results reveal the framework’s effectiveness in ex-\n",
            "tending to other stellar libraries and properties, paving the way for more\n",
            "sophisticated and automated techniques in stellar characterization.\n",
            "1 Introduction\n",
            "Advancements in computers, telescope designs, and funding have revolutionized astronomy,\n",
            "leading to expanded survey volumes and resolution [ 1,2]. These developments have enabled\n",
            "surveys like SDSS, SEGUE [ 3], RAVE [ 4], BOSS [ 5], and LAMOST [ 6] to collect spectra for\n",
            "millions of stars, creating a data-rich environment for cutting-edge research. Complementing\n",
            "this, the PHOENIX library has provided a comprehensive set of synthetic spectra, essential for\n",
            "developing and testing new spectral analysis techniques. The latest data releases from Gaia-\n",
            "ESO Survey [ 7], DESI [ 8], and LSST [ 9] are set to further expand this corpus, highlighting\n",
            "the necessity for advanced computational methods capable of handling such vast and complex\n",
            "datasets. The increasingly prominent role of Machine Learning (ML), particularly Deep\n",
            "Learning (DL), in processing and analyzing these data reflects this need, with techniques\n",
            "such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) proving\n",
            "effective for tasks like modeling SEDs of galaxies [ 10], improving image quality at telescopes\n",
            "[11, 12], stellar spectrum analysis [13] and inferring redshifts from gamma ray bursts [14].\n",
            "Traditional spectral analysis methods, though successful in the past, now face significant\n",
            "challenges due to the surge in data volume and the diversity of spectral resolutions provided\n",
            "by these surveys. High-resolution techniques like the Equivalent Widths (EW) Method\n",
            "[15,16] are less suited for the moderate-resolution spectra that are now increasingly common.\n",
            "Spectral synthesis, an alternative, has shortcomings including dependency on atomic line\n",
            "databases [17].\n",
            "Recent years have seen the development of ML algorithms for stellar parameterization, from\n",
            "multi-layer perceptrons [ 18–20] to convolutional neural networks (CNNs) like “The Cannon”\n",
            "∗www.linkedin.com/in/sankalp-gilda\n",
            "Machine Learning and the Physical Sciences Workshop, NeurIPS 2023.arXiv:2311.03738v2  [astro-ph.SR]  21 Nov 2023Figure 1: Schematic representation of the deep−REMAP neural network architecture\n",
            "highlighting the input layer, convolutional layers, pooling layers, dense layers, and the two\n",
            "heads for of the three stellar parameters.\n",
            "[21], “The Payne” [ 22], and “StarNet” [ 23]. However, these works either use simple networks,\n",
            "require significant labelled data, or lack generalization to out-of-sample distributions. Our\n",
            "proposed framework, deep−REMAP (see Figure 1), is designed to leverage the synergy\n",
            "between the synthetic spectra from the PHOENIX library and the observational data from\n",
            "the MARVELS survey. It addresses these contemporary challenges by employing a multi-task\n",
            "learning approach, enabling the simultaneous prediction of multiple atmospheric parameters.\n",
            "We apply it to MARVELS survey spectra [ 24], validate it on stars with known parameters,\n",
            "and extract stellar atmospheric parameters for 732 FGK giant star candidates.\n",
            "deep−REMAP ’s architecture, which integrates recent advancements in regularization and\n",
            "loss function design, is particularly well-suited to the complex task of extracting stellar\n",
            "characteristics from the vast and varied data produced by today’s spectroscopic surveys.\n",
            "2 Data and Pre-Processing\n",
            "2.1 MARVELS Spectra\n",
            "We leverage the rich dataset from SDSS-III’s Multi-object APO Radial Velocity Exoplanet\n",
            "Large-area Survey (MARVELS), featuring spectra collected from 5,500 stars using a medium-\n",
            "resolution spectrograph at Apache Point Observatory [ 24–26]. Prior analyses have scrutinized\n",
            "the UF2D and UF1D data processing pipelines [ 27], along with the meticulous target selection\n",
            "process [28].\n",
            "Expanding upon the spectral indices method [ 17], [29] refined the derivation of stellar\n",
            "parameters for all observed stars. Following quality control and uniqueness checks, as well as\n",
            "parameter filtering, we narrowed the dataset down to 2,343 dwarfs, whose stellar parameters\n",
            "have subsequently been used for the fine-tuning of our network. The validation process\n",
            "involved 30 calibration stars, and the network was also tasked with predicting parameters\n",
            "for the remaining 732 giant and sub-giant stars (Nolan Grieves, private communication).\n",
            "The pre-processing of MARVELS spectra entailed meticulous normalization, continuum\n",
            "removal, and a novel approach to cosmic ray mitigation. These steps were critical in ensuring\n",
            "the quality of the dataset, which is paramount when employing deep learning models for\n",
            "spectral analysis. The curated dataset enables the deep−REMAP framework to effectively\n",
            "learn the intricate patterns and nuances within the spectra, facilitating a more accurate\n",
            "prediction of stellar atmospheric parameters.\n",
            "22.2 PHOENIX Spectra\n",
            "The PHOENIX library offers an extensive collection of high-resolution synthetic spectra,\n",
            "pivotal for the calibration and benchmarking of spectral analysis algorithms. Utilizing the\n",
            "state-of-the-art synthetic modeling techniques, the library provides coverage over a wide range\n",
            "of stellar parameters, making it an indispensable resource for this study. For the purposes\n",
            "ofdeep−REMAP , we selected a subset of the PHOENIX spectra that closely matches the\n",
            "parameter space of the MARVELS survey to ensure a consistent and comprehensive training\n",
            "regimen for our model. Since MARVELS stars are expected to be F,G and K type stars, we\n",
            "limit the PHOENIX stellar parameter to: 4,850 ≤λ≤5,750 ˚A, 3,000 ≤Teff≤7,000 K, 1.0\n",
            "≤logg≤5.5, -2 ≤[Fe/H] ≤1. Below, we describe how the synthetic PHOENIX spectra\n",
            "are brought in line with MARVELS spectra to reduce the discrepancies between them, thus\n",
            "allowing for successful transfer learning.\n",
            "Incorporating these synthetic spectra allows for the fine-tuning of deep−REMAP ’s predictive\n",
            "capabilities, providing the network with robust training data that encapsulates the theoretical\n",
            "variance across different stellar atmospheres. This integration is particularly beneficial for\n",
            "the recognition of subtle spectral features that may not be as prominent in observational\n",
            "data, thereby enhancing the model’s generalizability and accuracy in parameter estimation.\n",
            "2.3 Pre-processing and Data Augmentation\n",
            "Synthetic spectra often require calibration to align with observed spectra, a process termed\n",
            "traversing the synthetic gap . To utilize the PHOENIX synthetic grid for the MARVELS\n",
            "spectra, we perform several pre-processing and data augmentation steps:\n",
            "1.Down-convolution : PHOENIX spectra are convolved with Gaussian kernels to lower\n",
            "resolution, accommodating resolution variation in the MARVELS spectra.\n",
            "2.Resampling : PHOENIX spectra are resampled onto the MARVELS wavelength grid\n",
            "via cubic spline interpolation.\n",
            "3.Noise Addition : Gaussian noise is added to PHOENIX spectra, reflecting the\n",
            "Signal-to-Noise (SNR) distribution in MARVELS spectra.\n",
            "4.Continuum Normalization : Continuum is removed from both datasets using a\n",
            "uniform method, ensuring consistency.\n",
            "Steps 1 and 3 also serve as data augmentation, enhancing the training set size and model\n",
            "robustness. For step 4, we develop a novel continuum normalization routine (see Algorithm\n",
            "1), superior to existing methods in tests on synthetic spectra. In future we plan to compare\n",
            "this with advanced algorithms in literature [30–32].\n",
            "MARVELS spectra required careful pre-processing to accurately estimate their continuum\n",
            "levels, which included removal of false features [ 27,29], and application of the continuum\n",
            "finding routine. Despite varying SNRs in the MARVELS spectra, our method effectively\n",
            "managed potential misestimations. Concluding these steps, we have 100,000 processed\n",
            "PHOENIX spectra, and 3,075 processed MARVELS spectra.\n",
            "3 Implementation\n",
            "We design the deep−REMAP architecture to accurately predict stellar parameters by\n",
            "discretizing the continuous spectral labels into discrete classes. We demonstrate this using\n",
            "the effective temperature (T eff) – we divide the range into bins and model these bins\n",
            "using Gaussian distributions, which reflects the measurement uncertainties inherent in T eff\n",
            "estimates and is expressed by the equation:\n",
            "p(Teff|x) =1√\n",
            "2πσ2exp\u0012\n",
            "−(Teff−µ)2\n",
            "2σ2\u0013\n",
            ", (1)\n",
            "where xis the input spectrum, Teffis the effective temperature, µis the mean value for the\n",
            "bin, and σis the standard deviation.\n",
            "3We apply asymmetric label smoothing, a recent advancement in training classification models,\n",
            "to mitigate the impact of these uncertainties, thereby forming a cost vector that guides the\n",
            "neural network towards more reliable predictions. We introduce triplet loss to our training\n",
            "process, a significant improvement over traditional loss functions. This loss function helps in\n",
            "embedding the parameters in a space that enhances the model’s predictive accuracy by:\n",
            "Ltriplet = max ( d(a, p)−d(a, n) + margin ,0), (2)\n",
            "where d(·) denotes a distance metric, ais an anchor input, pis a positive input of the same\n",
            "class as the anchor, and nis a negative input of a different class.\n",
            "Our final loss function, Lfinal, combines cross-entropy loss and triplet loss to balance classifi-\n",
            "cation accuracy with embedding effectiveness:\n",
            "Lfinal =LTeff,focal+λTeffLTeff,triplet (3)\n",
            "+Llog g,focal+λlog gLlog g,triplet\n",
            "+L[Fe/H],focal+λ[Fe/H]L[Fe/H],triplet\n",
            "with λs being weights that balance the two loss components. See Fig. 2) for the results on\n",
            "the validation set. The three λs are empirically chosen to be 0.001.\n",
            "The learning rates for the fine-tuning phase undergo a gradual reduction, thus preserving\n",
            "the knowledge in the shallower layers while facilitating faster modifications in the deeper\n",
            "layers. The number of epochs per cycle and the number of cycles are determined empirically.\n",
            "The weights of the model are set to the running average of the weights recorded at the end\n",
            "of each cycle, a technique known as Stochastic Weight Averaging (SWA). Lastly, the model’s\n",
            "performance is evaluated on the stellar parameters of 30 calibration stars from MARVELS.\n",
            "We determine the optimal model complexity through an exhaustive 10-fold cross-validation,\n",
            "selecting the appropriate number of residual blocks and filters to minimize Lfinal. This\n",
            "minimization ensures that our model generalizes well and remains robust against overfitting.\n",
            "Through this strategic implementation, we position deep−REMAP at the forefront of modern\n",
            "machine learning applications in spectral analysis, showcasing its prowess in automated\n",
            "stellar parameter inference.\n",
            "4 Results\n",
            "We applied the deep−REMAP framework to analyze a cohort of 732 FGK giant star\n",
            "candidates, extracting three key parameters: Teff, [Fe/H], and log(g). Our results confirm\n",
            "that approximately 80% of these candidates are FGK giants, as their 3 σparameter values\n",
            "align with the expected ranges for giants ( 4000 K≤Teff≤6000 K,−0.5≤[Fe/H] ≤0.3,\n",
            "and0.0≤log(g)≤3.0). The remaining 20% were re-classified as FGK dwarfs, with their\n",
            "parameters falling within the dwarf ranges ( 5000 K≤Teff≤7000 K,−0.3≤[Fe/H] ≤0.5,\n",
            "and 3 .5≤log(g)≤5.0).\n",
            "The efficacy of the triplet loss function in our deep−REMAP model is evident in Figure 2,\n",
            "which showcases the improved separability of stellar classes in the learned feature space. This\n",
            "improvement is quantified by a marked increase in classification accuracy and a reduction in\n",
            "overlap between the parameter distributions of giants and dwarfs.\n",
            "5 Conclusions and Discussion\n",
            "We have introduced deep−REMAP , a pioneering neural network designed for the spectral\n",
            "analysis of 1D spectra, which we have utilized to parameterize MARVELS targets. Our model\n",
            "is the first of its kind to integrate an array of contemporary machine learning techniques\n",
            "including transfer learning, multi-task learning, temperature scaling, focal loss, triplet loss,\n",
            "stochastic weight averaging, cosine annealing-based learning rate, and probabilistic inference\n",
            "with an asymmetric cost function.\n",
            "4Epoch 0Effective Temperature\n",
            " Surface Gravity\n",
            " Metallicity\n",
            "Epoch 25\n",
            " Epoch 50\n",
            "Figure 2: Visualization of the improved separability of stellar classes in the embedded feature\n",
            "space for the validation sample, facilitated by the triplet loss function.\n",
            "Our findings clearly demonstrate that, with carefully curated data augmentation and the\n",
            "application of transfer learning, our network can transcend the observational idiosyncrasies\n",
            "of specific spectroscopes. As a result, it is capable of estimating stellar parameters from real\n",
            "observed spectra with remarkable accuracy. Moreover, we have showcased how adopting a\n",
            "regression-as-classification approach allows us to capture the non-Gaussian distributions of\n",
            "the output parameters effectively. Furthermore, the incorporation of an embedding loss not\n",
            "only enhances the classification results but also significantly improves model interpretability.\n",
            "For the first time, we present a methodology that predicts the parameters of stellar spectra\n",
            "with unprecedented accuracy, marking a paradigm shift in automated spectral analysis. This\n",
            "advancement paves the way for future surveys, providing a scalable and efficient tool for\n",
            "the rapid classification of an ever-growing number of stellar objects. The deep-REMAP\n",
            "network stands as a testament to the potential of machine learning in revolutionizing our\n",
            "understanding of the cosmos.\n",
            "5A Acklowledgements\n",
            "The author would like to thank Dr. Jian Ge and Mr. Kevin Willis, formerly at the University\n",
            "of Florida, for assistance with this project.\n",
            "References\n",
            "[1]D. G. York, J. Adelman, J. E. Anderson Jr, S. F. Anderson, J. Annis, N. A. Bahcall, J. Bakken,\n",
            "R. Barkhouser, S. Bastian, E. Berman et al. , “The sloan digital sky survey: Technical summary,”\n",
            "The Astronomical Journal , vol. 120, no. 3, p. 1579, 2000.\n",
            "[2]D. J. Eisenstein, D. H. Weinberg, E. Agol, H. Aihara, C. A. Prieto, S. F. Anderson, J. A. Arns,\n",
            "´E. Aubourg, S. Bailey, E. Balbinot et al. , “Sdss-iii: Massive spectroscopic surveys of the distant\n",
            "universe, the milky way, and extra-solar planetary systems,” The Astronomical Journal , vol.\n",
            "142, no. 3, p. 72, 2011.\n",
            "[3]B. Yanny, C. Rockosi, H. J. Newberg, G. R. Knapp, J. K. Adelman-McCarthy, B. Alcorn,\n",
            "S. Allam, C. A. Prieto, D. An, K. S. Anderson et al. , “Segue: A spectroscopic survey of 240,000\n",
            "stars with g= 14-20,” The Astronomical Journal , vol. 137, no. 5, p. 4377, 2009.\n",
            "[4]G. Kordopatis, G. Gilmore, M. Steinmetz, C. Boeche, G. M. Seabroke, A. Siebert, T. Zwitter,\n",
            "J. Binney, P. De Laverny, A. Recio-Blanco et al. , “The radial velocity experiment (rave): Fourth\n",
            "data release,” The Astronomical Journal , vol. 146, no. 5, p. 134, 2013.\n",
            "[5]K. S. Dawson, D. J. Schlegel, C. P. Ahn, S. F. Anderson, ´E. Aubourg, S. Bailey, R. H. Barkhouser,\n",
            "J. E. Bautista, A. Beifiori, A. A. Berlind et al. , “The baryon oscillation spectroscopic survey of\n",
            "sdss-iii,” The Astronomical Journal , vol. 145, no. 1, p. 10, 2012.\n",
            "[6]X.-Q. Cui, Y.-H. Zhao, Y.-Q. Chu, G.-P. Li, Q. Li, L.-P. Zhang, H.-J. Su, Z.-Q. Yao, Y.-N.\n",
            "Wang, X.-Z. Xing et al. , “The large sky area multi-object fiber spectroscopic telescope (lamost),”\n",
            "Research in Astronomy and Astrophysics , vol. 12, no. 9, p. 1197, 2012.\n",
            "[7]G. Gilmore, S. Randich, M. Asplund, J. Binney, P. Bonifacio, J. Drew, S. Feltzing, A. Ferguson,\n",
            "R. Jeffries, G. Micela et al. , “The gaia-eso public spectroscopic survey,” The Messenger , vol.\n",
            "147, pp. 25–31, 2012.\n",
            "[8]M. Levi, C. Bebek, T. Beers, R. Blum, R. Cahn, D. Eisenstein, B. Flaugher, K. Honscheid,\n",
            "R. Kron, O. Lahav et al. , “The desi experiment, a whitepaper for snowmass 2013,” arXiv\n",
            "preprint arXiv:1308.0847 , 2013.\n",
            "[9]ˇZ. Ivezi´ c, S. M. Kahn, J. A. Tyson, B. Abel, E. Acosta, R. Allsman, D. Alonso, Y. AlSayyad,\n",
            "S. F. Anderson, J. Andrew et al. , “Lsst: from science drivers to reference design and anticipated\n",
            "data products,” The Astrophysical Journal , vol. 873, no. 2, p. 111, 2019.\n",
            "[10]S. Gilda, S. Lower, and D. Narayanan, “mirkwood: Fast and accurate sed modeling using\n",
            "machine learning,” The Astrophysical Journal , vol. 916, no. 1, p. 43, jul 2021. [Online].\n",
            "Available: https://dx.doi.org/10.3847/1538-4357/ac0058\n",
            "[11]S. Gilda, S. C. Draper, S. Fabbro, W. Mahoney, S. Prunet, K. Withington, M. Wilson, Y.-S.\n",
            "Ting, and A. Sheinis, “Uncertainty-aware learning for improvements in image quality of the\n",
            "Canada–France–Hawaii Telescope,” Monthly Notices of the Royal Astronomical Society , vol.\n",
            "510, no. 1, pp. 870–902, 11 2021. [Online]. Available: https://doi.org/10.1093/mnras/stab3243\n",
            "[12]S. Gilda, Y.-S. Ting, K. Withington, M. Wilson, S. Prunet, W. Mahoney, S. Fabbro, S. C.\n",
            "Draper, and A. Sheinis, “Astronomical Image Quality Prediction based on Environmental and\n",
            "Telescope Operating Conditions,” arXiv e-prints , p. arXiv:2011.03132, Nov. 2020.\n",
            "[13] S. Gilda, J. Ge, and MARVELS, “Parameterization of MARVELS Spectra Using Deep Learning,”\n",
            "inAmerican Astronomical Society Meeting Abstracts #231 , ser. American Astronomical Society\n",
            "Meeting Abstracts, vol. 231, Jan. 2018, p. 349.02.\n",
            "[14]M. Dainotti, V. Petrosian, M. Bogdan, B. Miasojedow, S. Nagataki, T. Hastie, Z. Nuyngen,\n",
            "S. Gilda, X. Hernandez, and D. Krol, “Gamma-ray Bursts as distance indicators through a\n",
            "machine learning approach,” arXiv e-prints , p. arXiv:1907.05074, Jul. 2019.\n",
            "[15]J. A. Valenti and D. A. Fischer, “Spectroscopic properties of cool stars (spocs). i. 1040 f, g,\n",
            "and k dwarfs from keck, lick, and aat planet search programs,” The Astrophysical Journal\n",
            "Supplement Series , vol. 159, no. 1, p. 141, 2005.\n",
            "6[16]S. G. Sousa, ARES + MOOG: A Practical Overview of an Equivalent Width (EW) Method to\n",
            "Derive Stellar Parameters . Cham: Springer International Publishing, 2014, pp. 297–310.\n",
            "[Online]. Available: https://doi.org/10.1007/978-3-319-06956-2 26\n",
            "[17] L. Ghezzi, L. Dutra-Ferreira, D. Lorenzo-Oliveira, G. F. P. De Mello, B. X. Santiago, N. De Lee,\n",
            "B. L. Lee, L. N. Da Costa, M. A. Maia, R. L. Ogando et al. , “Accurate atmospheric parameters\n",
            "at moderate resolution using spectral indices: Preliminary application to the marvels survey,”\n",
            "The Astronomical Journal , vol. 148, no. 6, p. 105, 2014.\n",
            "[18]C. A. Bailer-Jones, “Stellar parameters from very low resolution spectra and medium band\n",
            "filters: teff, logg and [m/h] using neural networks,” arXiv preprint astro-ph/0003071 , 2000.\n",
            "[19]S. Snider, C. A. Prieto, T. von Hippel, T. C. Beers, C. Sneden, Y. Qu, and S. Rossi, “Three-\n",
            "dimensional spectral classification of low-metallicity stars using artificial neural networks,” The\n",
            "Astrophysical Journal , vol. 562, no. 1, p. 528, 2001.\n",
            "[20] M. Manteiga, D. Ord´ o˜ nez, C. Dafonte, and B. Arcay, “Anns and wavelets: A strategy for gaia\n",
            "rvs low s/n stellar spectra parameterization,” Publications of the Astronomical Society of the\n",
            "Pacific , vol. 122, no. 891, p. 608, 2010.\n",
            "[21]M. Ness, D. W. Hogg, H.-W. Rix, A. Y. Ho, and G. Zasowski, “The cannon: a data-driven\n",
            "approach to stellar label determination,” The Astrophysical Journal , vol. 808, no. 1, p. 16, 2015.\n",
            "[22] Y.-S. Ting, C. Conroy, H.-W. Rix, and P. Cargile, “The payne: self-consistent ab initio fitting\n",
            "of stellar spectra,” arXiv preprint arXiv:1804.01530 , 2018.\n",
            "[23] S. Bialek, S. Fabbro, K. A. Venn, N. Kumar, T. O’Briain, and K. M. Yi, “Deep learning analyses\n",
            "of synthetic spectral libraries with an application to the gaia-eso database,” arXiv preprint\n",
            "arXiv:1911.02602 , 2019.\n",
            "[24]J. Ge, S. Mahadevan, B. Lee, X. Wan, B. Zhao, J. van Eyken, S. Kane, P. Guo, E. Ford,\n",
            "S. Fleming et al. , “The multi-object apo radial-velocity exoplanet large-area survey (marvels),”\n",
            "inExtreme Solar Systems , vol. 398, 2008, p. 449.\n",
            "[25] J. E. Gunn, W. A. Siegmund, E. J. Mannery, R. E. Owen, C. L. Hull, R. F. Leger, L. N. Carey,\n",
            "G. R. Knapp, D. G. York, W. N. Boroski et al. , “The 2.5 m telescope of the sloan digital sky\n",
            "survey,” The Astronomical Journal , vol. 131, no. 4, p. 2332, 2006.\n",
            "[26]J. Ge, B. Lee, N. D. Lee, X. Wan, J. Groot, B. Zhao, F. Varosi, K. Hanna, S. Mahadevan,\n",
            "F. Hearty, L. Chang, J. Liu, J. van Eyken, J. Wang, R. Pais, Z. Chen, A. Shelden, and\n",
            "E. Costello, “A new generation multi-object Doppler instrument for the SDSS-III Multi-object\n",
            "APO Radial Velocity Exoplanet Large-area Survey,” in Techniques and Instrumentation for\n",
            "Detection of Exoplanets IV , S. B. Shaklan, Ed., vol. 7440, International Society for Optics and\n",
            "Photonics. SPIE, 2009, pp. 187 – 196. [Online]. Available: https://doi.org/10.1117/12.826651\n",
            "[27] N. Grieves, J. Ge, N. Thomas, B. Ma, S. Sithajan, L. Ghezzi, B. Kimock, K. Willis, N. De Lee,\n",
            "B. Lee et al. , “Exploring the brown dwarf desert: new substellar companions from the sdss-\n",
            "iii marvels survey,” Monthly Notices of the Royal Astronomical Society , vol. 467, no. 4, pp.\n",
            "4264–4281, 2017.\n",
            "[28]M. Paegert, K. G. Stassun, N. D. Lee, J. Pepper, S. W. Fleming, T. Sivarani, S. Mahadevan,\n",
            "C. E. M. III, S. Dhital, L. Hebb, and J. Ge, “TARGET SELECTION FOR THE SDSS-III\n",
            "MARVELS SURVEY,” The Astronomical Journal , vol. 149, no. 6, p. 186, may 2015. [Online].\n",
            "Available: https://doi.org/10.1088%2F0004-6256%2F149%2F6%2F186\n",
            "[29]N. Grieves, J. Ge, N. Thomas, K. Willis, B. Ma, D. Lorenzo-Oliveira, A. Queiroz, L. Ghezzi,\n",
            "C. Chiappini, F. Anders et al. , “Chemo-kinematics of the milky way from the sdss-iii marvels\n",
            "survey,” Monthly Notices of the Royal Astronomical Society , vol. 481, no. 3, pp. 3244–3265,\n",
            "2018.\n",
            "[30]R. Zhao and A. Luo, “A novel method for continuum normalization of astronomical spectrum\n",
            "signals,” Guang pu xue yu guang pu fen xi= Guang pu , vol. 26, no. 3, pp. 587–590, 2006.\n",
            "[31]H. Wang, S. Li, and A. Luo, “Astronomical spectral lines auto-extraction based on framelet\n",
            "transform,” in 2010 3rd International Congress on Image and Signal Processing , vol. 3, Oct\n",
            "2010, pp. 1045–1048.\n",
            "[32]S. Gilda and Z. Slepian, “Automatic Kalman-filter-based wavelet shrinkage denoising of 1D\n",
            "stellar spectra,” Monthly Notices of the Royal Astronomical Society , vol. 490, no. 4, pp.\n",
            "5249–5269, 09 2019. [Online]. Available: https://doi.org/10.1093/mnras/stz2577\n",
            "7Algorithm 1 Continuum Normalization\n",
            "Input: a vector of spectrum flux measurements of length N,Z={zn}N\n",
            "n=1.\n",
            "Output: the normalized spectrum flux vector of length N,X={xn}N\n",
            "n=1.\n",
            "1:Setaas the maximum quantity of flux values in each bin that are allowed to be larger\n",
            "than the continuum.\n",
            "2:Define the initial continuum estimate, C, as a vector of length Nwith all elements set to\n",
            "10 + max( Z).\n",
            "3:forbin count vin range [2, 11] do\n",
            "4: Partition the index vector, [1, N], into vuniform bins. In each partition, determine\n",
            "the center element and place into vector H,H= [N/v/ 2,{[N/v/ 2, N]}N\n",
            "n=1+N/v]\n",
            "5: while the quantity of true values in C>Zis greater than a,do\n",
            "6: forbin index hin a random permuted vector of H do\n",
            "7: Create a subset of vector Cby duplicating indices HofCinto vector E.\n",
            "8: if C(h)>Z(h)then\n",
            "9: Lower the continuum estimate in bin hby multiplying E(h) by 0.999.\n",
            "10: ifv <5then\n",
            "11: Calculate the continuum estimate, C, by fitting a v−1degree polynomial to\n",
            "(H,E) and evaluate at range [1 , N].\n",
            "12: To avoid undesired inflections at the continuum edges we perform linear\n",
            "interpolations to force flat edges in the endpoint bins. At the left edge we\n",
            "find the slope between the points C(H(1))andC(H(1) + 1) , then extrapolate\n",
            "to replace the index range [1,H(1)−1]. The right edge is altered in the same\n",
            "manner at the index range [ H(v) + 1, N].\n",
            "13: else\n",
            "14: Calculate the continuum estimate, C, by linear interpolation of (H,E)and\n",
            "evaluate at range [1 , N].\n",
            "15: end if\n",
            "16: end if\n",
            "17: end for\n",
            "18: end while\n",
            "19:end for\n",
            "20:Calculate the continuum estimate, C, by fitting a 6degree polynomial to (H,E)and\n",
            "evaluate at range [1 , N].\n",
            "21:To avoid undesired inflections at the continuum edges we perform linear interpolations\n",
            "to force flat edges in the endpoint bins. At the left edge we find the slope between the\n",
            "points C(H(1))andC(H(1) + 1) , then extrapolate to replace the index range [1,H(1)−1].\n",
            "The right edge is altered in the same manner at the index range [ H(v) + 1, N].\n",
            "22:To smooth out the linear interpolations performed at the edges the continuum estimate\n",
            "is convolved with a gaussian kernel with sigma of 150.\n",
            "8Neural MMO 2.0: A Massively Multi-task Addition to\n",
            "Massively Multi-agent Learning\n",
            "Joseph Suárez JSUAREZ @MIT.EDU\n",
            "Phillip Isola PHILLIPI @MIT.EDU\n",
            "Massachusetts Institute of Technology\n",
            "Kyoung Whan Choe CHOE .KYOUNG @GMAIL .COM\n",
            "David Bloomin DAVEEY @GMAIL .COM\n",
            "Hao Xiang Li HXL23@ CAM .AC.UK\n",
            "Nikhil Pinnaparaju NIKHILPINNAPARAJU @GMAIL .COM\n",
            "Nishaanth Kanna NISHAANTHKANNA @GMAIL .COM\n",
            "Daniel Scott DSCOTT 45@ GATECH .EDU\n",
            "Ryan Sullivan RSULLI @UMD .EDU\n",
            "Rose S. Shuman ROSE .SHUMAN @ALUMNI .BROWN .EDU\n",
            "Lucas de Alcântara LUCASAGLLEITE @GMAIL .COM\n",
            "Herbie Bradley HB574@ CAM .AC.UK\n",
            "Louis Castricato LOUIS _CASTRICATO @BROWN .EDU\n",
            "CarperAI\n",
            "Kirsty You KIRSTYYOU @CHAOCANSHU .AI\n",
            "Yuhao Jiang YUHAOJIANG @CHAOCANSHU .AI\n",
            "Qimai Li QIMAILI @CHAOCANSHU .AI\n",
            "Jiaxin Chen JIAXINCHEN @CHAOCANSHU .AI\n",
            "Xiaolong Zhu XIAOLONGZHU @CHAOCANSHU .AI\n",
            "Parametrix.AI\n",
            "Abstract\n",
            "Neural MMO 2.0 is a massively multi-agent environment for reinforcement learning\n",
            "research. The key feature of this new version is a flexible task system that allows\n",
            "users to define a broad range of objectives and reward signals. We challenge\n",
            "researchers to train agents capable of generalizing to tasks, maps, and opponents\n",
            "never seen during training. Neural MMO features procedurally generated maps\n",
            "with 128 agents in the standard setting and support for up to. Version 2.0 is a\n",
            "complete rewrite of its predecessor with three-fold improved performance and\n",
            "compatibility with CleanRL. We release the platform as free and open-source\n",
            "software with comprehensive documentation available at neuralmmo.github.io and\n",
            "an active community Discord. To spark initial research on this new platform, we\n",
            "are concurrently running a competition at NeurIPS 2023.\n",
            "37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.arXiv:2311.03736v1  [cs.AI]  7 Nov 2023Figure 1: Overview of Neural MMO 2.0. Users can define tasks to specify a broad range of agent\n",
            "objective. In general, these involve using tools to gather resources, using resources to make items\n",
            "and weapons, using weapons to fight enemies, and fighting enemies to gain armor and tools. Full\n",
            "documentation is available at neuralmmo.github.io.\n",
            "1 Novelty and Impact\n",
            "Neural MMO is a reinforcement learning platform first released in 2019 [Suarez et al., 2019], with\n",
            "updates featured in short-form at AAMAS 2020 [Suarez et al., 2020] and ICML 2020, and a new\n",
            "version published in the 2021 NeurIPS Datasets & Benchmarks track [Suarez et al., 2021]. Since\n",
            "then, the platform has gained traction through competitions at IJCAI 2022 and NeurIPS 2022, totaling\n",
            "3500+ submission from 1200+ users, which significantly improved state-of-the-art on the platform.\n",
            "Alongside these developments, our community on Discord has grown to nearly 1000 members.\n",
            "While previous versions of the environment defined fixed objectives through only the reward signal,\n",
            "Neural MMO 2.0 introduces a flexible task system that allows users to define per-agent or per-team\n",
            "objectives and rewards, expanding the platform’s applicability to a broader range of problems. In\n",
            "particular, Neural MMO 2.0 enables research on generalization, open-endedness, and curriculum\n",
            "learning—areas that were difficult to explore with prior versions and which require sophisticated,\n",
            "flexible simulators. There are few if any other environments of comparable scope to Neural MMO\n",
            "available for these problems.\n",
            "2Figure 2: Neural MMO 2.0 features procedurally generated terrain, 7 resources to collect, 3 combat\n",
            "styles, 5 gathering and 3 combat professions to train and level up, scripted NPCs that roam the\n",
            "map, and 16 types of items in 10 quality levels including weapons, armor, consumables, tools, and\n",
            "ammunition. An environment-wide market allows agents to trade items with each other.\n",
            "Practical engineering improvements are at the core of Neural MMO 2.0. These include:\n",
            "1.A 3x faster engine. This was developed as part of a complete rewrite of our 5+ year old code\n",
            "base and is particularly important for reinforcement learning research, where simulation is\n",
            "often the bottleneck. For example, the upcoming competition would not be practical on the\n",
            "old engine.\n",
            "2.Simple baselines with CleanRL, a popular and user-friendly reinforcement learning library.\n",
            "CleanRL and most other reinforcement learning frameworks are not natively compatible with\n",
            "environments of this complexity, and previous versions required convoluted, environment-\n",
            "specific compatibility wrappers. Neural MMO 2.0 integrates PufferLib to solve this problem.\n",
            "3.A web client available at neuralmmo.github.io/client, generously open-sourced by\n",
            "Parametrix.AI. This client offers improved visualization capabilities and eliminates setup\n",
            "requirements.\n",
            "Additionally, the platform’s documentation has been professionally rewritten in consultation with\n",
            "the development team. This, along with a more intuitive and accessible website layout, marks\n",
            "a significant step towards improving user engagement. A collection of papers detailing previous\n",
            "versions and competitions is available on neuralmmo.github.io.\n",
            "2 Neural MMO 2.0\n",
            "Neural MMO (NMMO) is an open-source research platform that is computationally accessible. It\n",
            "enables populations of agents to be simulated in procedurally generated virtual worlds. Each world\n",
            "features unique landscapes, non-playable characters (NPCs), and resources that change each round.\n",
            "The platform draws inspiration from Massively Multiplayer Online games (MMOs), which are online\n",
            "video games that facilitate interaction among a large number of players. NMMO is a platform for\n",
            "intelligent agent creation, typically parameterized by a neural network. Agents in teams must forage\n",
            "for resources to stay alive, mine materials to increase their combat and task completion capabilities,\n",
            "level up their fighting styles and equipment, practice different professions, and engage in trade based\n",
            "on market demand.\n",
            "3In the canonical setting of NMMO that will support the upcoming competition, users control 8\n",
            "out of a total of 128 simulated agents. The ultimate goal is to score more points by completing\n",
            "more tasks than the other 118 agents present in the same environment. Originally, we planned to\n",
            "introduce team-based tasks and objectives, but we decided to postpone the introduction of these\n",
            "given the practical limitations of learning libraries. After the conclusion of the competition, top\n",
            "submissions will be provided as baseline opponents. NMMO includes the following mechanisms to\n",
            "induce complexity into the environment:\n",
            "• Terrain: Navigate procedurally generated maps\n",
            "• Survival: Forage for food and water to maintain your health\n",
            "• NPC: Interact with Non-Playable Characters of varying friendliness\n",
            "• Combat: Fight other agents and NPCs with Melee, Range, and Magic\n",
            "• Profession: Use tools to practice Herbalism, Fishing, Prospecting, Carving, and Alchemy\n",
            "• Item: Acquire consumables and ammunition through professions\n",
            "• Equipment: Increase offensive and defensive capabilities with weapons and armor\n",
            "•Progression: Train combat and profession skills to access higher level items and equipment\n",
            "• Exchange: Trade items and equipment with other agents on a global market\n",
            "A detailed wiki is available on the project’s document site.\n",
            "3 Background and Related Work\n",
            "In the initial development phase of Neural MMO from 2017 to 2021, the reinforcement learning\n",
            "community witnessed the release of numerous influential environments and platforms. Particularly\n",
            "noteworthy among these are Griddly [Bamford et al., 2020], NetHack [Küttler et al., 2020], and\n",
            "MineRL [Guss et al., 2021]. A comprehensive comparison of these with the initial Neural MMO\n",
            "can be found in our previous publication [Suarez et al., 2021]. The present work primarily focuses\n",
            "on recent advancements in the reinforcement learning environments sphere. Griddly has sustained\n",
            "ongoing enhancements, while MineRL has inspired several competitive initiatives. Since 2021, only\n",
            "a few new environments have emerged, with the most pertinent ones being Melting Pot [Leibo et al.,\n",
            "2021], and XLand [Team et al., 2021]. Melting Pot and its successor, Melting Pot 2.0 [Agapiou\n",
            "et al., 2023], comprise many multiagent scenarios intended for evaluating specific facets of learning\n",
            "and intelligence. XLand and its sequel, XLand 2.0 [Team et al., 2023], present large-scale projects\n",
            "focusing on training across a varied curriculum of tasks within a procedurally generated environment,\n",
            "with a subsequent emphasis on generalization to novel tasks. Compared to Melting Pot, Neural\n",
            "MMO is a larger environment with flexible task specifications, as opposed to a set of individual\n",
            "scenarios. XLand, while architecturally akin to Neural MMO, predominantly explores two-agent\n",
            "settings, whereas Neural MMO typically accommodates 128. A crucial distinction is that XLand is\n",
            "primarily a research contribution enabling the specific experiments presented in the publication. It\n",
            "does not provide open-source access and is not computationally practical for academic-scale research.\n",
            "Conversely, Neural MMO is an open-source platform designed for computational efficiency and\n",
            "user-friendliness.\n",
            "4 Task System\n",
            "The task system of Neural MMO 2.0, a central component of the new version, comprises three\n",
            "interconnected modules: GameState, Predicates, and Tasks. This system leverages the new Neural\n",
            "MMO engine to provide full access to the game state in a structured and computationally efficient\n",
            "manner. This architectural enhancement surpasses the capabilities of Neural MMO 1.x, allowing\n",
            "users to precisely specify the tasks for agents, paving the way for task-conditional learning and testing\n",
            "generalization to unseen tasks during training.\n",
            "4.1 GameState\n",
            "The GameState module acts as a high-performance data manager, hosting the entire game state in\n",
            "a flattened tensor format instead of traditional object hierarchies. This vectorization serves a dual\n",
            "4purpose: first, it accelerates simulation speeds—a crucial factor in generating data for reinforcement\n",
            "learning; and second, it offers researchers an efficient tool to cherry-pick the required bits of data for\n",
            "defining objectives. While this format was originally inspired by the data storage patterns used in\n",
            "MMOs, adaptations were needed to support the computation of observations and definition of tasks.\n",
            "Alongside GameState, we also introduced auxiliary datastores to capture event data—unique in-game\n",
            "occurrences that would be not be captured otherwise. These datastores record things that happen,\n",
            "such as when an agent lands a successful hit on an opponent or gathers a resource, rather than just the\n",
            "outcomes, i.e. damage inflicted or a change in tile state. Events enable the task system to encompass\n",
            "a broader range of objectives in a computationally efficient manner.\n",
            "To illustrate the flexibility provided by GameState access, let’s walk through some representative\n",
            "query examples. The snippets in the GameState Appendix employ both the global and agent-specific\n",
            "GameState queries. Global access is useful for game dynamics such as time and environmental\n",
            "constants. We also provide a convenience wrapper for accessing agent-specific data.\n",
            "This query API gives researchers direct access to the mechanics of the game environment, offering a\n",
            "rich playground for studying complex multi-agent interactions, resource management strategies, and\n",
            "competitive and cooperative dynamics in a reinforcement learning context.\n",
            "4.2 Predicates\n",
            "The Predicates module offers a robust syntax for defining completion conditions within the Neural\n",
            "MMO environment.\n",
            "Predicates interface with the game state (the \"subject\") to provide convenient access to agent data and\n",
            "any additional arguments desired. Predicates return a float ranging from 0 to 1, rather than a boolean.\n",
            "This design choice supports partial completion of predicates—crucial for generating dense reward\n",
            "functions—while still allowing tasks to be considered complete when the return value equals 1. As\n",
            "a starting point, Neural MMO offers 25 built-in predicates that can access every aspect of NMMO.\n",
            "The first example in the Predicates Appendix illustrates the creation of a more complex objective,\n",
            "building on the game state and subject from the previous section.\n",
            "The second example in the Predicates Appendix demonstrates how the Predicate system can be\n",
            "used to articulate complex, high-level objectives. The FullyArmed predicate demands that a specific\n",
            "number of agents in a team be thoroughly equipped. An agent is considered fully equipped if it\n",
            "has an entire set of equipment (hat, top, bottom, weapon, ammo) of a given level. To acquire a\n",
            "complete equipment set, agents would need to utilize various professions in different locations on the\n",
            "game map, which could take several minutes to accomplish. This task’s complexity could be further\n",
            "amplified by setting a condition that each team member be outfitted specifically with melee, ranged,\n",
            "or magical equipment, necessitating the coordinated use of all eight professions.\n",
            "4.3 Tasks\n",
            "The Task API allows users to formulate tasks by combining predicates and assigning per-agent\n",
            "rewards based on the outcomes of intermediary predicates. This approach not only maintains an\n",
            "account of tasks completed but also provides a denser reward signal during training. We expect that\n",
            "most users will form tasks using the library of pre-built predicates. For advanced users, direct access\n",
            "to GameState enables mapping conditions on the game’s internal variables to rewards, circumventing\n",
            "the need for intermediate predicates. The predicate can then be turned into a task. See the Tasks\n",
            "Appendix for an example.\n",
            "5 Performance and Baselines\n",
            "Neural MMO 2.0’s new engine runs at approximately 3,000 agent steps per CPU core per second, up\n",
            "from the approximately 800 to 1,000 in the previous version. Its design focuses on native compatibility\n",
            "with a vectorized datastore that represents game state. This allows us to keep the environment in\n",
            "Python while maintaining efficiency, providing easier access for researchers looking to modify or\n",
            "extend Neural MMO.\n",
            "5Simulation throughput is highly dependent upon agent actions within the game. We compute statistics\n",
            "by having agents take random actions, but to maintain a fair estimate, we eliminate mortality since\n",
            "dead agents do not require any computation time. Given that NMMO equates one action to 0.6\n",
            "seconds of real time, a single modern CPU core can simulate at 5,000 times real-time per-agent,\n",
            "equivalent to 250M agent steps or roughly 2.5 terabytes of data per day at approximately 10 KB per\n",
            "observation.\n",
            "We also release a baseline model with training code and pretrained checkpoints. Compared to the\n",
            "previous TorchBeast [Küttler et al., 2019] baseline, our new model builds on top of CleanRL. This\n",
            "is a simpler library that is much easier to work with, but it is not designed to work with complex\n",
            "environments like Neural MMO by default. To achieve interoperability, we integrate with PufferLib,\n",
            "a library designed to streamline the various complexities of working with sophisticated environments.\n",
            "6 Limitations\n",
            "Despite its enhancements, Neural MMO 2.0 does not incorporate any novel game mechanics absent\n",
            "in version 1.x. However, in the most recent competition, even the top approaches did not learn to\n",
            "comprehend and utilize all of the game systems, and there is substantial room for improvement.\n",
            "Moreover, agent specialization within a team remained limited. These circumstances are likely\n",
            "attributable to the overly broad survival objective that invariably promotes dominant strategies, posing\n",
            "a challenge to balance. However, with the introduction of a more flexible task system in Neural MMO\n",
            "2.0, we redefine performance as the capability to execute novel tasks, thereby enabling researchers to\n",
            "harness the existing game mechanics in a way not feasible in earlier versions.\n",
            "7 Accessibility and Accountability\n",
            "Neural MMO has been under active development with continuous support for the past 6 years.\n",
            "Each of the six major releases in this period was accompanied by comprehensive documentation\n",
            "updates, a guarantee of timely user support, and direct access to the development team via through\n",
            "the community Discord. The project will continue to support and maintenance. A fourth competition\n",
            "has been accepted to NeurIPS 2023 and is expected to improve the current baseline. The code for\n",
            "this project is hosted in perpetuity by the Neural MMO GitHub organization under the MIT license.\n",
            "We provide both a pip package and a containerized setup including the baselines. Documentation is\n",
            "consistently available on neuralmmo.github.io with no major outages recorded to date. The entire\n",
            "project is available as free and open-source software under the MIT license.\n",
            "Neural MMO implements the standard PettingZoo [Terry et al., 2021] ParallelEnv API, a direct\n",
            "generalization of the OpenAI Gym [Brockman et al., 2016] API for multi-agent environments. Our\n",
            "baselines utilize CleanRL’s [Huang et al., 2021] Proximal Policy Optimization (PPO) [Schulman\n",
            "et al., 2017] implementation, one of the simplest and most widely used reinforcement learning\n",
            "frameworks, with all algorithmic details encapsulated in a single file of approximately 400 lines.\n",
            "While CleanRL was originally designed for simpler environments like single-agent Atari [Bellemare\n",
            "et al., 2012] games, Neural MMO extends its capabilities through PufferLib, which provides native\n",
            "compatibility through a multiagent vectorization backend. The details of this library are available at\n",
            "pufferai.github.io.\n",
            "8 Ethics and Responsible Use\n",
            "Neural MMO is an abstract game simulation featuring systems of combat and commerce. These\n",
            "elements are incorporated for visual interpretability and are not representative of any actual violence\n",
            "or commerce systems. We are confident that these systems are sufficiently removed from their\n",
            "real-world counterparts that Neural MMO would not be a useful training platform for developing such\n",
            "systems. The use of game-like elements in Neural MMO is a deliberate choice to align with human\n",
            "intuition and does not reflect any specific real-world scenario. Neural MMO’s primary objective\n",
            "is to facilitate research on understanding and advancing the capabilities of learning agents. The\n",
            "project does not include any real-world human data other than the code and documentation voluntarily\n",
            "submitted by contributors and some 3D asset files commissioned at fair market rate.\n",
            "69 Conclusion\n",
            "Neural MMO 2.0 is a significant evolution of the platform. We invite researchers to tackle a new\n",
            "challenge in generalization across unseen new tasks, maps, and adversaries. Furthermore, we have\n",
            "achieved significant advancements in computational efficiency, yielding a performance improvement\n",
            "of over 300%, and have ensured compatibility with popular reinforcement learning frameworks like\n",
            "CleanRL. This opens up the potential for broader utilization by researchers and makes the environment\n",
            "significantly more accessible, especially to those working with more modest computational resources.\n",
            "Neural MMO has a five-year history of continuous support and development, and we commit to\n",
            "maintaining this support, making necessary adaptations, and facilitating a lively and active community\n",
            "of users and contributors. With the concurrent NeurIPS 2023 competition, we look forward to sparking\n",
            "new research ideas, encouraging scientific exploration, and contributing to progress in multi-agent\n",
            "reinforcement learning.\n",
            "Acknowledgements\n",
            "Training compute for baselines provided by Stability AI, Carper AI, and Eleuther AI. Development for\n",
            "2.0 was an open-source project under CarperAI led by Joseph Suarez and managed by Louis Castricato.\n",
            "Web client by Parametrix.AI with artwork by Lucas de Alcântara. Technical documentation by Rose S.\n",
            "Shuman in collaboration with the development team. Engine work for 2.0 by David Bloomin. Special\n",
            "thanks to Kyoung Whan Choe for major contributions to development and ongoing environment\n",
            "support. Original project by Joseph Suarez.\n",
            "This work was supported in part by ONR MURI grant N00014-22-1-2740.\n",
            "References\n",
            "John P. Agapiou, Alexander Sasha Vezhnevets, Edgar A. Duéñez-Guzmán, Jayd Matyas, Yiran\n",
            "Mao, Peter Sunehag, Raphael Köster, Udari Madhushani, Kavya Kopparapu, Ramona Comanescu,\n",
            "DJ Strouse, Michael B. Johanson, Sukhdeep Singh, Julia Haas, Igor Mordatch, Dean Mobbs, and\n",
            "Joel Z. Leibo. Melting pot 2.0, 2023.\n",
            "Chris Bamford, Shengyi Huang, and Simon M. Lucas. Griddly: A platform for AI research in games.\n",
            "CoRR , abs/2011.06363, 2020. URL https://arxiv.org/abs/2011.06363 .\n",
            "Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning en-\n",
            "vironment: An evaluation platform for general agents. CoRR , abs/1207.4708, 2012. URL\n",
            "http://arxiv.org/abs/1207.4708 .\n",
            "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\n",
            "and Wojciech Zaremba. Openai gym, 2016. URL http://arxiv.org/abs/1606.01540 . cite\n",
            "arxiv:1606.01540.\n",
            "William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie\n",
            "Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, Manuela\n",
            "Veloso, and Phillip Wang. The minerl 2019 competition on sample efficient reinforcement learning\n",
            "using human priors, 2021.\n",
            "Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, and Jeff Braga. Cleanrl: High-quality\n",
            "single-file implementations of deep reinforcement learning algorithms. CoRR , abs/2111.08819,\n",
            "2021. URL https://arxiv.org/abs/2111.08819 .\n",
            "Heinrich Küttler, Nantas Nardelli, Thibaut Lavril, Marco Selvatici, Viswanath Sivakumar, Tim\n",
            "Rocktäschel, and Edward Grefenstette. Torchbeast: A pytorch platform for distributed RL. CoRR ,\n",
            "abs/1910.03552, 2019. URL http://arxiv.org/abs/1910.03552 .\n",
            "Heinrich Küttler, Nantas Nardelli, Alexander H. Miller, Roberta Raileanu, Marco Selvatici, Edward\n",
            "Grefenstette, and Tim Rocktäschel. The nethack learning environment, 2020.\n",
            "Joel Z. Leibo, Edgar A. Duéñez-Guzmán, Alexander Sasha Vezhnevets, John P. Agapiou, Peter\n",
            "Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie, Igor Mordatch, and Thore Graepel.\n",
            "7Scalable evaluation of multi-agent reinforcement learning with melting pot. CoRR , abs/2107.06857,\n",
            "2021. URL https://arxiv.org/abs/2107.06857 .\n",
            "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n",
            "optimization algorithms. CoRR , abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.\n",
            "06347 .\n",
            "Joseph Suarez, Yilun Du, Phillip Isola, and Igor Mordatch. Neural mmo: A massively multiagent\n",
            "game environment for training and evaluating intelligent agents, 2019.\n",
            "Joseph Suarez, Yilun Du, Igor Mordach, and Phillip Isola. Neural mmo v1.3: A massively multiagent\n",
            "game environment for training and evaluating neural networks. In Proceedings of the 19th\n",
            "International Conference on Autonomous Agents and MultiAgent Systems , AAMAS ’20, page\n",
            "2020–2022, Richland, SC, 2020. International Foundation for Autonomous Agents and Multiagent\n",
            "Systems. ISBN 9781450375184.\n",
            "Joseph Suarez, Yilun Du, Clare Zhu, Igor Mordatch, and Phillip Isola. The neural mmo plat-\n",
            "form for massively multiagent research. In J. Vanschoren and S. Yeung, editors, Proceed-\n",
            "ings of the Neural Information Processing Systems Track on Datasets and Benchmarks , vol-\n",
            "ume 1, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/\n",
            "2021/file/44f683a84163b3523afe57c2e008bc8c-Paper-round1.pdf .\n",
            "Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar\n",
            "Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari\n",
            "Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson,\n",
            "Hannah Openshaw, Jack Parker-Holder, Shreya Pathak, Nicolas Perez-Nieves, Nemanja Rakicevic,\n",
            "Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander\n",
            "Zacherl, and Lei Zhang. Human-timescale adaptation in an open-ended task space, 2023.\n",
            "Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob\n",
            "Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie\n",
            "Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin\n",
            "Dalibard, and Wojciech Marian Czarnecki. Open-ended learning leads to generally capable agents,\n",
            "2021.\n",
            "J. K. Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan,\n",
            "Luis Santos, Rodrigo Perez, Caroline Horsch, Clemens Dieffendahl, Niall L. Williams, Yashas\n",
            "Lokesh, and Praveen Ravi. Pettingzoo: Gym for multi-agent reinforcement learning, 2021.\n",
            "A Game State\n",
            "# True if any agent in subject can see a tile of tile_type\n",
            "any(tile_type.index in t for t in subject.obs.tile.material_id)\n",
            "# True if all subjects are alive.\n",
            "np.count_nonzero(subject.health > 0) == len(subject)\n",
            "# Computes the maximum l-inf distance between teammates\n",
            "current_dist = max(subject.row.max()-subject.row.min(),\n",
            "subject.col.max()-subject.col.min())\n",
            "# Tracks hits scored with a specific combat style\n",
            "hits = subject.event.SCORE_HIT.combat_style == combat_style.SKILL_ID\n",
            "# Computes the summed gold of all teammates\n",
            "subject.gold.sum()\n",
            "# Evalutaes to >= 1 if the current game tick is >= the specified tick\n",
            "gs.current_tick / num_tick\n",
            "8B Predicates\n",
            "B.1 Example 1\n",
            "# Signature for predicates\n",
            "def predicate(gs: GameState, subject: Group, **kwargs) -> float:\n",
            "def DistanceTraveled(gs: GameState, subject: Group, dist: int):\n",
            "\"\"\"True if the summed l-inf distance between each agent 's current pos and\n",
            "spawn pos is greater than or equal to the specified _dist.\"\"\"\n",
            "if not any(subject.health > 0):\n",
            "return 0\n",
            "r, c = subject.row, subject.col\n",
            "dists = utils.linf(list(zip(r,c)),[gs.spawn_pos[id_]\n",
            "for id_ in subject.entity.id])\n",
            "return max(min(dists.sum() / dist, 1.0), 0.0)\n",
            "B.2 Example 2\n",
            "def FullyArmed(gs: GameState, subject: Group,\n",
            "combat_style: nmmo_skill.CombatSkill,\n",
            "level: int, num_agent: int):\n",
            "\"\"\"True if the number of fully equipped agents >= _num_agent\n",
            "To determine fully equipped, we compare the levels of\n",
            "the hat, top, bottom, weapon, ammo to _level.\"\"\"\n",
            "WEAPON_IDS = {\n",
            "nmmo_skill.Melee: { 'weapon ':5, 'ammo ':13}, # Spear, Whetstone\n",
            "nmmo_skill.Range: { 'weapon ':6, 'ammo ':14}, # Bow, Arrows\n",
            "nmmo_skill.Mage: { 'weapon ':7, 'ammo ':15} # Wand, Runes\n",
            "}\n",
            "item_ids = { 'hat':2, 'top':3, 'bottom ':4 }\n",
            "item_ids.update(WEAPON_IDS[combat_style])\n",
            "lvl_flt = (subject.item.level >= level) & (subject.item.equipped > 0)\n",
            "type_flt = np.isin(subject.item.type_id,list(item_ids.values()))\n",
            "_, equipment_numbers = np.unique(\n",
            "subject.item.owner_id[lvl_flt & type_flt], return_counts=True)\n",
            "if num_agent == 0:\n",
            "return 1.0\n",
            "return max(min((equipment_numbers >= len(\n",
            "item_ids.items())).sum() / num_agent, 1.0), 0.0)\n",
            "C Tasks\n",
            "def KillPredicate(gs: GameState, subject: Group):\n",
            "\"\"\"The progress, the max of which is 1, should\n",
            "* increase small for each player kill\n",
            "* increase big for the 1st and 3rd kills\n",
            "* reach 1 with 10 kills\"\"\"\n",
            "num_kills = len(subject.event.PLAYER_KILL)\n",
            "progress = num_kills * 0.06\n",
            "if num_kills >= 1:\n",
            "progress += .1\n",
            "if num_kills >= 3:\n",
            "progress += .3\n",
            "return min(progress, 1.0)\n",
            "This predicate can be turned into a task like this.\n",
            "9from nmmo.task import predicate_api\n",
            "from nmmo.task.group import Group\n",
            "# Create a Predicate class with interfaces to GameState and Group\n",
            "pred_cls = predicate_api.make_predicate(KillPredicate)\n",
            "# Create a task for each agent\n",
            "task_list = []\n",
            "for agent_id in agent_list:\n",
            "task_list.append(pred_cls(subject=Group(agent_id)).create_task())\n",
            "# Create a task that evaluates and rewards a whole team together\n",
            "team_task = pred_cls(subject=Group(agent_list)).create_task()\n",
            "# Make a task that agent 1 gets rewarded for the agent 2 's evaluation\n",
            "pred_cls = predicate_api.make_predicate(AllDead)\n",
            "task_for_agent_1 = pred_cls(\n",
            "subject=Group(agent_2)).create_task(assignee=agent_1)\n",
            "Checklist\n",
            "1. For all authors...\n",
            "(a)Do the main claims made in the abstract and introduction accurately reflect the paper’s\n",
            "contributions and scope? [Yes] We claim only a release of the platform with new\n",
            "features and improvements. These are available for download now.\n",
            "(b)Did you describe the limitations of your work? [Yes] See Discussion and Limitations:\n",
            "comparable game mechanics to 1.x\n",
            "(c)Did you discuss any potential negative societal impacts of your work? [Yes] See Ethics\n",
            "and Responsible Use. This is a simulated game with no personal information that is\n",
            "substantially abstracted from real world capabilities.\n",
            "(d)Have you read the ethics review guidelines and ensured that your paper conforms to\n",
            "them? [Yes] No concerns for our work. The only portions of our work that are not\n",
            "currently open source are a set of held-out tasks. These will be released following the\n",
            "competition.\n",
            "2. If you are including theoretical results...\n",
            "(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n",
            "(b) Did you include complete proofs of all theoretical results? [N/A]\n",
            "3. If you ran experiments (e.g. for benchmarks)...\n",
            "(a)Did you include the code, data, and instructions needed to reproduce the main experi-\n",
            "mental results (either in the supplemental material or as a URL)? [Yes] The baselines\n",
            "repository is available as part of our release\n",
            "(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\n",
            "were chosen)? [Yes] Included in our training scripts. Tables to be included with the\n",
            "competition summary instead of in this manuscript.\n",
            "(c)Did you report error bars (e.g., with respect to the random seed after running exper-\n",
            "iments multiple times)? [N/A] To appear as part of the post-competition analysis of\n",
            "models\n",
            "(d)Did you include the total amount of compute and the type of resources used (e.g., type\n",
            "of GPUs, internal cluster, or cloud provider)? [Yes] Individual models are trainable in\n",
            "8 A100 hours. Desktop-grade GPUs are also fine.\n",
            "4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n",
            "(a)If your work uses existing assets, did you cite the creators? [Yes] Contributors are\n",
            "displayed prominently on our homepage. Our baseline is built on CleanRL, cited in\n",
            "this paper, and PufferLib, currently unpublished\n",
            "(b) Did you mention the license of the assets? [Yes] Everything is MIT\n",
            "10(c)Did you include any new assets either in the supplemental material or as a URL? [No]\n",
            "(d)Did you discuss whether and how consent was obtained from people whose data you’re\n",
            "using/curating? [N/A] No human data\n",
            "(e)Did you discuss whether the data you are using/curating contains personally identifiable\n",
            "information or offensive content? [Yes] No PII\n",
            "5. If you used crowdsourcing or conducted research with human subjects...\n",
            "(a)Did you include the full text of instructions given to participants and screenshots, if\n",
            "applicable? [N/A] This publication covers the environment release, not the associated\n",
            "competition\n",
            "(b)Did you describe any potential participant risks, with links to Institutional Review\n",
            "Board (IRB) approvals, if applicable? [N/A]\n",
            "(c)Did you include the estimated hourly wage paid to participants and the total amount\n",
            "spent on participant compensation? [N/A]\n",
            "11Spatio-Temporal Similarity Measure based\n",
            "Multi-Task Learning for Predicting Alzheimer’s\n",
            "Disease Progression using MRI Data\n",
            "Xulong Wang\n",
            "Department of Computer Science\n",
            "University of Sheffield\n",
            "Sheffield, UK\n",
            "xl.wang@sheffield.ac.ukYu Zhang\n",
            "Department of Computer Science\n",
            "University of Sheffield\n",
            "Sheffield, UK\n",
            "yzhang489@sheffield.ac.ukMenghui Zhou\n",
            "Department of Computer Science\n",
            "University of Sheffield\n",
            "Sheffield, UK\n",
            "mzhou47@sheffield.ac.uk\n",
            "Tong Liu\n",
            "Department of Computer Science\n",
            "University of Sheffield\n",
            "Sheffield, UK\n",
            "tliu.soton@gmail.comJun Qi\n",
            "Department of Computing\n",
            "Xi’an JiaoTong-Liverpool University\n",
            "Suzhou, China\n",
            "Jun.Qi@xjtlu.edu.cnPo Yang∗\n",
            "Department of Computer Science\n",
            "University of Sheffield\n",
            "Sheffield, UK\n",
            "po.yang@sheffield.ac.uk\n",
            "Abstract —Identifying and utilising various biomarkers for\n",
            "tracking Alzheimer’s disease (AD) progression have received\n",
            "many recent attentions and enable helping clinicians make\n",
            "the prompt decisions. Traditional progression models focus\n",
            "on extracting morphological biomarkers in regions of interest\n",
            "(ROIs) from MRI/PET images, such as regional average cortical\n",
            "thickness and regional volume. They are effective but ignore\n",
            "the relationships between brain ROIs over time, which would\n",
            "lead to synergistic deterioration. For exploring the synergistic\n",
            "deteriorating relationship between these biomarkers, in this\n",
            "paper, we propose a novel spatio-temporal similarity measure\n",
            "based multi-task learning approach for effectively predicting AD\n",
            "progression and sensitively capturing the critical relationships\n",
            "between biomarkers. Specifically, we firstly define a temporal\n",
            "measure for estimating the magnitude and velocity of biomarker\n",
            "change over time, which indicate a changing trend(temporal).\n",
            "Converting this trend into the vector, we then compare this\n",
            "variability between biomarkers in a unified vector space(spatial).\n",
            "The experimental results show that compared with directly\n",
            "ROI based learning, our proposed method is more effective in\n",
            "predicting disease progression. Our method also enables per-\n",
            "forming longitudinal stability selection to identify the changing\n",
            "relationships between biomarkers, which play a key role in\n",
            "disease progression. We prove that the synergistic deteriorating\n",
            "biomarkers between cortical volumes or surface areas have a\n",
            "significant effect on the cognitive prediction.\n",
            "Index Terms —Alzheimer’s disease, brain biomarker correla-\n",
            "tion, cosine similarity, multi-task learning\n",
            "I. I NTRODUCTION\n",
            "Alzheimer’s disease (AD) is a serious neurodegenerative\n",
            "disease, which is characterized by memory loss and cognitive\n",
            "decline due to the progressive damage of neurons and their\n",
            "connections, which directly leads to death [1]. According\n",
            "to World Health Organization (WHO), it is estimated that\n",
            "there are globally 47.5 million people with dementia in 2016\n",
            "§Po Yang is the corresponding author.with 7.7 million new cases every year. Previous research has\n",
            "focused on using biomarkers combined with machine learning\n",
            "algorithms to predict patients’ Mini Mental State Examination\n",
            "(MMSE) and Alzheimer’s Disease Assessment Scale cognitive\n",
            "subscale (ADAS-Cog) scores as the target data to predict\n",
            "whether a patient is an AD patient and find the weight of each\n",
            "biomarker feature at different prediction time points, existing\n",
            "AD disease progression models mainly use machine learning\n",
            "regression algorithms [2], survival models based on statistical\n",
            "probabilities [3], [4], and deep learning methods based on neu-\n",
            "ral networks [5]–[7]. The above-mentioned research focuses\n",
            "on using the data obtained by the patient during the first test\n",
            "(baseline data) to make predictions, which is a method that\n",
            "uses a small number of input features to make predictions.\n",
            "The disadvantage is that it ignores the information contained\n",
            "in the biomarkers in the process of changing over time.\n",
            "Previous studies focusing ROIs of brain have studied the\n",
            "differences in the correlation between brain biomarkers for\n",
            "AD, cognitively normal older individuals (NL) and mild cog-\n",
            "nitive impairment (MCI). [8] proposed a deformation-based\n",
            "framework to jointly model the effects of aging and AD on\n",
            "the evolution of brain morphology, confirming the existence of\n",
            "components that significantly accelerate aging in AD patients.\n",
            "[9] evaluated the correlation of MRI and CSF biomarkers with\n",
            "clinical diagnosis and cognitive performance in subjects with\n",
            "NL and aMCI (amnestic mild cognitive impairment) and AD\n",
            "patients. It is concluded that MRI provides stronger cross-\n",
            "sectional grouping and recognition ability and has better corre-\n",
            "lation with general cognitive and functional status on the cross-\n",
            "section, and MRI can reflect the clinically determined disease\n",
            "stage than CSF biomarkers. On the longitudinal studies, [10]\n",
            "described a novel perspective on volume trajectories and brain\n",
            "atrophy progression of single biomarkers’ differences betweenarXiv:2311.03557v1  [cs.LG]  6 Nov 2023Cortical\n",
            "Subcortical\n",
            "Region 1\n",
            "Region 2\n",
            ".\n",
            ".\n",
            ".\n",
            "average,standard deviation\n",
            "cortical volumes\n",
            "white matter volumes\n",
            "surface area  \n",
            "Region N\n",
            "subcortical volume\n",
            "Cortical\n",
            "reconstruction,\n",
            "segmentation\n",
            "Original feature extraction\n",
            "Structuring\n",
            "Spatial\n",
            " feature mapping\n",
            "≈\n",
            "Temporal\n",
            " feature mapping\n",
            "Magnitude\n",
            "Velocity\n",
            "Feature\n",
            "selection\n",
            "Stability selection\n",
            "Predicting multiple\n",
            "cognitive scores\n",
            "Multi-task learning\n",
            " framework\n",
            "Synergistic deterioration\n",
            "between biomarkers in\n",
            "AD progression\n",
            "Time point \n",
            "t\n",
            "Time point \n",
            "t+m\n",
            "Temporal\n",
            "matrix\n",
            "samples\n",
            "features\n",
            "Temporal-spatio matrix\n",
            "Calculating\n",
            "similarity\n",
            "+\n",
            "Cognitive scalesFig. 1. The protocol of MTL approach using spatio-temporal measure.\n",
            "normal aging and AD. Some previous studies focused on the\n",
            "similarity between biomarkers from ROI, [11] employed the\n",
            "correlation of regional average cortical thickness and multi-\n",
            "kernel support vector machine to integrate relevant informa-\n",
            "tion with ROI-based information to improve the classification\n",
            "performance. However, the above-mentioned researches only\n",
            "focused on the use of a single biomarker or the same type of\n",
            "biomarkers and did not focus on the relationships of temporal\n",
            "and spatial changes between different types of biomarkers.\n",
            "To address the above challenges and uncover the critical\n",
            "relationships between biomarkers, we propose to utilise the\n",
            "temporal and spatial information of brain changes to model\n",
            "the disease process of AD. Additionally, to reinforce tem-\n",
            "poral relationships between follow-up time points, a multi-\n",
            "task learning method [12] based on temporal smoothness is\n",
            "introduced for interpretably modelling disease progression.\n",
            "In this paper, we propose to utilise the spatio-temporal simi-\n",
            "larity between biomarkers changes to predict clinical scores of\n",
            "patients. Specifically, we firstly define a temporal measure for\n",
            "estimating the magnitude and velocity of biomarker change\n",
            "over time, which indicate a changing trend(Fig.1:temporal\n",
            "feature mapping). Converting this trend into the vector, we\n",
            "then compare this variability between biomarkers in a unified\n",
            "vector space(Fig.1:spatial feature mapping). The computation\n",
            "of spatial similarity results in an increase in data dimension\n",
            "by an order of magnitude of square. Faced with the scarcity\n",
            "of samples and a large number of feature dimensions, we\n",
            "introduce multiple loss terms with L1[13] and its variant norm\n",
            "[12] to overcome the Curse of dimensionality and interpretably\n",
            "capture the key relationships. The contributions of this work\n",
            "are summarized as follows:\n",
            "•A novel spatio-temporal similarity measure approach is\n",
            "proposed of analysing and extracting reliable features\n",
            "from MRI. This similarity measure will effectively quan-tify the synergistic deterioration between these biomark-\n",
            "ers over time;\n",
            "•A multi-task learning (MTL) algorithm with spatio-\n",
            "temporal embedding is designed for effectively predicting\n",
            "AD progression, visualising brain biomarkers related to\n",
            "this progression;\n",
            "•A comprehensive experimental analysis is carried out by\n",
            "accessing impact of AD progression on brain function\n",
            "synergistic deteriorating biomarkers.\n",
            "II. R ELATED WORK\n",
            "In traditional machine learning paradigm, an accurate\n",
            "learner is usually treated as one single learning task (e.g.,\n",
            "classification, regression) and learnt by a large number of\n",
            "training samples. For instance, deep learning model [5], [14]\n",
            "can train an accurate AD prediction model of neural net-\n",
            "work with hundreds of layers contacting a great amount of\n",
            "parameters via massive labelled biomarkers at baseline from\n",
            "ADNI. But one key challenge here is that sufficient and well-\n",
            "labelled longitudinal AD data at multiple time points are\n",
            "hardly collected from AD patients. The problem of missing,\n",
            "sparse and insufficient data strongly impacts on learning a\n",
            "fine model. Differing with traditional ML approaches, Multi-\n",
            "Task Learning [15] considers the prediction of AD progression\n",
            "as multiple learning tasks each of which can be a general\n",
            "prediction task art certain time point. Among these prediction\n",
            "tasks, all of them are assumed to be related to each other in\n",
            "time domain with relevant temporal features (e.g., biomarkers\n",
            "in MRI). We demonstrate a typical pipeline of leveraging\n",
            "MTL algorithms for predicting cognitive functionality of AD\n",
            "patients from their brain imaging scans [16], where the pre-\n",
            "dictive information is shared and transferred among related\n",
            "models to reinforce their generalization performance. The data\n",
            "sources employed are Freesufer (Extracted features from MRI\n",
            "like V olume of Hippocampus) and cognitive functional scores(AD cognitive scales like MMSE [17] or ADAS-cog [18])\n",
            "from selected AD patients repeatedly by multiple time points.\n",
            "By considering the prediction of cognitive scales at a single\n",
            "time point (like 6, 12 or 18 months) as a regression task. The\n",
            "prediction of clinical scores at multiple future time points as\n",
            "a multi-task regression problem. Weights of MTL are trained\n",
            "and optimized through processing pre-extracted features from\n",
            "MRI and baseline cognitive scales.\n",
            "Two important issues affect the progress of applying MTL in\n",
            "AD modelling problems. First, it is important to obtain good\n",
            "quality of baselines from AD raw data, where MRI reflects\n",
            "changes in brain structure, such as the cerebral cortex and\n",
            "ventricle; cognitive scale directly shows cognitive functions of\n",
            "AD patients. Sparse representation [19] is a popular method\n",
            "in MTL for capturing key biomarkers in AD, which uses\n",
            "sparseness as a regularization condition, image blocks with key\n",
            "characteristics. Cognitive measure can be achieved by using\n",
            "worldwide standard AD cognitive assessment, such as MMSE\n",
            "[17], ADAS-cog [18] and Rey Auditory Verbal Learning\n",
            "Test (RA VLT) [20], [21]. As the second issue, utilizing and\n",
            "improving advanced regression models [22] in MTL are highly\n",
            "critical, where they could better explore the relationship and\n",
            "correlations between MRI features and cognitive measures.\n",
            "Here, structural regularization [12] is a common approach in\n",
            "MTL for minimize the penalized empirical loss and bundling\n",
            "the correlations between tasks in the assumption. In the field\n",
            "of MTL in AD, there are many prior work that model rela-\n",
            "tionships among tasks using novel regularizations [23], [24].\n",
            "The addition of kernel method problems allows the algorithm\n",
            "to fit non-linear relationships [25]. The benchmark of this\n",
            "paradigm is derived from [26] and subsequent achievements\n",
            "are mostly aimed at theoretical structure, relevance, and fusing\n",
            "the multi-modality data applications. So far to our best knowl-\n",
            "edge, above regularized MTL approaches deliver promising\n",
            "performance in many AD prediction applications.\n",
            "III. M ETHODOLOGY\n",
            "A. Problem formulation\n",
            "Consider a MTL of ktasks with ntraining samples of d\n",
            "features. Let x1, x2, ..., x nbe the input data for the patients,\n",
            "andy1, y2, ..., y nbe the predicted cognitive scale for each\n",
            "patient, where each xi∈Rdrepresents the feature data of\n",
            "an AD patient, and yi∈Ris the predicted value of different\n",
            "cognitive scales. Specifically, xj\n",
            "i= [m ,v]denotes spatio-\n",
            "temporal ROIs similarity on features jthand(j+r)thof the\n",
            "ithsample, m,vrepresent the magnitude and velocity of two\n",
            "specific biomarkers over time scale, where j,(j+r)∈(0, d].\n",
            "Then, let X= [x1, ..., x n]T∈Rn×dbe the data matrix,\n",
            "Y= [y1, ..., y n]T∈Rn×kbe the predicted matrix, and W=\n",
            "[w1, ..., w k]T∈Rd×kbe the weight matrix. The process of\n",
            "establishing a MTL model is to estimate the value of W, which\n",
            "is the parameter to be estimated from the training samples.\n",
            "In order to solve above problem, many prior works in MTL\n",
            "that model relationships among tasks using regularization\n",
            "methods. Normally, they assume the empirical loss to be\n",
            "square loss and common regularization terms are L1andL2norms, separately named as Lasso regression and ridge\n",
            "regression models as shown in Eq. 1 and 2. Ridge regression\n",
            "constrains variables to a smaller range for reducing some\n",
            "factors with little impacts on model’s prediction. Unfortu-\n",
            "nately, this reduction means that these variables are still con-\n",
            "sidered. To solve this problem, Lasso was proposed as a new\n",
            "sparse representation linear algorithm, which simultaneously\n",
            "performs feature selection and regression. Some variables are\n",
            "set to zero directly to achieve sparsity and dimensionality\n",
            "reduction.\n",
            "min\n",
            "wL(Y, X, W ) +λ||W||1 (1)\n",
            "min\n",
            "wL(Y, X, W ) +λ||W||2 (2)\n",
            "In AD study, the task of predicting AD patient’s cognitive\n",
            "scale at certain time point is strongly associated with other\n",
            "tasks at adjacent time points. Thus, many recent studies have\n",
            "focused on designing novel structural regularization methods\n",
            "to improve their performance in AD study.\n",
            "In this paper, we concentrate on two AD progression\n",
            "prediction models : Temporal Group Lasso (TGL) [16] and\n",
            "Convex Fused Sparse Group Lasso (cFSGL) [27]. Specifically,\n",
            "TGL contains a time smoothing term and a group Lasso term\n",
            "as constraints, which ensures that all regression models at\n",
            "different time points share a common set of features. The TGL\n",
            "formulation solves the following convex optimization problem:\n",
            "min\n",
            "w||XW−Y||2\n",
            "F+θ1||W||2\n",
            "F+θ2||WH||2\n",
            "F+δ||W||2,1(3)\n",
            "where the first term measures the empirical error on the\n",
            "training data, ||W||Fis the Frobenius norm, ||WH||2\n",
            "Fis the\n",
            "temporal smoothness term, which ensures a small deviation\n",
            "between two regression models at successive time points, and\n",
            "||W||2,1is the group lasso penalty, which ensures that a small\n",
            "subset of features will be selected for the regression models\n",
            "at all-time points.\n",
            "cFSGL involves sparsity between tasks, where it considers\n",
            "both common features at different points in time and unique\n",
            "features to each task. This feature is helpful to improve the\n",
            "overall performance of the model. cFSGL formulation solves\n",
            "the following convex optimization problem:\n",
            "min\n",
            "w||XW−Y||2\n",
            "F+θ1||W||1+θ2||RWT||1+δ||W||2,1(4)\n",
            "where the first term measures the empirical error on the\n",
            "training data, ||W||1is the lasso penalty, ||RWT||1is the fused\n",
            "lasso penalty, and ||W||2,1is the group lasso penalty.\n",
            "Lasso and group lasso combined employ is called sparse\n",
            "group lasso, which allows simultaneous selection of a common\n",
            "feature for all time points and internally generates sparse\n",
            "solutions in response to different time points. Fused lasso\n",
            "penalty having a given temporal smoothness, which makes\n",
            "selected features at nearby time points similar to each other.\n",
            "In addition, notice that cFSGL’s formula involves three non-\n",
            "smooth terms. Accelerated gradient descent method is utilised\n",
            "to solve this problem.B. Definition of spatio-temporal similarity\n",
            "Two consecutive MRI scans are used to calculate the\n",
            "temporal and spatial changes of brain biomarkers. For instance,\n",
            "we utilise BL and M06 MRI to calculate the magnitude and\n",
            "velocity for biomarkers, let x be the detection value of brain\n",
            "biomarkers and t be the MRI test dates, the magnitude is\n",
            "xM06−xBL\n",
            "xBL, the velocity isxM06−xBL\n",
            "tM06−tBLper month. Use the\n",
            "magnitude and velocity to compose a vector that represents\n",
            "the changing trend of the brain biomarker.\n",
            "Cosine similarity is used to calculate the similarity between\n",
            "two vectors to express the similarity of the temporal and\n",
            "spatial changes of two MRI biomarkers. Cosine similarity\n",
            "uses the cosine value of the angle between two vectors in\n",
            "the vector space as a measure of the difference between two\n",
            "individuals. As the values of different types of biomarkers are\n",
            "different in MRI dataset, while the cosine similarity measures\n",
            "the difference in trend rather than the value. The temporal\n",
            "and spatial relationships of brain biomarkers of AD, NL and\n",
            "MCI displayed by cosine similarity, euclidean distance and\n",
            "mahalanobis distance.\n",
            "C. Experiment protocol\n",
            "Firstly, we verified that MTL is superior in following AD\n",
            "progression. Combined with randomization techniques, we\n",
            "locate stable and sensitive cortical biomarkers identified by\n",
            "MTL algorithm. Our empirical protocol design are shown in\n",
            "Fig.1. The complete experimental process mainly includes 6\n",
            "steps:\n",
            "1)Original feature extraction. Statistical features based on\n",
            "ROIs in cerebral cortex/sub-cortex are extracted from\n",
            "MRI images.\n",
            "2)Temporal feature mapping. Potential biomarkers (ROIs)\n",
            "change over time are characterized from the magnitude\n",
            "and velocity. Describing temporal changes in biomarkers\n",
            "using a two-dimensional vector.\n",
            "3)Spatial feature mapping. Calculating spatial similarity\n",
            "(cosine similarity) between vectors.\n",
            "4)Feature selection. Through this stage, the features di-\n",
            "mension is greatly reduced, and the key features of\n",
            "temporal-spatial is retained.\n",
            "5)Predicting multiple cognitive scores. Modelling the AD\n",
            "progression between biomarkers and cognitive scales via\n",
            "MTL methods.\n",
            "6)Stability selection. Embedding MTL methods in the\n",
            "general stability selection to excavate synergistic deteri-\n",
            "oration between biomarkers in AD progression.\n",
            "Secondly, cross-validation is employed to split the training\n",
            "and test data. We utilise different metrics to evaluate the\n",
            "model performance on test data. The regression performance\n",
            "metric often employed in MTL is normalized mean square\n",
            "error (nMSE) and root mean square error (rMSE) is employed\n",
            "to measure the performance of each specific regression task.\n",
            "In particular, nMSE has been normalized to each task before\n",
            "evaluation, so it is widely used in MTL methods based on\n",
            "regression tasks. Also, weighted correlation coefficient (wR) asemployed in the medical literature addressing AD progression\n",
            "problems [26], [28], [29]. nMSE, rMSE and wR are defined\n",
            "as follows:\n",
            "nMSE( Y,ˆY) =Pt\n",
            "i=1\r\r\rYi−,ˆYi\r\r\r2\n",
            "2/σ(Yi)\n",
            "Pt\n",
            "i=1ni(5)\n",
            "rMSE( y,ˆy) =r\n",
            "∥y−ˆy∥2\n",
            "2\n",
            "n(6)\n",
            "wR(Y ,ˆY) =Pt\n",
            "i=1Corr\u0010\n",
            "Yi,ˆYi\u0011\n",
            "ni\n",
            "Pt\n",
            "i=1ni(7)\n",
            "Finally, as for repeated experimental times, one evaluation\n",
            "consensus in MTL models for AD study is that one experiment\n",
            "result is usually accidental and unreliable. To reduce exper-\n",
            "iment accidental errors, repeated experiments are required.\n",
            "We also evaluate the performance of four selected regularized\n",
            "MTL models under different repeated experimental times and\n",
            "lastly evaluate typical factors like data size and number of\n",
            "tasks affecting MTL models.\n",
            "D. Stability Selection via MTL\n",
            "In order to improve the interpretability and robustness of\n",
            "the results, stability selection was modified to meet our actual\n",
            "needs. The original strategy of feature selection was included\n",
            "a Lasso algorithm as core feature subsets searches approaches.\n",
            "In this paper, MTL algorithms were utilised to embedded in\n",
            "stability selection.\n",
            "LetFbe the overall set of features and let f∈Fbe the\n",
            "subset of features by sub-sampling. Let γdenote the iteration\n",
            "number of sub-sampling and Di={X(i), Y(i)}denote one\n",
            "random sub-sample operation of number i∈(0, γ]. Each\n",
            "operation size account for ⌞n\n",
            "2⌟. Let Λbe the regularization\n",
            "parameter space. For a λ∈Λ, let ˆW(i)denote the model\n",
            "coefficient of MTFL that fitted on a subset of D(i). Then, the\n",
            "subset of features generated in task jby the sparse constraints\n",
            "of the MTFL algorithm can be denote as:\n",
            "Sλ\n",
            "j\u0000\n",
            "D(i)\u0001\n",
            "=n\n",
            "f:ˆW(i)\n",
            "j̸= 0o\n",
            ". (8)\n",
            "With stability selection, we do not simply select one model in\n",
            "the parameter space λ. Instead the data are perturbed (e.g. by\n",
            "sub-sampling) γtimes at task j and we choose all structures or\n",
            "variables that occur in a large fraction of the resulting selection\n",
            "sets:\n",
            "ˆπλ\n",
            "j=Pγ\n",
            "i=1I\u0000\n",
            "f∈Sλ\n",
            "j(Dij)\u0001\n",
            "γ. (9)\n",
            "Where indicator function I(•)denote I(x) =(\n",
            "1, x= 0\n",
            "0, others\n",
            "andˆπλ\n",
            "j∈[0,1]denote the stability probability of task j\n",
            "at MTFL approaches which feature selection is not based\n",
            "on individual operations but on multiple task collaboration\n",
            "constraints.\n",
            "Repeat the above procedure for all λ∈Λ, we obtain the\n",
            "stability score Sj(f)for each feature fat task j:\n",
            "Sj(f) = max\n",
            "λ∈Λ\u0000\n",
            "ˆπλ\n",
            "j\u0001\n",
            ". (10)TABLE I\n",
            "SCREENING SUBJECTS\n",
            "Time Span Scanning Subjects MMSE Baseline Subjects\n",
            "Baseline to M06 700 429 408\n",
            "Baseline to M12 670 429 402\n",
            "Baseline to M24 533 429 373\n",
            "Baseline to M36 337 429 327\n",
            "Finally, for a cut-off πthwith 0< π th<1and a set\n",
            "of regularization parameters Λ, the set of stable variables is\n",
            "defined as:\n",
            "ˆSstable={k:Sj(f)≥πth}=\u001a\n",
            "k: max\n",
            "λ∈Λ\u0000\n",
            "ˆπλ\n",
            "j\u0001\n",
            "≥πth\u001b\n",
            ".\n",
            "(11)\n",
            "The embedded multi-task approach ensures that the selected\n",
            "features have the following properties:1) Stability. A cortical\n",
            "region of the brain that is closely related to the subject’s\n",
            "disease progression. 2) Global significance. MTL makes sure\n",
            "that the selected features are important for each task. One\n",
            "technique that arises here is to pick the coefficient value for\n",
            "one of the tasks when doing statistics on the stability of the\n",
            "selected features at equation 4.\n",
            "IV. EXPERIMENTAL SETTINGS\n",
            "A. Subjects\n",
            "To track the effectiveness of disease progression models,\n",
            "ADNI-1 subjects with all corresponding MRI and cognitive\n",
            "scales are evaluated. As shown in the Table I. Subjects are\n",
            "between 55–90 years of age, the male accounts for 52.18%,\n",
            "the degree of suffering from the dementia, the data ratio of\n",
            "AD, MCI and NL are 25%, 50% and 25% respectively.\n",
            "To explore the impact of the correlation between ROIs on\n",
            "AD progression, MRI data from two follow-up points in the\n",
            "longitudinal cohort were extracted to facilitate observation of\n",
            "this spatiotemporal variation. At the same time, the cognitive\n",
            "scales (like MMSE or ADAS-cog) of longitudinal cohorts are\n",
            "employed to estimate the patients’ cognitive functional decline\n",
            "during the AD progression. During the screening period, all\n",
            "the subject must satisfy the data integrity for verifying the\n",
            "reliable result. Namely, the cohort subjects must complete\n",
            "participation in two follow-up point MRI scans and multiple\n",
            "cognitive scoring assessments.\n",
            "B. Data pre-processing\n",
            "For guarantees high image quality and reliable data han-\n",
            "dling, the MR images used in the paper were derived from\n",
            "standardized datasets, which provide the intensity normalized\n",
            "and gradient unwrapped TI image volumes. Subsequently, the\n",
            "FreeSurfer [30] was performed to feature extraction of the\n",
            "MR, which execute cortical reconstruction and volumetric\n",
            "segmentations for processing and analysing brain MR images.\n",
            "For each MRI, cortical regions and subcortical regions are\n",
            "generated after this pre-processing suite. For each cortical\n",
            "region, the cortical thickness average, standard deviation ofTABLE II\n",
            "DIFFERENT SIMILARITY MEASURE\n",
            "Original ROI Mahalanobis Distance Euclidean Distance Cosine Similarity\n",
            "Target: MMSE\n",
            "nMSE 0.827±0.065 0.973±0.076 0.944±0.080 0.743±0.060\n",
            "wR 0.461±0.053 0.273±0.080 0.552±0.043 0.552±0.043\n",
            "BL rMSE 1.750±0.157 1.782±1.509 1.901±0.176 1.436±0.134\n",
            "M06 rMSE 2.326±0.302 2.420±0.195 2.240±0.286 2.190±0.215\n",
            "M12 rMSE 2.599±0.366 2.943±0.291 2.505±0.415 2.541±0.415\n",
            "M24 rMSE 3.516±0.777 3.747±0.627 3.689±0.694 3.227±0.575\n",
            "M36 rMSE 4.169±0.831 4.394±0.803 5.020±0.866 4.125±0.846\n",
            "Target: ADAS-cog\n",
            "nMSE 0.693±0.054 0.773±0.087 0.790±0.067 0.666±0.058\n",
            "wR 0.579±0.041 0.514±0.057 0.488±0.055 0.604±0.043\n",
            "BL rMSE 4.093±0.388 3.809±0.371 4.238±0.470 3.670±0.606\n",
            "M06 rMSE 4.540±0.609 4.375±0.497 4.665±0.529 4.399±0.740\n",
            "M12 rMSE 4.932±0.781 4.759±0.627 4969±0.590 4.693±0.562\n",
            "M24 rMSE 5.466±0.774 6.234±1.104 6.537±1.023 5.706±0.899\n",
            "M36 rMSE 7.661±1.092 8.943±1.969 8.851±1.352 8.133±1.720\n",
            "thickness, surface area, and cortical volume were calculated as\n",
            "features. For each subcortical region, subcortical volume was\n",
            "calculated as feature. Data cleaning operations are performed\n",
            "as the following steps: 1) removal of individuals who failed\n",
            "cortical reconstruction and failed quality control; 2) removal\n",
            "of features with more than half of the sample missing values;\n",
            "3) individual subject whose removal of baseline did not screen\n",
            "for MRI; 4) using the average of the features to fill in missing\n",
            "data; and 5) removal of cognitive function tests in individuals\n",
            "with missing follow-up points in longitudinal studies.\n",
            "C. Feature selection\n",
            "To discover the impact of the similarity between ROIs on\n",
            "progression with AD, we couple all the regions in pairs, which\n",
            "allows 326 ROIs statistic features to combine 52975 features.\n",
            "For a given sample size, the higher the dimensionality, the\n",
            "sparser the distribution of the sample in space.\n",
            "To solve this issue, we utilised TGL combined with a\n",
            "stability selection algorithm to obtain features that play an\n",
            "important role for all tasks. Finally, 300 significant features\n",
            "are selected for the training of MTL algorithm.\n",
            "V. E XPERIMENTAL RESULTS AND ANALYSIS\n",
            "A. Spatio-Temporal similarity measure\n",
            "We first accomplish three relevance approaches of estimat-\n",
            "ing ROIs relevant criteria: Euclidean Distance (ED), Maha-\n",
            "lanobis Distance (MD) and Cosine Similarity (CS). And then,\n",
            "each criterion between vectors composed of the magnitude\n",
            "and the non-absolute value of velocity of the biomarker are\n",
            "used and the feature subset selecting the original feature space\n",
            "to evaluate the subjects cognitive scales. Table II shows the\n",
            "different criterion tracking the AD progression. Note that\n",
            "Table II shows only the averaged results and variance of\n",
            "30 independent experiments; and the temporal distance from\n",
            "baseline to M06 period. Besides, we also reproduced the model\n",
            "achieved by [16], [26], [27], with only MRI data as features.\n",
            "Overall the cosine similarity representation of our proposed\n",
            "ROIs synchronization approaches outperforms the original\n",
            "ROIs feature. We have the following observations: 1) The\n",
            "collaborative expression of ROIs is better than independent\n",
            "ROI to a certain extent. 2) The expression of cosine similarity\n",
            "performs better than that of cosine similarity and MahalanobisTABLE III\n",
            "ROIS SYNCHRONIZATION REPRESENTS THE PROGRESS OF AD\n",
            "Original ROI BL to M06 BL to M12 BL to M24 BL to M36\n",
            "Target: MMSE\n",
            "nMSE 0.827±0.065 0.743±0.060 0.726±0.092 0.693±0.070 0.724±0.121\n",
            "wR 0.461±0.053 0.552±0.043 0.581±0.047 0.595±0.044 0.582±0.065\n",
            "BL rMSE 1.750±0.157 1.436±0.134 1.472±0.152 1.408±0.177 1.335±0.152\n",
            "M06 rMSE 2.326±0.302 2.190±0.215 2.265±0.268 2.134±0.194 1.983±0.332\n",
            "M12 rMSE 2.599±0.366 2.541±0.415 2.440±0.335 2.559±0.481 2.053±0.306\n",
            "M24 rMSE 3.516±0.777 3.227±0.575 3.197±0.549 3.244±0.644 2.710±0.517\n",
            "M36 rMSE 4.169±0.831 4.125±0.846 4.157±0.704 3.847±0.829 3.345±0.701\n",
            "Target: ADAS-cog\n",
            "nMSE 0.693±0.054 0.666±0.058 0.691±0.087 0.653±0.075 0.881±0.056\n",
            "wR 0.579±0.041 0.604±0.043 0.592±0.051 0.626±0.049 0.387±0.053\n",
            "BL rMSE 4.093±0.388 3.670±0.606 3.522±0.329 3.648±0.479 4.084±0.414\n",
            "M06 rMSE 4.540±0.609 4.399±0.740 4.406±0.514 4.206±0.530 4.462±0.609\n",
            "M12 rMSE 4.932±0.781 4.693±0.562 4.847±0.681 4.872±0.702 5.067±0.630\n",
            "M24 rMSE 5.466±0.774 5.706±0.899 5.953±0.929 5.707±1.124 5.530±0.625\n",
            "M36 rMSE 7.661±1.092 8.133±1.720 8.100±1.349 8.255±1.727 7.761±1.518\n",
            "Distance. 3) The proposed cosine similarity representation\n",
            "witnesses significant improvement for the early time point.\n",
            "This may be due to the data spanning from baseline and M06\n",
            "period.\n",
            "B. Modelling AD progression via MTL\n",
            "Inspired by the above experiments, we further explored the\n",
            "influence of temporal span on the progress of positioning AD\n",
            "under the collaborative expression of ROIs. In this section,\n",
            "only cosine similarity was utilized to estimate the cognitive\n",
            "functional progression.\n",
            "There are four temporal span group performed, namely\n",
            "baseline to M06 period, baseline to M12 period, baseline to\n",
            "M24 period and baseline to M36 period. Table III shows that\n",
            "the normalized results of different visited time span and the\n",
            "root mean square error of each sub-task results. We follow\n",
            "the same experimental procedure as above. The experimental\n",
            "results are presented in Table III.\n",
            "We can observe from the table that as the time span\n",
            "increases, the overall generalization performance of the model\n",
            "improves. When the temporal span growths, we also have the\n",
            "following observations: 1) The performance of the subtasks\n",
            "will gradually improve. 2) The task of the latter point in\n",
            "time has been greatly enhanced. This may be due to the\n",
            "latter MRI scanning support more collaborative expression\n",
            "of ROIs and these results further validate the efficacy of the\n",
            "proposed method for temporal-spatial collaborative expression\n",
            "of ROIs. 3) during the BL to M24, the overall task performance\n",
            "outperforms others. 4) during the BL to M36, Although the\n",
            "performance of the global model has decreased, the perfor-\n",
            "mance of each subtask has been greatly improved.\n",
            "C. Stable synergistic deterioration pattern\n",
            "Firstly, we use the data from a set of experiments with\n",
            "the best performance in experiment: Temporal Span of MRI\n",
            "Scan, namely the temporal span for baseline to M24 periods,\n",
            "which contains 94 dimensions corresponding a crucial couples\n",
            "of ROIs pairs. Secondly, a set of environmental parameters\n",
            "are clearly indicated: 1) Only half of the overall sample in\n",
            "each sampling subset is randomly selected. 2) A total of\n",
            "210 combinations of model hyperparameters. 3) during every\n",
            "ST101SV-ST13SAST103TA-ST44CVST105TA-ST46CVST107CV-ST70SVST107TS-ST119TSST109SA-ST12SVST109TS-ST14CVST109TS-ST60CVST110TA-ST44SAST110TS-ST25CVST111TA-ST25CVST113SA-ST43SAST114TA-ST43SAST115CV-ST48CVST115TS-ST72CVST116TS-ST119CVST116TS-ST93CVST119TA-ST72CVST123SA-ST83CVST129CV-ST24CVST129CV-ST50SAST129CV-ST84SAST129SA-ST48CVST14TA-ST46SABLM06M12M24M360.20.40.60.81.0\n",
            "ST15SA-ST24CVST15SA-ST9SVST19SV-ST43SAST19SV-ST55TAST24SA-ST34CVST24TA-ST89SVST31CV-ST98SAST31TS-ST48SAST32TS-ST39SAST34CV-ST93CVST35SA-ST54CVST39SA-ST70SVST45TA-ST93SAST47CV-ST73CVST51CV-ST60CVST53SV-ST7SVST54TS-ST83CVST55TA-ST83CVST55TS-ST95TSST59SA-ST62SAST60SA-ST70SVST64SA-ST9SVST74CV-ST93CVST85CV-ST93CVBLM06M12M24M360.20.40.60.81.0\n",
            "ST102TS-ST109SAST104CV-ST47SAST104TA-ST46SAST105TS-ST13SAST107CV-ST39SAST107SA-ST47SAST109CV-ST13SAST109SA-ST42SVST109TA-ST25CVST109TA-ST47SAST109TA-ST50SAST109TS-ST30SVST110CV-ST62CVST110SA-ST47SAST110TS-ST13SAST113CV-ST43SAST114CV-ST13SAST116TS-ST12SVST117TS-ST48SAST118TS-ST62CVST119TS-ST48SAST129CV-ST43SABLM06M12M24M360.20.40.60.81.0\n",
            "ST129CV-ST50SAST129SA-ST13SAST129SA-ST47SAST129TA-ST93SAST13SA-ST48SAST15TS-ST50SAST24SA-ST39SAST24SA-ST45SAST24SA-ST82SAST24TA-ST50SAST30SV-ST50SAST30SV-ST71SVST32TA-ST39SAST32TA-ST98SAST45TA-ST93SAST47TS-ST71SVST48TS-ST62CVST49TA-ST71SVST51SA-ST71SVST56TS-ST93SAST82SA-ST84CVST84SA-ST93SABLM06M12M24M360.20.40.60.81.0（a）\n",
            "(b)\n",
            "(c)\n",
            "(d)Fig. 2. the vectors of stability temporal collaborative patterns. A total of 94\n",
            "and 87 stable deteriorating pairs respectively. Specifically, (a) and (b) belong\n",
            "to the MMSE-targeted model of AD progress; (c) and (d) belong to ADAS-\n",
            "cog-targeted model of AD progress.\n",
            "combination, 10 samplings were executed. Finally, the vectors\n",
            "of stability temporal collaborative patterns are showed in Fig.2.\n",
            "For the MMSE set, the result shows that the synergistic\n",
            "effect of left insula on left entorhinal cortex, left posterior\n",
            "cingulate cortex, right bankssts, left caudal anterior cingulate\n",
            "cortex, left pars triangularis. The synergistic effect of right\n",
            "posterior cingulate gyrus on right isthmus of cingulate cortex,\n",
            "left temporal pole. For the ADAS-cog set, the result shows\n",
            "that the synergistic effect of left insula on left entorhinal\n",
            "cortex, left posterior cingulate cortex, left bankssts, left pars\n",
            "triangularis. The synergistic effect of left entorhinal on left\n",
            "parahippocampal, right cuneus, medial orbitofrontal cortex.\n",
            "The synergistic effect of right posterior cingulate cortex on\n",
            "left pars triangularis, left parahippocampal. The fact that our\n",
            "findings are in line with those of previous studies [31]–[33]\n",
            "demonstrates the validity of our proposed model.\n",
            "In the selection of longitudinal stability, we observed 29\n",
            "most stable features with MMSE score, which are shown\n",
            "in Fig.2, where the horizontal axis represents the markers\n",
            "between each salient ROI pair, and details are available in the\n",
            "Appendix. The correlation features based on Cortical V olume\n",
            "and Cortical V olume are the majority (6 features), which shows\n",
            "that the similarity of the change trend of the biomarkers based\n",
            "on Cortical V olumes have important effect in AD prediction.\n",
            "Previous studies have also observed a significant improvementin the classification performance of abnormal cortical patterns\n",
            "and the coordinated patterns of cortical morphology are widely\n",
            "altered in AD patients [11]. In addition, the number of\n",
            "correlation features based on the similarity of changes between\n",
            "Surface Area and Surface Area is also relatively large (5\n",
            "features).\n",
            "VI. D ISCUSSION\n",
            "Although we modelled the spatio-temporal correlation of\n",
            "ROIs between time points, we focused only on the cognitive\n",
            "scales of the latter time point. The alignment of two cognitive\n",
            "scales would provide valuable context in the MTL settings as\n",
            "the cognitive scales might potentially also change over time.\n",
            "Additionally, we only focuses on the comparison of methods\n",
            "based on temporal smoothness and does not consider methods\n",
            "such as spatial assumptions. More comparisons will be carried\n",
            "out in future work.\n",
            "VII. CONCLUSION\n",
            "Identifying the synergistic deteriorating relationship of\n",
            "biomarkers can help clinicians assess AD progress in early\n",
            "intervention. We propose a new method to model and pre-\n",
            "dict AD progress by extracting morphological information\n",
            "from MRI. This paper has three main contributions. Firstly,\n",
            "we employ cosine similarity to represent a temporal-spatial\n",
            "relationships between brain biomarkers. We then regard the\n",
            "disease progression prediction as a MTL problem and combine\n",
            "the cosine similarity to predict the disease progression of\n",
            "AD. Finally, the stability selection is utilised to analyze the\n",
            "temporal and spatial dynamic patterns between biomarkers.\n",
            "We prove that correlate information can better describe the\n",
            "brain structural changes in patients with NL, MCI and AD.\n",
            "Experiments shows that the effectiveness of the impact of\n",
            "AD progression on brain function synergistic deteriorating\n",
            "biomarkers.\n",
            "ACKNOWLEDGMENT\n",
            "This work was supported by the China Scholarship Coun-\n",
            "cil (No.202107030007), Engineering and Physical Sciences\n",
            "Research Council (EPSRC) Doctoral Training Partnership\n",
            "(EP/T517835/1) and Young Scientists Fund of the National\n",
            "Natural Science Foundation of China (Grant No.62301452).\n",
            "REFERENCES\n",
            "[1] A. Association, “2019 alzheimer’s disease facts and figures,” Alzheimer’s\n",
            "& dementia , vol. 15, no. 3, pp. 321–387, 2019.\n",
            "[2] S. Tabarestani, M. Aghili, M. Eslami, M. Cabrerizo, A. Barreto,\n",
            "N. Rishe, R. E. Curiel, D. Loewenstein, R. Duara, and M. Adjouadi,\n",
            "“A distributed multitask multimodal approach for the prediction of\n",
            "alzheimer’s disease in a longitudinal study,” NeuroImage , vol. 206, p.\n",
            "116317, 2020.\n",
            "[3] R. S. Doody, V . Pavlik, P. Massman, S. Rountree, E. Darby, and W. Chan,\n",
            "“Predicting progression of alzheimer’s disease,” Alzheimer’s research &\n",
            "therapy , vol. 2, pp. 1–9, 2010.\n",
            "[4] C. Green, J. Shearer, C. W. Ritchie, and J. P. Zajicek, “Model-based\n",
            "economic evaluation in alzheimer’s disease: a review of the methods\n",
            "available to model alzheimer’s disease progression,” Value in health ,\n",
            "vol. 14, no. 5, pp. 621–630, 2011.[5] M. Nguyen, N. Sun, D. C. Alexander, J. Feng, and B. T. Yeo, “Modeling\n",
            "alzheimer’s disease progression using deep recurrent neural networks,”\n",
            "in2018 International Workshop on Pattern Recognition in Neuroimaging\n",
            "(PRNI) . IEEE, 2018, pp. 1–4.\n",
            "[6] S. Liu, S. Liu, W. Cai, S. Pujol, R. Kikinis, and D. Feng, “Early\n",
            "diagnosis of alzheimer’s disease with deep learning,” in 2014 IEEE 11th\n",
            "international symposium on biomedical imaging (ISBI) . IEEE, 2014,\n",
            "pp. 1015–1018.\n",
            "[7] P. Yang, G. Yang, J. Liu, J. Qi, Y . Yang, X. Wang, and T. Wang,\n",
            "“Duapm: An effective dynamic micro-blogging user activity prediction\n",
            "model towards cyber-physical-social systems,” IEEE Transactions on\n",
            "Industrial Informatics , vol. 16, no. 8, pp. 5317–5326, 2019.\n",
            "[8] R. Sivera, H. Delingette, M. Lorenzi, X. Pennec, N. Ayache, A. D. N.\n",
            "Initiative et al. , “A model of brain morphological changes related\n",
            "to aging and alzheimer’s disease from cross-sectional assessments,”\n",
            "NeuroImage , vol. 198, pp. 255–270, 2019.\n",
            "[9] P. Vemuri, H. Wiste, S. Weigand, L. Shaw, J. Trojanowski, M. Weiner,\n",
            "D. S. Knopman, R. C. Petersen, C. Jack et al. , “Mri and csf biomarkers\n",
            "in normal, mci, and ad subjects: predicting future clinical change,”\n",
            "Neurology , vol. 73, no. 4, pp. 294–301, 2009.\n",
            "[10] V . Planche, J. V . Manjon, B. Mansencal, E. Lanuza, T. Tourdias,\n",
            "G. Catheline, and P. Coup ´e, “Structural progression of alzheimer’s\n",
            "disease over decades: the mri staging scheme,” Brain Communications ,\n",
            "vol. 4, no. 3, p. fcac109, 2022.\n",
            "[11] C.-Y . Wee, P.-T. Yap, D. Shen, and A. D. N. Initiative, “Prediction\n",
            "of alzheimer’s disease and mild cognitive impairment using cortical\n",
            "morphological patterns,” Human brain mapping , vol. 34, no. 12, pp.\n",
            "3411–3425, 2013.\n",
            "[12] J. Zhou, J. Chen, and J. Ye, “Malsar: Multi-task learning via structural\n",
            "regularization,” Arizona State University , vol. 21, pp. 1–50, 2011.\n",
            "[13] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Jour-\n",
            "nal of the Royal Statistical Society Series B: Statistical Methodology ,\n",
            "vol. 58, no. 1, pp. 267–288, 1996.\n",
            "[14] M. M. Ghazi, M. Nielsen, A. Pai, M. J. Cardoso, M. Modat, S. Ourselin,\n",
            "L. Sørensen, A. D. N. Initiative et al. , “Training recurrent neural\n",
            "networks robust to incomplete data: Application to alzheimer’s disease\n",
            "progression modeling,” Medical image analysis , vol. 53, pp. 39–46,\n",
            "2019.\n",
            "[15] K.-H. Thung and C.-Y . Wee, “A brief review on multi-task learning,”\n",
            "Multimedia Tools and Applications , vol. 77, pp. 29 705–29 725, 2018.\n",
            "[16] J. Zhou, L. Yuan, J. Liu, and J. Ye, “A multi-task learning formulation\n",
            "for predicting disease progression,” in Proceedings of the 17th ACM\n",
            "SIGKDD international conference on Knowledge discovery and data\n",
            "mining , 2011, pp. 814–822.\n",
            "[17] L. Kurlowicz and M. Wallace, “The mini-mental state examination\n",
            "(mmse),” pp. 8–9, 1999.\n",
            "[18] L. Chu, K. Chiu, S. Hui, G. Yu, W. Tsui, and P. Lee, “The reliability and\n",
            "validity of the alzheimer’s disease assessment scale cognitive subscale\n",
            "(adas-cog) among the elderly chinese in hong kong.” Annals of the\n",
            "Academy of Medicine, Singapore , vol. 29, no. 4, pp. 474–485, 2000.\n",
            "[19] J. Qi, P. Yang, L. Newcombe, X. Peng, Y . Yang, and Z. Zhao, “An\n",
            "overview of data fusion techniques for internet of things enabled physical\n",
            "activity recognition and measure,” Information Fusion , vol. 55, pp. 269–\n",
            "280, 2020.\n",
            "[20] M. Schmidt et al. ,Rey auditory verbal learning test: A handbook .\n",
            "Western Psychological Services Los Angeles, CA, 1996, vol. 17.\n",
            "[21] E. Vakil and H. Blachstein, “Rey auditory-verbal learning test: structure\n",
            "analysis,” Journal of clinical psychology , vol. 49, no. 6, pp. 883–890,\n",
            "1993.\n",
            "[22] J. Wan, Z. Zhang, B. D. Rao, S. Fang, J. Yan, A. J. Saykin, and\n",
            "L. Shen, “Identifying the neuroanatomical basis of cognitive impairment\n",
            "in alzheimer’s disease by correlation-and nonlinearity-aware sparse\n",
            "bayesian learning,” IEEE transactions on medical imaging , vol. 33,\n",
            "no. 7, pp. 1475–1487, 2014.\n",
            "[23] P. Cao, X. Shan, D. Zhao, M. Huang, and O. Zaiane, “Sparse shared\n",
            "structure based multi-task learning for mri based cognitive performance\n",
            "prediction of alzheimer’s disease,” Pattern Recognition , vol. 72, pp. 219–\n",
            "235, 2017.\n",
            "[24] M. Wang, D. Zhang, D. Shen, and M. Liu, “Multi-task exclusive\n",
            "relationship learning for alzheimer’s disease progression prediction with\n",
            "longitudinal data,” Medical image analysis , vol. 53, pp. 111–122, 2019.\n",
            "[25] J. Peng, X. Zhu, Y . Wang, L. An, and D. Shen, “Structured sparsity\n",
            "regularized multiple kernel learning for alzheimer’s disease diagnosis,”\n",
            "Pattern recognition , vol. 88, pp. 370–382, 2019.[26] J. Zhou, J. Liu, V . A. Narayan, J. Ye, A. D. N. Initiative et al. , “Modeling\n",
            "disease progression via multi-task learning,” NeuroImage , vol. 78, pp.\n",
            "233–248, 2013.\n",
            "[27] J. Zhou, J. Liu, V . A. Narayan, and J. Ye, “Modeling disease progression\n",
            "via fused sparse group lasso,” in Proceedings of the 18th ACM SIGKDD\n",
            "international conference on Knowledge discovery and data mining ,\n",
            "2012, pp. 1095–1103.\n",
            "[28] K. Ito, B. Corrigan, Q. Zhao, J. French, R. Miller, H. Soares, E. Katz,\n",
            "T. Nicholas, B. Billing, R. Anziano et al. , “Disease progression model\n",
            "for cognitive deterioration from alzheimer’s disease neuroimaging ini-\n",
            "tiative database,” Alzheimer’s & Dementia , vol. 7, no. 2, pp. 151–160,\n",
            "2011.\n",
            "[29] C. M. Stonnington, C. Chu, S. Kl ¨oppel, C. R. Jack Jr, J. Ashburner, R. S.\n",
            "Frackowiak, A. D. N. Initiative et al. , “Predicting clinical scores from\n",
            "magnetic resonance scans in alzheimer’s disease,” Neuroimage , vol. 51,\n",
            "no. 4, pp. 1405–1413, 2010.\n",
            "[30] B. Fischl, “Freesurfer,” Neuroimage , vol. 62, no. 2, pp. 774–781, 2012.\n",
            "[31] Y . Liu, T. Paajanen, Y . Zhang, E. Westman, L.-O. Wahlund, A. Simmons,\n",
            "C. Tunnard, T. Sobow, P. Mecocci, M. Tsolaki et al. , “Combination\n",
            "analysis of neuropsychological tests and structural mri measures in\n",
            "differentiating ad, mci and control groups—the addneuromed study,”\n",
            "Neurobiology of Aging , vol. 32, no. 7, pp. 1198–1206, 2011.\n",
            "[32] C. Davatzikos, P. Bhatt, L. M. Shaw, K. N. Batmanghelich, and\n",
            "J. Q. Trojanowski, “Prediction of mci to ad conversion, via mri, csf\n",
            "biomarkers, and pattern classification,” Neurobiology of aging , vol. 32,\n",
            "no. 12, pp. 2322–e19, 2011.\n",
            "[33] L. Wang, F. C. Goldstein, E. Veledar, A. I. Levey, J. J. Lah, C. C.\n",
            "Meltzer, C. A. Holder, and H. Mao, “Alterations in cortical thickness\n",
            "and white matter integrity in mild cognitive impairment measured by\n",
            "whole-brain cortical thickness mapping and diffusion tensor imaging,”\n",
            "American Journal of Neuroradiology , vol. 30, no. 5, pp. 893–899, 2009.APPENDIX\n",
            "TABLE IV\n",
            "THE STABILITY ROIS PAIRS AND THE CORRESPONDING NEUROANATOMY (ADAS-COG-TARGETED)\n",
            "RIO Pairs Defination\n",
            "ST104SA-ST62CV [’Surface Area of RightParsOpercularis ⇔V olume (Cortical Parcellation) of LeftTransverseTemporal’]\n",
            "ST109CV-ST13SA [’V olume (Cortical Parcellation) of RightPosteriorCingulate ⇔Surface Area of LeftBankssts’]\n",
            "ST109SA-ST12SV [’Surface Area of RightPosteriorCingulate ⇔V olume (WM Parcellation) of LeftAmygdala’]\n",
            "ST109SA-ST47SA [’Surface Area of RightPosteriorCingulate ⇔Surface Area of LeftParsTriangularis’]\n",
            "ST109TA-ST44SA [’Cortical Thickness Average of RightPosteriorCingulate ⇔Surface Area of LeftParahippocampal’]\n",
            "ST109TA-ST48SA [’Cortical Thickness Average of RightPosteriorCingulate ⇔Surface Area of LeftPericalcarine’]\n",
            "ST109TS-ST30SV [’Cortical Thickness Standard Deviation of RightPosteriorCingulate ⇔V olume (WM Parcellation) of LeftInferiorLateralVentricle’]\n",
            "ST112SV-ST50SA [’V olume (WM Parcellation) of RightPutamen ⇔Surface Area of LeftPosteriorCingulate’]\n",
            "ST116TS-ST12SV [’Cortical Thickness Standard Deviation of RightSuperiorParietal ⇔V olume (WM Parcellation) of LeftAmygdala’]\n",
            "ST118TS-ST62CV [’Cortical Thickness Standard Deviation of RightSupramarginal ⇔V olume (Cortical Parcellation) of LeftTransverseTemporal’]\n",
            "ST129SA-ST13SA [’Surface Area of LeftInsula ⇔Surface Area of LeftBankssts’]\n",
            "ST129SA-ST25CV [’Surface Area of LeftInsula ⇔V olume (Cortical Parcellation) of LeftFrontalPole’]\n",
            "ST129SA-ST47SA [’Surface Area of LeftInsula ⇔Surface Area of LeftParsTriangularis’]\n",
            "ST13SA-ST48SA [’Surface Area of LeftBankssts ⇔Surface Area of LeftPericalcarine’]\n",
            "ST15TA-ST48SA [’Cortical Thickness Average of LeftCaudalMiddleFrontal ⇔Surface Area of LeftPericalcarine’]\n",
            "ST24SA-ST44SA [’Surface Area of LeftEntorhinal ⇔Surface Area of LeftParahippocampal’]\n",
            "ST24SA-ST71SV [’Surface Area of LeftEntorhinal ⇔V olume (WM Parcellation) of RightAmygdala’]\n",
            "ST24SA-ST82SA [’Surface Area of LeftEntorhinal ⇔Surface Area of RightCuneus’]\n",
            "ST24SA-ST98SA [’Surface Area of LeftEntorhinal ⇔Surface Area of RightMedialOrbitofrontal’]\n",
            "ST24TA-ST50SA [’Cortical Thickness Average of LeftEntorhinal ⇔Surface Area of LeftPosteriorCingulate’]\n",
            "ST32TA-ST71SV [’Cortical Thickness Average of LeftInferiorTemporal ⇔V olume (WM Parcellation) of RightAmygdala’]\n",
            "ST34TA-ST50SA [’Cortical Thickness Average of LeftIsthmusCingulate ⇔Surface Area of LeftPosteriorCingulate’]\n",
            "ST45TA-ST93SA [’Cortical Thickness Average of LeftParsOpercularis ⇔Surface Area of RightIsthmusCingulate’]\n",
            "ST49TA-ST62CV [’Cortical Thickness Average of LeftPostcentral ⇔V olume (Cortical Parcellation) of LeftTransverseTemporal’]\n",
            "ST51SA-ST62CV [’Surface Area of LeftPrecentral ⇔V olume (Cortical Parcellation) of LeftTransverseTemporal’]\n",
            "ST51SA-ST71SV [’Surface Area of LeftPrecentral ⇔V olume (WM Parcellation) of RightAmygdala’]\n",
            "ST56TS-ST93SA [’Cortical Thickness Standard Deviation of LeftSuperiorFrontal ⇔Surface Area of RightIsthmusCingulate’]\n",
            "ST73TS-ST93SA [’Cortical Thickness Standard Deviation of RightCaudalAnteriorCingulate ⇔Surface Area of RightIsthmusCingulate’]\n",
            "ST90TA-ST93SA [’Cortical Thickness Average of RightInferiorParietal ⇔Surface Area of RightIsthmusCingulate’]TABLE V\n",
            "THE STABILITY ROIS PAIRS AND THE CORRESPONDING NEUROANATOMY (MMSE-TARGETED)\n",
            "RIO Pairs Defination\n",
            "ST101SV-ST13SA [’V olume (WM Parcellation) of RightPallidum ⇔Surface Area of LeftBankssts’]\n",
            "ST102SA-ST46CV [’Surface Area of RightParacentral ⇔V olume (Cortical Parcellation) of LeftParsOrbitalis’]\n",
            "ST103TA-ST44CV [’Cortical Thickness Average of RightParahippocampal ⇔V olume (Cortical Parcellation) of LeftParahippocampal’]\n",
            "ST104CV-ST46SA [’V olume (Cortical Parcellation) of RightParsOpercularis ⇔Surface Area of LeftParsOrbitalis’]\n",
            "ST105TA-ST46CV [’Cortical Thickness Average of RightParsOrbitalis ⇔V olume (Cortical Parcellation) of LeftParsOrbitalis’]\n",
            "ST107CV-ST70SV [’V olume (Cortical Parcellation) of RightPericalcarine ⇔V olume (WM Parcellation) of RightAccumbensArea’]\n",
            "ST107TA-ST93CV [’Cortical Thickness Average of RightPericalcarine ⇔V olume (Cortical Parcellation) of RightIsthmusCingulate’]\n",
            "ST108TS-ST60TA [’Cortical Thickness Standard Deviation of RightPostcentral ⇔Cortical Thickness Average of LeftTemporalPole’]\n",
            "ST109SA-ST12SV [’Surface Area of RightPosteriorCingulate ⇔V olume (WM Parcellation) of LeftAmygdala’]\n",
            "ST109SA-ST93CV [’Surface Area of RightPosteriorCingulate ⇔V olume (Cortical Parcellation) of RightIsthmusCingulate’]\n",
            "ST109TS-ST14CV [’Cortical Thickness Standard Deviation of RightPosteriorCingulate ⇔V olume (Cortical Parcellation) of LeftCaudalAnteriorCingulate’]\n",
            "ST109TS-ST48TA [’Cortical Thickness Standard Deviation of RightPosteriorCingulate ⇔Cortical Thickness Average of LeftPericalcarine’]\n",
            "ST109TS-ST60CV [’Cortical Thickness Standard Deviation of RightPosteriorCingulate ⇔V olume (Cortical Parcellation) of LeftTemporalPole’]\n",
            "ST110SA-ST98SA [’Surface Area of RightPrecentral ⇔Surface Area of RightMedialOrbitofrontal’]\n",
            "ST110TA-ST44SA [’Cortical Thickness Average of RightPrecentral ⇔Surface Area of LeftParahippocampal’]\n",
            "ST110TA-ST74SA [’Cortical Thickness Average of RightPrecentral ⇔Surface Area of RightCaudalMiddleFrontal’]\n",
            "ST110TS-ST25CV [’Cortical Thickness Standard Deviation of RightPrecentral ⇔V olume (Cortical Parcellation) of LeftFrontalPole’]\n",
            "ST111CV-ST46SA [’V olume (Cortical Parcellation) of RightPrecuneus ⇔Surface Area of LeftParsOrbitalis’]\n",
            "ST111TA-ST25CV [’Cortical Thickness Average of RightPrecuneus ⇔V olume (Cortical Parcellation) of LeftFrontalPole’]\n",
            "ST113SA-ST39SA [’Surface Area of RightRostralAnteriorCingulate ⇔Surface Area of LeftMedialOrbitofrontal’]\n",
            "ST113SA-ST43SA [’Surface Area of RightRostralAnteriorCingulate ⇔Surface Area of LeftParacentral’]\n",
            "ST113SA-ST74SA [’Surface Area of RightRostralAnteriorCingulate ⇔Surface Area of RightCaudalMiddleFrontal’]\n",
            "ST114TA-ST43SA [’Cortical Thickness Average of RightRostralMiddleFrontal ⇔Surface Area of LeftParacentral’]\n",
            "ST114TA-ST93CV [’Cortical Thickness Average of RightRostralMiddleFrontal ⇔V olume (Cortical Parcellation) of RightIsthmusCingulate’]\n",
            "ST115CV-ST48CV [’V olume (Cortical Parcellation) of RightSuperiorFrontal ⇔V olume (Cortical Parcellation) of LeftPericalcarine’]\n",
            "ST115TS-ST42SV [’Cortical Thickness Standard Deviation of RightSuperiorFrontal ⇔V olume (WM Parcellation) of LeftPallidum’]\n",
            "ST115TS-ST72CV [’Cortical Thickness Standard Deviation of RightSuperiorFrontal ⇔V olume (Cortical Parcellation) of RightBankssts’]\n",
            "ST116TS-ST25CV [’Cortical Thickness Standard Deviation of RightSuperiorParietal ⇔V olume (Cortical Parcellation) of LeftFrontalPole’]\n",
            "ST116TS-ST93CV [’Cortical Thickness Standard Deviation of RightSuperiorParietal ⇔V olume (Cortical Parcellation) of RightIsthmusCingulate’]\n",
            "ST119TA-ST72CV [’Cortical Thickness Average of RightTemporalPole ⇔V olume (Cortical Parcellation) of RightBankssts’]\n",
            "ST120SV-ST46SA [’V olume (WM Parcellation) of RightThalamus ⇔Surface Area of LeftParsOrbitalis’]\n",
            "ST123SA-ST83CV [’Surface Area of RightUnknown ⇔V olume (Cortical Parcellation) of RightEntorhinal’]\n",
            "ST124SV-ST71SV [’V olume (WM Parcellation) of RightVentralDC ⇔V olume (WM Parcellation) of RightAmygdala’]\n",
            "ST129CV-ST24CV [’V olume (Cortical Parcellation) of LeftInsula ⇔V olume (Cortical Parcellation) of LeftEntorhinal’]\n",
            "ST129CV-ST50SA [’V olume (Cortical Parcellation) of LeftInsula ⇔Surface Area of LeftPosteriorCingulate’]\n",
            "ST129CV-ST72CV [’V olume (Cortical Parcellation) of LeftInsula ⇔V olume (Cortical Parcellation) of RightBankssts’]\n",
            "ST129CV-ST84SA [’V olume (Cortical Parcellation) of LeftInsula ⇔Surface Area of RightFrontalPole’]\n",
            "ST129SA-ST14CV [’Surface Area of LeftInsula ⇔V olume (Cortical Parcellation) of LeftCaudalAnteriorCingulate’]\n",
            "ST14TA-ST46SA [’Cortical Thickness Average of LeftCaudalAnteriorCingulate ⇔Surface Area of LeftParsOrbitalis’]\n",
            "ST15SA-ST24CV [’Surface Area of LeftCaudalMiddleFrontal ⇔V olume (Cortical Parcellation) of LeftEntorhinal’]\n",
            "ST15SA-ST30SV [’Surface Area of LeftCaudalMiddleFrontal ⇔V olume (WM Parcellation) of LeftInferiorLateralVentricle’]\n",
            "ST15SA-ST9SV [’Surface Area of LeftCaudalMiddleFrontal ⇔V olume (WM Parcellation) of FourthVentricle’]\n",
            "ST19SV-ST43SA [’V olume (WM Parcellation) of LeftCerebralCortex ⇔Surface Area of LeftParacentral’]\n",
            "ST19SV-ST54CV [’V olume (WM Parcellation) of LeftCerebralCortex ⇔V olume (Cortical Parcellation) of LeftRostralAnteriorCingulate’]\n",
            "ST19SV-ST55TA [’V olume (WM Parcellation) of LeftCerebralCortex ⇔Cortical Thickness Average of LeftRostralMiddleFrontal’]\n",
            "ST24SA-ST34CV [’Surface Area of LeftEntorhinal ⇔V olume (Cortical Parcellation) of LeftIsthmusCingulate’]\n",
            "ST24TA-ST30SV [’Cortical Thickness Average of LeftEntorhinal ⇔V olume (WM Parcellation) of LeftInferiorLateralVentricle’]\n",
            "ST31CV-ST98SA [’V olume (Cortical Parcellation) of LeftInferiorParietal ⇔Surface Area of RightMedialOrbitofrontal’]\n",
            "ST31SA-ST46CV [’Surface Area of LeftInferiorParietal ⇔V olume (Cortical Parcellation) of LeftParsOrbitalis’]\n",
            "ST32TA-ST62CV [’Cortical Thickness Average of LeftInferiorTemporal ⇔V olume (Cortical Parcellation) of LeftTransverseTemporal’]\n",
            "ST34CV-ST44CV [’V olume (Cortical Parcellation) of LeftIsthmusCingulate ⇔V olume (Cortical Parcellation) of LeftParahippocampal’]\n",
            "ST34CV-ST93CV [’V olume (Cortical Parcellation) of LeftIsthmusCingulate ⇔V olume (Cortical Parcellation) of RightIsthmusCingulate’]\n",
            "ST35SA-ST47SA [’Surface Area of LeftLateralOccipital ⇔Surface Area of LeftParsTriangularis’]\n",
            "ST39SA-ST43SA [’Surface Area of LeftMedialOrbitofrontal ⇔Surface Area of LeftParacentral’]\n",
            "ST39SA-ST70SV [’Surface Area of LeftMedialOrbitofrontal ⇔V olume (WM Parcellation) of RightAccumbensArea’]\n",
            "ST45TA-ST93SA [’Cortical Thickness Average of LeftParsOpercularis ⇔Surface Area of RightIsthmusCingulate’]\n",
            "ST47CV-ST72CV [’V olume (Cortical Parcellation) of LeftParsTriangularis ⇔V olume (Cortical Parcellation) of RightBankssts’]\n",
            "ST47CV-ST73CV [’V olume (Cortical Parcellation) of LeftParsTriangularis ⇔V olume (Cortical Parcellation) of RightCaudalAnteriorCingulate’]\n",
            "ST48TS-ST74SA [’Cortical Thickness Standard Deviation of LeftPericalcarine ⇔Surface Area of RightCaudalMiddleFrontal’]\n",
            "ST51CV-ST60CV [’V olume (Cortical Parcellation) of LeftPrecentral ⇔V olume (Cortical Parcellation) of LeftTemporalPole’]\n",
            "ST53SV-ST5SV [’V olume (WM Parcellation) of LeftPutamen ⇔V olume (WM Parcellation) of CorpusCallosumMidPosterior’]\n",
            "ST53SV-ST7SV [’V olume (WM Parcellation) of LeftPutamen ⇔V olume (WM Parcellation) of Csf’]\n",
            "ST54TS-ST69SV [’Cortical Thickness Standard Deviation of LeftRostralAnteriorCingulate ⇔V olume (WM Parcellation) of OpticChiasm’]\n",
            "ST55TA-ST83CV [’Cortical Thickness Average of LeftRostralMiddleFrontal ⇔V olume (Cortical Parcellation) of RightEntorhinal’]\n",
            "ST55TS-ST73CV [’Cortical Thickness Standard Deviation of LeftRostralMiddleFrontal ⇔V olume (Cortical Parcellation) of RightCaudalAnteriorCingulate’]\n",
            "ST55TS-ST95TS [’Cortical Thickness Standard Deviation of LeftRostralMiddleFrontal ⇔Cortical Thickness Standard Deviation of RightLateralOrbitofrontal’]\n",
            "ST56TS-ST84SA [’Cortical Thickness Standard Deviation of LeftSuperiorFrontal ⇔Surface Area of RightFrontalPole’]\n",
            "ST59SA-ST62SA [’Surface Area of LeftSupramarginal ⇔Surface Area of LeftTransverseTemporal’]\n",
            "ST60SA-ST70SV [’Surface Area of LeftTemporalPole ⇔V olume (WM Parcellation) of RightAccumbensArea’]\n",
            "ST62TS-ST83CV [’Cortical Thickness Standard Deviation of LeftTransverseTemporal ⇔V olume (Cortical Parcellation) of RightEntorhinal’]\n",
            "ST73CV-ST93CV [’V olume (Cortical Parcellation) of RightCaudalAnteriorCingulate ⇔V olume (Cortical Parcellation) of RightIsthmusCingulate’]Multi-task Learning for Optical Coherence\n",
            "Tomography Angiography (OCTA) Vessel\n",
            "Segmentation\n",
            "Can Koz Onat Dalmaz Mertay Dayanc\n",
            "Department of Computer Science Department of Electrical Engineering Google\n",
            "University of Oxford, OX, UK Stanford University, CA, USA Mountain View, CA, USA\n",
            "can.koz@some.ox.ac.uk onat@stanford.edu mertaydayancc@gmail.com\n",
            "Abstract\n",
            "Optical Coherence Tomography Angiography (OCTA) is a non-invasive imaging\n",
            "technique that provides high-resolution cross-sectional images of the retina, which\n",
            "are useful for diagnosing and monitoring various retinal diseases. However, manual\n",
            "segmentation of OCTA images is a time-consuming and labor-intensive task, which\n",
            "motivates the development of automated segmentation methods. In this paper,\n",
            "we propose a novel multi-task learning method for OCTA segmentation, called\n",
            "OCTA-MTL, that leverages an image-to-DT (Distance Transform) branch and an\n",
            "adaptive loss combination strategy. The image-to-DT branch predicts the distance\n",
            "from each vessel voxel to the vessel surface, which can provide useful shape prior\n",
            "and boundary information for the segmentation task. The adaptive loss combination\n",
            "strategy dynamically adjusts the loss weights according to the inverse of the average\n",
            "loss values of each task, to balance the learning process and avoid the dominance\n",
            "of one task over the other. We evaluate our method on the ROSE-2 dataset its\n",
            "superiority in terms of segmentation performance against two baseline methods:\n",
            "a single-task segmentation method and a multi-task segmentation method with a\n",
            "fixed loss combination.\n",
            "1 Introduction\n",
            "In recent years, Optical Coherence Tomography (OCT) has emerged as a powerful non-invasive\n",
            "imaging technique that has revolutionized the field of ophthalmology by providing high-resolution\n",
            "cross-sectional images of biological tissues, particularly the retina [ 1,2]. One of the pivotal down-\n",
            "stream analysis of OCT angiography (OCTA) lies in segmentation, a process that involves extracting\n",
            "and delineating Henle’s fiber layer (HFL) and retinal blood vessels within these images [ 3]. OCT\n",
            "segmentation plays a critical role in diagnosing and monitoring various retinal diseases, such as dia-\n",
            "betic retinopathy and neovascularization, while aiding in understanding complex structures within the\n",
            "images [ 4]. Unfortunately, manual segmentation of OCTA images by clinicians is a time-consuming\n",
            "and labor-intensive task. Consequently, this has sparked interest in the development of automated\n",
            "segmentation methods [ 3–5]. Deep learning, a cornerstone of modern medical image analysis,\n",
            "unsuprisingly have paved the way in automatic segmentation of OCTA images [ 6]. Despite their\n",
            "prowess, deep models are commonly trained via single objective functions such as binary cross\n",
            "entropy (BCE) loss or dice loss [ 7–9]. Such singular objective functions may not capture the rich\n",
            "complexity and diversity of representations that can arise from OCTA segmentation, especially for\n",
            "tubular structures such as vessels [ 8]. Multi-task learning trains a single model to handle multiple\n",
            "related tasks together, leveraging shared knowledge to improve performance across all tasks [ 10].\n",
            "By jointly optimizing on multiple tasks, the model can learn common patterns and representations,\n",
            "leading to enhanced efficiency and understanding of the data [ 11]. Previous studies have shown that\n",
            "deep multi-task methods can significantly improve performance and generalization of medical image\n",
            "analysis models [11].\n",
            "37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2311.02266v1  [eess.IV]  3 Nov 2023Figure 1: The overall architecture of OCTA-MTL, a novel multi-task learning method for OCTA\n",
            "segmentation.\n",
            "In this study, we introduce a multi-task approach (OCTA-MTL) that effectively captures complex\n",
            "and diverse features of OCTA images. Unlike vanilla segmentation architectures, OCTA-MTL\n",
            "leverages an image-to-DT (Distance Transform) branch that computes the distance from each vessel\n",
            "voxel to the vessel surface which can provide useful shape prior and boundary information for the\n",
            "encoder during training. Image-to-DT branch and segmentation head employs a joint encoder, and the\n",
            "aggregate network is simultaneously optimized via pixel-wise (regression) and binary cross-entropy\n",
            "(classification) losses. We then introduce a novel adaptive combination strategy that controls relative\n",
            "weighings of these loss functions. Demonstrations on ROSE-2 dataset [ 3] shows OCTA-MTL’s\n",
            "superiority in terms of accuracy, robustness, and generalization against a single-task (Unet) method\n",
            "and a multi-task variant that employs an existing loss weighting strategy.\n",
            "2 Methods\n",
            "2.1 OCTA-MTL\n",
            "OCTA-MTL, consists of two main components: a segmentation head and an image-to-DT branch.\n",
            "The segmentation head is responsible for predicting the binary mask of the vessels, whereas the\n",
            "image-to-DT branch is responsible for predicting the distance transform (DT) of the vessels. DT is\n",
            "a mathematical operator that assigns to each voxel in the image the distance to the nearest vessel\n",
            "surface. DT can provide useful shape prior and boundary information for the segmentation task,\n",
            "as well as facilitate the evaluation of segmentation quality. Both the segmentation head and the\n",
            "image-to-DT branch share a common encoder, which extracts high-level features from the input\n",
            "OCTA image. The overall architecture of OCTA-MTL is illustrated in Figure 1.\n",
            "Segmentation head The segmentation head of OCTA-MTL is a decoder that reconstructs the binary\n",
            "mask of the vessels from the encoder output. The final output of the decoder is passed through a\n",
            "sigmoid activation function to obtain the probability of each voxel belonging to the vessel class. Here,\n",
            "a binary cross-entropy (BCE) loss is calculated.\n",
            "Image-to-DT branch The image-to-DT branch of OCTA-MTL is a dense-pixel regression network\n",
            "that predicts the DT of the vessels from the encoder output. The predicted DT is then compared with\n",
            "the ground truth DT, which is computed from the ground truth binary mask of the vessels using the\n",
            "Euclidean distance transform algorithm [12].\n",
            "Adaptive loss combination To train OCTA-MTL, we use a combination of two loss functions: a\n",
            "binary cross-entropy (BCE) loss for the segmentation task and a mean squared error (MSE) loss\n",
            "for the image-to-DT task. Simply adding these two loss functions may not result in an optimal\n",
            "performance, as the scale and importance of each task may vary [ 13]. Therefore, we introduce a\n",
            "novel adaptive loss combination strategy that controls the relative weighting of these loss functions.\n",
            "Our strategy is based on the idea of dynamically adjusting the loss weights according to the inverse\n",
            "of the average loss values for each task. Specifically, we define a loss weight as follows:\n",
            "2Table 1: Quantitative comparison of the segmentation performance in terms of average dice coefficient\n",
            "(Dice) and the intersection over union (IoU) scores of competing methods on test set.\n",
            "Proposed Multi-task [13] Single Unet [14]\n",
            "Dice 0.57 0.57 0.54\n",
            "IoU 0.35 0.30 0.20\n",
            "Figure 2: OCTA images, ground truth segmentation maps, and maps predicted by the competing\n",
            "methods are shown in the figure.\n",
            "α=E[||LBCE||]/E[||LMSE||] (1)\n",
            "where||LBCE||and||LMSE||are the loss values of BCE and MSE. The intuition behind this strategy\n",
            "is that the task with a higher average loss value should be assigned a lower weight, as it indicates\n",
            "that the task is more difficult or less important than the other task. By doing so, we can balance the\n",
            "learning process of both tasks and avoid the dominance of one task over the other. In implementation,\n",
            "value of αis empirically estimated and updated within a batch. The final loss function for OCTA-MTL\n",
            "is then calculated via weighing MSE loss with αand linearly combining it with BCE loss:\n",
            "L=||LBCE||+α· ||LBCE|| (2)\n",
            "3 Results\n",
            "We evaluated our proposed method, OCTA-MTL, on the ROSE-2 dataset [ 3], and compared it\n",
            "with two baseline methods: a single-task segmentation method (Single Unet) [ 14] and a multi-task\n",
            "segmentation method that combined losses via an other common algorithm (Multi-task) [ 13]. We\n",
            "used the dice coefficient (Dice) and the intersection over union (IoU) scores as the evaluation metrics.\n",
            "Table 1 shows that our method achieved the highest Dice and IoU scores, indicating its accuracy\n",
            "and consistency. Figure 2 shows a qualitative comparison of the segmentation results for a sample\n",
            "test slice. The proposed method produced more accurate and consistent segmentation results than\n",
            "the other methods, as it can better capture the complex and diverse features of the vessels, while\n",
            "avoiding the false positives and false negatives that are present in the other methods, especially in the\n",
            "regions with low contrast or high noise. These findings indicate that the proposed method benefits\n",
            "from effective leverage of image-to-DT branch and adaptive loss combination strategy.\n",
            "4 Discussion\n",
            "In this paper, we proposed OCTA-MTL, a novel multi-task learning method for OCTA segmentation\n",
            "that leverages an image-to-DT branch and an adaptive loss combination strategy. We showed that\n",
            "our method can effectively capture the complex and diverse features of the OCTA images, and\n",
            "improve the segmentation performance and generalization over the baseline methods. The proposed\n",
            "image-to-DT branch provides useful shape prior and boundary information for the segmentation task,\n",
            "while the adaptive loss combination strategy balances the learning process of both tasks and avoids\n",
            "the dominance of one task over the other. Our method has several limitations and directions for future\n",
            "work, such as evaluating it on other OCTA datasets, exploring transformer-based architectures and\n",
            "loss functions, and extending it to segment other structures in the OCTA images.\n",
            "35 Potential Negative Societal Impact\n",
            "This study aims to utilize a multi-task learning approach for improved performance and reliability\n",
            "in medical image segmentation. Consequently, our work does not put society in a negative position.\n",
            "This work could be utilized to improve the effectiveness of automated image analysis tasks in the\n",
            "future. Lastly, we are confident that the suggested approach does not exploit any potential bias in the\n",
            "data.\n",
            "References\n",
            "[1]T. E. De Carlo, A. Romano, N. K. Waheed, and J. S. Duker, “A review of optical coherence\n",
            "tomography angiography (octa),” International journal of retina and vitreous , vol. 1, no. 1, p. 5,\n",
            "2015.\n",
            "[2]N. Feucht, M. Maier, G. Lepennetier, M. Pettenkofer, C. Wetzlmair, T. Daltrozzo, and et\n",
            "al., “Optical coherence tomography angiography indicates associations of the retinal vascular\n",
            "network and disease activity in multiple sclerosis,” Ophthalmic research , vol. 61, no. 1, pp. 1–8,\n",
            "2019.\n",
            "[3]Y . Ma, H. Hao, J. Xie, H. Fu, J. Zhang, J. Yang, Z. Wang, J. Liu, Y . Zheng, and Y . Zhao, “Rose:\n",
            "a retinal oct-angiography vessel segmentation dataset and new model,” IEEE Transactions on\n",
            "Medical Imaging , vol. 40, no. 3, pp. 928–939, 2021.\n",
            "[4]K. M. Meiburger, M. Salvi, G. Rotunno, W. Drexler, and M. Liu, “Automatic segmentation and\n",
            "classification methods using optical coherence tomography angiography (octa): A review and\n",
            "handbook,” Applied Sciences , vol. 11, no. 20, 2021.\n",
            "[5]G. Kim, J. Kim, W. J. Choi, C. Kim, and S. Lee, “Integrated deep learning framework for\n",
            "accelerated optical coherence tomography angiography,” Scientific Reports , vol. 12, 01 2022.\n",
            "[6]D. Le, T. Son, and X. Yao, “Machine learning in optical coherence tomography angiography,”\n",
            "Experimental Biology and Medicine , vol. 246, pp. 2170–2183, July 2021.\n",
            "[7]J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and Y . Zhou, “Transunet:\n",
            "Transformers make strong encoders for medical image segmentation,” 2021.\n",
            "[8]S. Cansiz, C. Kesim, S. N. Bektas, Z. Kulali, M. Hasanreisoglu, and C. Gunduz-Demir, “Fouri-\n",
            "ernet: Shape-preserving network for henle’s fiber layer segmentation in optical coherence\n",
            "tomography images,” IEEE Journal of Biomedical and Health Informatics , vol. 27, no. 2,\n",
            "pp. 1036–1047, 2023.\n",
            "[9]P. Moeskops, J. M. Wolterink, B. H. M. van der Velden, K. G. A. Gilhuijs, T. Leiner, M. A.\n",
            "Viergever, and I. Iš gum, “Deep learning for multi-task medical image segmentation in multiple\n",
            "modalities,” in Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016 ,\n",
            "pp. 478–486, Springer International Publishing, 2016.\n",
            "[10] Y . Zhao, X. Wang, T. Che, G. Bao, and S. Li, “Multi-task deep learning for medical image\n",
            "computing and analysis: A review,” Comput. Biol. Med. , vol. 153, feb 2023.\n",
            "[11] R. Caruana Machine Learning , vol. 28, no. 1, pp. 41–75, 1997.\n",
            "[12] G. Borgefors, “Distance transformations in digital images,” Computer Vision, Graphics, and\n",
            "Image Processing , vol. 34, no. 3, pp. 344–371, 1986.\n",
            "[13] A. Kendall, Y . Gal, and R. Cipolla, “Multi-task learning using uncertainty to weigh losses for\n",
            "scene geometry and semantics,” 2018.\n",
            "[14] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image\n",
            "segmentation,” in Medical Image Computing and Computer-Assisted Intervention - MICCAI\n",
            "2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part\n",
            "III(N. Navab, J. Hornegger, W. M. W. III, and A. F. Frangi, eds.), vol. 9351 of Lecture Notes in\n",
            "Computer Science , pp. 234–241, Springer, 2015.\n",
            "4LEARNING A MULTI -TASK TRANSFORMER VIA\n",
            "UNIFIED AND CUSTOMIZED INSTRUCTION TUNING FOR\n",
            "CHEST RADIOGRAPH INTERPRETATION\n",
            "A P REPRINT\n",
            "Lijian Xu∗,B,1,2, Ziyu Ni∗,3, Xinglong Liu3, Xiaosong WangB,2, Hongsheng Li1,4, and Shaoting Zhang2\n",
            "1Centre for Perceptual and Interactive Intelligence, the Chinese University of Hong Kong, Hong Kong\n",
            "2Shanghai Artificial Intelligence Laboratory, Shanghai\n",
            "3Sensetime Research, Shanghai\n",
            "4Department of Electronic Engineering, the Chinese University of Hong Kong, Hong Kong\n",
            "∗Equal contributions\n",
            "ABSTRACT\n",
            "The emergence of multi-modal deep learning models has made significant impacts on clinical\n",
            "applications in the last decade. However, the majority of models are limited to single-tasking, without\n",
            "considering disease diagnosis is indeed a multi-task procedure. Here, we demonstrate a unified\n",
            "transformer model specifically designed for multi-modal clinical tasks by incorporating customized\n",
            "instruction tuning. We first compose a multi-task training dataset comprising 13.4 million instruction\n",
            "and ground-truth pairs (with approximately one million radiographs) for the customized tuning,\n",
            "involving both image- and pixel-level tasks. Thus, we can unify the various vision-intensive tasks\n",
            "in a single training framework with homogeneous model inputs and outputs to increase clinical\n",
            "interpretability in one reading. Finally, we demonstrate the overall superior performance of our model\n",
            "compared to prior arts on various chest X-ray benchmarks across multi-tasks in both direct inference\n",
            "and finetuning settings. Three radiologists further evaluate the generated reports against the recorded\n",
            "ones, which also exhibit the enhanced explainability of our multi-task model.\n",
            "Keywords Instruction Tuning ·Multi-task Learning ·Chest X-ray ·Explainability ·Computer-aided Diagnosis\n",
            "1 Introduction\n",
            "Chest radiography (CXR) is a non-invasive and relatively low-cost diagnostic radiology examination for screening and\n",
            "diagnosis of various thoracic diseases affecting the lung and heart [ 1]. However, the interpretation of CXR is greatly\n",
            "challenged by its low sensitivity of subtle abnormalities, overlapping structures, and limited soft tissue details, and\n",
            "therefore, depends heavily on the capability and experience of radiologists [ 2]. On the other hand, the growing demand\n",
            "for CXR examination has brought a burden on medical professionals, which also limits the clinical application of CXR,\n",
            "especially in community clinics or primary hospitals. In this context, automated diagnosis by AI could potentially\n",
            "contribute to reducing the workload of radiologists.\n",
            "Large Language Models [ 3;4] have revolutionized natural language processing and developed the capability to generate\n",
            "responses that closely resemble those from humans. They excel at a wide range of tasks, including language translation,\n",
            "question answering, and text generation[ 5;6;7;8;9;10;11]. Models like ChatGPT (OpenAI) [ 12;13] and Med-PaLm\n",
            "(Google) [ 14;15]have also demonstrated the powerful reasoning capabilities of language models in complex scenarios\n",
            "like medical diagnosis to assist professionals in delivering care. Nonetheless, such tasks are limited to a more general\n",
            "medical scope and largely rely on the visual features on the image level, without touching the pixel-level vision tasks,\n",
            "e.g., disease localization and segmentation. Moreover, to further improve the model’s ability on downstream tasks,\n",
            "supervised instruction tuning with specific downstream task-oriented data is often required on language only [ 16;17]\n",
            "and vision-language tasks[18; 19; 20], individually.arXiv:2311.01092v1  [cs.CV]  2 Nov 2023LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "On the other hand, the development of the generalist model in the medical field has lagged. Most models are designed\n",
            "primarily for \"pure\" language tasks [ 21;14]. Several generalist models for the biomedical field have been recently\n",
            "proposed and achieved progress in the VQA task [ 22;23;24;25;26;27;28;29]. On the other hand, the present\n",
            "multi-modality models are not well-suited for traditional image processing tasks like detection and segmentation.\n",
            "Existing methods face discrepancies in input, output, and training processes between visual tasks (e.g., detection,\n",
            "segmentation) and language tasks (e.g., image captioning and VQA), which hinders efficient collaboration. Furthermore,\n",
            "relying solely on textual outputs restricts the answer capacity and interpretability to some extent. For instance, in\n",
            "computer-aided diagnosis (CAD) using medical images, while the model can identify the disease type and provide\n",
            "treatment recommendations, it is unable to pinpoint the exact location and region of the pathologies, limiting its clinical\n",
            "usefulness as a reference for explainable diagnosis prediction.\n",
            "To address these limitations on both technical and application aspects, we propose OmniFM-DR, a multi-modal\n",
            "generalist model for reading chest radiographs, by providing more detailed evidence of associated diseases instead\n",
            "of rushing to the diagnosis directly. For the proposed multi-task transformer model, we unify the input and output\n",
            "labels of all sub-tasks into a uniform format for consistent modeling and joint training, which is detailed in the\n",
            "customized instruction tuning section. Figure 1 illustrates the four main tasks performed by OmniFM-DR and provides\n",
            "an illustrative comparison of model capabilities in each task with other state-of-the-art (SOTA) methods, measured\n",
            "in each individual evaluation metric. OmniFM-DR is designed to handle a wide range of downstream tasks relevant\n",
            "to chest X-ray analysis, including diagnosis of common thoracic diseases and localization of those image-visible\n",
            "disease patterns. Additionally, it can perform image segmentation for pneumothorax, lungs, and heart regions. Most\n",
            "importantly, OmniFM-DR is capable of generating reports summarizing the findings by leveraging all the provided\n",
            "evidence mentioned here. Notably, OmniFM-DR is capable of reaching equivalent or even better performance compared\n",
            "to SOTA models (dedicated to specific tasks) in all downstream clinical applications, showing its effectiveness and\n",
            "generality in chest X-ray interpretation.\n",
            "Our contributions are three-fold:\n",
            "•Our proposed model offers a versatile approach to analyzing chest X-ray images, allowing for comprehensive\n",
            "and accurate radiography image analysis across various application tasks. It enhances the interpretability of\n",
            "chest X-ray reporting by generating more detailed information on disease attributes. This includes disease size,\n",
            "location, severity, and contour, providing stronger evidence for diagnosis and treatment.\n",
            "•We develop a unique framework for building datasets that are tailored for customized instruction tuning. Unlike\n",
            "the conventional method of organizing pair-wise supervision (consisting of an image and its corresponding\n",
            "label), our framework involves cross-task training supervision for each sample, which enhances the learning of\n",
            "correlations among tasks. We will release the composed datasets with detailed image-instruction-label triplets,\n",
            "which is the very first dataset of this kind to our knowledge.\n",
            "•We applied the proposed model on various downstream application benchmarks, and an overall superior\n",
            "performance is shown compared to SOTA approaches. Furthermore, we conducted a controlled trial and\n",
            "evaluation on the generated reports performed by three radiologists. In the blinded comparison of 160\n",
            "retrospective cases from four centers, three radiologists perceived the quality of 59% of the generated reports\n",
            "to be equivalent to or even better than the original physician reports.\n",
            "2 Results\n",
            "2.1 Overview\n",
            "As illustrated in Figure 1, we train the multi-task model for analyzing chest X-ray images with a dataset specially\n",
            "designed for customized instruction tuning. It aims at a comprehensive analysis of chest X-ray images (providing\n",
            "detailed evidence) and enhanced interpretability (evidence-based diagnosis) as a tool for computer-aided diagnosis\n",
            "(CADx). For the performance evaluation purpose, we applied the proposed model to various downstream tasks and\n",
            "benchmarks, and an overall superior performance is achieved compared to SOTA models (dedicated to each individual\n",
            "sub-task). Furthermore, we conducted a controlled trial for the evaluation of the generated report in comparison to the\n",
            "original reports, assessed by a group of radiologists. In a blinded comparison involving 160 historical reports from\n",
            "four different centers, three radiologists consistently rated the quality of the generated reports as comparable to or\n",
            "better than original radiological reports, with a success rate of over 59%. Moreover, our proposed model exhibited\n",
            "an average omission rate of 2.53% and an average error rate of 2.72% per report, which are close to those of the\n",
            "radiologist-provided reference reports (i.e., 1.25% and 2.00%)\n",
            "In Table 1, we illustrate the overall performance of the proposed OmniFM-DR across four main tasks on nine datasets,\n",
            "along with individual SOTA results, using a total of 150 thousand testing cases. Our unified transformer model achieves\n",
            "2LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 1: (a) Overview performance of the proposed method OmniFM-DR on multiple datasets across multi-tasks\n",
            "(i.e., disease classification, localization, segmentation, and report generation) and (b) Training dataset DR-VQA . The\n",
            "attribute classification extracts disease phrases and related attributes (severity level and location) from the report. (c)\n",
            "Typical VQA examples of instruction set .\n",
            "superior results among the majority of the tasks on unseen datasets when performing direct inference. In the fine-tuning\n",
            "setting, the advantage remains significant in most of the tasks and metrics. Here, the proposed model is trained using\n",
            "a list of public datasets (detailed below). ChestXray14 and RSNA pneumonia datasets are utilized to evaluate the\n",
            "multi-label classification task performance, while MS-CXR, ChestXray14, and RSNA pneumonia datasets are used\n",
            "for the disease localization task. Furthermore, the largest available dataset for CXR reports (i.e., MIMIC-CXR) is\n",
            "3LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "utilized to evaluate the report generation performance. The metrics (AUC and F1) refer to the macro average on the 14\n",
            "diseases for ChestXray14. Accuracy (ACC) and mean Intersection over Union (mIoU) are utilized for the evaluation of\n",
            "disease localization, while Dice is utilized for segmentation tasks in three datasets (JSRT, CheXMark, and MS-PS).\n",
            "Furthermore, the performance of the report generation task is assessed by both clinical efficacy (CE, i.e., F1, Precision,\n",
            "Recall) and natural language processing (NLP, i.e., BL-4, METEOR, Rouge-L) metrics.\n",
            "Table 1: Performance comparison on MultiMedBench with the direct inference and fine-tune setting. We compare\n",
            "OmniFM-DR with specialist SOTA models. Across all tasks, datasets, and metrics combinations in MultiMedBench,\n",
            "we observe OmniFM-DR performance is equivalent to or exceeding SOTA.\n",
            "Task Dataset Metric Direct Inference Finetuning\n",
            "SOTA OmniFM-DR SOTA OmniFM-DR\n",
            "Disease classification ChestXray14 AUC 72.6[30] 73.4 77.8[31] 77.9\n",
            "F1 24.4[30] 26.2 32.9 [31] 32.6\n",
            "RSNA Pneumonia AUC 82.8[32] 84.3 88.5[33] 88.9\n",
            "F1 58.4[31] 59.5 67.2 [33] 66.2\n",
            "Disease localization ChestXray14 ACC 31.5[34] 56.7 47.7[34] 61.5\n",
            "mIoU 32.7[34] 49.2 41.5[34] 51.6\n",
            "MS-CXR ACC 26.1[34] 46.5 43.7[34] 54.7\n",
            "mIoU 28.2[34] 46.2 46.5[34] 50.6\n",
            "RSNA Pneumonia ACC 35.7[34] 42.7 33.4[34] 55.0\n",
            "mIoU 36.7[34] 47.6 33.2[34] 49.7\n",
            "Segmentation JSRT Dice 93.8[33] 90.8 95.2[33] 91.6\n",
            "CheXmask Dice 90.2[33] 88.5 92.1[33] 93.9\n",
            "MS-PS Dice 48.9 [33] 65.0 52.6[33] 59.4\n",
            "Report generation MIMIC-CXR F1 30.3[35] 33.3 - -\n",
            "Precision 35.2[35] 43.2 - -\n",
            "Recall 29.8[35] 31.3 - -\n",
            "BL-4 10.9[35] 11.0 - -\n",
            "METEOR 15.1[35] 14.0 - -\n",
            "Rouge-L 28.3[35] 26.5 - -\n",
            "2.2 Disease Classification\n",
            "We explore two types of classification tasks: disease entity classification and attribute classification. The entity\n",
            "classification task focuses on classifying disease categories, while the attribute classification task determines the disease\n",
            "attributes, e.g., location and severity. As illustrated in Table 1, we evaluate the disease entity classification task on\n",
            "the ChestXray14 and RSNA Pneumonia datasets with both direct inference and fine-tuning settings. Compared to the\n",
            "previous best results, Our model shows improvements of 0.8% in AUC and 1.8% in F1 for the ChestXRay14 dataset,\n",
            "with improvements of 1.5% in AUC and 1.1% in F1 for the RSNA Pneumonia dataset with direct inference setting, in\n",
            "comparison to models explicitly trained for each classification task. We further conduct the fine-tuning experiments and\n",
            "notice the average results of various methods are similar when models are fine-tuned with 50 samples for each finding\n",
            "on the target dataset. Figure2(a) further shows the detailed distribution of F1 results for 26 different disease entities\n",
            "that are aligned with the long-tail distribution on the MIMIC-CXR dataset. Additionally, our model attains high F1\n",
            "scores across the diseases in the attribute classification task. For instance, the severity classification ACC for Effusion\n",
            "and Pneumothorax reaches 65.6% and 65.9%, respectively, with an average of 59.2% for seven diseases with attribute\n",
            "descriptions.\n",
            "When evaluating the domain-shifted ChestXray14 dataset, our method achieves SOTA results on eight diseases (i.e.,\n",
            "Atelectasis, Mass, Nodule, Pneumonia, Consolidation, Fibrosis, Pleural Thicken, and Herina) shown in Figure 2(b). For\n",
            "instance, our method achieves an AUC and F1 of 74.5% and 36.8% for Atelectasis and 65.5% and 20.6% for Nodule,\n",
            "respectively. These results surpass the previous best results, demonstrating the effectiveness of our method in accurately\n",
            "identifying and localizing relatively small abnormalities in Chest X-ray images.\n",
            "2.3 Disease Localization\n",
            "We herein conduct extensive experiments on MS-CXR, ChestXray14, and RSNA Pneumonia datasets to evaluate\n",
            "disease localization under both direct inference and 20-shot fine-tuning settings. Under the direct inference setting,\n",
            "4LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 2: (a) In domain evaluation of 26 disease entities on the MIMIC-CXR dataset (upper panel) and attribute\n",
            "classification task in disease severity level and location (lower panel). (b) Out-of-domain evaluation between OmniFM-\n",
            "DR and other classification models (i.e., ConVIRT, GloRIA, BioViL, and MedKLIP) on the ChestXray14 dataset. AUC\n",
            "score and F1 are utilized for assessing the classification task, and \"mean\" is the weighted average of all attributes\n",
            "according to their frequency of occurrence.\n",
            "our model generally achieves the best performances over all existing visual-grounding models. As shown in Table 1,\n",
            "OmniFM-DR gets an average ACC of 56.7%, 46.5%, and 42.7% on the ChestXray14, MS-CXR, and RSNA Pneumonia\n",
            "datasets respectively, surpassing other methods by a large margin, 7% to 25%. When fine-tuned on the downstream\n",
            "dataset with 20 shots for each label, OmniFM-DR consistently scores the highest ACC of 61.5%, 54.7%, and 55.0% on\n",
            "the three datasets.\n",
            "Figure 3 further presents the model’s capability of disease localization across multiple diseases and comparations of\n",
            "OmniFM-DR against three approaches (i.e., VGTR, SeqTR, TransVG) on MS-CXR and ChestXray14 dataset with\n",
            "fine-tuning setting. The two datasets share five common diseases: Cardiomegaly, Effusion, Pneumothorax, Atelectasis,\n",
            "and Pneumonia. Moreover, the MS-CXR dataset comprises three additional diseases (i.e., Consolidation, Edema,\n",
            "and Opacity), while the ChestXray14 dataset includes Infiltrate, Mass, and Nodule. It is observed that OmniFM-DR\n",
            "5LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 3: Assessment of the disease localization task with ACC and mIoU metrics on the MS-CXR and ChestXray14\n",
            "datasets. (a) Example cases with boundary box predictions and (b) comparisons between OmniFM-DR and other\n",
            "disease localization methods.\n",
            "is capable of accurately identifying disease locations across multiple diseases. Furthermore, the model consistently\n",
            "achieves large advantages over other approaches for most disease categories. For seven of eight diseases on the MS-CXR\n",
            "dataset, OmniFM-DR gets significantly higher ACC and mIoU than the best-performing baseline, with the largest\n",
            "improvements in Pleural Effusion (>15%). Supplementary Table 2 further provides detailed comparisons of the eight\n",
            "diseases among OmniFM-DR and other approaches.\n",
            "2.4 Segmentation\n",
            "We adopt a polygon-based contour representation for the segmentation task to achieve a uniform input-output format\n",
            "with other tasks, i.e., predicting a list of polygon vertexes instead of region masks. As shown in Table 1, the Dice\n",
            "coefficient is utilized for evaluating the segmentation of lung and cardiac contours. Our unified model is comparable\n",
            "to pixel-based segmentation methods. To assess the severity of cardiomegaly and pneumothorax in potentially more\n",
            "detailed analyses, we perform post-processing on the segmented lung and cardiac masks to calculate the cardiothoracic\n",
            "ratio (CTR) and pneumothorax ratio (PCR). CTR is calculated as the ratio between the maximum transverse diameter\n",
            "of the heart and the chest, commonly used to evaluate the cardiomegaly severity as mild, moderate, or severe. The\n",
            "area method is used to calculate the PCR, which is represented by the ratio of the pneumothorax area to the affected\n",
            "lung area. Figure 4 illustrates the segmentation results of cardiomegaly and Pneumothorax with different severity. We\n",
            "observe that the overall performance of lung and cardiac contours is satisfactory across different CTR (see Figure 4(a)).\n",
            "Furthermore, Pneumothorax exhibits significant variations in location and size (see Figure 4(b)). For more accurate\n",
            "segmentation of mild pneumothorax, we incorporate a segmentation head with a U-Net decoder and achieve a Dice\n",
            "value of 59.4% and 65.0% under direct inference and fine-tune settings.\n",
            "6LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 4: Results of Heart, Lung, and Pneumothorax Segmentation. (a) Segmentation of the heart and lungs can be\n",
            "used to assess the severity of cardiomegaly. (b) Pneumothorax segmentation is performed to assess the severity of\n",
            "pneumothorax.\n",
            "2.5 Report Generation\n",
            "As the summary of the radiological reading process, reports contain major findings and possible disease diagnoses from\n",
            "the radiologists. As the unique feature of our proposed framework, we hypothesize adding pre-generated evidence could\n",
            "significantly improve the quality of AI-generated reports. Figure 5 demonstrates this quality improvement with the\n",
            "disease attributes prompt. During the inference stage, the customized prompt is initially derived from the classification\n",
            "task, including disease entity, severity, and rough location. The disease attribute prompt will be updated when a\n",
            "more accurate boundary box or severity of Pneumothorax/Cardiomegaly is available from the disease localization\n",
            "or segmentation task. The generated reports of typical cases of Pneumothorax and Pleural Effusion are provided in\n",
            "Figure 5(a). The proposed model is capable of identifying the diseases with an accurate disease localization box and\n",
            "generated report. Detailed descriptions of pneumothorax entities and attributes such as \"small left apical pneumothorax\"\n",
            "are highlighted by blue text in the ground truth report and are well predicted in the generated report. The location and\n",
            "severity information of Pneumothorax is further verified by the boundary boxes and calculated metric (i.e., PCR). On\n",
            "the other hand, the descriptions of disease attributes (e.g., \"small\" of pneumothorax, \"moderate cardiomegaly\") are\n",
            "omitted in the generated report without proper prompt. For the Effusion case, we also find the importance of proper\n",
            "input prompts in the task of report generation. The descriptions of pleural effusion and cardiomegaly are more detailed\n",
            "and accurate in the generated report with prompt. The boundary box of pleural effusion and mild cardiomegaly is\n",
            "indicated by the disease localization and segmentation task (i.e., CTR=0.54). With more specific prompts, the generated\n",
            "report shows accurate descriptions as \"bilateral pleural effusion, no pneumothorax, and mild heart size.\" In contrast, the\n",
            "general model without the designed prompt provides an inaccurate assessment of cardiomegaly, as described as \"stable\n",
            "heart size.\"\n",
            "Figure 5(b) further compares the accuracy of three different conditions (i.e., baseline, with phrase prompt, and with\n",
            "phrase-GT prompt). With the help of disease attributes prompt, the accuracy of severity and location description\n",
            "is improved across multiple diseases such as Atelectasis, Pneumothorax, and Cardiomegaly. The proposed method\n",
            "surpasses the baseline in accuracy by 9% for the severity (see details in Supplementary Table 3). When the ground truth\n",
            "disease phrase is utilized as the prompt, we find the quality of the report improved further as an upbound for our model.\n",
            "7LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 5: (a) Comparison results of two examples (e.g., Pneumothorax and Pleural Effusion). We further compare\n",
            "the generated report with and without designed instruction. The customized instruction includes the disease category,\n",
            "location, and severity level that are inferred from other tasks. (b) Comparison of generated reports in three ways:\n",
            "without prompt, with prompt from multi-task results, and with prompt from GT, using ACC of disease attributes from\n",
            "severity level and location.\n",
            "Subjective Comparison Study To assess the clinical interpretation, radiologists’ evaluations are employed to examine\n",
            "the quality of radiology reports. This side-by-side comparison study focuses on errors associated with the presence,\n",
            "location, and severity of clinical findings. Non-clinical errors, such as referring to views or previous studies that do not\n",
            "exist, are excluded from our evaluation.\n",
            "Figure 6(a) presents the average score comparison result from three radiologists. The average ratings for the radiologist-\n",
            "provided reference reports and generated reports are 3.74/3.48, 3.79/3.58, and 4.33/3.91, respectively. The average\n",
            "result across three radiologists is 3.95 (95% CI, 3.47-4.05)/3.65 (95% CI, 3.38-3.91), indicating that the quality of the\n",
            "generated reports is comparable to the reference reports. Figure 6(b) presents the pairwise comparison result, three\n",
            "radiologists believe that the quality of generated reports is equal to or even better than the original reports by 60%,\n",
            "64%, and 53%, respectively. Despite the observed fluctuations in results across radiologists, the collective analysis\n",
            "8LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 6: Side-by-side comparison of the quality of radiology reports from the radiologist (R) and center (c) perspective.\n",
            "In the side-by-side comparison , three radiologists reviewed and scored the clinically derived reports from four centers\n",
            "and reports generated by OmniFM-DR. (a) shows average score comparison, and (b) shows pairwise comparison results\n",
            "among three radiologists. The independent evaluation identifies rates of omissions and clinical errors for reports\n",
            "generated by OmniFM-DR. Significant errors are related to the presence, location, or severity of clinical findings, which\n",
            "are identified by clinical raters.\n",
            "suggests that our generated reports exhibit a comparable level of quality to the original reports generated by medical\n",
            "professionals.\n",
            "In the detailed evaluation, omission rate refers to missed disease diagnoses and error rate refers to inaccuracies in\n",
            "severity or location descriptions or false disease diagnosis. We report the results on the report level. Figure 6(c)\n",
            "shows the total omission rate for the reference and generated reports in the four centers are as follows: 0.89%/2.11%,\n",
            "1.67%/2.56%, 1.00%/3.22%, 1.44%/2.22%. On average, the omission rate for the generated reports 2.53% is close to\n",
            "that of the radiologist-provided reference reports, i.e., 1.25%, with a significant omission rate of 0.42%/1.00%. Figure\n",
            "6(d) shows the error rate, i.e., inaccuracies in severity or location descriptions or false positives. The total error rate for\n",
            "the reference and generated reports in the four centers are 0.67%/2.33%, 2.22%/3.11%, 2.78%/3.11%, 2.33%/2.33%.\n",
            "On average, the error rate for the generated reports 2.72% is comparable to that of the radiologist-provided reference\n",
            "reports, i.e., 2.00%, with few clinically significant errors observed.\n",
            "3 Discussion\n",
            "Our research focuses on developing a single and unified model for Chest X-ray image analysis, with the goal of\n",
            "addressing multi-tasks for better clinical interpretation, including disease classification, localization, segmentation, and\n",
            "report generation. By leveraging a carefully designed instruct tuning framework and incorporating diverse training\n",
            "strategies, our method can effectively extract relevant features and make predictions for multiple tasks in Chest X-\n",
            "ray images. This allows for a comprehensive analysis and diagnosis of various diseases and abnormalities in the\n",
            "images. With unseen evaluation datasets, the proposed unified model has exhibited impressive performance in disease\n",
            "classification and grounding tasks (via direct inference), surpassing the capabilities of existing state-of-the-art methods\n",
            "with a few-shot fine-tune setting.\n",
            "OmniFM-DR provides reliable evidence for a better explainability\n",
            "9LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "It is crucial that automated medical report generators produce trustworthy, easily understandable, and accurate reports\n",
            "for effective utilization in practice. To achieve the desired goals, it is crucial to have high-quality explanations about how\n",
            "the report was generated and the process involved in reaching the diagnosis. An explainable system helps developers to\n",
            "pinpoint any weaknesses or inefficiencies, while clinicians can rely on the decisions made with the aid of these systems.\n",
            "Here, we believe the interpretability of the reports can be validated mutually with the results obtained from other tasks\n",
            "within the model. For instance, the disease category, severity level, and approximate location of the lesion could be\n",
            "preliminary verified with the disease entity classification and attribute classification task. The disease localization\n",
            "task could further provide a more accurate bounding box of the lesion. For Pneumothorax and Cardiomegaly, the\n",
            "segmentation function could provide an accurate assessment of disease degree by postprocessing the contours of the\n",
            "pneumothorax/lung/heart mask. These results together contribute to a better verification of the generated reports.\n",
            "Furthermore, we improve the explainability of the generated reports by incorporating customized prompts that provide\n",
            "information regarding disease attributes. Our results indicate that disease-specific prompts have improved recall and\n",
            "F1 score of disease entities — key metrics in evaluating the performance of automated report generation models. The\n",
            "results of this study underscore the significance of incorporating prompts (evidence) into the model, thereby contributing\n",
            "to the model’s performance in the report generation process. This technique can prove to be a valuable strategy for\n",
            "improving the overall accuracy of AI models, specifically in the medical field.\n",
            "Image-instruction-label triplet dataset is designed for the promoting multi-task learning\n",
            "While there exist various single-task datasets, there have been limited attempts to unify them and create benchmarks for\n",
            "the development of a single and more comprehensive model. As one of our major contributions, we design and plan to\n",
            "release a comprehensive dataset of Chest X-ray data. This dataset includes full-label annotations, enabling researchers\n",
            "and practitioners to explore and leverage the benefits of multi-task learning in this domain. By sharing this dataset, we\n",
            "aim to encourage and support further advancements in multi-task learning approaches for Chest X-ray analysis. This\n",
            "can potentially create new opportunities in clinical applications [ 36;37;38]. The benchmark has several limitations,\n",
            "including the limited size of the individual datasets and limited modality and task diversity. Another key barrier to\n",
            "developing models for use across an even wider variety of biomedical data types is the lack of large-scale multimodal\n",
            "datasets, which would permit joint learning and alignment of the modality-specific encoders with the decoder.\n",
            "OmniFM-DR achieves comparable reporting results with expert radiologists\n",
            "We conduct the evaluation of report quality based on the assessment of radiologists, which indicates that the model\n",
            "performs well on the complex multimodal task of generating radiology reports. In 59% of the cases, the generated\n",
            "reports are of equal or even better quality compared with the reference reports generated by medical professionals.\n",
            "Additionally, the average omission rate and error rate in the model-generated reports are similar to those found in reports\n",
            "generated by medical professionals on the same dataset. These promising findings demonstrate the rapid progress in the\n",
            "field of automatic radiology report generation and suggest the potential for future clinical applications.\n",
            "Our proposed model exhibits competent performance across multiple tasks and enhances the explainability of the\n",
            "generated results. However, it still faces challenges when it comes to generalizing to unseen disease categories and\n",
            "undefined instructions (tasks). These limitations highlight the need for further research and development to improve the\n",
            "model’s ability to handle novel diseases and adapt to unfamiliar instructions. In the future, given the wider array of\n",
            "modalities and tasks in instruction tuning, more generalized models are expected to understand the tasks better and\n",
            "render modality-wise and task-wise emergent capability.\n",
            "4 Material and Method\n",
            "4.1 Building Dataset for Customized Instruction Tuning\n",
            "In this work, we construct a multi-task dataset for joint training of disease classification, localization, segmentation, and\n",
            "report generation. In general, we unify the input and output labels of all sub-tasks into a uniform format for consistent\n",
            "modeling and joint training, i.e., a set of image-instruction-label triplets as samples shown in Figure 1(c) and more\n",
            "in the supplementary materials. We further built a subset including the attributes and phrases for chest X-ray images\n",
            "like \"small base effusion, normal cardiac silhouette,\" which can be used as instruction for the report generation task.\n",
            "Additionally, the dataset underwent quality assurance by radiologists to ensure its accuracy and reliability.\n",
            "Instruction Design To build and utilize multiple instruction sets (for each of four sub-tasks) during the joint training\n",
            "approach, we design a set of seed instructions with placeholders (later replaced with corresponding targets) and employ\n",
            "LLMs to create diverse related task descriptions for coarse-grained task-level customization, such as samples illustrated\n",
            "in Figure 1(c). Following various instructions, our model can elegantly switch among different vision-centric tasks\n",
            "and accomplish them in a unified manner like LLMs. More details about the organization of instructions for task-level\n",
            "10LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "customization, including disease classification, localization, segmentation, and report generation, are introduced in the\n",
            "supplementary material.\n",
            "We employ a list of public datasets for training our proposed transformer model, e.g., MIMIC-CXR, VinDr-CXR, and\n",
            "ChestX-Det10. We first sort out the aligned lesion categories of each dataset and the associated radiology report data\n",
            "and detection bounding box (BBox) data. We exclude the image datasets that are included in the test and validation\n",
            "datasets of downstream tasks to avoid data leakage. Each dataset is described in detail as follows:\n",
            "•MIMIC-CXR [36] contains more than 377,110 radiograph images from over 227,835 radiographic studies.\n",
            "Each radiograph is paired with lesion classification and associated radiology report. We employ this dataset\n",
            "for multi-label classification and report generation tasks.\n",
            "•Padchest [39] includes 160,840 images obtained from 67,000 patients, covering six different position views.\n",
            "174 different radiographic findings were labeled and used for the classification task in this study.\n",
            "•CXR-AL14 [40] is a large-scale dataset for chest X-ray image detection. It has more than 140,000 chest X-ray\n",
            "radiographs containing 253,844 bounding boxes in 14 chest abnormal object categories.\n",
            "•VinDr-CXR [41] includes chest radiographs with annotations for the classification of 28 common chest\n",
            "diseases. The dataset contains 15,000 CXR scans in the training set. We select eight diseases from the dataset\n",
            "along with their corresponding BBox for the disease localization task.\n",
            "•ChestX-Det [42] consists of 3,578 images from NIH ChestXray14[ 38] for 13 common disease. We select\n",
            "seven diseases from the dataset along with BBox for the disease localization task.\n",
            "•CheXmask [43] contains 676,803 lung and heart segmentation masks of chest images from six publicly\n",
            "available databases: CANDID-PTX, ChestXray14, Chexpert, MIMIC-CXR, Padchest, and VinDr-CXR. We\n",
            "include 224,316 data for training and 10,000 data from ChestXray14 for downstream evaluation.\n",
            "•SIIM [44] comes from the SIIM-ACR Pneumothorax Segmentation competition and contains 12,090 images,\n",
            "among which approximately 3,000 cases are positive for pneumothorax disease with masks.\n",
            "•In-house dataset consists of 2,531 chest X-ray images, encompassing nine disease categories relevant to the\n",
            "disease localization task, along with BBox. All the images are captured from the front view.\n",
            "4.2 Model Architecture\n",
            "In this work, we propose a multimodal model that leverages the sequence-to-sequence learning paradigm for joint multi-\n",
            "task training. The specific tasks encompass disease classification, localization, segmentation, and report generation.\n",
            "For each task, we design specific task instructions to facilitate the model’s differentiation between tasks. Inspired by\n",
            "the multi-modal models [ 45;46;47;48], OmniFM-DR leverages an encoder-decoder architecture for perceiving pixel\n",
            "inputs and generating the target sequence and performs unified modeling and joint training on downstream visual and\n",
            "language tasks as shown in Figure 7. Bounding boxes and class labels are converted into sequences of discrete tokens.\n",
            "This enables OmniFM-DR to robustly perform diverse language and vision tasks based on instructions, providing\n",
            "diverse and complex output results.\n",
            "Image and Language Encoder With an input chest X-ray image xi∈RH×W, visual features are extracted by image\n",
            "encoder and further projected to the feature space:\n",
            "vi=Pimg(Eimg(xi))∈R(hf×wf)×d(1)\n",
            "where hfandwfare the output size of visual features, and drepresents the feature dimension. Eimgcan be any\n",
            "common visual backbones and we use ResNet152 in this study. Specifically, we take output features from the 4th\n",
            "residual block. Visual features are then projected to a pre-defined feature dimension by using Pimg, which is composed\n",
            "of two linear layers. With any processed input instruction sequence ti, text features are extraced by language encoder:\n",
            "li=Etxt(ti)∈Rnt×d(2)\n",
            "where ntis the number of input tokens and drepresents the feature dimension. In our case, Bert [ 49] is used as a\n",
            "language encoder.\n",
            "Multi-modality Module This module follows an encoder-decoder architecture format. Given the input visual features\n",
            "viand text features li, we first generate fused multi-modal representations by combining the image and text embeddings.\n",
            "These fused features serve as the keys and values in the cross-attention blocks in the decoder. By conditioning on the\n",
            "11LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 7: Overview of OmniFM-DR. Our model processes the chest X-ray image and instruction query utilizing\n",
            "a Resnet-based encoder and BPE/tokenizer, respectively. We then fuse the visual and textual information with a\n",
            "transformer encoder and generate the output text with a unified multi-task decoder. The instruction tuning includes four\n",
            "tasks, i.e., multi-disease classification, disease localization, segmentation, and report generation.\n",
            "partial sequence yi,<jpredicted so far, the decoder recursively makes predictions for the token at position j, effectively\n",
            "generating aligned descriptions across modalities.\n",
            "yi,j=Dmm(Emm(concat (vi, li)), yi,<j)∈R1×d(3)\n",
            "In our experiments, we leverage BART [ 50] for multi-modal encoding and decoding. BART utilizes BERT [ 49] as the\n",
            "encoder. The decoder is based on GPT [51] and generates output sequences in an autoregressive manner.\n",
            "Model joint-training and Inference We optimize the sequence-to-sequence model using cross-entropy loss as follows.\n",
            "L=−nX\n",
            "i=1|y|X\n",
            "j=1logPθ(yi,j|yi,<j, xi, ti) (4)\n",
            "where nis the batch size, θrepresents the model parameters, xirepresents the input image, tistands for the input\n",
            "instruction, and yi,jdenotes the output token at position jfor the ithsample at each batch. To enhance the quality of\n",
            "generation during inference, we employ various decoding techniques, such as beam search.\n",
            "4.3 Downstream Finetuning and Evaluation\n",
            "The benchmark evaluates four tasks, using a total of nine datasets with over 150 thousand samples. Among them,\n",
            "ChestX-ray14 and RSNA pneumonia datasets are utilized for evaluating the performance on the multi-label classification\n",
            "task, while MS-CXR, ChestX-ray14, and RSNA pneumonia datasets for the disease localization task. Meanwhile,\n",
            "we assess the report generation on the MIMIC-CXR dataset. Several standard metrics are introduced for various\n",
            "tasks. For example, F1 stands for “F1 score”, ACC stands for “Accuracy”, BLEU stands for “BiLingual Evaluation\n",
            "12LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Understudy” [ 52], ROUGE stands for “Recall-Oriented Understudy for Gisting Evaluation” [ 53]. For BLEU and\n",
            "ROUGE, we all use 1-gram by default. We compared the proposed method with other methods across four tasks\n",
            "[31;33;32;54;30;55;34;56;57;58;59;60;35;61] and the detailed comparison results can be found in the\n",
            "supplementary materials. The evaluation datasets include\n",
            "•ChestXray14 [38] is a publicly accessible dataset for classification and localization. It has 112,120 images\n",
            "with 14 common disease labels, with 984 images having eight localization of key findings with hand-labeled\n",
            "BBOX. We randomly split it into training/validation/test sets by 7:1:2 for the classification and localization\n",
            "task.\n",
            "•MS-CXR [62] consists of 1,153 samples with BBOX and concise radiology reports for eight diseases (sourced\n",
            "from the testing set ofMIMIC-CXR), which is utilized for the disease localization task. We randomly split it\n",
            "into training/validation/test sets by 7:1:2 based on the patients and evaluated the average performance of the\n",
            "model in all eight diseases.\n",
            "•RSNA Pneumonia [63] is a binary classification chest X-ray dataset that comprises 26,683 images, where\n",
            "each radiograph is categorized as either pneumonia or normal. We randomly sample 3,000 data from the\n",
            "official training set to build the test set for direct inference and finetuning experiments for disease classification\n",
            "and localization tasks.\n",
            "•JSRT [64] contains 247 chest images, among which 154 cases have lung nodules. We selected 93 healthy\n",
            "samples for the lung segmentation task.\n",
            "•MS-PS is a privately annotated pneumothorax segmentation dataset, comprising 233 images from the MS-\n",
            "CXR dataset. It was annotated by three expert physicians and is split into a 4:1 ratio for training and testing,\n",
            "respectively, for downstream pneumothorax segmentation evaluation.\n",
            "4.4 Clinical Evaluation of Generated Reports\n",
            "To examine the quality of generated reports from a clinical usefulness perspective, we conducted a comprehensive\n",
            "evaluation performed by three experienced radiologists. A total of 160 cases were evaluated, including 120 cases from\n",
            "three medical institutes (listed in details below) and 40 cases from the MIMIC-CXR test set. To match the intended\n",
            "inputs of our model, we excluded cases that mentioned multiple imaging views or comparisons to prior test results in\n",
            "the generated reports. Our study involved two distinct yet complementary human evaluations: (a) a parallel evaluation,\n",
            "where raters compared and ranked alternative reports based on their quality, and (b) an independent evaluation conducted\n",
            "to assess the quality of each individual report.\n",
            "Ethical Statement The private data used in this retrospective study was approved by the Ethics Committee of three\n",
            "institutes, i.e., Fengcheng People’s Hospital, Huanggang Hospital of Traditional Chinese Medicine, and Longkou\n",
            "People’s Hospital. and the committees waived the consent since the retrospective research would not change the\n",
            "examination process of the patients. All data were adequately anonymized, and the risk of disclosing patient privacy via\n",
            "imaging data was minimal.\n",
            "Parallel Evaluation All 160 original and generated reports were randomly chosen from a pool of four centers and\n",
            "evaluated by three radiologists. The radiologists were unaware of the source of the reports and reviewed them in a\n",
            "randomized order. The quality of the reports will be scored subjectively on a 1-5 scale, with 1 being the worst and 5 the\n",
            "best. The detailed guidelines are provided as follows:\n",
            "• Report without diagnosis errors and with comprehensive description should be scored 5;\n",
            "• Report with significant clinical diagnosis errors should be scored 1;\n",
            "•Report can be considered correct if the content is reasonable based on the given image. For instance, an\n",
            "accurate diagnosis of pleural effusion may not be obtained based on a frontal view image;\n",
            "•Report can be considered correct if there are descriptions of related diseases. For instance, lung collapse can\n",
            "be indicative of atelectasis;\n",
            "•Report with the better description should be scored higher, if two reports are error-free or exhibit similar errors;\n",
            "• Repetitive descriptions can be overlooked;\n",
            "Independent Evaluation Radiologists were provided with one chest X-ray image paired with the disease findings, and\n",
            "tasked with assessing the generated reports and original reports. During the evaluation, the radiologists were unaware of\n",
            "the source of the reports. They aimed to determine whether there are discrepancies or errors, any missing elements, or\n",
            "inaccurate descriptions (e.g., location and severity) in the reports and evaluate their clinical significance referring to the\n",
            "13LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "methodology [ 65;22]. Six types of diseases are evaluated, i.e., Pneumothorax, Pleural Effusion, Edema, Consolidation\n",
            "or Pneumonia (grouped together), Atelectasis, and normal. Radiologists were required to assess whether every type of\n",
            "error exists for each specific disease when evaluating reports. The considered errors are agreed by the radiologists and\n",
            "listed as follows:\n",
            "• False positives. Incorrect disease detection;\n",
            "• False negatives. Missed disease detection;\n",
            "• Inaccurate location. For instance, left lung pneumonia is described as right lung pneumonia;\n",
            "• Inaccurate severity. For instance, a minor pleural effusion is described as a major pleural effusion;\n",
            "• Non-existent references. For instance, \"compared with previous\" and \"based on front-lateral image\";\n",
            "Code and Data Availability\n",
            "Code for training and evaluation is available at https://github.com/MedHK23/OmniFM-DR . The new dataset\n",
            "released in this study can be found at https://huggingface.co/datasets/MedHK23/OmniFM-DR . The MultiMed-\n",
            "Bench is all open source, and the respective download link is described in Git Hub.\n",
            "Author contributions\n",
            "All authors have contributed fully to the concept and design of the study. LX and ZN collected the clinical data,\n",
            "performed the experiments, and analyzed the experiment results. XL performed the comparative experiments with other\n",
            "methods. LX and XW drafted the manuscript. XW, SZ, and HL supervised the projects and gave final approval of the\n",
            "manuscript. All authors have carefully read and approved the final manuscript.\n",
            "Competing interests\n",
            "The authors declare no competing interests.\n",
            "References\n",
            "[1]B.J. Stevens, L. Skermer, and J. Davies. Radiographers reporting chest x-ray images: Identifying the service\n",
            "enablers and challenges in england, uk. Radiography , 27(4):1006–1013, 2021.\n",
            "[2]Erdi Çallı, Ecem Sogancioglu, Bram van Ginneken, Kicky G. van Leeuwen, and Keelin Murphy. Deep learning\n",
            "for chest x-ray analysis: A survey. Medical Image Analysis , 72:102125, 2021.\n",
            "[3]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n",
            "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\n",
            "image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n",
            "[4]Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\n",
            "transformer: Hierarchical vision transformer using shifted windows, 2021.\n",
            "[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n",
            "transformers for language understanding, 2019.\n",
            "[6]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
            "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\n",
            "models. arXiv preprint arXiv:2302.13971 , 2023.\n",
            "[7]Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman Castagné,\n",
            "Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access\n",
            "multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.\n",
            "[8]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\n",
            "Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\n",
            "pathways. arXiv preprint arXiv:2204.02311 , 2022.\n",
            "[9]Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\n",
            "Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint\n",
            "arXiv:2205.01068 , 2022.\n",
            "14LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "[10] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\n",
            "Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In\n",
            "International Conference on Machine Learning , pages 5547–5569. PMLR, 2022.\n",
            "[11] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\n",
            "Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.\n",
            "[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\n",
            "lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\n",
            "in neural information processing systems , 33:1877–1901, 2020.\n",
            "[13] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n",
            "Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human\n",
            "feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.\n",
            "[14] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\n",
            "Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv\n",
            "preprint arXiv:2212.13138 , 2022.\n",
            "[15] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\n",
            "Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language\n",
            "models. arXiv preprint arXiv:2305.09617 , 2023.\n",
            "[16] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,\n",
            "and Quoc V . Le. Finetuned language models are zero-shot learners, 2022.\n",
            "[17] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\n",
            "Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\n",
            "arXiv:2210.11416 , 2022.\n",
            "[18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,\n",
            "Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction\n",
            "tuning, 2023.\n",
            "[19] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language\n",
            "understanding with advanced large language models, 2023.\n",
            "[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n",
            "[21] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. Med-bert: pretrained contextualized embeddings\n",
            "on large-scale structured electronic health records for disease prediction. NPJ digital medicine , 4(1):86, 2021.\n",
            "[22] Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung Weng,\n",
            "Attila Kiraly, Sahar Kazemzadeh, Zakkai Melamed, et al. Elixr: Towards a general purpose x-ray artificial\n",
            "intelligence system through alignment of large language models and radiology vision encoders. arXiv preprint\n",
            "arXiv:2308.01317 , 2023.\n",
            "[23] Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou,\n",
            "Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained transformer for vision,\n",
            "language, and multimodal tasks. arXiv preprint arXiv:2305.17100 , 2023.\n",
            "[24] Qiuhui Chen, Xinyue Hu, Zirui Wang, and Yi Hong. Medblip: Bootstrapping language-image pre-training from\n",
            "3d medical images and texts, 2023.\n",
            "[25] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll,\n",
            "Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. arXiv preprint arXiv:2307.14334 ,\n",
            "2023.\n",
            "[26] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for\n",
            "radiology, 2023.\n",
            "[27] Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M.\n",
            "Summers, and Yingying Zhu. Expert knowledge-aware image difference graph representation learning for\n",
            "difference-aware medical visual question answering. In Proceedings of the 29th ACM SIGKDD Conference on\n",
            "Knowledge Discovery and Data Mining . ACM, aug 2023.\n",
            "[28] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis,\n",
            "Pranav Rajpurkar, and Jure Leskovec. Med-flamingo: a multimodal medical few-shot learner, 2023.\n",
            "[29] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\n",
            "Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day,\n",
            "2023.\n",
            "15LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "[30] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Medklip: Medical knowledge enhanced\n",
            "language-image pre-training in radiology, 2023.\n",
            "[31] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Contrastive learning\n",
            "of medical visual representations from paired images and text, 2022.\n",
            "[32] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Stephanie Hyland,\n",
            "Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, Hoifung Poon, and Ozan Oktay. Making\n",
            "the most of text semantics to improve biomedical vision–language processing. In Lecture Notes in Computer\n",
            "Science , pages 1–21. Springer Nature Switzerland, 2022.\n",
            "[33] Shih-Cheng Huang, Liyue Shen, Matthew P. Lungren, and Serena Yeung. Gloria: A multimodal global-local\n",
            "representation learning framework for label-efficient medical image recognition. 2021 IEEE/CVF International\n",
            "Conference on Computer Vision (ICCV) , pages 3922–3931, 2021.\n",
            "[34] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao,\n",
            "Xiaoshuai Sun, and Rongrong Ji. SeqTR: A simple yet universal network for visual grounding. In Lecture Notes\n",
            "in Computer Science , pages 598–615. Springer Nature Switzerland, 2022.\n",
            "[35] Zhihong Chen, Yaling Shen, Yan Song, and Xiang Wan. Cross-modal memory networks for radiology report\n",
            "generation, 2022.\n",
            "[36] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-\n",
            "ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest\n",
            "radiographs with free-text reports. Scientific data , 6(1):317, 2019.\n",
            "[37] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,\n",
            "Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty\n",
            "labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence , volume 33, pages\n",
            "590–597, 2019.\n",
            "[38] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8:\n",
            "Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of\n",
            "common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\n",
            "pages 2097–2106, 2017.\n",
            "[39] Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vayá. Padchest: A large chest x-ray\n",
            "image dataset with multi-label annotated reports. Medical Image Analysis , 66, 2020.\n",
            "[40] CXR-AL14 dataset. http://47.108.59.218/user/data/ . Accessed: 2023-08-30.\n",
            "[41] Ha Q Nguyen, Khanh Lam, Linh T Le, Hieu H Pham, Dat Q Tran, Dung B Nguyen, Dung D Le, Chi M Pham,\n",
            "Hang TT Tong, Diep H Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologist’s annotations.\n",
            "Scientific Data , 9(1):429, 2022.\n",
            "[42] Jie Lian, Jingyu Liu, Shu Zhang, Kai Gao, Xiaoqing Liu, Dingwen Zhang, and Yizhou Yu. A structure-aware\n",
            "relation network for thoracic diseases detection and segmentation. IEEE Transactions on Medical Imaging ,\n",
            "40(8):2042–2052, 2021.\n",
            "[43] Nicolás Gaggion, Candelaria Mosquera, Lucas Mansilla, Martina Aineseder, Diego H Milone, and Enzo Ferrante.\n",
            "Chexmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images. arXiv\n",
            "preprint arXiv:2307.03293 , 2023.\n",
            "[44] Anna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail Fomitchev, Mohannad Hussain, ParasLakhani, Phil\n",
            "Culliton, and Shunxing Bao. Siim-acr pneumothorax segmentation. Kaggle. https://kaggle.com/competitions/siim-\n",
            "acr-pneumothorax-segmentation , 2019.\n",
            "[45] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\n",
            "and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence\n",
            "learning framework, 2022.\n",
            "[46] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and Geoffrey Hinton. Pix2seq: A language modeling\n",
            "framework for object detection, 2022.\n",
            "[47] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J. Fleet, and Geoffrey Hinton. A unified sequence\n",
            "interface for vision tasks, 2022.\n",
            "[48] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified\n",
            "model for vision, language, and multi-modal tasks, 2022.\n",
            "[49] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n",
            "transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\n",
            "16LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "[50] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves\n",
            "Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language\n",
            "generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 , 2019.\n",
            "[51] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.\n",
            "[52] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of\n",
            "machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics ,\n",
            "pages 311–318, 2002.\n",
            "[53] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out ,\n",
            "pages 74–81, 2004.\n",
            "[54] Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Knowledge-enhanced visual-language\n",
            "pre-training on chest radiology images, 2023.\n",
            "[55] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual\n",
            "grounding with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision\n",
            "(ICCV) , pages 1769–1779, October 2021.\n",
            "[56] Ye Du, Zehua Fu, Qingjie Liu, and Yunhong Wang. Visual grounding with transformers. 2022 IEEE International\n",
            "Conference on Multimedia and Expo (ICME) , pages 1–6, 2022.\n",
            "[57] Muchen Li and Leonid Sigal. Referring transformer: A one-step approach to multi-task visual grounding, 2021.\n",
            "[58] Zhihao Chen, Yang Zhou, Anh Tran, Junting Zhao, Liang Wan, Gideon Ooi, Lionel Cheng, Choon Hua Thng,\n",
            "Xinxing Xu, Yong Liu, and Huazhu Fu. Medical phrase grounding with region-phrase context contrastive\n",
            "alignment, 2023.\n",
            "[59] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.\n",
            "Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the\n",
            "IEEE conference on computer vision and pattern recognition , pages 6077–6086, 2018.\n",
            "[60] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence\n",
            "training for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\n",
            "pages 7008–7024, 2017.\n",
            "[61] Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang, and Xu Sun. Contrastive attention for automatic\n",
            "chest X-ray report generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 ,\n",
            "pages 269–280, Online, August 2021. Association for Computational Linguistics.\n",
            "[62] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer, Stephanie Hyland,\n",
            "Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, et al. Making the most of text semantics\n",
            "to improve biomedical vision–language processing. In European conference on computer vision , pages 1–21.\n",
            "Springer, 2022.\n",
            "[63] George Shih, Carol C Wu, Safwan S Halabi, Marc D Kohli, Luciano M Prevedello, Tessa S Cook, Arjun Sharma,\n",
            "Judith K Amorosa, Veronica Arteaga, Maya Galperin-Aizenberg, et al. Augmenting the national institutes of\n",
            "health chest radiograph dataset with expert annotations of possible pneumonia. Radiology: Artificial Intelligence ,\n",
            "1(1):e180041, 2019.\n",
            "[64] Junji Shiraishi, Shigehiko Katsuragawa, Junpei Ikezoe, Tsuneo Matsumoto, Takeshi Kobayashi, Ken-ichi Komatsu,\n",
            "Mitate Matsui, Hiroshi Fujita, Yoshie Kodera, and Kunio Doi. Development of a digital image database for chest\n",
            "radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists’ detection of\n",
            "pulmonary nodules. American Journal of Roentgenology , 174(1):71–74, 2000.\n",
            "[65] Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Uru-\n",
            "rahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y Ng, et al. Evaluating\n",
            "progress in automatic chest x-ray radiology report generation. medRxiv , pages 2022–08, 2022.\n",
            "17LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Material\n",
            "Instruction Design\n",
            "In the clinical context of chest X-ray images, physicians typically identify potential diseases, locate relevant regions,\n",
            "and subsequently generate a comprehensive report based on observation. This process involves tasks such as disease\n",
            "classification, localization, and report generation. Historically, either multiple single-task models or a single multi-task\n",
            "model were employed to accomplish these goals, but these approaches lacked intrinsic correlations between tasks. By\n",
            "utilizing multiple instruction sets during the joint training approach, we not only enable the model to learn task-related\n",
            "features but also activate its potential capabilities to adapt to other tasks. As described in Figure 1(c), we design a set of\n",
            "seed instructions with placeholders and employ LLMs to create diverse related task descriptions for coarse-grained\n",
            "task-level customization. Following various instructions, our model can elegantly switch among different vision-centric\n",
            "tasks and accomplish them in a unified manner like LLMs. Here, we introduce the organization of instructions for\n",
            "task-level customization, including disease classification, localization, segmentation, and report generation as follows.\n",
            "Disease Classification Dataset includes entity information across 174 diseases from 0.54M images. For the entity\n",
            "classification task, the instruction is \"What disease does this image have?\". The answer includes all possible diseases\n",
            "present in the data, such as \"pneumonia\" and \"atelectasis.\", \"Is Pneumonia in this image?\". The response can be either\n",
            "\"yes\" or \"no\". We further extracted the textual phrases from the disease attributes (e.g., small left pneumothorax,\n",
            "normal cardiac silhouette) described in the original report of MIMIC-CXR and developed a subset that matches 135,751\n",
            "images with phrases. The subset comprises position descriptions (e.g., left, right, base, mid) and severity descriptions\n",
            "(e.g., mild, moderate, severe) for ten common diseases, i.e., Cardiomegaly, Pneumonia, Effusion, Atelectasis, Edema,\n",
            "Consolidation, Pneumothorax, Opacity, Fracture, and Supported Devices. For the severity classification task, the\n",
            "instruction is \"What is the level of cardiomegaly?\". The response can be \"moderate\" or \"severe\". The instruction for the\n",
            "location classification task is like \"Where is pneumothorax?\". The response can be \"on the left apical side\".\n",
            "Disease Localization Dataset incorporates CXR-AL14, VinDR-CXR, ChestX-Det, and In-house datasets, consisting\n",
            "of 187,097 images and corresponding BBOX for 12 diseases. The instruction given for the disease localization task is\n",
            "\"Give the accurate bounding box of {}.\". Here, the placeholder {} represents the category of the specific disease, such\n",
            "as \"pneumonia, in the lower left lung\". The response is a distinct bounding box area defined by coordinates [x 1, y1, x2,\n",
            "y2], representing the top-left and bottom-right points.\n",
            "Segmentation Dataset includes the subset of CheXmark for the segmentation task, comprising 224,316 images. We\n",
            "calculate the Cardiothoracic Ratio (CTR) for each image and compare it with the corresponding relationship described\n",
            "in the reports (e.g., CTR < 0.51: normal cardiac silhouette; 0.51 < CTR < 0.55: mild cardiomegaly; 0.55 < CTR < 0.6:\n",
            "moderate cardiomegaly; CTR > 0.6: severe cardiomegaly). This comparison allows us to filter the data accordingly.\n",
            "The SIIM dataset is collected for pneumothorax segmentation, consisting of 2,668 positive cases and 6422 negative\n",
            "cases. We further supplement the disease phrase subset and segmentation subsets as follows. The pneumothorax subset\n",
            "includes contour points (polygon vertexes, recomputed from the region mask) for 233 cases of pneumothorax. The\n",
            "respective instruction is \"Please segment the {} from the given image.\" For instance, \"Please segment the heart from the\n",
            "image.\" The response is a polygon area defined by a set of 30 points (coordinates).\n",
            "Report Instruction Dataset includes the original MIMIC-CXR dataset of 243,324 front images, and paired radiology\n",
            "reports. The instruction provided for the report generation task is \"describe the image\". This task specifically involves\n",
            "generating comprehensive reports based on chest X-ray images. Such brief instruction generates reports that lack\n",
            "accurate descriptions. We thus incorporate disease attributions in the instruction to improve the quality of the reports.\n",
            "During the training stage, we extract disease entities from ground truth reports and relevant severity and position\n",
            "attributes of the diseases within the corresponding sentences. These attributes are then combined with the original\n",
            "instruction for training. During the inference stage, we construct instructions for report generation using the results\n",
            "of the classification, segmentation, and disease localization tasks. First, we obtain the disease category from the\n",
            "classification task. Then, we use disease localization to determine the location and size of the lesion and compare it\n",
            "with the lung mask to determine the precise positional information.\n",
            "Experiment Details\n",
            "In this section, we introduce the detailed setting of the direct inference and fine-tune across all four tasks.\n",
            "Based on empirical findings, we set the proportional distribution of training data across each batch for classification,\n",
            "disease localization, report generation, and segmentation tasks to be 0.15/0.2/0.5/0.15. All the images are resized to a\n",
            "uniform size of 512x512 and subsequently adjusted by contrast and brightness. We selected the huge version of the\n",
            "OFA model as the pre-training model. We set a learning rate of 10−5, warm-up learning rate of 10−7, and dropout rate\n",
            "18LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "of 0.1, and train on eight V100 with batch size 256 for 30 epochs. We fine-tune all models using a learning rate of 10−4\n",
            "for all datasets with a batch size of 64.\n",
            "•Classification and Segmentation We have selected ConVIRT[ 31], GLoRIA[ 33], and BioVil[ 32] as the baseline\n",
            "models for the disease classification task. In all three models, ResNet-50 and BERT are chosen as the visual and text\n",
            "encoders, respectively. To perform direct inference of classification, we adopt the methods proposed in GLoRIA and\n",
            "BioVil, which transform the image classification task into a text-image matching task. Specifically, the test image is fed\n",
            "into the image encoder to generate image features. The test disease labels are subsequently formulated as text prompts\n",
            "and fed into the text encoder to generate text features. We then calculate the similarity between the image and text\n",
            "features. The prediction scores are set with normalized similarities. For ACC and F1, we utilize the validation dataset to\n",
            "determine the best score threshold for each class. Furthermore, we adhere to the official training strategies and train\n",
            "each model for 50 epochs during the fine-tuning process. Supplementary Table 1 shows the classification tasks achieve\n",
            "satisfactory results across all diseases. The results are comparable with KAD[54] and MedKLIP[30] claimed SOTA.\n",
            "•Disease Localization We employ TransVG, SeqTR, and VGTR as the baseline models for comparison with our\n",
            "OmniFM-DR. For TransVG[ 55], Resnet-50 is selected as the backbone. The BERT and ViT encoding length are 12 and\n",
            "6 separately, while the maximum query length is set to 20, following the authors’ recommendation. TransVG[ 55] has\n",
            "been trained on five datasets and all models are validated, while the most competitive on RefCOCOg is reported in\n",
            "Supplementary Table 2. For SeqTR[ 34], we follow the default settings of RefCOCOg, and DarkNet53 is selected as\n",
            "the detection backbone. The corresponding pre-calculated word embeddings are used to accommodate the pre-trained\n",
            "models. The authors have released three models on different datasets and training settings. We validate each model and\n",
            "the most competitive on RefCOCOg is reported in Supplementary Table 2. For VGTR[ 56], we followed the default\n",
            "settings as RefCOCOg and selected ResNet-50 and Bi-LSTM as the vision backbone and text encoder, respectively. For\n",
            "the evaluation of disease localization, the IoU threshold of TransVG, SeqTR, VGTR, and OmniFM-DR is set as 0.5\n",
            "consistently. The results of RefTR[57] and MedRPG[58] can not be reproduced due to the absence of code.\n",
            "•Report Generation We utilize Up-down[ 59], Att2in[ 60], and R2GenCMN[ 35] as the baseline models for report\n",
            "generation. Both Up-down and Att2in employ LSTM as the text encoder. Following their official implementation, Faster\n",
            "R-CNN and ResNet-101 are chosen as the image encoders for Up-down and Att2in, respectively. For R2GenCMN\n",
            "method, ResNet-101 serves as the image encoder, while a transformer-based module is utilized as the language model.\n",
            "We retrain the three models following their training procedures with our preprocessed dataset and evaluate them on the\n",
            "official test dataset. As for OmniFM-DR, leveraging its multi-task capability, we find it beneficial to incorporate disease\n",
            "attributes as prompts during both the training and inference stages. During training, we include extracted phrases from\n",
            "radiologist reports as additional prompts. During inference, we utilize phrases predicted by our model as supplementary\n",
            "prompts. Supplementary Table 3 shows that our report generation task achieved SOTA on clinical efficacy metrics and\n",
            "comparable results on natural language processing metrics. Supplementary Figure 1 provides more examples of multi-\n",
            "task results generated by our model. It can be found that the proposed model is capable of identifying Pneumothorax(a),\n",
            "Pneumonia (b), Edema (c), and Atelectasis(d) with a disease localization box, classification, and generated report.\n",
            "Take Supplementary Figure 1(b) for example, the generated report demonstrates the accurate pneumonia features and\n",
            "position described as \"increased opacification of the bilateral bases, right greater than left\", which are well consistent\n",
            "with the blue highlighted text in the golden standard report. The disease localization and classification results also agree\n",
            "with the gold standard. Furthermore, the generated report shows a stable cardiomediastinal contour which could be\n",
            "verified by the cardiothoracic ratio of 0.4 calculated by the segmentation task. Through the validation of multi-tasks,\n",
            "the explainability of the generated reports could be greatly enhanced.\n",
            "19LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Table 1: Comparison with other state-of-the-art methods of Disease classification task with direct\n",
            "inference setting on ChestXray14 dataset. The mean metrics (e.g., AUC and F1) refer to the macro average on the 14\n",
            "diseases.\n",
            "Metric Model\n",
            "Mean\n",
            "Atelectasis\n",
            "Cardiomegaly\n",
            "Effusion\n",
            "Infiltration\n",
            "Mass\n",
            "Nodule\n",
            "Pneumonia\n",
            "Pneumothorax\n",
            "Consolidation\n",
            "Edema\n",
            "Emphysema\n",
            "Fibrosis\n",
            "Pleural Thicken\n",
            "Hernia\n",
            "AUC ConVIRT[31] 56.0 45.9 43.3 64.6 65.4 60.1 58.0 64.0 53.3 64.6 69.2 43.1 48.2 54.5 49.4\n",
            "GLoRIA [33] 61.0 65.3 70.4 76.2 66.0 61.3 50.8 58.7 57.2 69.7 76.2 49.9 45.9 61.3 45.0\n",
            "BioViL[32] 66.2 51.7 68.8 74.3 60.1 66.3 63.9 66.9 68.3 65.0 79.5 65.6 63.2 63.7 69.8\n",
            "MedKLIP[30] 72.6 67.1 84.2 81.3 70.6 74.2 62.1 69.8 82.1 71.9 80.3 78.3 60.4 49.9 84.1\n",
            "Ours 73.6 74.5 76.1 78.8 60.4 72.3 65.5 69.6 81.6 71.9 79.1 72.6 64.5 72.1 90.9\n",
            "F1 ConVIRT[31] 13.5 0.1 0.2 36.7 43.6 15.7 14.2 6.0 20.5 17.7 12.1 8.3 3.4 9.7 0.7\n",
            "GLoRIA [33] 17.4 28.1 16.7 45.2 44.2 15.5 12.2 5.3 20.9 20.0 14.6 8.6 0.4 10.9 0.7\n",
            "BioViL[32] 19.2 23.5 20.9 43.8 41.4 17.8 16.4 6.7 27.4 17.7 18.7 12.3 5.6 11.9 4.5\n",
            "MedKLIP[30] 24.4 29.2 30.1 51.6 48.3 25.6 17.5 7.6 43.8 21.8 17.3 24.6 7.9 1.0 15.4\n",
            "Ours 26.3 36.8 21.9 48.8 41.1 29.5 20.6 9.7 42.8 21.9 17.4 16.4 8.9 17.8 35.1\n",
            "Supplementary Table 2: Comparison with other state-of-the-art methods of Disease localization task with 20-shot\n",
            "setting on MS-CXR and ChestXray14 dataset. The metrics (i.e. ACC and mIoU) refer to the macro average on the eight\n",
            "diseases.\n",
            "Dataset Metric Model Mean\n",
            "Atelectasis\n",
            "Cardiomegaly\n",
            "Effusion\n",
            "Pneumonia\n",
            "Pneumothorax\n",
            "Consolidation\n",
            "Edema\n",
            "Opacity\n",
            "Infiltrate\n",
            "Mass\n",
            "Nodule\n",
            "MS-CXR ACC VGTR[56] 30.1 40.0 91.8 5.6 40.0 13.8 28.3 10.0 11.1 - - -\n",
            "SeqTR[34] 43.7 31.3 93.5 6.25 41.6 51.3 41.7 34.4 50.0 - - -\n",
            "TransVG[55] 32.3 35.0 89.4 27.8 29.3 12.5 32.6 15.0 16.7 - - -\n",
            "Ours 54.7 50.0 90.6 45.0 51.3 53.7 47.8 45.0 54.4 - - -\n",
            "mIoU VGTR[56] 28.7 35.2 65.2 10.6 32.8 20.3 25.8 16.9 22.8 - - -\n",
            "SeqTR[34] 46.5 37.7 77.4 8.9 47.7 49.4 45.0 57.1 48.5 - - -\n",
            "TransVG[55] 29.3 29.8 61.9 23.2 26.6 18.7 27.4 19.9 26.6 - - -\n",
            "Ours 50.6 43.7 67.8 39.9 52.5 52.5 48.0 48.3 51.8 - - -\n",
            "ChestXray14 ACC VGTR[56] 35.6 14.8 100.0 29.4 54.8 4.2 - - - 40.0 23.8 17.7\n",
            "SeqTR[34] 47.7 50.3 82.9 62.5 29.6 56.3 - - - 55.0 38.8 6.3\n",
            "TransVG[55] 28.5 14.8 97.6 6.9 51.6 16.7 - - - 31.1 9.5 0.0\n",
            "Ours 61.5 51.8 97.6 67.0 61.3 55.8 - - - 62.2 61.9 34.7\n",
            "mIoU VGTR[56] 35.2 21.7 75.1 33.7 51.3 18.6 - - - 42.7 26.7 11.6\n",
            "SeqTR[34] 41.5 45.8 67.7 46.5 25.7 53.0 - - - 52.0 25.0 16.7\n",
            "TransVG[55] 28.7 23.3 72.8 22.5 37.6 22.5 - - - 32.1 15.9 3.3\n",
            "Ours 51.6 45.3 73.3 56.0 53.7 46.1 - - - 57.4 48.2 32.4\n",
            "20LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Table 3: Ablation experiment of multi-task and prompt capability. The quality of the generated report is\n",
            "evaluated by report, entity, and attribute levels, with the overall performance assessed by metrics (i.e., BL-4, METEOR,\n",
            "and Rouge-L), and the accuracy of the disease category evaluated by the CE metric (i.e., Precision, Recall, F1). The\n",
            "attribute metric focuses on the performance of disease severity and location described in the report.\n",
            "Report Entity Attribute\n",
            "BL-4 METEOR Rouge-L Precision Recall F1 ACC_S ACC_L\n",
            "Baseline Ours 10.97 14.02 26.48 43.22 31.31 33.29 18.34 8.17\n",
            "Task - LOC 10.85 14.06 26.49 44.96 31.06 33.01 - -\n",
            "- CLS 10.81 13.93 26.43 46.42 30.37 32.73 - -\n",
            "Prompt + Phrase 10.22 13.65 24.42 35.71 38.11 35.08 22.19 12.87\n",
            "+ Phrase-GT 11.42 14.33 26.99 71.47 44.82 49.55 30.18 23.57\n",
            "Supplementary Table 4: Diagnostic accuracy comparison with various report generation methods on MIMIC-CXR.\n",
            "Dataset Model BL-1 BL-4 METEOR Rouge-L Precision Recall F1\n",
            "MIMIC-CXR Up-down[59] 31.5 9.1 12.8 26.3 32.2 23.4 23.9\n",
            "Att2in[60] 33.1 9.7 13.6 27.5 32.5 23.6 25.7\n",
            "R2GenCMN[35] 35.6 10.4 14.7 28.1 33.3 28.4 28.6\n",
            "Constrastive[61] 35.0 10.9 15.1 28.3 35.2 29.8 30.3\n",
            "Ours 35.1 11.0 14.0 26.5 43.2 31.3 33.3\n",
            "21LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Figure 1: Typical cases of OmniFM-DR and ground truth for three tasks: multi-disease classification,\n",
            "visual grounding, and report generation. (a) Pneumothorax; (b) Pneumonia; (c) Edema; (d) Atelectasis. In the left Chest\n",
            "X-ray image, the BBOX with a green solid line denotes the ground truth, and the BBOX with a red-white dashed line\n",
            "represents the region detected by OmniFM-DR. In the right reports, the blue highlighted text represents the matched\n",
            "classified lesions compared to the ground truth report, and the yellow highlighted area represents the matched report\n",
            "describing other categories (e.g. cardiomegaly). CTR and PCR denote the Cardiothoracic Ratio and Pneumothorax\n",
            "Compress Ratio, respectively.\n",
            "22LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Figure 2: Typical examples of instruction set for six disease labels: Pneumothorax, Atelectasis,\n",
            "Pneumonia, Pleural Effusion, Condilidation, and Opacity. The left panel indicates the multiple instruction sets utilized\n",
            "during the training and testing phase. In the Chest X-ray image, the red dash line BBOX denotes the region detected by\n",
            "OmniFM-DR.\n",
            "23LEARNING A MULTI -TASK TRANSFORMER VIA\n",
            "UNIFIED AND CUSTOMIZED INSTRUCTION TUNING FOR\n",
            "CHEST RADIOGRAPH INTERPRETATION\n",
            "A P REPRINT\n",
            "Lijian Xu∗,B,1,2, Ziyu Ni∗,3, Xinglong Liu3, Xiaosong WangB,2, Hongsheng Li1,4, and Shaoting Zhang2\n",
            "1Centre for Perceptual and Interactive Intelligence, the Chinese University of Hong Kong, Hong Kong\n",
            "2Shanghai Artificial Intelligence Laboratory, Shanghai\n",
            "3Sensetime Research, Shanghai\n",
            "4Department of Electronic Engineering, the Chinese University of Hong Kong, Hong Kong\n",
            "∗Equal contributions\n",
            "ABSTRACT\n",
            "The emergence of multi-modal deep learning models has made significant impacts on clinical\n",
            "applications in the last decade. However, the majority of models are limited to single-tasking, without\n",
            "considering disease diagnosis is indeed a multi-task procedure. Here, we demonstrate a unified\n",
            "transformer model specifically designed for multi-modal clinical tasks by incorporating customized\n",
            "instruction tuning. We first compose a multi-task training dataset comprising 13.4 million instruction\n",
            "and ground-truth pairs (with approximately one million radiographs) for the customized tuning,\n",
            "involving both image- and pixel-level tasks. Thus, we can unify the various vision-intensive tasks\n",
            "in a single training framework with homogeneous model inputs and outputs to increase clinical\n",
            "interpretability in one reading. Finally, we demonstrate the overall superior performance of our model\n",
            "compared to prior arts on various chest X-ray benchmarks across multi-tasks in both direct inference\n",
            "and finetuning settings. Three radiologists further evaluate the generated reports against the recorded\n",
            "ones, which also exhibit the enhanced explainability of our multi-task model.\n",
            "Keywords Instruction Tuning ·Multi-task Learning ·Chest X-ray ·Explainability ·Computer-aided Diagnosis\n",
            "1 Introduction\n",
            "Chest radiography (CXR) is a non-invasive and relatively low-cost diagnostic radiology examination for screening and\n",
            "diagnosis of various thoracic diseases affecting the lung and heart [ 1]. However, the interpretation of CXR is greatly\n",
            "challenged by its low sensitivity of subtle abnormalities, overlapping structures, and limited soft tissue details, and\n",
            "therefore, depends heavily on the capability and experience of radiologists [ 2]. On the other hand, the growing demand\n",
            "for CXR examination has brought a burden on medical professionals, which also limits the clinical application of CXR,\n",
            "especially in community clinics or primary hospitals. In this context, automated diagnosis by AI could potentially\n",
            "contribute to reducing the workload of radiologists.\n",
            "Large Language Models [ 3;4] have revolutionized natural language processing and developed the capability to generate\n",
            "responses that closely resemble those from humans. They excel at a wide range of tasks, including language translation,\n",
            "question answering, and text generation[ 5;6;7;8;9;10;11]. Models like ChatGPT (OpenAI) [ 12;13] and Med-PaLm\n",
            "(Google) [ 14;15]have also demonstrated the powerful reasoning capabilities of language models in complex scenarios\n",
            "like medical diagnosis to assist professionals in delivering care. Nonetheless, such tasks are limited to a more general\n",
            "medical scope and largely rely on the visual features on the image level, without touching the pixel-level vision tasks,\n",
            "e.g., disease localization and segmentation. Moreover, to further improve the model’s ability on downstream tasks,\n",
            "supervised instruction tuning with specific downstream task-oriented data is often required on language only [ 16;17]\n",
            "and vision-language tasks[18; 19; 20], individually.arXiv:2311.01092v1  [cs.CV]  2 Nov 2023LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "On the other hand, the development of the generalist model in the medical field has lagged. Most models are designed\n",
            "primarily for \"pure\" language tasks [ 21;14]. Several generalist models for the biomedical field have been recently\n",
            "proposed and achieved progress in the VQA task [ 22;23;24;25;26;27;28;29]. On the other hand, the present\n",
            "multi-modality models are not well-suited for traditional image processing tasks like detection and segmentation.\n",
            "Existing methods face discrepancies in input, output, and training processes between visual tasks (e.g., detection,\n",
            "segmentation) and language tasks (e.g., image captioning and VQA), which hinders efficient collaboration. Furthermore,\n",
            "relying solely on textual outputs restricts the answer capacity and interpretability to some extent. For instance, in\n",
            "computer-aided diagnosis (CAD) using medical images, while the model can identify the disease type and provide\n",
            "treatment recommendations, it is unable to pinpoint the exact location and region of the pathologies, limiting its clinical\n",
            "usefulness as a reference for explainable diagnosis prediction.\n",
            "To address these limitations on both technical and application aspects, we propose OmniFM-DR, a multi-modal\n",
            "generalist model for reading chest radiographs, by providing more detailed evidence of associated diseases instead\n",
            "of rushing to the diagnosis directly. For the proposed multi-task transformer model, we unify the input and output\n",
            "labels of all sub-tasks into a uniform format for consistent modeling and joint training, which is detailed in the\n",
            "customized instruction tuning section. Figure 1 illustrates the four main tasks performed by OmniFM-DR and provides\n",
            "an illustrative comparison of model capabilities in each task with other state-of-the-art (SOTA) methods, measured\n",
            "in each individual evaluation metric. OmniFM-DR is designed to handle a wide range of downstream tasks relevant\n",
            "to chest X-ray analysis, including diagnosis of common thoracic diseases and localization of those image-visible\n",
            "disease patterns. Additionally, it can perform image segmentation for pneumothorax, lungs, and heart regions. Most\n",
            "importantly, OmniFM-DR is capable of generating reports summarizing the findings by leveraging all the provided\n",
            "evidence mentioned here. Notably, OmniFM-DR is capable of reaching equivalent or even better performance compared\n",
            "to SOTA models (dedicated to specific tasks) in all downstream clinical applications, showing its effectiveness and\n",
            "generality in chest X-ray interpretation.\n",
            "Our contributions are three-fold:\n",
            "•Our proposed model offers a versatile approach to analyzing chest X-ray images, allowing for comprehensive\n",
            "and accurate radiography image analysis across various application tasks. It enhances the interpretability of\n",
            "chest X-ray reporting by generating more detailed information on disease attributes. This includes disease size,\n",
            "location, severity, and contour, providing stronger evidence for diagnosis and treatment.\n",
            "•We develop a unique framework for building datasets that are tailored for customized instruction tuning. Unlike\n",
            "the conventional method of organizing pair-wise supervision (consisting of an image and its corresponding\n",
            "label), our framework involves cross-task training supervision for each sample, which enhances the learning of\n",
            "correlations among tasks. We will release the composed datasets with detailed image-instruction-label triplets,\n",
            "which is the very first dataset of this kind to our knowledge.\n",
            "•We applied the proposed model on various downstream application benchmarks, and an overall superior\n",
            "performance is shown compared to SOTA approaches. Furthermore, we conducted a controlled trial and\n",
            "evaluation on the generated reports performed by three radiologists. In the blinded comparison of 160\n",
            "retrospective cases from four centers, three radiologists perceived the quality of 59% of the generated reports\n",
            "to be equivalent to or even better than the original physician reports.\n",
            "2 Results\n",
            "2.1 Overview\n",
            "As illustrated in Figure 1, we train the multi-task model for analyzing chest X-ray images with a dataset specially\n",
            "designed for customized instruction tuning. It aims at a comprehensive analysis of chest X-ray images (providing\n",
            "detailed evidence) and enhanced interpretability (evidence-based diagnosis) as a tool for computer-aided diagnosis\n",
            "(CADx). For the performance evaluation purpose, we applied the proposed model to various downstream tasks and\n",
            "benchmarks, and an overall superior performance is achieved compared to SOTA models (dedicated to each individual\n",
            "sub-task). Furthermore, we conducted a controlled trial for the evaluation of the generated report in comparison to the\n",
            "original reports, assessed by a group of radiologists. In a blinded comparison involving 160 historical reports from\n",
            "four different centers, three radiologists consistently rated the quality of the generated reports as comparable to or\n",
            "better than original radiological reports, with a success rate of over 59%. Moreover, our proposed model exhibited\n",
            "an average omission rate of 2.53% and an average error rate of 2.72% per report, which are close to those of the\n",
            "radiologist-provided reference reports (i.e., 1.25% and 2.00%)\n",
            "In Table 1, we illustrate the overall performance of the proposed OmniFM-DR across four main tasks on nine datasets,\n",
            "along with individual SOTA results, using a total of 150 thousand testing cases. Our unified transformer model achieves\n",
            "2LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 1: (a) Overview performance of the proposed method OmniFM-DR on multiple datasets across multi-tasks\n",
            "(i.e., disease classification, localization, segmentation, and report generation) and (b) Training dataset DR-VQA . The\n",
            "attribute classification extracts disease phrases and related attributes (severity level and location) from the report. (c)\n",
            "Typical VQA examples of instruction set .\n",
            "superior results among the majority of the tasks on unseen datasets when performing direct inference. In the fine-tuning\n",
            "setting, the advantage remains significant in most of the tasks and metrics. Here, the proposed model is trained using\n",
            "a list of public datasets (detailed below). ChestXray14 and RSNA pneumonia datasets are utilized to evaluate the\n",
            "multi-label classification task performance, while MS-CXR, ChestXray14, and RSNA pneumonia datasets are used\n",
            "for the disease localization task. Furthermore, the largest available dataset for CXR reports (i.e., MIMIC-CXR) is\n",
            "3LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "utilized to evaluate the report generation performance. The metrics (AUC and F1) refer to the macro average on the 14\n",
            "diseases for ChestXray14. Accuracy (ACC) and mean Intersection over Union (mIoU) are utilized for the evaluation of\n",
            "disease localization, while Dice is utilized for segmentation tasks in three datasets (JSRT, CheXMark, and MS-PS).\n",
            "Furthermore, the performance of the report generation task is assessed by both clinical efficacy (CE, i.e., F1, Precision,\n",
            "Recall) and natural language processing (NLP, i.e., BL-4, METEOR, Rouge-L) metrics.\n",
            "Table 1: Performance comparison on MultiMedBench with the direct inference and fine-tune setting. We compare\n",
            "OmniFM-DR with specialist SOTA models. Across all tasks, datasets, and metrics combinations in MultiMedBench,\n",
            "we observe OmniFM-DR performance is equivalent to or exceeding SOTA.\n",
            "Task Dataset Metric Direct Inference Finetuning\n",
            "SOTA OmniFM-DR SOTA OmniFM-DR\n",
            "Disease classification ChestXray14 AUC 72.6[30] 73.4 77.8[31] 77.9\n",
            "F1 24.4[30] 26.2 32.9 [31] 32.6\n",
            "RSNA Pneumonia AUC 82.8[32] 84.3 88.5[33] 88.9\n",
            "F1 58.4[31] 59.5 67.2 [33] 66.2\n",
            "Disease localization ChestXray14 ACC 31.5[34] 56.7 47.7[34] 61.5\n",
            "mIoU 32.7[34] 49.2 41.5[34] 51.6\n",
            "MS-CXR ACC 26.1[34] 46.5 43.7[34] 54.7\n",
            "mIoU 28.2[34] 46.2 46.5[34] 50.6\n",
            "RSNA Pneumonia ACC 35.7[34] 42.7 33.4[34] 55.0\n",
            "mIoU 36.7[34] 47.6 33.2[34] 49.7\n",
            "Segmentation JSRT Dice 93.8[33] 90.8 95.2[33] 91.6\n",
            "CheXmask Dice 90.2[33] 88.5 92.1[33] 93.9\n",
            "MS-PS Dice 48.9 [33] 65.0 52.6[33] 59.4\n",
            "Report generation MIMIC-CXR F1 30.3[35] 33.3 - -\n",
            "Precision 35.2[35] 43.2 - -\n",
            "Recall 29.8[35] 31.3 - -\n",
            "BL-4 10.9[35] 11.0 - -\n",
            "METEOR 15.1[35] 14.0 - -\n",
            "Rouge-L 28.3[35] 26.5 - -\n",
            "2.2 Disease Classification\n",
            "We explore two types of classification tasks: disease entity classification and attribute classification. The entity\n",
            "classification task focuses on classifying disease categories, while the attribute classification task determines the disease\n",
            "attributes, e.g., location and severity. As illustrated in Table 1, we evaluate the disease entity classification task on\n",
            "the ChestXray14 and RSNA Pneumonia datasets with both direct inference and fine-tuning settings. Compared to the\n",
            "previous best results, Our model shows improvements of 0.8% in AUC and 1.8% in F1 for the ChestXRay14 dataset,\n",
            "with improvements of 1.5% in AUC and 1.1% in F1 for the RSNA Pneumonia dataset with direct inference setting, in\n",
            "comparison to models explicitly trained for each classification task. We further conduct the fine-tuning experiments and\n",
            "notice the average results of various methods are similar when models are fine-tuned with 50 samples for each finding\n",
            "on the target dataset. Figure2(a) further shows the detailed distribution of F1 results for 26 different disease entities\n",
            "that are aligned with the long-tail distribution on the MIMIC-CXR dataset. Additionally, our model attains high F1\n",
            "scores across the diseases in the attribute classification task. For instance, the severity classification ACC for Effusion\n",
            "and Pneumothorax reaches 65.6% and 65.9%, respectively, with an average of 59.2% for seven diseases with attribute\n",
            "descriptions.\n",
            "When evaluating the domain-shifted ChestXray14 dataset, our method achieves SOTA results on eight diseases (i.e.,\n",
            "Atelectasis, Mass, Nodule, Pneumonia, Consolidation, Fibrosis, Pleural Thicken, and Herina) shown in Figure 2(b). For\n",
            "instance, our method achieves an AUC and F1 of 74.5% and 36.8% for Atelectasis and 65.5% and 20.6% for Nodule,\n",
            "respectively. These results surpass the previous best results, demonstrating the effectiveness of our method in accurately\n",
            "identifying and localizing relatively small abnormalities in Chest X-ray images.\n",
            "2.3 Disease Localization\n",
            "We herein conduct extensive experiments on MS-CXR, ChestXray14, and RSNA Pneumonia datasets to evaluate\n",
            "disease localization under both direct inference and 20-shot fine-tuning settings. Under the direct inference setting,\n",
            "4LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 2: (a) In domain evaluation of 26 disease entities on the MIMIC-CXR dataset (upper panel) and attribute\n",
            "classification task in disease severity level and location (lower panel). (b) Out-of-domain evaluation between OmniFM-\n",
            "DR and other classification models (i.e., ConVIRT, GloRIA, BioViL, and MedKLIP) on the ChestXray14 dataset. AUC\n",
            "score and F1 are utilized for assessing the classification task, and \"mean\" is the weighted average of all attributes\n",
            "according to their frequency of occurrence.\n",
            "our model generally achieves the best performances over all existing visual-grounding models. As shown in Table 1,\n",
            "OmniFM-DR gets an average ACC of 56.7%, 46.5%, and 42.7% on the ChestXray14, MS-CXR, and RSNA Pneumonia\n",
            "datasets respectively, surpassing other methods by a large margin, 7% to 25%. When fine-tuned on the downstream\n",
            "dataset with 20 shots for each label, OmniFM-DR consistently scores the highest ACC of 61.5%, 54.7%, and 55.0% on\n",
            "the three datasets.\n",
            "Figure 3 further presents the model’s capability of disease localization across multiple diseases and comparations of\n",
            "OmniFM-DR against three approaches (i.e., VGTR, SeqTR, TransVG) on MS-CXR and ChestXray14 dataset with\n",
            "fine-tuning setting. The two datasets share five common diseases: Cardiomegaly, Effusion, Pneumothorax, Atelectasis,\n",
            "and Pneumonia. Moreover, the MS-CXR dataset comprises three additional diseases (i.e., Consolidation, Edema,\n",
            "and Opacity), while the ChestXray14 dataset includes Infiltrate, Mass, and Nodule. It is observed that OmniFM-DR\n",
            "5LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 3: Assessment of the disease localization task with ACC and mIoU metrics on the MS-CXR and ChestXray14\n",
            "datasets. (a) Example cases with boundary box predictions and (b) comparisons between OmniFM-DR and other\n",
            "disease localization methods.\n",
            "is capable of accurately identifying disease locations across multiple diseases. Furthermore, the model consistently\n",
            "achieves large advantages over other approaches for most disease categories. For seven of eight diseases on the MS-CXR\n",
            "dataset, OmniFM-DR gets significantly higher ACC and mIoU than the best-performing baseline, with the largest\n",
            "improvements in Pleural Effusion (>15%). Supplementary Table 2 further provides detailed comparisons of the eight\n",
            "diseases among OmniFM-DR and other approaches.\n",
            "2.4 Segmentation\n",
            "We adopt a polygon-based contour representation for the segmentation task to achieve a uniform input-output format\n",
            "with other tasks, i.e., predicting a list of polygon vertexes instead of region masks. As shown in Table 1, the Dice\n",
            "coefficient is utilized for evaluating the segmentation of lung and cardiac contours. Our unified model is comparable\n",
            "to pixel-based segmentation methods. To assess the severity of cardiomegaly and pneumothorax in potentially more\n",
            "detailed analyses, we perform post-processing on the segmented lung and cardiac masks to calculate the cardiothoracic\n",
            "ratio (CTR) and pneumothorax ratio (PCR). CTR is calculated as the ratio between the maximum transverse diameter\n",
            "of the heart and the chest, commonly used to evaluate the cardiomegaly severity as mild, moderate, or severe. The\n",
            "area method is used to calculate the PCR, which is represented by the ratio of the pneumothorax area to the affected\n",
            "lung area. Figure 4 illustrates the segmentation results of cardiomegaly and Pneumothorax with different severity. We\n",
            "observe that the overall performance of lung and cardiac contours is satisfactory across different CTR (see Figure 4(a)).\n",
            "Furthermore, Pneumothorax exhibits significant variations in location and size (see Figure 4(b)). For more accurate\n",
            "segmentation of mild pneumothorax, we incorporate a segmentation head with a U-Net decoder and achieve a Dice\n",
            "value of 59.4% and 65.0% under direct inference and fine-tune settings.\n",
            "6LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 4: Results of Heart, Lung, and Pneumothorax Segmentation. (a) Segmentation of the heart and lungs can be\n",
            "used to assess the severity of cardiomegaly. (b) Pneumothorax segmentation is performed to assess the severity of\n",
            "pneumothorax.\n",
            "2.5 Report Generation\n",
            "As the summary of the radiological reading process, reports contain major findings and possible disease diagnoses from\n",
            "the radiologists. As the unique feature of our proposed framework, we hypothesize adding pre-generated evidence could\n",
            "significantly improve the quality of AI-generated reports. Figure 5 demonstrates this quality improvement with the\n",
            "disease attributes prompt. During the inference stage, the customized prompt is initially derived from the classification\n",
            "task, including disease entity, severity, and rough location. The disease attribute prompt will be updated when a\n",
            "more accurate boundary box or severity of Pneumothorax/Cardiomegaly is available from the disease localization\n",
            "or segmentation task. The generated reports of typical cases of Pneumothorax and Pleural Effusion are provided in\n",
            "Figure 5(a). The proposed model is capable of identifying the diseases with an accurate disease localization box and\n",
            "generated report. Detailed descriptions of pneumothorax entities and attributes such as \"small left apical pneumothorax\"\n",
            "are highlighted by blue text in the ground truth report and are well predicted in the generated report. The location and\n",
            "severity information of Pneumothorax is further verified by the boundary boxes and calculated metric (i.e., PCR). On\n",
            "the other hand, the descriptions of disease attributes (e.g., \"small\" of pneumothorax, \"moderate cardiomegaly\") are\n",
            "omitted in the generated report without proper prompt. For the Effusion case, we also find the importance of proper\n",
            "input prompts in the task of report generation. The descriptions of pleural effusion and cardiomegaly are more detailed\n",
            "and accurate in the generated report with prompt. The boundary box of pleural effusion and mild cardiomegaly is\n",
            "indicated by the disease localization and segmentation task (i.e., CTR=0.54). With more specific prompts, the generated\n",
            "report shows accurate descriptions as \"bilateral pleural effusion, no pneumothorax, and mild heart size.\" In contrast, the\n",
            "general model without the designed prompt provides an inaccurate assessment of cardiomegaly, as described as \"stable\n",
            "heart size.\"\n",
            "Figure 5(b) further compares the accuracy of three different conditions (i.e., baseline, with phrase prompt, and with\n",
            "phrase-GT prompt). With the help of disease attributes prompt, the accuracy of severity and location description\n",
            "is improved across multiple diseases such as Atelectasis, Pneumothorax, and Cardiomegaly. The proposed method\n",
            "surpasses the baseline in accuracy by 9% for the severity (see details in Supplementary Table 3). When the ground truth\n",
            "disease phrase is utilized as the prompt, we find the quality of the report improved further as an upbound for our model.\n",
            "7LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 5: (a) Comparison results of two examples (e.g., Pneumothorax and Pleural Effusion). We further compare\n",
            "the generated report with and without designed instruction. The customized instruction includes the disease category,\n",
            "location, and severity level that are inferred from other tasks. (b) Comparison of generated reports in three ways:\n",
            "without prompt, with prompt from multi-task results, and with prompt from GT, using ACC of disease attributes from\n",
            "severity level and location.\n",
            "Subjective Comparison Study To assess the clinical interpretation, radiologists’ evaluations are employed to examine\n",
            "the quality of radiology reports. This side-by-side comparison study focuses on errors associated with the presence,\n",
            "location, and severity of clinical findings. Non-clinical errors, such as referring to views or previous studies that do not\n",
            "exist, are excluded from our evaluation.\n",
            "Figure 6(a) presents the average score comparison result from three radiologists. The average ratings for the radiologist-\n",
            "provided reference reports and generated reports are 3.74/3.48, 3.79/3.58, and 4.33/3.91, respectively. The average\n",
            "result across three radiologists is 3.95 (95% CI, 3.47-4.05)/3.65 (95% CI, 3.38-3.91), indicating that the quality of the\n",
            "generated reports is comparable to the reference reports. Figure 6(b) presents the pairwise comparison result, three\n",
            "radiologists believe that the quality of generated reports is equal to or even better than the original reports by 60%,\n",
            "64%, and 53%, respectively. Despite the observed fluctuations in results across radiologists, the collective analysis\n",
            "8LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 6: Side-by-side comparison of the quality of radiology reports from the radiologist (R) and center (c) perspective.\n",
            "In the side-by-side comparison , three radiologists reviewed and scored the clinically derived reports from four centers\n",
            "and reports generated by OmniFM-DR. (a) shows average score comparison, and (b) shows pairwise comparison results\n",
            "among three radiologists. The independent evaluation identifies rates of omissions and clinical errors for reports\n",
            "generated by OmniFM-DR. Significant errors are related to the presence, location, or severity of clinical findings, which\n",
            "are identified by clinical raters.\n",
            "suggests that our generated reports exhibit a comparable level of quality to the original reports generated by medical\n",
            "professionals.\n",
            "In the detailed evaluation, omission rate refers to missed disease diagnoses and error rate refers to inaccuracies in\n",
            "severity or location descriptions or false disease diagnosis. We report the results on the report level. Figure 6(c)\n",
            "shows the total omission rate for the reference and generated reports in the four centers are as follows: 0.89%/2.11%,\n",
            "1.67%/2.56%, 1.00%/3.22%, 1.44%/2.22%. On average, the omission rate for the generated reports 2.53% is close to\n",
            "that of the radiologist-provided reference reports, i.e., 1.25%, with a significant omission rate of 0.42%/1.00%. Figure\n",
            "6(d) shows the error rate, i.e., inaccuracies in severity or location descriptions or false positives. The total error rate for\n",
            "the reference and generated reports in the four centers are 0.67%/2.33%, 2.22%/3.11%, 2.78%/3.11%, 2.33%/2.33%.\n",
            "On average, the error rate for the generated reports 2.72% is comparable to that of the radiologist-provided reference\n",
            "reports, i.e., 2.00%, with few clinically significant errors observed.\n",
            "3 Discussion\n",
            "Our research focuses on developing a single and unified model for Chest X-ray image analysis, with the goal of\n",
            "addressing multi-tasks for better clinical interpretation, including disease classification, localization, segmentation, and\n",
            "report generation. By leveraging a carefully designed instruct tuning framework and incorporating diverse training\n",
            "strategies, our method can effectively extract relevant features and make predictions for multiple tasks in Chest X-\n",
            "ray images. This allows for a comprehensive analysis and diagnosis of various diseases and abnormalities in the\n",
            "images. With unseen evaluation datasets, the proposed unified model has exhibited impressive performance in disease\n",
            "classification and grounding tasks (via direct inference), surpassing the capabilities of existing state-of-the-art methods\n",
            "with a few-shot fine-tune setting.\n",
            "OmniFM-DR provides reliable evidence for a better explainability\n",
            "9LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "It is crucial that automated medical report generators produce trustworthy, easily understandable, and accurate reports\n",
            "for effective utilization in practice. To achieve the desired goals, it is crucial to have high-quality explanations about how\n",
            "the report was generated and the process involved in reaching the diagnosis. An explainable system helps developers to\n",
            "pinpoint any weaknesses or inefficiencies, while clinicians can rely on the decisions made with the aid of these systems.\n",
            "Here, we believe the interpretability of the reports can be validated mutually with the results obtained from other tasks\n",
            "within the model. For instance, the disease category, severity level, and approximate location of the lesion could be\n",
            "preliminary verified with the disease entity classification and attribute classification task. The disease localization\n",
            "task could further provide a more accurate bounding box of the lesion. For Pneumothorax and Cardiomegaly, the\n",
            "segmentation function could provide an accurate assessment of disease degree by postprocessing the contours of the\n",
            "pneumothorax/lung/heart mask. These results together contribute to a better verification of the generated reports.\n",
            "Furthermore, we improve the explainability of the generated reports by incorporating customized prompts that provide\n",
            "information regarding disease attributes. Our results indicate that disease-specific prompts have improved recall and\n",
            "F1 score of disease entities — key metrics in evaluating the performance of automated report generation models. The\n",
            "results of this study underscore the significance of incorporating prompts (evidence) into the model, thereby contributing\n",
            "to the model’s performance in the report generation process. This technique can prove to be a valuable strategy for\n",
            "improving the overall accuracy of AI models, specifically in the medical field.\n",
            "Image-instruction-label triplet dataset is designed for the promoting multi-task learning\n",
            "While there exist various single-task datasets, there have been limited attempts to unify them and create benchmarks for\n",
            "the development of a single and more comprehensive model. As one of our major contributions, we design and plan to\n",
            "release a comprehensive dataset of Chest X-ray data. This dataset includes full-label annotations, enabling researchers\n",
            "and practitioners to explore and leverage the benefits of multi-task learning in this domain. By sharing this dataset, we\n",
            "aim to encourage and support further advancements in multi-task learning approaches for Chest X-ray analysis. This\n",
            "can potentially create new opportunities in clinical applications [ 36;37;38]. The benchmark has several limitations,\n",
            "including the limited size of the individual datasets and limited modality and task diversity. Another key barrier to\n",
            "developing models for use across an even wider variety of biomedical data types is the lack of large-scale multimodal\n",
            "datasets, which would permit joint learning and alignment of the modality-specific encoders with the decoder.\n",
            "OmniFM-DR achieves comparable reporting results with expert radiologists\n",
            "We conduct the evaluation of report quality based on the assessment of radiologists, which indicates that the model\n",
            "performs well on the complex multimodal task of generating radiology reports. In 59% of the cases, the generated\n",
            "reports are of equal or even better quality compared with the reference reports generated by medical professionals.\n",
            "Additionally, the average omission rate and error rate in the model-generated reports are similar to those found in reports\n",
            "generated by medical professionals on the same dataset. These promising findings demonstrate the rapid progress in the\n",
            "field of automatic radiology report generation and suggest the potential for future clinical applications.\n",
            "Our proposed model exhibits competent performance across multiple tasks and enhances the explainability of the\n",
            "generated results. However, it still faces challenges when it comes to generalizing to unseen disease categories and\n",
            "undefined instructions (tasks). These limitations highlight the need for further research and development to improve the\n",
            "model’s ability to handle novel diseases and adapt to unfamiliar instructions. In the future, given the wider array of\n",
            "modalities and tasks in instruction tuning, more generalized models are expected to understand the tasks better and\n",
            "render modality-wise and task-wise emergent capability.\n",
            "4 Material and Method\n",
            "4.1 Building Dataset for Customized Instruction Tuning\n",
            "In this work, we construct a multi-task dataset for joint training of disease classification, localization, segmentation, and\n",
            "report generation. In general, we unify the input and output labels of all sub-tasks into a uniform format for consistent\n",
            "modeling and joint training, i.e., a set of image-instruction-label triplets as samples shown in Figure 1(c) and more\n",
            "in the supplementary materials. We further built a subset including the attributes and phrases for chest X-ray images\n",
            "like \"small base effusion, normal cardiac silhouette,\" which can be used as instruction for the report generation task.\n",
            "Additionally, the dataset underwent quality assurance by radiologists to ensure its accuracy and reliability.\n",
            "Instruction Design To build and utilize multiple instruction sets (for each of four sub-tasks) during the joint training\n",
            "approach, we design a set of seed instructions with placeholders (later replaced with corresponding targets) and employ\n",
            "LLMs to create diverse related task descriptions for coarse-grained task-level customization, such as samples illustrated\n",
            "in Figure 1(c). Following various instructions, our model can elegantly switch among different vision-centric tasks\n",
            "and accomplish them in a unified manner like LLMs. More details about the organization of instructions for task-level\n",
            "10LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "customization, including disease classification, localization, segmentation, and report generation, are introduced in the\n",
            "supplementary material.\n",
            "We employ a list of public datasets for training our proposed transformer model, e.g., MIMIC-CXR, VinDr-CXR, and\n",
            "ChestX-Det10. We first sort out the aligned lesion categories of each dataset and the associated radiology report data\n",
            "and detection bounding box (BBox) data. We exclude the image datasets that are included in the test and validation\n",
            "datasets of downstream tasks to avoid data leakage. Each dataset is described in detail as follows:\n",
            "•MIMIC-CXR [36] contains more than 377,110 radiograph images from over 227,835 radiographic studies.\n",
            "Each radiograph is paired with lesion classification and associated radiology report. We employ this dataset\n",
            "for multi-label classification and report generation tasks.\n",
            "•Padchest [39] includes 160,840 images obtained from 67,000 patients, covering six different position views.\n",
            "174 different radiographic findings were labeled and used for the classification task in this study.\n",
            "•CXR-AL14 [40] is a large-scale dataset for chest X-ray image detection. It has more than 140,000 chest X-ray\n",
            "radiographs containing 253,844 bounding boxes in 14 chest abnormal object categories.\n",
            "•VinDr-CXR [41] includes chest radiographs with annotations for the classification of 28 common chest\n",
            "diseases. The dataset contains 15,000 CXR scans in the training set. We select eight diseases from the dataset\n",
            "along with their corresponding BBox for the disease localization task.\n",
            "•ChestX-Det [42] consists of 3,578 images from NIH ChestXray14[ 38] for 13 common disease. We select\n",
            "seven diseases from the dataset along with BBox for the disease localization task.\n",
            "•CheXmask [43] contains 676,803 lung and heart segmentation masks of chest images from six publicly\n",
            "available databases: CANDID-PTX, ChestXray14, Chexpert, MIMIC-CXR, Padchest, and VinDr-CXR. We\n",
            "include 224,316 data for training and 10,000 data from ChestXray14 for downstream evaluation.\n",
            "•SIIM [44] comes from the SIIM-ACR Pneumothorax Segmentation competition and contains 12,090 images,\n",
            "among which approximately 3,000 cases are positive for pneumothorax disease with masks.\n",
            "•In-house dataset consists of 2,531 chest X-ray images, encompassing nine disease categories relevant to the\n",
            "disease localization task, along with BBox. All the images are captured from the front view.\n",
            "4.2 Model Architecture\n",
            "In this work, we propose a multimodal model that leverages the sequence-to-sequence learning paradigm for joint multi-\n",
            "task training. The specific tasks encompass disease classification, localization, segmentation, and report generation.\n",
            "For each task, we design specific task instructions to facilitate the model’s differentiation between tasks. Inspired by\n",
            "the multi-modal models [ 45;46;47;48], OmniFM-DR leverages an encoder-decoder architecture for perceiving pixel\n",
            "inputs and generating the target sequence and performs unified modeling and joint training on downstream visual and\n",
            "language tasks as shown in Figure 7. Bounding boxes and class labels are converted into sequences of discrete tokens.\n",
            "This enables OmniFM-DR to robustly perform diverse language and vision tasks based on instructions, providing\n",
            "diverse and complex output results.\n",
            "Image and Language Encoder With an input chest X-ray image xi∈RH×W, visual features are extracted by image\n",
            "encoder and further projected to the feature space:\n",
            "vi=Pimg(Eimg(xi))∈R(hf×wf)×d(1)\n",
            "where hfandwfare the output size of visual features, and drepresents the feature dimension. Eimgcan be any\n",
            "common visual backbones and we use ResNet152 in this study. Specifically, we take output features from the 4th\n",
            "residual block. Visual features are then projected to a pre-defined feature dimension by using Pimg, which is composed\n",
            "of two linear layers. With any processed input instruction sequence ti, text features are extraced by language encoder:\n",
            "li=Etxt(ti)∈Rnt×d(2)\n",
            "where ntis the number of input tokens and drepresents the feature dimension. In our case, Bert [ 49] is used as a\n",
            "language encoder.\n",
            "Multi-modality Module This module follows an encoder-decoder architecture format. Given the input visual features\n",
            "viand text features li, we first generate fused multi-modal representations by combining the image and text embeddings.\n",
            "These fused features serve as the keys and values in the cross-attention blocks in the decoder. By conditioning on the\n",
            "11LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Figure 7: Overview of OmniFM-DR. Our model processes the chest X-ray image and instruction query utilizing\n",
            "a Resnet-based encoder and BPE/tokenizer, respectively. We then fuse the visual and textual information with a\n",
            "transformer encoder and generate the output text with a unified multi-task decoder. The instruction tuning includes four\n",
            "tasks, i.e., multi-disease classification, disease localization, segmentation, and report generation.\n",
            "partial sequence yi,<jpredicted so far, the decoder recursively makes predictions for the token at position j, effectively\n",
            "generating aligned descriptions across modalities.\n",
            "yi,j=Dmm(Emm(concat (vi, li)), yi,<j)∈R1×d(3)\n",
            "In our experiments, we leverage BART [ 50] for multi-modal encoding and decoding. BART utilizes BERT [ 49] as the\n",
            "encoder. The decoder is based on GPT [51] and generates output sequences in an autoregressive manner.\n",
            "Model joint-training and Inference We optimize the sequence-to-sequence model using cross-entropy loss as follows.\n",
            "L=−nX\n",
            "i=1|y|X\n",
            "j=1logPθ(yi,j|yi,<j, xi, ti) (4)\n",
            "where nis the batch size, θrepresents the model parameters, xirepresents the input image, tistands for the input\n",
            "instruction, and yi,jdenotes the output token at position jfor the ithsample at each batch. To enhance the quality of\n",
            "generation during inference, we employ various decoding techniques, such as beam search.\n",
            "4.3 Downstream Finetuning and Evaluation\n",
            "The benchmark evaluates four tasks, using a total of nine datasets with over 150 thousand samples. Among them,\n",
            "ChestX-ray14 and RSNA pneumonia datasets are utilized for evaluating the performance on the multi-label classification\n",
            "task, while MS-CXR, ChestX-ray14, and RSNA pneumonia datasets for the disease localization task. Meanwhile,\n",
            "we assess the report generation on the MIMIC-CXR dataset. Several standard metrics are introduced for various\n",
            "tasks. For example, F1 stands for “F1 score”, ACC stands for “Accuracy”, BLEU stands for “BiLingual Evaluation\n",
            "12LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Understudy” [ 52], ROUGE stands for “Recall-Oriented Understudy for Gisting Evaluation” [ 53]. For BLEU and\n",
            "ROUGE, we all use 1-gram by default. We compared the proposed method with other methods across four tasks\n",
            "[31;33;32;54;30;55;34;56;57;58;59;60;35;61] and the detailed comparison results can be found in the\n",
            "supplementary materials. The evaluation datasets include\n",
            "•ChestXray14 [38] is a publicly accessible dataset for classification and localization. It has 112,120 images\n",
            "with 14 common disease labels, with 984 images having eight localization of key findings with hand-labeled\n",
            "BBOX. We randomly split it into training/validation/test sets by 7:1:2 for the classification and localization\n",
            "task.\n",
            "•MS-CXR [62] consists of 1,153 samples with BBOX and concise radiology reports for eight diseases (sourced\n",
            "from the testing set ofMIMIC-CXR), which is utilized for the disease localization task. We randomly split it\n",
            "into training/validation/test sets by 7:1:2 based on the patients and evaluated the average performance of the\n",
            "model in all eight diseases.\n",
            "•RSNA Pneumonia [63] is a binary classification chest X-ray dataset that comprises 26,683 images, where\n",
            "each radiograph is categorized as either pneumonia or normal. We randomly sample 3,000 data from the\n",
            "official training set to build the test set for direct inference and finetuning experiments for disease classification\n",
            "and localization tasks.\n",
            "•JSRT [64] contains 247 chest images, among which 154 cases have lung nodules. We selected 93 healthy\n",
            "samples for the lung segmentation task.\n",
            "•MS-PS is a privately annotated pneumothorax segmentation dataset, comprising 233 images from the MS-\n",
            "CXR dataset. It was annotated by three expert physicians and is split into a 4:1 ratio for training and testing,\n",
            "respectively, for downstream pneumothorax segmentation evaluation.\n",
            "4.4 Clinical Evaluation of Generated Reports\n",
            "To examine the quality of generated reports from a clinical usefulness perspective, we conducted a comprehensive\n",
            "evaluation performed by three experienced radiologists. A total of 160 cases were evaluated, including 120 cases from\n",
            "three medical institutes (listed in details below) and 40 cases from the MIMIC-CXR test set. To match the intended\n",
            "inputs of our model, we excluded cases that mentioned multiple imaging views or comparisons to prior test results in\n",
            "the generated reports. Our study involved two distinct yet complementary human evaluations: (a) a parallel evaluation,\n",
            "where raters compared and ranked alternative reports based on their quality, and (b) an independent evaluation conducted\n",
            "to assess the quality of each individual report.\n",
            "Ethical Statement The private data used in this retrospective study was approved by the Ethics Committee of three\n",
            "institutes, i.e., Fengcheng People’s Hospital, Huanggang Hospital of Traditional Chinese Medicine, and Longkou\n",
            "People’s Hospital. and the committees waived the consent since the retrospective research would not change the\n",
            "examination process of the patients. All data were adequately anonymized, and the risk of disclosing patient privacy via\n",
            "imaging data was minimal.\n",
            "Parallel Evaluation All 160 original and generated reports were randomly chosen from a pool of four centers and\n",
            "evaluated by three radiologists. The radiologists were unaware of the source of the reports and reviewed them in a\n",
            "randomized order. The quality of the reports will be scored subjectively on a 1-5 scale, with 1 being the worst and 5 the\n",
            "best. The detailed guidelines are provided as follows:\n",
            "• Report without diagnosis errors and with comprehensive description should be scored 5;\n",
            "• Report with significant clinical diagnosis errors should be scored 1;\n",
            "•Report can be considered correct if the content is reasonable based on the given image. For instance, an\n",
            "accurate diagnosis of pleural effusion may not be obtained based on a frontal view image;\n",
            "•Report can be considered correct if there are descriptions of related diseases. For instance, lung collapse can\n",
            "be indicative of atelectasis;\n",
            "•Report with the better description should be scored higher, if two reports are error-free or exhibit similar errors;\n",
            "• Repetitive descriptions can be overlooked;\n",
            "Independent Evaluation Radiologists were provided with one chest X-ray image paired with the disease findings, and\n",
            "tasked with assessing the generated reports and original reports. During the evaluation, the radiologists were unaware of\n",
            "the source of the reports. They aimed to determine whether there are discrepancies or errors, any missing elements, or\n",
            "inaccurate descriptions (e.g., location and severity) in the reports and evaluate their clinical significance referring to the\n",
            "13LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "methodology [ 65;22]. Six types of diseases are evaluated, i.e., Pneumothorax, Pleural Effusion, Edema, Consolidation\n",
            "or Pneumonia (grouped together), Atelectasis, and normal. Radiologists were required to assess whether every type of\n",
            "error exists for each specific disease when evaluating reports. The considered errors are agreed by the radiologists and\n",
            "listed as follows:\n",
            "• False positives. Incorrect disease detection;\n",
            "• False negatives. Missed disease detection;\n",
            "• Inaccurate location. For instance, left lung pneumonia is described as right lung pneumonia;\n",
            "• Inaccurate severity. For instance, a minor pleural effusion is described as a major pleural effusion;\n",
            "• Non-existent references. For instance, \"compared with previous\" and \"based on front-lateral image\";\n",
            "Code and Data Availability\n",
            "Code for training and evaluation is available at https://github.com/MedHK23/OmniFM-DR . The new dataset\n",
            "released in this study can be found at https://huggingface.co/datasets/MedHK23/OmniFM-DR . The MultiMed-\n",
            "Bench is all open source, and the respective download link is described in Git Hub.\n",
            "Author contributions\n",
            "All authors have contributed fully to the concept and design of the study. LX and ZN collected the clinical data,\n",
            "performed the experiments, and analyzed the experiment results. XL performed the comparative experiments with other\n",
            "methods. LX and XW drafted the manuscript. XW, SZ, and HL supervised the projects and gave final approval of the\n",
            "manuscript. All authors have carefully read and approved the final manuscript.\n",
            "Competing interests\n",
            "The authors declare no competing interests.\n",
            "References\n",
            "[1]B.J. Stevens, L. Skermer, and J. Davies. Radiographers reporting chest x-ray images: Identifying the service\n",
            "enablers and challenges in england, uk. Radiography , 27(4):1006–1013, 2021.\n",
            "[2]Erdi Çallı, Ecem Sogancioglu, Bram van Ginneken, Kicky G. van Leeuwen, and Keelin Murphy. Deep learning\n",
            "for chest x-ray analysis: A survey. Medical Image Analysis , 72:102125, 2021.\n",
            "[3]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n",
            "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\n",
            "image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n",
            "[4]Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\n",
            "transformer: Hierarchical vision transformer using shifted windows, 2021.\n",
            "[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n",
            "transformers for language understanding, 2019.\n",
            "[6]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
            "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\n",
            "models. arXiv preprint arXiv:2302.13971 , 2023.\n",
            "[7]Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman Castagné,\n",
            "Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access\n",
            "multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.\n",
            "[8]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\n",
            "Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\n",
            "pathways. arXiv preprint arXiv:2204.02311 , 2022.\n",
            "[9]Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\n",
            "Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint\n",
            "arXiv:2205.01068 , 2022.\n",
            "14LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "[10] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\n",
            "Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In\n",
            "International Conference on Machine Learning , pages 5547–5569. PMLR, 2022.\n",
            "[11] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\n",
            "Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.\n",
            "[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\n",
            "lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\n",
            "in neural information processing systems , 33:1877–1901, 2020.\n",
            "[13] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n",
            "Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human\n",
            "feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.\n",
            "[14] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\n",
            "Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv\n",
            "preprint arXiv:2212.13138 , 2022.\n",
            "[15] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\n",
            "Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language\n",
            "models. arXiv preprint arXiv:2305.09617 , 2023.\n",
            "[16] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,\n",
            "and Quoc V . Le. Finetuned language models are zero-shot learners, 2022.\n",
            "[17] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\n",
            "Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\n",
            "arXiv:2210.11416 , 2022.\n",
            "[18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,\n",
            "Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction\n",
            "tuning, 2023.\n",
            "[19] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language\n",
            "understanding with advanced large language models, 2023.\n",
            "[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n",
            "[21] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. Med-bert: pretrained contextualized embeddings\n",
            "on large-scale structured electronic health records for disease prediction. NPJ digital medicine , 4(1):86, 2021.\n",
            "[22] Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung Weng,\n",
            "Attila Kiraly, Sahar Kazemzadeh, Zakkai Melamed, et al. Elixr: Towards a general purpose x-ray artificial\n",
            "intelligence system through alignment of large language models and radiology vision encoders. arXiv preprint\n",
            "arXiv:2308.01317 , 2023.\n",
            "[23] Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou,\n",
            "Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained transformer for vision,\n",
            "language, and multimodal tasks. arXiv preprint arXiv:2305.17100 , 2023.\n",
            "[24] Qiuhui Chen, Xinyue Hu, Zirui Wang, and Yi Hong. Medblip: Bootstrapping language-image pre-training from\n",
            "3d medical images and texts, 2023.\n",
            "[25] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll,\n",
            "Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. arXiv preprint arXiv:2307.14334 ,\n",
            "2023.\n",
            "[26] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for\n",
            "radiology, 2023.\n",
            "[27] Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M.\n",
            "Summers, and Yingying Zhu. Expert knowledge-aware image difference graph representation learning for\n",
            "difference-aware medical visual question answering. In Proceedings of the 29th ACM SIGKDD Conference on\n",
            "Knowledge Discovery and Data Mining . ACM, aug 2023.\n",
            "[28] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis,\n",
            "Pranav Rajpurkar, and Jure Leskovec. Med-flamingo: a multimodal medical few-shot learner, 2023.\n",
            "[29] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\n",
            "Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day,\n",
            "2023.\n",
            "15LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "[30] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Medklip: Medical knowledge enhanced\n",
            "language-image pre-training in radiology, 2023.\n",
            "[31] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Contrastive learning\n",
            "of medical visual representations from paired images and text, 2022.\n",
            "[32] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Stephanie Hyland,\n",
            "Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, Hoifung Poon, and Ozan Oktay. Making\n",
            "the most of text semantics to improve biomedical vision–language processing. In Lecture Notes in Computer\n",
            "Science , pages 1–21. Springer Nature Switzerland, 2022.\n",
            "[33] Shih-Cheng Huang, Liyue Shen, Matthew P. Lungren, and Serena Yeung. Gloria: A multimodal global-local\n",
            "representation learning framework for label-efficient medical image recognition. 2021 IEEE/CVF International\n",
            "Conference on Computer Vision (ICCV) , pages 3922–3931, 2021.\n",
            "[34] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao,\n",
            "Xiaoshuai Sun, and Rongrong Ji. SeqTR: A simple yet universal network for visual grounding. In Lecture Notes\n",
            "in Computer Science , pages 598–615. Springer Nature Switzerland, 2022.\n",
            "[35] Zhihong Chen, Yaling Shen, Yan Song, and Xiang Wan. Cross-modal memory networks for radiology report\n",
            "generation, 2022.\n",
            "[36] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-\n",
            "ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest\n",
            "radiographs with free-text reports. Scientific data , 6(1):317, 2019.\n",
            "[37] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,\n",
            "Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty\n",
            "labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence , volume 33, pages\n",
            "590–597, 2019.\n",
            "[38] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8:\n",
            "Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of\n",
            "common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\n",
            "pages 2097–2106, 2017.\n",
            "[39] Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vayá. Padchest: A large chest x-ray\n",
            "image dataset with multi-label annotated reports. Medical Image Analysis , 66, 2020.\n",
            "[40] CXR-AL14 dataset. http://47.108.59.218/user/data/ . Accessed: 2023-08-30.\n",
            "[41] Ha Q Nguyen, Khanh Lam, Linh T Le, Hieu H Pham, Dat Q Tran, Dung B Nguyen, Dung D Le, Chi M Pham,\n",
            "Hang TT Tong, Diep H Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologist’s annotations.\n",
            "Scientific Data , 9(1):429, 2022.\n",
            "[42] Jie Lian, Jingyu Liu, Shu Zhang, Kai Gao, Xiaoqing Liu, Dingwen Zhang, and Yizhou Yu. A structure-aware\n",
            "relation network for thoracic diseases detection and segmentation. IEEE Transactions on Medical Imaging ,\n",
            "40(8):2042–2052, 2021.\n",
            "[43] Nicolás Gaggion, Candelaria Mosquera, Lucas Mansilla, Martina Aineseder, Diego H Milone, and Enzo Ferrante.\n",
            "Chexmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images. arXiv\n",
            "preprint arXiv:2307.03293 , 2023.\n",
            "[44] Anna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail Fomitchev, Mohannad Hussain, ParasLakhani, Phil\n",
            "Culliton, and Shunxing Bao. Siim-acr pneumothorax segmentation. Kaggle. https://kaggle.com/competitions/siim-\n",
            "acr-pneumothorax-segmentation , 2019.\n",
            "[45] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\n",
            "and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence\n",
            "learning framework, 2022.\n",
            "[46] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and Geoffrey Hinton. Pix2seq: A language modeling\n",
            "framework for object detection, 2022.\n",
            "[47] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J. Fleet, and Geoffrey Hinton. A unified sequence\n",
            "interface for vision tasks, 2022.\n",
            "[48] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified\n",
            "model for vision, language, and multi-modal tasks, 2022.\n",
            "[49] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n",
            "transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\n",
            "16LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "[50] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves\n",
            "Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language\n",
            "generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 , 2019.\n",
            "[51] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.\n",
            "[52] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of\n",
            "machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics ,\n",
            "pages 311–318, 2002.\n",
            "[53] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out ,\n",
            "pages 74–81, 2004.\n",
            "[54] Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Knowledge-enhanced visual-language\n",
            "pre-training on chest radiology images, 2023.\n",
            "[55] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual\n",
            "grounding with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision\n",
            "(ICCV) , pages 1769–1779, October 2021.\n",
            "[56] Ye Du, Zehua Fu, Qingjie Liu, and Yunhong Wang. Visual grounding with transformers. 2022 IEEE International\n",
            "Conference on Multimedia and Expo (ICME) , pages 1–6, 2022.\n",
            "[57] Muchen Li and Leonid Sigal. Referring transformer: A one-step approach to multi-task visual grounding, 2021.\n",
            "[58] Zhihao Chen, Yang Zhou, Anh Tran, Junting Zhao, Liang Wan, Gideon Ooi, Lionel Cheng, Choon Hua Thng,\n",
            "Xinxing Xu, Yong Liu, and Huazhu Fu. Medical phrase grounding with region-phrase context contrastive\n",
            "alignment, 2023.\n",
            "[59] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.\n",
            "Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the\n",
            "IEEE conference on computer vision and pattern recognition , pages 6077–6086, 2018.\n",
            "[60] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence\n",
            "training for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\n",
            "pages 7008–7024, 2017.\n",
            "[61] Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang, and Xu Sun. Contrastive attention for automatic\n",
            "chest X-ray report generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 ,\n",
            "pages 269–280, Online, August 2021. Association for Computational Linguistics.\n",
            "[62] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer, Stephanie Hyland,\n",
            "Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, et al. Making the most of text semantics\n",
            "to improve biomedical vision–language processing. In European conference on computer vision , pages 1–21.\n",
            "Springer, 2022.\n",
            "[63] George Shih, Carol C Wu, Safwan S Halabi, Marc D Kohli, Luciano M Prevedello, Tessa S Cook, Arjun Sharma,\n",
            "Judith K Amorosa, Veronica Arteaga, Maya Galperin-Aizenberg, et al. Augmenting the national institutes of\n",
            "health chest radiograph dataset with expert annotations of possible pneumonia. Radiology: Artificial Intelligence ,\n",
            "1(1):e180041, 2019.\n",
            "[64] Junji Shiraishi, Shigehiko Katsuragawa, Junpei Ikezoe, Tsuneo Matsumoto, Takeshi Kobayashi, Ken-ichi Komatsu,\n",
            "Mitate Matsui, Hiroshi Fujita, Yoshie Kodera, and Kunio Doi. Development of a digital image database for chest\n",
            "radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists’ detection of\n",
            "pulmonary nodules. American Journal of Roentgenology , 174(1):71–74, 2000.\n",
            "[65] Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Uru-\n",
            "rahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y Ng, et al. Evaluating\n",
            "progress in automatic chest x-ray radiology report generation. medRxiv , pages 2022–08, 2022.\n",
            "17LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Material\n",
            "Instruction Design\n",
            "In the clinical context of chest X-ray images, physicians typically identify potential diseases, locate relevant regions,\n",
            "and subsequently generate a comprehensive report based on observation. This process involves tasks such as disease\n",
            "classification, localization, and report generation. Historically, either multiple single-task models or a single multi-task\n",
            "model were employed to accomplish these goals, but these approaches lacked intrinsic correlations between tasks. By\n",
            "utilizing multiple instruction sets during the joint training approach, we not only enable the model to learn task-related\n",
            "features but also activate its potential capabilities to adapt to other tasks. As described in Figure 1(c), we design a set of\n",
            "seed instructions with placeholders and employ LLMs to create diverse related task descriptions for coarse-grained\n",
            "task-level customization. Following various instructions, our model can elegantly switch among different vision-centric\n",
            "tasks and accomplish them in a unified manner like LLMs. Here, we introduce the organization of instructions for\n",
            "task-level customization, including disease classification, localization, segmentation, and report generation as follows.\n",
            "Disease Classification Dataset includes entity information across 174 diseases from 0.54M images. For the entity\n",
            "classification task, the instruction is \"What disease does this image have?\". The answer includes all possible diseases\n",
            "present in the data, such as \"pneumonia\" and \"atelectasis.\", \"Is Pneumonia in this image?\". The response can be either\n",
            "\"yes\" or \"no\". We further extracted the textual phrases from the disease attributes (e.g., small left pneumothorax,\n",
            "normal cardiac silhouette) described in the original report of MIMIC-CXR and developed a subset that matches 135,751\n",
            "images with phrases. The subset comprises position descriptions (e.g., left, right, base, mid) and severity descriptions\n",
            "(e.g., mild, moderate, severe) for ten common diseases, i.e., Cardiomegaly, Pneumonia, Effusion, Atelectasis, Edema,\n",
            "Consolidation, Pneumothorax, Opacity, Fracture, and Supported Devices. For the severity classification task, the\n",
            "instruction is \"What is the level of cardiomegaly?\". The response can be \"moderate\" or \"severe\". The instruction for the\n",
            "location classification task is like \"Where is pneumothorax?\". The response can be \"on the left apical side\".\n",
            "Disease Localization Dataset incorporates CXR-AL14, VinDR-CXR, ChestX-Det, and In-house datasets, consisting\n",
            "of 187,097 images and corresponding BBOX for 12 diseases. The instruction given for the disease localization task is\n",
            "\"Give the accurate bounding box of {}.\". Here, the placeholder {} represents the category of the specific disease, such\n",
            "as \"pneumonia, in the lower left lung\". The response is a distinct bounding box area defined by coordinates [x 1, y1, x2,\n",
            "y2], representing the top-left and bottom-right points.\n",
            "Segmentation Dataset includes the subset of CheXmark for the segmentation task, comprising 224,316 images. We\n",
            "calculate the Cardiothoracic Ratio (CTR) for each image and compare it with the corresponding relationship described\n",
            "in the reports (e.g., CTR < 0.51: normal cardiac silhouette; 0.51 < CTR < 0.55: mild cardiomegaly; 0.55 < CTR < 0.6:\n",
            "moderate cardiomegaly; CTR > 0.6: severe cardiomegaly). This comparison allows us to filter the data accordingly.\n",
            "The SIIM dataset is collected for pneumothorax segmentation, consisting of 2,668 positive cases and 6422 negative\n",
            "cases. We further supplement the disease phrase subset and segmentation subsets as follows. The pneumothorax subset\n",
            "includes contour points (polygon vertexes, recomputed from the region mask) for 233 cases of pneumothorax. The\n",
            "respective instruction is \"Please segment the {} from the given image.\" For instance, \"Please segment the heart from the\n",
            "image.\" The response is a polygon area defined by a set of 30 points (coordinates).\n",
            "Report Instruction Dataset includes the original MIMIC-CXR dataset of 243,324 front images, and paired radiology\n",
            "reports. The instruction provided for the report generation task is \"describe the image\". This task specifically involves\n",
            "generating comprehensive reports based on chest X-ray images. Such brief instruction generates reports that lack\n",
            "accurate descriptions. We thus incorporate disease attributions in the instruction to improve the quality of the reports.\n",
            "During the training stage, we extract disease entities from ground truth reports and relevant severity and position\n",
            "attributes of the diseases within the corresponding sentences. These attributes are then combined with the original\n",
            "instruction for training. During the inference stage, we construct instructions for report generation using the results\n",
            "of the classification, segmentation, and disease localization tasks. First, we obtain the disease category from the\n",
            "classification task. Then, we use disease localization to determine the location and size of the lesion and compare it\n",
            "with the lung mask to determine the precise positional information.\n",
            "Experiment Details\n",
            "In this section, we introduce the detailed setting of the direct inference and fine-tune across all four tasks.\n",
            "Based on empirical findings, we set the proportional distribution of training data across each batch for classification,\n",
            "disease localization, report generation, and segmentation tasks to be 0.15/0.2/0.5/0.15. All the images are resized to a\n",
            "uniform size of 512x512 and subsequently adjusted by contrast and brightness. We selected the huge version of the\n",
            "OFA model as the pre-training model. We set a learning rate of 10−5, warm-up learning rate of 10−7, and dropout rate\n",
            "18LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "of 0.1, and train on eight V100 with batch size 256 for 30 epochs. We fine-tune all models using a learning rate of 10−4\n",
            "for all datasets with a batch size of 64.\n",
            "•Classification and Segmentation We have selected ConVIRT[ 31], GLoRIA[ 33], and BioVil[ 32] as the baseline\n",
            "models for the disease classification task. In all three models, ResNet-50 and BERT are chosen as the visual and text\n",
            "encoders, respectively. To perform direct inference of classification, we adopt the methods proposed in GLoRIA and\n",
            "BioVil, which transform the image classification task into a text-image matching task. Specifically, the test image is fed\n",
            "into the image encoder to generate image features. The test disease labels are subsequently formulated as text prompts\n",
            "and fed into the text encoder to generate text features. We then calculate the similarity between the image and text\n",
            "features. The prediction scores are set with normalized similarities. For ACC and F1, we utilize the validation dataset to\n",
            "determine the best score threshold for each class. Furthermore, we adhere to the official training strategies and train\n",
            "each model for 50 epochs during the fine-tuning process. Supplementary Table 1 shows the classification tasks achieve\n",
            "satisfactory results across all diseases. The results are comparable with KAD[54] and MedKLIP[30] claimed SOTA.\n",
            "•Disease Localization We employ TransVG, SeqTR, and VGTR as the baseline models for comparison with our\n",
            "OmniFM-DR. For TransVG[ 55], Resnet-50 is selected as the backbone. The BERT and ViT encoding length are 12 and\n",
            "6 separately, while the maximum query length is set to 20, following the authors’ recommendation. TransVG[ 55] has\n",
            "been trained on five datasets and all models are validated, while the most competitive on RefCOCOg is reported in\n",
            "Supplementary Table 2. For SeqTR[ 34], we follow the default settings of RefCOCOg, and DarkNet53 is selected as\n",
            "the detection backbone. The corresponding pre-calculated word embeddings are used to accommodate the pre-trained\n",
            "models. The authors have released three models on different datasets and training settings. We validate each model and\n",
            "the most competitive on RefCOCOg is reported in Supplementary Table 2. For VGTR[ 56], we followed the default\n",
            "settings as RefCOCOg and selected ResNet-50 and Bi-LSTM as the vision backbone and text encoder, respectively. For\n",
            "the evaluation of disease localization, the IoU threshold of TransVG, SeqTR, VGTR, and OmniFM-DR is set as 0.5\n",
            "consistently. The results of RefTR[57] and MedRPG[58] can not be reproduced due to the absence of code.\n",
            "•Report Generation We utilize Up-down[ 59], Att2in[ 60], and R2GenCMN[ 35] as the baseline models for report\n",
            "generation. Both Up-down and Att2in employ LSTM as the text encoder. Following their official implementation, Faster\n",
            "R-CNN and ResNet-101 are chosen as the image encoders for Up-down and Att2in, respectively. For R2GenCMN\n",
            "method, ResNet-101 serves as the image encoder, while a transformer-based module is utilized as the language model.\n",
            "We retrain the three models following their training procedures with our preprocessed dataset and evaluate them on the\n",
            "official test dataset. As for OmniFM-DR, leveraging its multi-task capability, we find it beneficial to incorporate disease\n",
            "attributes as prompts during both the training and inference stages. During training, we include extracted phrases from\n",
            "radiologist reports as additional prompts. During inference, we utilize phrases predicted by our model as supplementary\n",
            "prompts. Supplementary Table 3 shows that our report generation task achieved SOTA on clinical efficacy metrics and\n",
            "comparable results on natural language processing metrics. Supplementary Figure 1 provides more examples of multi-\n",
            "task results generated by our model. It can be found that the proposed model is capable of identifying Pneumothorax(a),\n",
            "Pneumonia (b), Edema (c), and Atelectasis(d) with a disease localization box, classification, and generated report.\n",
            "Take Supplementary Figure 1(b) for example, the generated report demonstrates the accurate pneumonia features and\n",
            "position described as \"increased opacification of the bilateral bases, right greater than left\", which are well consistent\n",
            "with the blue highlighted text in the golden standard report. The disease localization and classification results also agree\n",
            "with the gold standard. Furthermore, the generated report shows a stable cardiomediastinal contour which could be\n",
            "verified by the cardiothoracic ratio of 0.4 calculated by the segmentation task. Through the validation of multi-tasks,\n",
            "the explainability of the generated reports could be greatly enhanced.\n",
            "19LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Table 1: Comparison with other state-of-the-art methods of Disease classification task with direct\n",
            "inference setting on ChestXray14 dataset. The mean metrics (e.g., AUC and F1) refer to the macro average on the 14\n",
            "diseases.\n",
            "Metric Model\n",
            "Mean\n",
            "Atelectasis\n",
            "Cardiomegaly\n",
            "Effusion\n",
            "Infiltration\n",
            "Mass\n",
            "Nodule\n",
            "Pneumonia\n",
            "Pneumothorax\n",
            "Consolidation\n",
            "Edema\n",
            "Emphysema\n",
            "Fibrosis\n",
            "Pleural Thicken\n",
            "Hernia\n",
            "AUC ConVIRT[31] 56.0 45.9 43.3 64.6 65.4 60.1 58.0 64.0 53.3 64.6 69.2 43.1 48.2 54.5 49.4\n",
            "GLoRIA [33] 61.0 65.3 70.4 76.2 66.0 61.3 50.8 58.7 57.2 69.7 76.2 49.9 45.9 61.3 45.0\n",
            "BioViL[32] 66.2 51.7 68.8 74.3 60.1 66.3 63.9 66.9 68.3 65.0 79.5 65.6 63.2 63.7 69.8\n",
            "MedKLIP[30] 72.6 67.1 84.2 81.3 70.6 74.2 62.1 69.8 82.1 71.9 80.3 78.3 60.4 49.9 84.1\n",
            "Ours 73.6 74.5 76.1 78.8 60.4 72.3 65.5 69.6 81.6 71.9 79.1 72.6 64.5 72.1 90.9\n",
            "F1 ConVIRT[31] 13.5 0.1 0.2 36.7 43.6 15.7 14.2 6.0 20.5 17.7 12.1 8.3 3.4 9.7 0.7\n",
            "GLoRIA [33] 17.4 28.1 16.7 45.2 44.2 15.5 12.2 5.3 20.9 20.0 14.6 8.6 0.4 10.9 0.7\n",
            "BioViL[32] 19.2 23.5 20.9 43.8 41.4 17.8 16.4 6.7 27.4 17.7 18.7 12.3 5.6 11.9 4.5\n",
            "MedKLIP[30] 24.4 29.2 30.1 51.6 48.3 25.6 17.5 7.6 43.8 21.8 17.3 24.6 7.9 1.0 15.4\n",
            "Ours 26.3 36.8 21.9 48.8 41.1 29.5 20.6 9.7 42.8 21.9 17.4 16.4 8.9 17.8 35.1\n",
            "Supplementary Table 2: Comparison with other state-of-the-art methods of Disease localization task with 20-shot\n",
            "setting on MS-CXR and ChestXray14 dataset. The metrics (i.e. ACC and mIoU) refer to the macro average on the eight\n",
            "diseases.\n",
            "Dataset Metric Model Mean\n",
            "Atelectasis\n",
            "Cardiomegaly\n",
            "Effusion\n",
            "Pneumonia\n",
            "Pneumothorax\n",
            "Consolidation\n",
            "Edema\n",
            "Opacity\n",
            "Infiltrate\n",
            "Mass\n",
            "Nodule\n",
            "MS-CXR ACC VGTR[56] 30.1 40.0 91.8 5.6 40.0 13.8 28.3 10.0 11.1 - - -\n",
            "SeqTR[34] 43.7 31.3 93.5 6.25 41.6 51.3 41.7 34.4 50.0 - - -\n",
            "TransVG[55] 32.3 35.0 89.4 27.8 29.3 12.5 32.6 15.0 16.7 - - -\n",
            "Ours 54.7 50.0 90.6 45.0 51.3 53.7 47.8 45.0 54.4 - - -\n",
            "mIoU VGTR[56] 28.7 35.2 65.2 10.6 32.8 20.3 25.8 16.9 22.8 - - -\n",
            "SeqTR[34] 46.5 37.7 77.4 8.9 47.7 49.4 45.0 57.1 48.5 - - -\n",
            "TransVG[55] 29.3 29.8 61.9 23.2 26.6 18.7 27.4 19.9 26.6 - - -\n",
            "Ours 50.6 43.7 67.8 39.9 52.5 52.5 48.0 48.3 51.8 - - -\n",
            "ChestXray14 ACC VGTR[56] 35.6 14.8 100.0 29.4 54.8 4.2 - - - 40.0 23.8 17.7\n",
            "SeqTR[34] 47.7 50.3 82.9 62.5 29.6 56.3 - - - 55.0 38.8 6.3\n",
            "TransVG[55] 28.5 14.8 97.6 6.9 51.6 16.7 - - - 31.1 9.5 0.0\n",
            "Ours 61.5 51.8 97.6 67.0 61.3 55.8 - - - 62.2 61.9 34.7\n",
            "mIoU VGTR[56] 35.2 21.7 75.1 33.7 51.3 18.6 - - - 42.7 26.7 11.6\n",
            "SeqTR[34] 41.5 45.8 67.7 46.5 25.7 53.0 - - - 52.0 25.0 16.7\n",
            "TransVG[55] 28.7 23.3 72.8 22.5 37.6 22.5 - - - 32.1 15.9 3.3\n",
            "Ours 51.6 45.3 73.3 56.0 53.7 46.1 - - - 57.4 48.2 32.4\n",
            "20LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Table 3: Ablation experiment of multi-task and prompt capability. The quality of the generated report is\n",
            "evaluated by report, entity, and attribute levels, with the overall performance assessed by metrics (i.e., BL-4, METEOR,\n",
            "and Rouge-L), and the accuracy of the disease category evaluated by the CE metric (i.e., Precision, Recall, F1). The\n",
            "attribute metric focuses on the performance of disease severity and location described in the report.\n",
            "Report Entity Attribute\n",
            "BL-4 METEOR Rouge-L Precision Recall F1 ACC_S ACC_L\n",
            "Baseline Ours 10.97 14.02 26.48 43.22 31.31 33.29 18.34 8.17\n",
            "Task - LOC 10.85 14.06 26.49 44.96 31.06 33.01 - -\n",
            "- CLS 10.81 13.93 26.43 46.42 30.37 32.73 - -\n",
            "Prompt + Phrase 10.22 13.65 24.42 35.71 38.11 35.08 22.19 12.87\n",
            "+ Phrase-GT 11.42 14.33 26.99 71.47 44.82 49.55 30.18 23.57\n",
            "Supplementary Table 4: Diagnostic accuracy comparison with various report generation methods on MIMIC-CXR.\n",
            "Dataset Model BL-1 BL-4 METEOR Rouge-L Precision Recall F1\n",
            "MIMIC-CXR Up-down[59] 31.5 9.1 12.8 26.3 32.2 23.4 23.9\n",
            "Att2in[60] 33.1 9.7 13.6 27.5 32.5 23.6 25.7\n",
            "R2GenCMN[35] 35.6 10.4 14.7 28.1 33.3 28.4 28.6\n",
            "Constrastive[61] 35.0 10.9 15.1 28.3 35.2 29.8 30.3\n",
            "Ours 35.1 11.0 14.0 26.5 43.2 31.3 33.3\n",
            "21LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Figure 1: Typical cases of OmniFM-DR and ground truth for three tasks: multi-disease classification,\n",
            "visual grounding, and report generation. (a) Pneumothorax; (b) Pneumonia; (c) Edema; (d) Atelectasis. In the left Chest\n",
            "X-ray image, the BBOX with a green solid line denotes the ground truth, and the BBOX with a red-white dashed line\n",
            "represents the region detected by OmniFM-DR. In the right reports, the blue highlighted text represents the matched\n",
            "classified lesions compared to the ground truth report, and the yellow highlighted area represents the matched report\n",
            "describing other categories (e.g. cardiomegaly). CTR and PCR denote the Cardiothoracic Ratio and Pneumothorax\n",
            "Compress Ratio, respectively.\n",
            "22LEARNING A MULTI -TASK TRANSFORMER FOR CHEST RADIOGRAPH INTERPRETATION\n",
            "Supplementary Figure 2: Typical examples of instruction set for six disease labels: Pneumothorax, Atelectasis,\n",
            "Pneumonia, Pleural Effusion, Condilidation, and Opacity. The left panel indicates the multiple instruction sets utilized\n",
            "during the training and testing phase. In the Chest X-ray image, the red dash line BBOX denotes the region detected by\n",
            "OmniFM-DR.\n",
            "231\n",
            "CML-MOTS: Collaborative Multi-task Learning for\n",
            "Multi-Object Tracking and Segmentation\n",
            "Yiming Cui, Cheng Han, Dongfang Liu\n",
            "Abstract —The advancement of computer vision has pushed\n",
            "visual analysis tasks from still images to the video domain.\n",
            "In recent years, video instance segmentation, which aims to\n",
            "track and segment multiple objects in video frames, has drawn\n",
            "much attention for its potential applications in various emerging\n",
            "areas such as autonomous driving, intelligent transportation, and\n",
            "smart retail. In this paper, we propose an effective framework\n",
            "for instance-level visual analysis on video frames, which can\n",
            "simultaneously conduct object detection, instance segmentation,\n",
            "and multi-object tracking. The core idea of our method is collabo-\n",
            "rative multi-task learning which is achieved by a novel structure,\n",
            "named associative connections among detection, segmentation,\n",
            "and tracking task heads in an end-to-end learnable CNN. These\n",
            "additional connections allow information propagation across\n",
            "multiple related tasks, to benefit these tasks simultaneously. We\n",
            "evaluate the proposed method extensively on KITTI MOTS and\n",
            "MOTS Challenge datasets and obtain quite encouraging results.\n",
            "I. I NTRODUCTION\n",
            "In the past decade, the computer vision community has\n",
            "achieved significant progress in many tasks with the de-\n",
            "velopment of deep learning [1]–[4]. Among various visual\n",
            "tasks, instance segmentation [5] has drawn wide attention\n",
            "due to its importance in many emerging applications, such as\n",
            "autonomous driving [6]–[11], augmented reality [12], [13], and\n",
            "video captioning [14], [15]. Technically, it is quite challenging\n",
            "as it is a compound task consisting of both object detection\n",
            "and segmentation, each of which is a difficult task and has\n",
            "been studied for a long time.\n",
            "Compared to instance segmentation on images, multi-object\n",
            "tracking, and segmentation is much more challenging because\n",
            "it not only needs to perform instance-level segmentation on\n",
            "individual frames but also has to depict the coherence of each\n",
            "instance in consecutive video frames [16]. Due to these chal-\n",
            "lenges, multi-object tracking and segmentation have received\n",
            "much attention in recent years [17]–[22]. In general, multi-\n",
            "object tracking and segmentation contain object detection, seg-\n",
            "mentation, and tracking simultaneously in consecutive video\n",
            "frames. Compared to video object segmentation [23] that deals\n",
            "with object segmentation and tracking in videos, multi-object\n",
            "tracking and segmentation require additional object detection.\n",
            "It also has to generate object masks compared to video object\n",
            "detection which contains both object detection and tracking in\n",
            "videos [24]–[29].\n",
            "Currently, the state-of-the-art methods are mainly based on\n",
            "Mask R-CNN [5] while adding tracking sub-network. The\n",
            "Mask R-CNN family [19], [22] has demonstrated an appeal-\n",
            "ing performance on this task. However, these methods still\n",
            "Yiming Cui is with the University of Florida, Gainesville, FL 32611, USA.\n",
            "Cheng Han and Dongfang Liu are with the Rochester Institute of Technol-\n",
            "ogy, Rochester, New York, USA.have several drawbacks. For instance, TrackR-CNN [19] uses\n",
            "proposal-based ROI features, which are not fine enough for\n",
            "mask and tracking heads to produce accurate predictions. In\n",
            "detail, the current methods use the coarse proposal-based ROI\n",
            "features directly, which is not enough. Instead, our method\n",
            "processes the coarse proposal-based ROI features first and then\n",
            "uses the refined version for the downstream tasks. In addition,\n",
            "using proposal-based features not only has high computational\n",
            "complexity but also makes it prone to incorrect or redundant\n",
            "predictions. What is more, TrackR-CNN models object move-\n",
            "ments and scene consistency among video frames by using a\n",
            "3D convolutional operation, which is also parameter-heavy and\n",
            "computationally expensive. Although PointTrack [22] achieves\n",
            "a significant improvement in segmentation results by propos-\n",
            "ing a more powerful mask head, it is a multi-step architecture\n",
            "and so cannot be jointly optimized. In PointTrack, detection,\n",
            "and segmentation operations are applied to the input first. This\n",
            "partial model is optimized first without the tracking task. Then\n",
            "the detected and segmented objects are transformed into point\n",
            "cloud formats for the tracking task, which is optimized. In\n",
            "general, the whole model contains multiple steps and is not\n",
            "optimized jointly or end-to-end trainable. Moreover, its point-\n",
            "cloud strategy requires additional post-processing. Meanwhile,\n",
            "although these methods are trained by multiple learning ob-\n",
            "jectives corresponding to different tasks, the relations among\n",
            "these tasks have not been explored.\n",
            "Intuitively, object detection could benefit instance segmen-\n",
            "tation, and good object masks are also helpful for multi-\n",
            "object tracking. Inspired by this intuition, we propose a\n",
            "novel idea of collaborative multi-task learning for multi-object\n",
            "tracking and segmentation. Associative connections are added\n",
            "among different task heads to enable information propagation\n",
            "across them. Although our method is also based on Mask R-\n",
            "CNN [5], it fundamentally differs from other Mask R-CNN\n",
            "variants [19], [22] as our method exploits associative con-\n",
            "nections to facilitate accurate information propagation across\n",
            "the detection, segmentation, and tracking heads to benefit\n",
            "individual tasks. With associative connections, our segmen-\n",
            "tation and tracking can perform predictions on refined ROI\n",
            "features instead of using features pooled from the very coarse\n",
            "region proposals. Extensive experimental results demonstrate\n",
            "significant improvements over the existing state-of-the-art\n",
            "methods for multi-object tracking and segmentation on two\n",
            "benchmarks (KITTI MOTS [19] and MOTS Challenge [30]).\n",
            "The principal contributions of this work can be summarized\n",
            "as follows,\n",
            "•We propose a novel idea of collaborative multi-taskarXiv:2311.00987v1  [cs.CV]  2 Nov 20232\n",
            "Fig. 1. Illustration of the proposed framework. It adds associative connections among different task heads in TrackR-CNN [19] to enable collaboration among\n",
            "multiple tasks by facilitating information propagation across them. Particularly, the bounding boxes outputted by the detection head are linked to the mask\n",
            "head to extract object-aware features for instance segmentation. The outputs of the mask head are fused with the outputs of the detection head to obtain more\n",
            "reliable bounding boxes, which are then inputted into the tracking head for better tracking ability. Besides, a lightweight optical flow network is used to model\n",
            "the object movements across successive frames, which is used for aligning object features among these frames to obtain more powerful feature maps in the\n",
            "currently processed frame.\n",
            "learning for multiple object tracking and segmentation\n",
            "in videos.\n",
            "•We design associative connections among different tasks\n",
            "to enable information flow through different task heads,\n",
            "to simultaneously learn from multiple tasks and consider\n",
            "their intrinsic relations in the meantime.\n",
            "•Compared to existing methods, our method achieves\n",
            "better results on KITTI MOTS and MOTS Challenge\n",
            "benchmarks, especially on the consistency of multi-object\n",
            "tracking.\n",
            "II. R ELATED WORK\n",
            "Visual tasks in the video domain have been under-explored\n",
            "in the literature compared to image-level tasks. Particularly,\n",
            "video instance segmentation which includes detection, seg-\n",
            "mentation, and tracking is largely ignored due to its extreme\n",
            "difficulties. This section offers a review of recent works about\n",
            "several related tasks to video instance segmentation.\n",
            "A. Image Object Detection\n",
            "State-of-the-art object detection methods [25], [31]–[37] are\n",
            "generally based on deep CNNs for feature extraction and a\n",
            "shallow detection structure for detection prediction, including\n",
            "classification and bounding box regression. R-CNN [38] pro-\n",
            "posed a multi-stage pipeline to classify region proposals at\n",
            "different semantic levels for object detection. To speed up,\n",
            "Fast R-CNN [39] and Libra R-CNN [40] used ROI pooling\n",
            "on the feature maps which are shared on the entire image.\n",
            "As a representative work of the multi-stage detection family,\n",
            "Faster R-CNN [31] introduced a Region Proposal Network\n",
            "(RPN) to generate region proposals and then the proposal-\n",
            "based features are shared between classification and bounding\n",
            "box regression heads. R-FCN [41] replaced ROI poolingwith position-sensitivity ROI pooling to further improve the\n",
            "recognition accuracy while still facilitating feature sharing.\n",
            "Traditional two-stage object detectors, exemplified by the R-\n",
            "CNN family [31], [39], [42], rely on a plethora of predefined\n",
            "anchor boxes to designate initial object candidate locations.\n",
            "In contrast, one-stage methods [43], [44] were introduced to\n",
            "enhance the efficiency and inference speed of object detectors\n",
            "by forgoing the use of region proposals. Recently, query-\n",
            "based approaches [45]–[48] have emerged, replacing anchor\n",
            "boxes and region proposals with learned proposals or queries.\n",
            "DETR [45] adapts an encoder-decoder architecture based on\n",
            "transformers [49] to generate a sequence of prediction outputs.\n",
            "It introduces a set loss function to facilitate bipartite match-\n",
            "ing between predicted and ground-truth objects. Deformable-\n",
            "DETR [46] enhances the convergence of DETR by refining\n",
            "feature spatial resolutions. Sparse R-CNN [48] employs a fixed\n",
            "sparse set of learned object proposals to classify and localize\n",
            "objects in the image. It utilizes dynamic heads to generate final\n",
            "predictions directly, eliminating the need for post-processing\n",
            "techniques like non-maximum suppression.\n",
            "B. Image Instance Segmentation\n",
            "Instance segmentation not only predicts semantic classes\n",
            "on each pixel but also groups pixels into different object\n",
            "instances. Due to the effectiveness of R-CNN [38], many\n",
            "instance segmentation methods perform mask prediction on\n",
            "top of region proposals. Some early methods are based on\n",
            "the bottom-up segment strategy. For instance, the DeepMask\n",
            "family [17], [50] learns to segment proposal candidates first\n",
            "and then classify them using Fast R-CNN. These methods have\n",
            "segmentation precede detection, which is slow and inaccurate.\n",
            "Another strategy for instance segmentation is based on\n",
            "a parallel prediction of masks and class labels [5]. Li et3\n",
            "al. [51] proposed the fully convolutional instance segmentation\n",
            "by combining the segmentation proposal system in [50] and\n",
            "object detection system in R-FCN [41]. Basically, [50], [51]\n",
            "all use a set of position-sensitive output channels to simultane-\n",
            "ously predict object classes, bounding boxes, and masks, thus\n",
            "being fast and fundamentally similar. However, they struggle\n",
            "to deal with occlusion or truncation instances as they tend to\n",
            "create spurious edges in those cases.\n",
            "Besides two-stage methods, one-stage instance segmenta-\n",
            "tion frameworks like YOLACT [52], [53], SipMask [54], and\n",
            "SOLO [55], [56] have been introduced to strike a balance be-\n",
            "tween inference speed and accuracy. Recently, QueryInst [57]\n",
            "extended the query-based object detection method Sparse R-\n",
            "CNN [48] to the instance segmentation task by incorporating\n",
            "a dynamic mask head and parallel supervision. Nevertheless,\n",
            "all the two-stage and query-based methods mentioned earlier\n",
            "employ a fixed number of proposals, which may not be\n",
            "adaptable to images with varying objects or devices with\n",
            "different computational resource constraints.\n",
            "C. Object Tracking\n",
            "Tracking-by-detection is a popular strategy for multi-object\n",
            "tracking [58]–[60]. A common practice is to associate the\n",
            "tracks with detection based on their confidences [60]. To\n",
            "improve the tracking accuracy, Sun et al. [61] exploited\n",
            "multiple detectors by considering outputs from multiple over-\n",
            "detected detectors. However, this method enhances tracking\n",
            "performance at the cost of high computational complexity.\n",
            "More recently, Kim et al. [62] proposed to use a single object\n",
            "tracker based on a binary classifier for online multi-object\n",
            "tracking. Their proposed architecture has shared features for\n",
            "tracking and classification to speed up the process. Even\n",
            "though, [62] is still computationally expensive for real-time\n",
            "applications.\n",
            "Many previous methods deal with tracking tasks as a global\n",
            "optimization problem [17], [63]. This kind of method formu-\n",
            "lates temporal information from nearby frames to reduce noisy\n",
            "detection and handle ambiguities for object association. The\n",
            "principal strategy is to re-identify objects using the embedding\n",
            "vectors since each association vector represents the identity of\n",
            "an object [19], [63]. Inspired by the aforementioned methods,\n",
            "our work also leverages deeply learned ReID features. With the\n",
            "proposed associative connections, our work has better object-\n",
            "aware features to obtain improved identification performance.\n",
            "D. Video Object Detection\n",
            "Video object detection involves the identification and lo-\n",
            "calization of objects of interest in each frame, even in the\n",
            "presence of potential degradation due to rapid motion. Present\n",
            "approaches [27]–[29], [64]–[72] typically extend image-based\n",
            "object detectors into the realm of videos. These models\n",
            "fall into two categories: Post-processing-based and feature-\n",
            "aggregation-based.\n",
            "Post-processing-based models extend image object detectors\n",
            "to video by linking prediction results across frames based\n",
            "on temporal relationships [64]–[66]. Examples include T-\n",
            "CNN [64] and Seq-NMS [65]. T-CNN employs a CNN-based\n",
            "pipeline with straightforward object tracking for video object\n",
            "detection. Seq-NMS associates prediction results from eachframe using the IOU threshold. While these models outperform\n",
            "single-image object detectors, they heavily rely on individual\n",
            "frame detections and lack joint optimization. If single-frame\n",
            "outputs are erroneous, the post-processing pipeline cannot rec-\n",
            "tify them, resulting in suboptimal performance. Furthermore,\n",
            "these models tend to be slower as they process each frame\n",
            "independently before post-processing.\n",
            "In contrast, feature-aggregation-based and Transformer-\n",
            "based models can aggregate information across frames and\n",
            "jointly optimize predictions, yielding improved performance.\n",
            "These models effectively utilize temporal and spatial cues to\n",
            "track and detect objects across consecutive frames, making\n",
            "them better suited for video object detection tasks. While post-\n",
            "processing-based models offer some improvement over single-\n",
            "image object detectors, they are constrained by their reliance\n",
            "on individual frame results and slower inference. Feature-\n",
            "aggregation-based and Transformer-based models present a\n",
            "more promising approach, leveraging data from multiple\n",
            "frames and optimizing jointly.\n",
            "Feature-aggregation-based models enhance current frame\n",
            "representations by incorporating features from adjacent\n",
            "frames, assuming they can mitigate feature degradation. Sev-\n",
            "eral models embody this concept. For instance, FGFA [28]\n",
            "employs estimated optical flow to fuse neighboring features,\n",
            "while MANet combines pixel-level and instance-level object\n",
            "features. Conversely, SELSA [67] calibrates features based\n",
            "on semantic similarity rather than temporal relations. MEGA\n",
            "[70] integrates local and global temporal information to\n",
            "enhance performance. Although these models surpass post-\n",
            "processing-based ones in performance, they typically demand\n",
            "more computational resources, resulting in slower inference\n",
            "speeds. In summary, while feature-aggregation-based models\n",
            "have enhanced video object detection, they often come at\n",
            "the expense of slower inference. Transformer-based models\n",
            "offer a promising solution to this issue by efficiently fusing\n",
            "information across frames, suggesting further advancements in\n",
            "video object detection tasks.\n",
            "E. Video Instance Segmentation\n",
            "Recent endeavors on video instance segmentation [17], [19],\n",
            "[22] are intuitive extensions of Mask R-CNN [5] while con-\n",
            "sidering additional motion cues [73] or temporal consistency\n",
            "[74]–[76] in videos. There are three main categories of\n",
            "existing methods for Visual Instance Segmentation (VIS): two-\n",
            "stage, one-stage, and transformer-based. Two-stage approaches\n",
            "[16] build upon the Mask R-CNN family [5], [31], incorpo-\n",
            "rating an additional tracking branch for object association.\n",
            "One-stage methods [20], [77] employ anchor-free detectors\n",
            "[78], often utilizing linear mask basis combination [52] or\n",
            "conditional mask prediction generation [79]. Transformer-\n",
            "based models [80]–[83] introduce innovative adaptations of\n",
            "the transformer architecture for VIS tasks. VisTr [82] pioneers\n",
            "the application of transformers in VIS, and IFC [84] enhances\n",
            "efficiency through the use of memory tokens. Seqformer [85]\n",
            "introduces frame query decomposition, while Mask2Former\n",
            "[80] incorporates masked attention. VMT [86] extends the\n",
            "Mask Transfiner [87] to video for high-quality VIS, and IDOL\n",
            "[88] specializes in online VIS.4\n",
            "F . Multi-object Tracking and Segmentation\n",
            "Multi-object tracking (MOT) is a crucial task in autonomous\n",
            "driving, encompassing both object detection and tracking\n",
            "within video sequences. Numerous datasets have been cu-\n",
            "rated with a focus on driving scenarios, including KITTI\n",
            "tracking [89], MOTChallenge [30], UA-DETRAC [90], Path-\n",
            "Track [91], and PoseTrack [92]. However, none of these\n",
            "datasets offer segmentation masks for annotated objects,\n",
            "thus lacking pixel-level representations and intricate inter-\n",
            "actions seen in MOTS data. More advanced datasets, such\n",
            "as Cityscapes [93], ApolloScape [94], BDD100K [95], and\n",
            "KITTI MOTS dataset [19], do provide instance segmentation\n",
            "data for autonomous driving. Nevertheless, Cityscapes only\n",
            "supplies instance annotations for a small subset (i.e., 5,000\n",
            "images), and ApolloScape does not offer temporal object de-\n",
            "scriptions over time. Consequently, neither dataset is suitable\n",
            "for the joint training of MOTS algorithms. In contrast, KITTI\n",
            "MOTS [19] stands as the first public dataset that addresses\n",
            "the scarcity of data for the MOTS task, albeit with a relatively\n",
            "modest number of training samples. To date, BDD100K boasts\n",
            "the largest scale of data from intensive sequential frames,\n",
            "which may be considered redundant for training purposes. In\n",
            "comparison to the aforementioned datasets, our DGL-MOTS\n",
            "dataset encompasses a wider range of diverse data with finely\n",
            "detailed annotations.\n",
            "III. T HEPROPOSED METHOD\n",
            "A. Overview\n",
            "Fig. 1 shows the proposed network for multi-objects track-\n",
            "ing and segmentation. It follows the basic network architec-\n",
            "tures for object detection, instance segmentation, and object\n",
            "tracking, containing two parts: one for extracting feature maps\n",
            "and the other for different tasks sharing the extracted features\n",
            "as inputs. More specifically, the first part of our method\n",
            "consists of two components: a backbone feature extraction\n",
            "network Nfmto compute per frame feature maps and an\n",
            "optical flow network Nflto estimate object movements across\n",
            "nearby frames. With the help of optical flow, feature maps\n",
            "from previous frames can be warped into the current frame,\n",
            "which is used to combine with the extracted feature maps at\n",
            "the current frame, to obtain an enhanced feature representation\n",
            "of the current frame that should be more robust to image blur,\n",
            "occlusion, etc. Accordingly, the second part of our method\n",
            "is constituted of three heads, each of which corresponds to a\n",
            "specific task, i.e., object detection, instance segmentation, and\n",
            "multi-object tracking. The detection head contains classifica-\n",
            "tion head Ncland bounding-box regression head Nbbwhile\n",
            "the instance segmentation is achieved by the mask prediction\n",
            "head Nmk. The tracking head Ntraims to identify the same\n",
            "objects that appeared in multiple frames.\n",
            "To train such a network with different heads end-to-end,\n",
            "existing methods resort to multi-task learning that simultane-\n",
            "ously optimizes losses related to individual tasks. However,\n",
            "those methods ignore the intrinsic correlations among these\n",
            "tasks, which could benefit each other if used properly. In detail,\n",
            "each loss is only designed and optimized specifically for one\n",
            "task and there is no interaction between different tasks and\n",
            "their corresponding losses. In other words, the performanceof the instance segmentation task will not affect that of the\n",
            "object detection task and vice versa. In this case, we argue\n",
            "that the designs of losses can be optimized. Therefore, we\n",
            "propose collaborative multi-task learning to enable information\n",
            "propagation among these individual tasks when optimizing\n",
            "the total learning objective containing all these tasks. For\n",
            "this purpose, we introduce associative connections among\n",
            "detection, segmentation, and tracking heads so that the network\n",
            "training could be aware of the interactions among these three\n",
            "tasks. The introduced associative connections are shown by the\n",
            "red arrows in Fig. 1. Besides enabling information propagation\n",
            "among different task heads, our network is more efficient\n",
            "compared to previous methods. Existing MOTS methods [19],\n",
            "[96], [97] that also use three task heads encounter the problem\n",
            "of computing redundant feature representations, as they rely\n",
            "on proposal features for instance segmentation and tracking.\n",
            "With the help of associative connections, our segmentation\n",
            "and tracking heads only take the detected bounding boxes\n",
            "or masks for input, thus avoiding computing features on\n",
            "unrelated proposals. In addition, due to the added associative\n",
            "connections, the mask head and tracking head can use more\n",
            "accurate features extracted on object bounding boxes instead\n",
            "of the region proposals. Therefore, higher-quality instance\n",
            "segmentation and tracking results can be expected.\n",
            "In the following subsections, we describe in detail the\n",
            "proposed method, including feature extraction network, as-\n",
            "sociative connections, training, and inference, as well as the\n",
            "network architecture. Table I lists the main symbols used in\n",
            "this paper for the neatness of description.\n",
            "B. Network Architecture\n",
            "Our network architectures have general and flexible de-\n",
            "sign options. We craft state-of-the-art architectures into the\n",
            "proposed methods for different visual tasks. Particularly, the\n",
            "proposed method has three contributing modules: (i) the CNN\n",
            "backbone architecture employed for feature extraction over\n",
            "the input frame, (ii) the optical flow architecture used for\n",
            "motion estimation across frames, and (iii) the task heads\n",
            "for classification, location regression (bounding box), mask\n",
            "generation, and object tracking. With associative connections,\n",
            "the mask head and tracking head are applied to object-aware\n",
            "RoI instead of using RoI from proposals.\n",
            "1) Feature extraction: The ResNet-101 [98] is used as\n",
            "our backbone feature extraction network Nfmin this paper.\n",
            "According to the practice of using ResNet-101 as the backbone\n",
            "in Faster R-CNN, the outputs of its final convolutional layer\n",
            "C4are feedforwarded to the task heads.\n",
            "2) Motion estimation: The simple version of FlowNet [99]\n",
            "is used as the flow network Nflfor motion estimation across\n",
            "video frames. It is pretrained on the synthetic Flying Chairs\n",
            "dataset [99]. According to [24], [28], we have the input\n",
            "frame half-sized and the output stride 4. Therefore, the output\n",
            "resolution of the generated flow field is 1/8of the original\n",
            "frame size. Since the output of the feature extraction network\n",
            "has a stride of 16, we use bilinear interpolation to further\n",
            "down-sample the flow field and scale the field by half, to\n",
            "match the resolution of extracted feature maps. The down-\n",
            "sample process is non-learnable as the bilinear interpolation is5\n",
            "TABLE I\n",
            "SUMMARIZATION OF THE MAIN NOTATIONS USED IN THIS PAPER .\n",
            "Notation\n",
            "Nfm Feature extraction network\n",
            "Nfl Optical flow network\n",
            "Ncl Classification head\n",
            "Nbb Bounding box head\n",
            "Nmk Mask head\n",
            "Ntr Tracking head\n",
            "Ft Feature maps at time t\n",
            "F′\n",
            "t Enhanced feature maps at time t\n",
            "It Input frame at time t\n",
            "Ft−i→t Warped feature maps from time t−itot\n",
            "∆Dt−i→t Object movement between tandt−i\n",
            "ωk→t Weight for fusing feature of the kth frame at the time t\n",
            "biThe bounding box of the ithobject\n",
            "bi\n",
            "mkThe bounding box computed from the predicted mask of the ithobject\n",
            "bi\n",
            "wbWeighted bounding box of the ithobject connecting the detection and segmentation\n",
            "heads to the tracking head\n",
            "Lcls Loss for classification\n",
            "Lbox Loss for bounding box regression\n",
            "Lmask Loss for mask generation\n",
            "Ltrack Loss for tracking\n",
            "Ltotal Total loss for training\n",
            "a parameter-free layer in the network and is also differentiated\n",
            "during training.\n",
            "3) Task heads: For the task heads, we follow architec-\n",
            "tures presented in Faster R-CNN [31], Mask RCNN [5], and\n",
            "TrackR-CNN [19] for different tasks. For detection, we craft\n",
            "Faster R-CNN, a two-stage detector for object classification\n",
            "and bounding box regression. For mask prediction, we follow\n",
            "Mask RCNN by adding a fully convolutional mask prediction\n",
            "branch. For the tracking task, we include an association\n",
            "layer [19] to calculate the distance of 128-D identity vectors to\n",
            "track different objects across frames. Among these heads, there\n",
            "are associative connections to collect object-aware features\n",
            "and propagate them across these tasks. Introducing associative\n",
            "connections among other task heads to facilitate information\n",
            "flow interactively among several tasks is also the main con-\n",
            "tribution of this work. Compared to previous proposal-based\n",
            "features, this work uses object-aware features by leveraging\n",
            "associative connections, which shows better performance in\n",
            "predicting masks and tracking the same identities.\n",
            "C. Feature Extraction Guided by Optical Flow\n",
            "Given an input frame Itat time t, the feature extraction\n",
            "process can be expressed as Ft=Nfm(It). Note that Ft\n",
            "are only intermediate feature maps, which will be enhanced\n",
            "with features from previous frames based on the estimated\n",
            "object movements by Nfl. It is the enhanced feature maps\n",
            "being passed into the following heads regarding different tasks.\n",
            "Although various backbone networks can be used here for\n",
            "feature extraction, we use ResNet-101 [98] in this paper due\n",
            "to its popularity.\n",
            "To enhance the feature representation of different objects\n",
            "on the input frame by leveraging on its previous frames, we\n",
            "exploit the temporal visual cues with the help of an optical\n",
            "flow network Nfl. Thus, the enhanced feature representation\n",
            "is also called flow-guided features. The warped feature from\n",
            "timet−itotis denote as:\n",
            "Ft−i→t=WP(Ft−i,∆Dt−i→t), (1)where WP is the feature warping function to predict the\n",
            "feature maps at tbased on the feature maps at t−i(i.e.,\n",
            "Ft−i) and the estimated object movement ∆Dt−i→tbetween\n",
            "tandt−iframes. The movement is predicted by the optical\n",
            "flow network, i.e., ∆Dt−i→t=Nfl(It−i, It). By modeling\n",
            "the feature map movements from nearby frames, we hope to\n",
            "improve the extracted features that may have been originally\n",
            "compromised by motion blur, defocus, or occlusion that often\n",
            "happened in the video domain.\n",
            "Since in real scenarios processing a specific video frame can\n",
            "only rely on its previous frames, given an input frame It, we\n",
            "obtain a set of warped feature maps for each of its previous\n",
            "frames to compute the enhanced feature maps of the current\n",
            "frame It. Specifically, with a predefined temporal range n,\n",
            "each feature map of the previous frames in this range is warped\n",
            "into frame taccording to Eq. (1), resulting in a set of predicted\n",
            "feature maps {Ft−i→t|i∈[1, n]}. Note that we warp previous\n",
            "features to every current frame for feature fusion instead of\n",
            "keyframes or using dense aggregation [24], [28]. Due to the\n",
            "high efficiency of the used flow network (i.e., FlowNet [99]),\n",
            "this enables our method to extract strong features but still with\n",
            "affordable computational cost. Runtime analysis is provided in\n",
            "Section III-E2.\n",
            "Given the set of warped feature maps {Ft−i→t|i∈[1, n]},\n",
            "the fused feature maps F′\n",
            "tat the frame Itis then computed\n",
            "by the weighted average of these warped features and Ft,\n",
            "F′\n",
            "t=P\n",
            "k∈[t−n,t−1]\u0000\n",
            "ωk→t·F′\n",
            "k→t\u0001\n",
            "+Ft,(2)\n",
            "where the weight ωk→tis adaptively computed based on the\n",
            "similarity between Fk→tandFt. Among many choices for\n",
            "defining the similarity between these feature maps, we follow\n",
            "the implementation in [100] to use a shallow fully convo-\n",
            "lutional network to output embedding vectors for similarity6\n",
            "computation. That is, ωk→tis computed as,\n",
            "ωk→t=exp\u0012Fe\n",
            "k→t·Fe\n",
            "t\n",
            "|Fe\n",
            "k→t||Fe\n",
            "t|\u0013\n",
            ", (3)\n",
            "where Feis the embedding vector of Foutputted by the shal-\n",
            "low fully convolutional network. Note that the dot productions\n",
            "in Eq. (2) and Eq. (3) is element-wise multiplication and all the\n",
            "obtained weights are normalized so thatP\n",
            "k∈[t−1,t−n]ωk→t=\n",
            "1.\n",
            "Compared to the existing architectures like TrackR-\n",
            "CNN [19] which naively uses heavy 3D convolutions for\n",
            "feature extraction and fusion, our method models the spatial\n",
            "movements of objects in successive video frames in a more\n",
            "reasonable fashion to enhance the extracted features, could\n",
            "improve the accuracy for upper-stream tasks. With the help\n",
            "of a lightweight optical flow network (i.e., FlowNet [99])\n",
            "to predict the object movements, our method for feature\n",
            "extraction has much fewer parameters than 3D convolutions.\n",
            "Therefore, our method keeps a faster runtime than TrackR-\n",
            "CNN and its variants. In addition, by tuning the temporal range\n",
            "n, we can actively control the tradeoff between inferring speed\n",
            "and accuracy.\n",
            "D. Associative Connections Across Tasks\n",
            "After the above-mentioned flow-guided feature fusion, the\n",
            "obtained feature representation could be significantly en-\n",
            "hanced. These enhanced feature maps are then fed into the\n",
            "individual upper-stream task heads. These tasks include ob-\n",
            "ject detection, instance segmentation, and object tracking.\n",
            "Although these tasks are intrinsically related, existing meth-\n",
            "ods (e.g., TrackR-CNN [19] and CAMOT [101]) simply add\n",
            "different learning objectives together. This paper explores the\n",
            "relations among these tasks for improving multiple object\n",
            "tracking and segmentation accuracy. Under this motivation, we\n",
            "propose collaborative multi-task learning by adding associative\n",
            "connections among different tasks to facilitate information\n",
            "flow through different heads. Different from previous methods\n",
            "where different task heads are independent, the three heads in\n",
            "our method jointly worked together. To be concrete, as shown\n",
            "in Fig. 1, there are three associative connections across the\n",
            "detection, segmentation, and tracking heads. The first one is\n",
            "the associative connection between the output of bounding box\n",
            "regression in the detection head and the input of the mask\n",
            "prediction head. The second one is a connection to link the\n",
            "outputs of detection and mask heads. The third one is the\n",
            "associative connection between the combined outputs of detec-\n",
            "tion and mask heads and the input of the tracking head. With\n",
            "this implementation, we achieve much lower computational\n",
            "complexity per instance than the mask head in TrackR-CNN,\n",
            "where the ROI-based operations are repeatedly performed\n",
            "for final dense predictions. For comparison, the associative\n",
            "connections facilitate information sharing across different task\n",
            "heads to keep a low computational cost. Meanwhile, owing\n",
            "to the interleaved information flow among all three heads, we\n",
            "can jointly optimize their objectives as a whole, while previous\n",
            "works optimize each task’s objective independently since all\n",
            "the task heads are independent in their methods.1) Connection from Detection to Segmentation: Due to the\n",
            "good performance of Mask R-CNN [5] for instance segmen-\n",
            "tation, we adopt its head architecture for object detection and\n",
            "segmentation. The detection and instance segmentation are\n",
            "based on region proposals, and our network training procedure\n",
            "contains two stages Mask R-CNN and Faster R-CNN [31].\n",
            "In the first stage, a Region Proposal Network (RPN) takes\n",
            "the extracted features from the backbone and outputs a set of\n",
            "region proposals for object detection. In the second stage, for\n",
            "the detection head, an ROI pooling layer is used to generate\n",
            "region features for each proposal, which are then used to\n",
            "predict the object class and the related bounding box by the\n",
            "classification Ncland regression Nbbheads respectively. Then,\n",
            "for the mask head, an ROI pooling layer is used to generate\n",
            "object-aware features based on the bounding boxes outputted\n",
            "from the detection head. These object-aware features are used\n",
            "by the following layers to conduct instance segmentation for\n",
            "producing object masks.\n",
            "Adding a connection from the output of the detection head\n",
            "to the ROI pooling layer of the mask head (i.e., the red arrow\n",
            "between the bounding box and mask heads in Fig. 1) can\n",
            "explore more accurate features focused on objects compared\n",
            "to using the region proposals that have been widely used in the\n",
            "literature [19], [22]. This is because the output of the detection\n",
            "head is the object bounding boxes that are finer than the region\n",
            "proposals regarding the object locations. Therefore, our mask\n",
            "head can accurately fire on the pixels of instances. In other\n",
            "words, by adding an associative connection between the output\n",
            "of the detection head and the input of the mask head, we\n",
            "achieve a better behavior for mask generation since the used\n",
            "features could be more focused on the object itself.\n",
            "The learning objective of our detection head is identical to\n",
            "that of Faster R-CNN,\n",
            "Lcls+Lbox=1\n",
            "DX\n",
            "d∈DL′(pd, p∗\n",
            "d) +1\n",
            "DX\n",
            "d∈DL′′(td, t∗\n",
            "d),(4)\n",
            "where Dis the total number of detections in a video se-\n",
            "quence, pdis the predicted probability of an object, p∗\n",
            "dis\n",
            "the corresponding ground-truth label, tdis the coordinates\n",
            "of the predicted bounding box, t∗\n",
            "dis the ground-truth for the\n",
            "corresponding bounding box.\n",
            "Similarly, the instance segmentation head is trained using\n",
            "the following loss function,\n",
            "Lmask =1\n",
            "DX\n",
            "d∈DL′′′(md, m∗\n",
            "d), (5)\n",
            "where mdandm∗\n",
            "dare predicted mask and the ground-truth\n",
            "mask.\n",
            "Note that the novelty of our method lies in introducing the\n",
            "associative connections to interact among different tasks, not\n",
            "the individual learning objective for each task.\n",
            "2) Connection from Detection and Segmentation to Track-\n",
            "ing: The tracking head aims to establish correspondences of\n",
            "the same identities across successive frames. Existing works,\n",
            "such as TrackR-CNN, extensively extract ROI features for all\n",
            "region proposals, which are then used to compute identity\n",
            "vectors. Those identity vectors from positive proposals are\n",
            "used to link the same identities across frames based on their7\n",
            "similarities. Such a tracking head computes many redundant\n",
            "proposal features and identity vectors. In addition, extracting\n",
            "ROI features from region proposals may not be accurate\n",
            "enough to facilitate good tracking results. To address these\n",
            "issues, we propose to extract ROI features from the detection\n",
            "and instance segmentation results. This not only avoids the\n",
            "redundant feature computation but also provides a more accu-\n",
            "rate description of the tracked objects. Such an idea inspires\n",
            "the associative connection from the outputs of detection and\n",
            "segmentation heads to the input of the tracking head.\n",
            "Usually, the bounding boxes generated by the detection head\n",
            "are larger than the ground truth boxes, while the bounding\n",
            "boxes computed from the masks produced by the mask head\n",
            "are often tighter than the ground truth ones due to the pixel-\n",
            "level prediction of the mask head. To effectively combine two\n",
            "kinds of bounding boxes for a better ROI feature extraction in\n",
            "the tracking head, we propose an adaptive fusion strategy to\n",
            "obtain a weighted bounding box as,\n",
            "bi\n",
            "wb=α1·bi+α2·bi\n",
            "mk, (6)\n",
            "where biis the bounding box outputted by the detection\n",
            "head and bi\n",
            "mkis the bounding box drawn from the predicted\n",
            "mask for the ithobject, α1,α2are adaptive weights fulfilling\n",
            "α1+α2= 1. The parameters α1andα2are hyperparameters\n",
            "but they do not vary for each scenario right now. We will\n",
            "investigate how to adjust them for each video sequence in\n",
            "future work.\n",
            "In this way, the weighted bounding boxes bwbwill be\n",
            "smaller than the bounding boxes bgenerated by the detection\n",
            "head, while larger than the mask-based bounding boxes bmk. If\n",
            "the object is on a large scale, we expect the weighted bounding\n",
            "box to be tighter for tracking. However, if the object is small,\n",
            "the mask-based bounding box may not be accurate enough\n",
            "so we expect the weighted bounding box to approach the\n",
            "detected bounding box to include more information. Under\n",
            "this consideration, α1is set proportional to the reciprocal of\n",
            "the scale of the object, to be specific, the area of bounding box\n",
            "bi. Therefore, we have a small α1(α2is large) and bi\n",
            "mkhas\n",
            "a larger impact on the weighted bounding box bi\n",
            "wbwhen the\n",
            "object is large. On the contrary, biwill have a larger impact\n",
            "onbi\n",
            "wbwhen the object is small.\n",
            "Owing to the good properties of these adaptive bounding\n",
            "boxes, we add an associative connection to link these boxes\n",
            "into the tracking head for ROI feature extraction. In other\n",
            "words, we use bwbto pool feature maps to be more object-\n",
            "aware. Following the idea of [19], [63] that use the embedding\n",
            "vectors to re-identify persons, our tracking head uses one fully\n",
            "connected layer to map ROI features into identity vectors v,\n",
            "each of which indicates a unique identified instance. These\n",
            "identity vectors vare used to link all detections across frames.\n",
            "Since our ROI features are extracted from object-aware feature\n",
            "maps and could be more discriminative for tracking multiple\n",
            "objects, our tracking head can predict more distinguishable\n",
            "identity vectors.\n",
            "In the training stage, we minimize the distances of all\n",
            "vectors belonging to the same object while maximizing the\n",
            "distances of vectors belonging to different objects. To this end,\n",
            "the tracking head is trained with the batch hard triplet rankingloss. For each detected object, we sample its hard positive\n",
            "detections and negative detections for network training. Let\n",
            "us denote Das all the detections in a video. At frame, t, each\n",
            "detection d∈Dis associated with a tracking vector vdand a\n",
            "ground truth track IDwhich is used to determine its overlap\n",
            "with the ground truth over frames. Thus, for a video sequence\n",
            "including Tframes, the tracking loss is computed by,\n",
            "Ltrack =1\n",
            "DX\n",
            "d∈Dmax\u0012\n",
            "m+ max\n",
            "n∈D\n",
            "idd̸=idnS(vd, vn)\n",
            "−min\n",
            "p∈D\n",
            "idd=idpS(vd, vp),0\u0013\n",
            ",(7)\n",
            "where the subscript pandnindicate the positive and negative\n",
            "detections respectively and Srepresents the similarity between\n",
            "the input vectors. By default, we use cosine similarity.\n",
            "E. Inference and Training\n",
            "In this section, we first elaborate on the inference algorithm,\n",
            "then analyze the runtime complexity, and finally discuss the\n",
            "training objective of the proposed method.\n",
            "1) Inference algorithm: We summarize the inference of\n",
            "our proposed method in Algorithm 1. The algorithm of our\n",
            "proposed method can be divided into two major stages. In\n",
            "the first stage, given a video frame It, the feature extraction\n",
            "network Nfmtakes it as input and produces a set of interme-\n",
            "diate feature maps Ft(line 2 in Algorithm 1). Based on the\n",
            "light-weight optical flow network Nfl, we warp the temporal\n",
            "feature maps in previous frames to the current frame (line 3\n",
            "to 4) according to Eq. (1). In the meantime, we calculate the\n",
            "similarities of the warped feature maps to that of the current\n",
            "frame. These similarities are used as adaptive weights (line\n",
            "5) to perform flow-guided feature fusion over the temporally\n",
            "aligned features, to enhance the feature representation for\n",
            "the current frame. In this way, the enhanced feature maps\n",
            "F′\n",
            "t(lines 6 to 7) can be obtained for different head tasks.\n",
            "In the second stage, the enhanced feature maps F′\n",
            "tare fed\n",
            "into the classification and bounding box heads to predict\n",
            "object class {ct}and bounding box {bt}respectively (line\n",
            "8). Using the predicted bounding box from {bt}, we further\n",
            "obtain the object-aware feature maps F′\n",
            "(bb)twhich are more\n",
            "accurate than the proposal-based features that are widely used\n",
            "in Mask RCNN [5] and its variants [19], [22]. Next, the\n",
            "feature maps F′\n",
            "(bb)tare fed into the mask head Nmk(line\n",
            "11). Since the mask head Nmktakes the object-aware feature\n",
            "maps as inputs, it tends to generate masks (denoted as {mt})\n",
            "with higher quality compared to those masks predicted from\n",
            "proposal based features. We draw a close rectangular over\n",
            "each instance in {mt}to obtain a mask-based bounding box\n",
            "{b(mk)t}. Considering the predictions from both {bt}and\n",
            "{b(mk)t}together, we compute adaptive weights to get a\n",
            "weighted bounding box {b(wk)t}for each detected object (line\n",
            "12) which could be more accurate than the original bounding\n",
            "boxes {bt}produced by the bounding box head. Intuitively,\n",
            "more accurate bounding boxes can benefit following object\n",
            "tracking heads as they only focus on the major component of\n",
            "objects, reducing the influence of background. The final video\n",
            "recognition result Rtconsists of a set of object classes {ct},8\n",
            "Algorithm 1 Online inference of the proposed method per frame\n",
            "1:input : frame {It} ◁Video frame at time t\n",
            "2:Ft=Nfm\u0000\n",
            "It\u0001\n",
            "◁Produce feature maps\n",
            "3:forj=t−ntot−1do ◁Use temporal features\n",
            "4: Fj→t=WP\u0010\n",
            "Fj,Nfl\u0000\n",
            "Ij, It\u0001\u0011\n",
            "◁Feature warping to time t\n",
            "5: ωj→t=exp\u0010Fe\n",
            "j→t·Fe\n",
            "t\n",
            "|Fe\n",
            "j→t||Fe\n",
            "t|\u0011\n",
            "◁Calculate adaptive weights\n",
            "6:end for\n",
            "7:F′\n",
            "t=Pt−1\n",
            "j=t−n\u0000\n",
            "ωj→t·Fj→t\u0001\n",
            "+Ft◁Flow-guided feature fusion\n",
            "8:{ct}=Ncl\u0000\n",
            "F′\n",
            "t\u0001\n",
            "{bt}=Nbb\u0000\n",
            "F′\n",
            "t\u0001 ◁Object detection results\n",
            "9:F′\n",
            "(bb)t={bt} →F′\n",
            "t ◁bounding-box features maps\n",
            "10:{mt}=Nmk\u0000\n",
            "F′\n",
            "(bb)t\u0001\n",
            "◁Produce mask results\n",
            "11:b(mk)t← {mt} ◁Mask-based bounding box\n",
            "12:{b(wb)t}=P\n",
            "i∈Rnα1·bi\n",
            "t+α2·bi\n",
            "(mk)t◁Weighed bounding box\n",
            "13:F′\n",
            "(tb)t={b(wb)t} →F′\n",
            "t ◁Object-aware features maps\n",
            "14:{tt}=Ntr\u0000\n",
            "F′\n",
            "(tb)t\u0001\n",
            "◁Produce tracking results\n",
            "13:Rt=\u0010\n",
            "{bt},{ct},{mt},{tt}\u0011\n",
            "◁Final recognition at t\n",
            "15:output : Video recognition result Rt.\n",
            "bounding-box locations {bt}, object masks {mt}, and tracked\n",
            "identity labels {tt}.\n",
            "2) Time complexity: Based on Algorithm 1, we give an\n",
            "analysis of the time complexity of the proposed method. Be-\n",
            "sides the backbone feature extraction network Nfm, there are\n",
            "five modules: (1) The optical flow network Nflwhich includes\n",
            "the bilinear warping and feature embedding functions; (2) The\n",
            "classification head Ncl; (3) The bounding-box regression head\n",
            "Nbb; (4) The mask head Nmk; (5) The tracking head Ntr.\n",
            "Since our associative connections are parameters-free and only\n",
            "work as information flow which is fast, we exclude it in our\n",
            "analysis. Given a temporal range of nin flow-guided feature\n",
            "fusion, the optical flow network Nflloops ntimes per frame\n",
            "for feature warping. Accordingly, the runtime complexity for\n",
            "the proposed method is,\n",
            "Oours=O(Nfm) +n· O(Nfl) +O(Ncl) +O(Nbb)\n",
            "+O(Nmk) +O(Ntr)(8)\n",
            "We compare the time complexity of our method to its\n",
            "predecessor, TrackR-CNN [19] as both of them have similar\n",
            "architecture and TrackR-CNN is the state-of-the-art multi-\n",
            "object tracking and segmentation. With the same symbols,\n",
            "TrackR-CNN has the following runtime complexity,\n",
            "Otr=O(Nfm) +m· O(N3D) +O(Ncl)\n",
            "+O(Nbb) +O(Nmk) +O(Ntr),(9)\n",
            "where O\u0000\n",
            "N3D\u0001\n",
            "is the 3D convolutions and mis the feature\n",
            "fusion length used in TrackR-CNN for per frame feature\n",
            "extraction.\n",
            "Typically, the backbone feature extraction network has a\n",
            "heavier architecture than the task heads because it contains\n",
            "many more layers for feature abstraction. Therefore, it is rea-\n",
            "sonable to assume O\u0000\n",
            "Ncl\u0001\n",
            "≪ O\u0000\n",
            "Nfm\u0001\n",
            ",O\u0000\n",
            "Nbb\u0001\n",
            "≪ O\u0000\n",
            "Nfm\u0001\n",
            ",\n",
            "O\u0000\n",
            "Nmk\u0001\n",
            "≪ O\u0000\n",
            "Nfm\u0001\n",
            ",O\u0000\n",
            "Ntr\u0001\n",
            "≪ O\u0000\n",
            "Nfm\u0001\n",
            ", andO\u0000\n",
            "Nmk\u0001\n",
            "≪\n",
            "O\u0000\n",
            "Nfm\u0001\n",
            ". Given these, the ratio of runtime complexity of the\n",
            "proposed method to TrackR-CNN [19] can be computed as:\n",
            "C=Oours\n",
            "Otr≈O\u0000\n",
            "Nfm\u0001\n",
            "+n· O\u0000\n",
            "Nfl\u0001\n",
            "O\u0000\n",
            "Nfm\u0001\n",
            "+m· O\u0000\n",
            "N3D\u0001. (10)Compared to N3Dused in TrackR-CNN, we use Nfl\n",
            "which is more efficient while still being effective for object\n",
            "movement modeling. Assuming n=m,Cin Eq. (10) is less\n",
            "than 1. Therefore, our method is faster than TrackR-CNN.\n",
            "Experimental results in Table II also demonstrate that our\n",
            "method is two times faster than TrackR-CNN.\n",
            "3) Training objective: Since our method simultaneously\n",
            "considers object detection, segmentation, and tracking, its\n",
            "training objective contains multiple losses accordingly,\n",
            "Ltotal=Lcls+Lbox+Lmask +Ltrack, (11)\n",
            "where Lcls,Lbox,Lmask , and Ltrack denotes the loss for\n",
            "classification, bounding box regression, mask segmentation,\n",
            "and object tracking respectively. Following the implementation\n",
            "from Mask R-CNN [5], we modify the classification loss\n",
            "Lclsand bounding-box loss Lboxbased those from Faster\n",
            "R-CNN [31]. In detail, we add extra losses before and after\n",
            "the associative connections across tasks for full supervision.\n",
            "Therefore, both the predictions before and after the associative\n",
            "connections are optimized. As we use a set of m×mfor\n",
            "mask generation, the mask head has a cm2-dimensional output\n",
            "for each ROI, which encodes cbinary masks. To achieve this\n",
            "goal, we use a per-pixel sigmoid, and Lmask is defined as the\n",
            "average binary cross-entropy loss. For an ROI associated with\n",
            "ground-truth class k, we only define Lmask on the kthmask\n",
            "while ignoring other mask outputs that do not contribute to the\n",
            "loss. The details of Ltrack are discussed in Section III-D2.\n",
            "As the entire framework of the proposed method is multiple-\n",
            "stage, we rely on predictions from the classification head for\n",
            "the label of object detection, instance segmentation (mask),\n",
            "and tracking. Using the classification result for each ROI,\n",
            "we allow each task head to generate its results. Namely, the\n",
            "bounding-box head, the mask head, and the tracking head only\n",
            "need to focus on their specific task based on ROI without\n",
            "competition among classes. With the associative connections,\n",
            "critical ROI information can forward pass from the bounding\n",
            "box head to the mask head and eventually to the tracking head\n",
            "respectively. In the same vein, the ground truth for instance\n",
            "segmentation and object tracking can be backpropagated to\n",
            "each task head in the training procedure.\n",
            "IV. E XPERIMENTS\n",
            "This section first describes the datasets and evaluation\n",
            "metrics that we use to assess our method. The implementation\n",
            "details with training parameter settings are then elaborated.\n",
            "Next, We supply ablation studies to investigate the improve-\n",
            "ments of our method from a strong baseline and its variants.\n",
            "We also compare our method with the state-of-the-art methods\n",
            "on two benchmarks. Finally, we conclude with the accuracy\n",
            "and runtime evaluation.\n",
            "A. Datasets and Evaluation Metrics\n",
            "KITTI MOTS [19] and MOTS Challenge [30] are used\n",
            "to evaluate the effectiveness of the proposed method. KITTI\n",
            "MOTS has 21 video sequences of 8,008 frames, while\n",
            "MOTSChallenge has four video sequences of 2,862 frames.\n",
            "For KITTI MOTS, it includes 11,420 pedestrian and 26,899\n",
            "car instances. There are 26,894 pedestrian instances in the\n",
            "MOTS Challenge.9\n",
            "TABLE II\n",
            "COMPARISON WITH THE STATE -OF-THE-ART METHODS ON THE KITTI MOTS DATASET .\n",
            "Method Detect + SegmentSpeedFPSCars / Pedestrians\n",
            "(s) sMOTSA ↑ MOTSA ↑ MOTSP ↑ IDS↓\n",
            "CAMOT [101] TRCNN 0.76 1.32 67.4 / 39.5 78.6 / 57.6 86.5 / 73.1 220 / 131\n",
            "CIWT [102] TRCNN 0.28 3.57 68.1 / 42.9 79.4 / 61.0 86.7 / 75.7 106 / 42\n",
            "ReMOTS [21] TRCNN + BB2SegNet 3.33 / - 0.30 70.4 / - 84.4 /- - / - 231 / -\n",
            "TrackR-CNN [19] TRCNN 0.50 2.00 76.2 / 46.8 87.8 / 65.1 87.2 / 75.7 93 / 78\n",
            "BePix [103] RRC [104] + TRCNN 0.36 2.77 76.9 / - 89.7 / - 86.5 / - 88 / -\n",
            "Stem-Seg [105] TRCNN 0.32 3.12 72.7 / 50.4 83.8 / 66.1 87.2 / 77.7 76 / 14\n",
            "Ours TRCNN 0.27 3.70 76.7 / 47.9 88.2 / 65.3 88.5 /76.1 62/ 32\n",
            "Following TrackR-CNN [19], we use MOTSA to mea-\n",
            "sure the accuracy for multi-object tracking and segmentation,\n",
            "MOTSP to measure the precision of mask-based multi-object\n",
            "tracking and segmentation results, and sMOTSA to evaluate\n",
            "the soft multi-object tracking and segmentation accuracy. They\n",
            "are defined as follows,\n",
            "MOTSA =|TP| − |FP| − |IDS|\n",
            "|M|,\n",
            "MOTSP =|gTP|\n",
            "|TP|,\n",
            "sMOTSA =gTP− |FP| − |IDS|\n",
            "|M|,(12)\n",
            "where TP,gTP,FP, and IDS are true positive, soft true pos-\n",
            "itive, false positive, and tracking ID switch score respectively.\n",
            "We refer readers to [19] for more details about each notation.\n",
            "These metrics collectively measure the performance of a multi-\n",
            "object tracking and segmentation system by considering three\n",
            "tasks (i.e., object detection, instance segmentation, and multi-\n",
            "object tracking) together.\n",
            "B. Implementation Details\n",
            "The proposed method is implemented on a workstation\n",
            "with one NVIDIA RTX GPU. To have a fair comparison\n",
            "with existing methods, we follow the same experimental\n",
            "setup as in TrackR-CNN [19]. To be specific, the back-\n",
            "bone feature extraction network ResNet-101 [98] is pretrained\n",
            "on COCO [106] and Mapillary [107] datasets. The optical\n",
            "flow network FlowNet is pretrained on the Flying Chair\n",
            "dataset [99]. During the training process, the weights of\n",
            "ResNet-101 and FlowNet are fixed, and the other weights\n",
            "related to different task heads (i.e., Nbb,Ncl,NmkandNtr) are\n",
            "updated by learning on the target dataset, i.e.KITTI MOTS\n",
            "or MOTS Challenge. We train our model for 40 epochs with a\n",
            "learning rate of 5×10−7using the Adam [108] optimizer and\n",
            "mini-batch size of eight. The temporal range nin Eq. (2) is\n",
            "set to eight, i.e., eight adjacent frames are used for the flow-\n",
            "guided feature extraction.\n",
            "For the KITTI MOTS benchmark, there are 21 videos in\n",
            "total and we use 12 of them for training and the remaining for\n",
            "testing, following the practice in TrackR-CNN. We randomly\n",
            "keep some training data for validation and choose the best\n",
            "model on the validation set for testing. For the MOTS Chal-\n",
            "lenge benchmark, since there are only four video sequences\n",
            "in total, we use cross-validation to test the performance of\n",
            "different methods. To be specific, we leave one video sequence\n",
            "for evaluation and train the model on the three others on theMOTS Challenge. This process is repeated four times and the\n",
            "average result is reported.\n",
            "C. Comparison with the State-of-the-art Methods\n",
            "Results on KITTI MOTS. The results compared with the\n",
            "state-of-the-art methods on KITTI MOTS dataset are shown\n",
            "in Table II. For a fair comparison, we list the leading meth-\n",
            "ods with detection and segmentation architectures based on\n",
            "TrackR-CNN, which is the same as ours. The best result for\n",
            "each metric is highlighted in the table. The results of our\n",
            "method in Table II is quite encouraging. For car recognition,\n",
            "our method achieves the best result on MOTSP and IDS.\n",
            "Especially, our method improves IDS over other methods by\n",
            "a significantly large margin, which leads to the second-best\n",
            "method by 26. sMOTSA and MOTSA of our method are on par\n",
            "with the best model, a.k.a., BePix [103]. Regarding speed, our\n",
            "method is faster than BePix. The fast inference speed of our\n",
            "method is due to its flow-guided feature extraction in which a\n",
            "lightweight flow network is used. Finally, for pedestrian recog-\n",
            "nition, our method is better than all the compared methods on\n",
            "these metrics.\n",
            "To qualitatively demonstrate the improvements of our\n",
            "method, we visualize our results and compare them with\n",
            "TrackR-CNN. As shown in Fig. 2, our method is less prone to\n",
            "false predictions. In the left two columns of Fig. 2, TrackR-\n",
            "CNN in the top row produces a ”car” prediction on the\n",
            "red minivan while our method in the bottom row does not.\n",
            "A minivan is not a labeled object in the KITTI MOTS\n",
            "dataset. For the right two columns of Fig. 2, TrackR-CNN\n",
            "produces a false detection on traffic signs, predicting them as\n",
            "”pedestrian”. In these examples, our method performs accurate\n",
            "predictions for both scenarios. What is more, TrackR-CNN\n",
            "frequently encounters missing detection. For comparison, our\n",
            "method can constantly detect objects through video frames\n",
            "as demonstrated in Fig. 3. The improvement of our method\n",
            "over TrackR-CNN mainly stems from the reliable object-aware\n",
            "features, which are brought from our associative connections.\n",
            "With the help of these connections, our method produces bet-\n",
            "ter feature representations than TrackR-CNN before the task\n",
            "heads produce individual predictions, thus achieving better\n",
            "results.\n",
            "We also observe that our method has improved performance\n",
            "on tracking and mask quality. We argue that the proposed\n",
            "associative connections benefit the mask and tracking predic-\n",
            "tions as for the two tasks, we use refined ROI features for the\n",
            "final prediction. In contrast, TrackR-CNN produces predictions\n",
            "using coarse features based on proposals.10\n",
            "Fig. 2. Qualitative comparison between TrackR-CNN and our method regarding false prediction. The false predictions are marked by red bounding boxes.\n",
            "Minivan is not an object in the KITTI MOTS dataset. In the first two rows, TrackR-CNN has a false prediction on the minivan as a car class. In the same\n",
            "vein, TrackR-CNN has a false recognition of traffic signs and predicts them as ”pedestrian”. For both scenarios, our method works correctly without false\n",
            "predictions. The images are cropped for better visualization.\n",
            "Fig. 3. Comparison of TrackR-CNN and our results on missed recognition. The missed recognition is marked in red bounding boxes. Our method can\n",
            "constantly perform recognition on video frames while TrackR-CNN encounters recognition lost for the middle two frames. The demonstrated examples are\n",
            "cropped for better visualization.\n",
            "Results on MOTS Challenge. Table III reports the results\n",
            "on the MOTS Challenge dataset. Our method outperforms all\n",
            "the other methods for all metrics. The improvements over the\n",
            "second best method (TrackR-CNN), are 0.5 on sMOTSA, 0.2\n",
            "on MOTSA, and 0.7 on MOTSP respectively. Besides TrackR-\n",
            "CNN, our method achieves significantly better results than\n",
            "other methods even if they use a domain-finetuned Mask R-\n",
            "CNN.\n",
            "D. Ablation Study\n",
            "To demonstrate the effectiveness of our design, we con-\n",
            "duct ablation studies from TrackR-CNN since our method is\n",
            "proposed for the same purpose as TrackR-CNN and shares\n",
            "a similar architecture to it. Specifically, we first replaceTABLE III\n",
            "COMPARISON WITH THE STATE -OF-THE-ART METHODS ON MOTS\n",
            "CHALLENGE DATASET . +MG DENOTES MASK GENERATION WITH A\n",
            "DOMAIN FINETUNED MASK R-CNN.\n",
            "Method sMOTSA ↑ MOTSA ↑ MOTSP ↑\n",
            "MOTDT [109] + MG 47.8 61.1 80.0\n",
            "MHT-DAM [110] + MG 48.0 62.7 79.8\n",
            "jCC [111] + MG 48.3 63.0 79.9\n",
            "FWT [112] + MG 48.3 64.0 79.7\n",
            "TrackR-CNN [19] 52.7 66.9 80.2\n",
            "Ours 53.2 67.1 80.9\n",
            "the computationally expensive conv3d in TrackR-CNN with\n",
            "more efficient flow-guided feature extraction proposed in\n",
            "Section III-C, which leads to Method A. Then, we add an\n",
            "associative connection from the bounding box head to the11\n",
            "Fig. 4. Comparison of TrackR-CNN and our results on ID switch. The incorrect cases for tracking are marked in red bounding boxes. Our method can\n",
            "constantly perform accurate tracking of the same instances on video frames while TrackR-CNN frequently produces incorrect predictions. The demonstrated\n",
            "examples are cropped for better visualization.\n",
            "mask head as described in Section III-D1 and obtain Method\n",
            "B. By further adding an associative connection from the\n",
            "bounding-box head to track, this leads to Method C in which\n",
            "bounding box features are used for both segmentation and\n",
            "tracking, rather than the proposal based features. A variant\n",
            "of Method C is to connect the mask-based bounding box to\n",
            "the tracking head, which we call Method D. We also test two\n",
            "kinds of strategies to fuse bounding boxes and mask-based\n",
            "bounding boxes, which are then input to the tracking head.\n",
            "Method E uses fixed weight α= 0.5to fuse these two kinds\n",
            "of bounding boxes, while Method F is the proposed one that\n",
            "fuses them with adaptive weights.\n",
            "Table IV demonstrates the results of each method on the\n",
            "KITTI MOTS dataset. For reference, we also include the\n",
            "results of TrackR-CNN. By comparing A to TrackR-CNN,\n",
            "we can find that using flow-guided feature extraction leads to\n",
            "slightly worse accuracy, however, it leads to faster runtime as\n",
            "shown in Table II. Based on A, B obtains better results, prov-\n",
            "ing our associative connection that using predicted bounding\n",
            "boxes is better than region proposals in pooling features for\n",
            "the mask head. C and D further improve B, demonstrating the\n",
            "effectiveness of introducing an associative connection to the\n",
            "tracking head. If these two kinds of bounding boxes together,\n",
            "better results are obtained. More importantly, the proposed\n",
            "adaptive box fusion strategy (Method F) achieves significant\n",
            "improvement over the hard fusion (Method E), especially for\n",
            "the tracking consistency. These results show that the proposed\n",
            "method effectively explores the properties of these two kinds\n",
            "for multi-object tracking and segmentation.E. Fusion Length and Accuracy\n",
            "We use eight frames as the default temporal range for\n",
            "feature fusion based on the object movements across frames.\n",
            "To evaluate the impact of this fusion length on the accuracy,\n",
            "we change the number of input frames for our method and test\n",
            "our method on the KITTI MOTS dataset. Table V reports the\n",
            "results concerning the number of input frames. For both cars\n",
            "and pedestrians, the recognition accuracy increases when the\n",
            "temporal range increases from 2 to 8. After that, the perfor-\n",
            "mance starts to degrade. Such results are reasonable, because\n",
            "only a few frames may not accumulate enough information\n",
            "to enhance the feature of the current frame while too many\n",
            "frames are also damageable as the long-term movements esti-\n",
            "mated by the optical flow network are unstable. In addition, it\n",
            "is also worth pointing out that a larger temporal range requires\n",
            "more time to process feature extraction, thus will result in a\n",
            "lower speed. Therefore, we need to pay more attention to the\n",
            "temporal range in practice to achieve a desirable recognition\n",
            "accuracy as well as the runtime speed.\n",
            "V. C ONCLUSION\n",
            "In this work, we propose a novel algorithm capable of\n",
            "associatively detecting, segmenting, and tracking multiple ob-\n",
            "jects for video analysis. By adding associative connections\n",
            "across detection, segmentation, and tracking heads in an\n",
            "end-to-end learnable CNN, our method enables information\n",
            "propagation through different tasks, which could benefit all the\n",
            "considered tasks. Therefore, our method achieves state-of-the-\n",
            "art performance on various metrics regarding object detection,12\n",
            "TABLE IV\n",
            "ABLATION STUDY ON THE KITTI MOTS DATASET . PLEASE SEE THE TEXTS FOR DETAILS ABOUT METHODS ATOE,AND FIS THE PROPOSED METHOD .\n",
            "Method TrackR-CNN A B C D E F\n",
            "CarssMOTSA 76.2 75.6 76.0 76.1 76.5 76.3 76.5\n",
            "MOTSA 87.8 87.0 87.1 87.5 88.1 87.9 88.2\n",
            "MOTSP 87.2 86.9 87.1 87.4 88.4 88.3 88.5\n",
            "IDS 93 95 87 76 74 73 62\n",
            "PedestrianssMOTSA 46.8 45.2 45.5 46.1 46.1 46.2 47.9\n",
            "MOTSA 65.1 63.5 63.8 64.3 64.3 64.4 65.3\n",
            "MOTSP 75.7 75.0 75.1 75.8 75.9 76.0 76.1\n",
            "IDS 78 76 43 39 38 36 32\n",
            "TABLE V\n",
            "INFLUENCE OF THE TEMPORAL RANGE USED IN THE FLOW -GUIDED\n",
            "FEATURE FUSION . THE RESULTS ARE ON THE KITTI MOTS DATASET .\n",
            "temporal range n 2 4 8 12 16\n",
            "CarssMOTSA ↑ 75.2 75.7 76.7 75.4 75.3\n",
            "MOTSA ↑ 86.5 87.6 88.2 86.8 86.7\n",
            "MOTSP ↑ 87.3 87.4 88.5 87.8 87.2\n",
            "IDS↓ 87 75 62 73 78\n",
            "PedestrianssMOTSA ↑ 46.2 46.9 47.9 46.5 46.3\n",
            "MOTSA ↑ 63.5 64.3 65.3 63.7 63.6\n",
            "MOTSP ↑ 75.4 75.7 76.1 75.5 75.4\n",
            "IDS↓ 39 36 32 38 35\n",
            "instance segmentation, and multi-object tracking, according\n",
            "to our evaluation of two benchmarks. We also conducted\n",
            "extensive ablation studies to demonstrate the effectiveness of\n",
            "each associative connection.\n",
            "REFERENCES\n",
            "[1] J. Liu, M. Gong, and H. He, “Deep associative neural network for\n",
            "associative memory based on unsupervised representation learning,”\n",
            "Neural Networks , vol. 113, pp. 41 – 53, 2019.\n",
            "[2] H. Luo, Y . Yang, B. Tong, F. Wu, and B. Fan, “Traffic sign recognition\n",
            "using a multi-task convolutional neural network,” IEEE Transactions\n",
            "on Intelligent Transportation Systems , vol. 19, no. 4, pp. 1100–1111,\n",
            "2018.\n",
            "[3] J. Han, D. Zhang, X. Hu, L. Guo, J. Ren, and F. Wu, “Background\n",
            "prior-based salient object detection via deep reconstruction residual,”\n",
            "IEEE Transactions on Circuits and Systems for Video Technology ,\n",
            "vol. 25, no. 8, pp. 1309–1321, 2015.\n",
            "[4] Z. Huang, X. Xu, H. He, J. Tan, and Z. Sun, “Parameterized batch\n",
            "reinforcement learning for longitudinal control of autonomous land\n",
            "vehicles,” IEEE Transactions on Systems, Man, and Cybernetics:\n",
            "Systems , vol. 49, no. 4, pp. 730–741, 2019.\n",
            "[5] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask r-cnn,” in ICCV .\n",
            "Venice, Italy: IEEE, 2017, pp. 2961–2969.\n",
            "[6] D. Liu, Y . Cui, X. Guo, W. Ding, B. Yang, and Y . Chen, “Visual\n",
            "localization for autonomous driving: Mapping the accurate location\n",
            "in the city maze,” in 2020 25th International Conference on Pattern\n",
            "Recognition (ICPR) . IEEE, 2021, pp. 3170–3177.\n",
            "[7] L. Liu, Z. Dong, Y . Wang, and W. Shi, “Prophet: Realizing a predictable\n",
            "real-time perception pipeline for autonomous vehicles,” in 2022 IEEE\n",
            "Real-Time Systems Symposium (RTSS) . IEEE, 2022, pp. 305–317.\n",
            "[8] L. Yan, Y . Cui, Y . Chen, and D. Liu, “Hierarchical attention fusion\n",
            "for geo-localization,” in ICASSP 2021-2021 IEEE International Con-\n",
            "ference on Acoustics, Speech and Signal Processing (ICASSP) , IEEE.\n",
            "virtual: IEEE, 2021, pp. 2220–2224.\n",
            "[9] D. Liu, Y . Cui, L. Yan, C. Mousas, B. Yang, and Y . Chen, “Densernet:\n",
            "Weakly supervised visual localization using multi-scale feature aggre-\n",
            "gation,” in AAAI . Virtual: AAAI, 2021, pp. 6101–6109.\n",
            "[10] D. Liu, Y . Cui, Z. Cao, and Y . Chen, “Indoor navigation for mobile\n",
            "agents: A multimodal vision fusion model,” in 2020 International Joint\n",
            "Conference on Neural Networks (IJCNN) , IEEE. virtual: IEEE, 2020,\n",
            "pp. 1–8.\n",
            "[11] Y . Wang, D. Liu, H. Jeon, Z. Chu, and E. T. Matson, “End-to-end\n",
            "learning approach for autonomous driving: A convolutional neural\n",
            "network model.” in ICAART (2) . Prague, Czechia: IEEE, 2019, pp.\n",
            "833–839.\n",
            "[12] Z. Cao, Z. Chu, D. Liu, and Y . Chen, “A vector-based representation to\n",
            "enhance head pose estimation,” in Proceedings of the IEEE/CVF Winter\n",
            "Conference on Applications of Computer Vision . virtual: IEEE, 2021,\n",
            "pp. 1188–1197.[13] Z. Cao, D. Liu, Q. Wang, and Y . Chen, “Towards unbiased la-\n",
            "bel distribution learning for facial pose estimation using anisotropic\n",
            "spherical gaussian,” in Computer Vision–ECCV 2022: 17th European\n",
            "Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\n",
            "XII, Springer. Tel Aviv, Israel: Springer, 2022, pp. 737–753.\n",
            "[14] L. Yan, Q. Wang, Y . Cui, F. Feng, X. Quan, X. Zhang, and D. Liu,\n",
            "“Gl-rg: Global-local representation granularity for video captioning,”\n",
            "inInternational Joint Conference on Artificial Intelligence , 2022.\n",
            "[15] L. Yan, S. Ma, Q. Wang, Y . Chen, X. Zhang, A. Savakis, and\n",
            "D. Liu, “Video captioning using global-local representation,” IEEE\n",
            "Transactions on Circuits and Systems for Video Technology , vol. 32,\n",
            "no. 10, pp. 6642–6656, 2022.\n",
            "[16] G. Bertasius and L. Torresani, “Classifying, segmenting, and tracking\n",
            "object instances in video with mask propagation,” in CVPR . Virtual:\n",
            "IEEE, 2020, pp. 9739–9748.\n",
            "[17] L. Yang, Y . Fan, and N. Xu, “Video instance segmentation,” in ICCV .\n",
            "Seoul, Korea: IEEE, 2019, pp. 5188–5197.\n",
            "[18] Y . Cui, Z. Cao, Y . Xie, X. Jiang, F. Tao, Y . V . Chen, L. Li, and\n",
            "D. Liu, “Dg-labeler and dgl-mots dataset: Boost the autonomous\n",
            "driving perception,” in WACV . Waikoloa, HI, USA: IEEE, 2022, pp.\n",
            "58–67.\n",
            "[19] P. V oigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar,\n",
            "A. Geiger, and B. Leibe, “Mots: Multi-object tracking and segmenta-\n",
            "tion,” in CVPR . Long Beach, CA, USA: IEEE, 2019, pp. 7942–7951.\n",
            "[20] D. Liu, Y . Cui, W. Tan, and Y . Chen, “Sg-net: Spatial granularity\n",
            "network for one-stage video instance segmentation,” in CVPR . Virtual:\n",
            "IEEE, 2021, pp. 9816–9825.\n",
            "[21] F. Yang, X. Chang, C. Dang, Z. Zheng, S. Sakti, S. Nakamura, and\n",
            "Y . Wu, “Remots: Self-supervised refining multi-object tracking and\n",
            "segmentation,” arXiv e-prints , pp. arXiv–2007, 2020.\n",
            "[22] Z. Xu, W. Zhang, X. Tan, W. Yang, X. Su, Y . Yuan, H. Zhang, S. Wen,\n",
            "E. Ding, and L. Huang, “Pointtrack++ for effective online multi-object\n",
            "tracking and segmentation,” arXiv preprint arXiv:2007.01549 , 2020.\n",
            "[23] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. Torr, “Fast online\n",
            "object tracking and segmentation: A unifying approach,” in CVPR .\n",
            "Long Beach, CA, USA: IEEE, 2019, pp. 1328–1338.\n",
            "[24] X. Zhu, Y . Xiong, J. Dai, L. Yuan, and Y . Wei, “Deep feature flow for\n",
            "video recognition,” in CVPR . Honolulu, Hawaii, USA: IEEE, 2017,\n",
            "pp. 2349–2358.\n",
            "[25] Y . Cui, L. Yang, and D. Liu, “Dynamic proposals for efficient object\n",
            "detection,” arXiv preprint arXiv:2207.05252 , 2022.\n",
            "[26] G. Wang, Z. Qin, S. Wang, H. Sun, Z. Dong, and D. Zhang, “To-\n",
            "wards accessible shared autonomous electric mobility with dynamic\n",
            "deadlines,” IEEE Transactions on Mobile Computing , 2022.\n",
            "[27] Y . Cui, “Dfa: Dynamic feature aggregation for efficient video object\n",
            "detection,” arXiv preprint arXiv:2210.00588 , 2022.\n",
            "[28] X. Zhu, Y . Wang, J. Dai, L. Yuan, and Y . Wei, “Flow-guided feature\n",
            "aggregation for video object detection,” in ICCV . Venice, Italy: IEEE,\n",
            "2017, pp. 408–417.\n",
            "[29] Y . Cui, L. Yan, Z. Cao, and D. Liu, “Tf-blender: Temporal feature\n",
            "blender for video object detection,” in Proceedings of the IEEE/CVF\n",
            "International Conference on Computer Vision , 2021, pp. 8138–8147.\n",
            "[30] A. Milan, L. Leal-Taix ´e, I. Reid, S. Roth, and K. Schindler,\n",
            "“Mot16: A benchmark for multi-object tracking,” arXiv preprint\n",
            "arXiv:1603.00831 , 2016.\n",
            "[31] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\n",
            "time object detection with region proposal networks,” in NeurIPS .\n",
            "Montreal, Canada: MIT Press, 2015, pp. 91–99.\n",
            "[32] Z.-Q. Zhao, P. Zheng, S.-T. Xu, and X. Wu, “Object detection with\n",
            "deep learning: A review,” IEEE Transactions on Neural Networks and\n",
            "Learning Systems , vol. 30, no. 11, pp. 3212–3232, 2019.13\n",
            "[33] Y . Cui, “Dynamic feature aggregation for efficient video object detec-\n",
            "tion,” in Proceedings of the Asian Conference on Computer Vision ,\n",
            "2022, pp. 944–960.\n",
            "[34] D. Liu, Y . Cui, Z. Cao, and Y . Chen, “A large-scale simulation dataset:\n",
            "Boost the detection accuracy for special weather conditions,” in 2020\n",
            "International Joint Conference on Neural Networks (IJCNN) . IEEE,\n",
            "2020, pp. 1–8.\n",
            "[35] L. Yang, C. Yiming, and D. Liu, “Techniques for using dynamic\n",
            "proposals in object detection,” Jul. 27 2023, uS Patent App. 17/581,423.\n",
            "[36] X. Zhang, Y . Cui, Y . Wang, M. Sun, and H. Hu, “An improved ae\n",
            "detection method of rail defect based on multi-level anc with vss-lms,”\n",
            "Mechanical Systems and Signal Processing , vol. 99, pp. 420–433, 2018.\n",
            "[37] Z. Dong, Y . Lu, G. Tong, Y . Shu, S. Wang, and W. Shi, “Watchdog:\n",
            "Real-time vehicle tracking on geo-distributed edge nodes,” ACM Trans-\n",
            "actions on Internet of Things , vol. 4, no. 1, pp. 1–23, 2023.\n",
            "[38] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\n",
            "hierarchies for accurate object detection and semantic segmentation,”\n",
            "inCVPR . IEEE, 2014, pp. 580–587.\n",
            "[39] R. Girshick, “Fast r-cnn,” in ICCV . Santiago, Chile: IEEE, 2015, pp.\n",
            "1440–1448.\n",
            "[40] J. Pang, K. Chen, J. Shi, H. Feng, W. Ouyang, and D. Lin, “Libra r-\n",
            "cnn: Towards balanced learning for object detection,” in CVPR . Long\n",
            "Beach, CA, USA: IEEE, 2019, pp. 821–830.\n",
            "[41] J. Dai, Y . Li, K. He, and J. Sun, “R-fcn: Object detection via region-\n",
            "based fully convolutional networks,” in NeurIPS . Barcelona, Spain:\n",
            "MIT Press, 2016, pp. 379–387.\n",
            "[42] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\n",
            "object detection with region proposal networks,” IEEE Transactions on\n",
            "Pattern Analysis and Machine Intelligence , Jun 2017.\n",
            "[43] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\n",
            "once: Unified, real-time object detection,” in Proceedings of the IEEE\n",
            "conference on computer vision and pattern recognition , 2016, pp. 779–\n",
            "788.\n",
            "[44] A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao, “Yolov4: Op-\n",
            "timal speed and accuracy of object detection,” arXiv preprint\n",
            "arXiv:2004.10934 , 2020.\n",
            "[45] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\n",
            "S. Zagoruyko, “End-to-end object detection with transformers,” in\n",
            "ECCV . Glasgow, UK: Springer, 2020, pp. 213–229.\n",
            "[46] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable\n",
            "detr: Deformable transformers for end-to-end object detection,” in\n",
            "ICLR , 2021. [Online]. Available: https://openreview.net/forum?id=\n",
            "gZ9hCDWe6ke\n",
            "[47] Y . Cui, X. Liu, H. Liu, J. Zhang, A. Zare, and B. Fan, “Geometric\n",
            "attentional dynamic graph convolutional neural networks for point\n",
            "cloud analysis,” Neurocomputing , vol. 432, pp. 300–310, 2021.\n",
            "[48] P. Sun, R. Zhang, Y . Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka,\n",
            "L. Li, Z. Yuan, C. Wang et al. , “Sparse r-cnn: End-to-end object\n",
            "detection with learnable proposals,” in CVPR . Virtual: IEEE, 2021,\n",
            "pp. 14 454–14 463.\n",
            "[49] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\n",
            "Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\n",
            "NeurIPS , vol. 30, 2017.\n",
            "[50] K. Chen, J. Pang, J. Wang, Y . Xiong, X. Li, S. Sun, W. Feng,\n",
            "Z. Liu, J. Shi, W. Ouyang et al. , “Hybrid task cascade for instance\n",
            "segmentation,” in CVPR . Long Beach, CA, USA: IEEE, 2019, pp.\n",
            "4974–4983.\n",
            "[51] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, “Fully convolutional instance-\n",
            "aware semantic segmentation,” in CVPR . Honolulu, Hawaii, USA:\n",
            "IEEE, 2017, pp. 2359–2367.\n",
            "[52] D. Bolya, C. Zhou, F. Xiao, and Y . J. Lee, “Yolact: Real-time instance\n",
            "segmentation,” in ICCV . Seoul, Korea: IEEE, 2019, pp. 9157–9166.\n",
            "[53] ——, “Yolact++: Better real-time instance segmentation,” IEEE Trans-\n",
            "actions on Pattern Analysis and Machine Intelligence , 2020.\n",
            "[54] J. Cao, R. M. Anwer, H. Cholakkal, F. S. Khan, Y . Pang, and L. Shao,\n",
            "“Sipmask: Spatial information preservation for fast image and video\n",
            "instance segmentation,” in ECCV . Glasgow, UK: Springer, 2020, pp.\n",
            "1–18.\n",
            "[55] X. Wang, T. Kong, C. Shen, Y . Jiang, and L. Li, “SOLO: Segmenting\n",
            "objects by locations,” in ECCV . Glasgow, UK: Springer, 2020, pp.\n",
            "649–665.\n",
            "[56] X. Wang, R. Zhang, T. Kong, L. Li, and C. Shen, “Solov2: Dynamic\n",
            "and fast instance segmentation,” in NeurIPS . virtual: MIT Press, 2020,\n",
            "pp. 17 721–17 732.\n",
            "[57] Y . Fang, S. Yang, X. Wang, Y . Li, C. Fang, Y . Shan, B. Feng, and\n",
            "W. Liu, “Instances as queries,” in ICCV . Virtual: IEEE, 2021, pp.\n",
            "6910–6919.[58] X. Liu, D. Tao, M. Song, L. Zhang, J. Bu, and C. Chen, “Learning\n",
            "to track multiple targets,” IEEE Transactions on Neural Networks and\n",
            "Learning Systems , vol. 26, no. 5, pp. 1060–1073, 2015.\n",
            "[59] X. Wang, B. Fan, S. Chang, Z. Wang, X. Liu, D. Tao, and T. S.\n",
            "Huang, “Greedy batch-based minimum-cost flows for tracking multiple\n",
            "objects,” IEEE Transactions on Image Processing , vol. 26, no. 10, pp.\n",
            "4765–4776, 2017.\n",
            "[60] P. Chu and H. Ling, “Famnet: Joint learning of feature, affinity and\n",
            "multi-dimensional assignment for online multiple object tracking,” in\n",
            "ICCV . Long Beach, CA, USA: IEEE, 2019, pp. 6172–6181.\n",
            "[61] S. Sun, N. Akhtar, H. Song, A. S. Mian, and M. Shah, “Deep affinity\n",
            "network for multiple object tracking,” IEEE Transactions on Pattern\n",
            "Analysis and Machine Intelligence , 2019.\n",
            "[62] C. Kim, F. Li, and J. M. Rehg, “Multi-object tracking with neural\n",
            "gating using bilinear lstm,” in ECCV . Springer, 2018, pp. 200–215.\n",
            "[63] S. Tang, M. Andriluka, B. Andres, and B. Schiele, “Multiple people\n",
            "tracking by lifted multicut and person re-identification,” in CVPR .\n",
            "Honolulu, Hawaii, USA: IEEE, 2017, pp. 3539–3548.\n",
            "[64] K. Kang, W. Ouyang, H. Li, and X. Wang, “Object detection from\n",
            "video tubelets with convolutional neural networks,” in CVPR . Las\n",
            "Vegas, Nevada, USA: IEEE, 2016, pp. 817–825.\n",
            "[65] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh,\n",
            "H. Shi, J. Li, S. Yan, and T. S. Huang, “Seq-nms for video object\n",
            "detection,” arXiv preprint arXiv:1602.08465 , 2016.\n",
            "[66] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang,\n",
            "R. Wang, X. Wang et al. , “T-cnn: Tubelets with convolutional neural\n",
            "networks for object detection from videos,” IEEE Transactions on\n",
            "Circuits and Systems for Video Technology , vol. 28, no. 10, pp. 2896–\n",
            "2907, 2017.\n",
            "[67] H. Wu, Y . Chen, N. Wang, and Z. Zhang, “Sequence level semantics ag-\n",
            "gregation for video object detection,” in Proceedings of the IEEE/CVF\n",
            "International Conference on Computer Vision , 2019, pp. 9217–9225.\n",
            "[68] Y . Cui, L. Yang, and H. Yu, “Dq-det: Learning dynamic query\n",
            "combinations for transformer-based object detection and segmentation,”\n",
            "arXiv preprint arXiv:2307.12239 , 2023.\n",
            "[69] X. Zhu, J. Dai, L. Yuan, and Y . Wei, “Towards high performance video\n",
            "object detection,” in Proceedings of the IEEE Conference on Computer\n",
            "Vision and Pattern Recognition , 2018, pp. 7210–7218.\n",
            "[70] Y . Chen, Y . Cao, H. Hu, and L. Wang, “Memory enhanced global-local\n",
            "aggregation for video object detection,” in CVPR . Virtual: IEEE, 2020,\n",
            "pp. 10 337–10 346.\n",
            "[71] S. Wang, Y . Zhou, J. Yan, and Z. Deng, “Fully motion-aware network\n",
            "for video object detection,” in Proceedings of the European conference\n",
            "on computer vision (ECCV) , 2018, pp. 542–557.\n",
            "[72] Y . Cui, “Feature aggregated queries for transformer-based video object\n",
            "detectors,” in Proceedings of the IEEE/CVF Conference on Computer\n",
            "Vision and Pattern Recognition , 2023, pp. 6365–6376.\n",
            "[73] J. Cheng, Y .-H. Tsai, S. Wang, and M.-H. Yang, “Segflow: Joint\n",
            "learning for video object segmentation and optical flow,” in ICCV .\n",
            "Honolulu, Hawaii, USA: IEEE, 2017, pp. 686–695.\n",
            "[74] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and A. Sorkine-\n",
            "Hornung, “Learning video object segmentation from static images,”\n",
            "inCVPR . Honolulu, Hawaii, USA: IEEE, 2017, pp. 2663–2672.\n",
            "[75] B. B. Elallid, S. E. Hamdani, N. Benamar, and N. Mrani, “Deep\n",
            "learning-based modeling of pedestrian perception and decision-making\n",
            "in refuge island for autonomous driving,” in Computational Intelligence\n",
            "in Recent Communication Networks . Springer, 2022, pp. 135–146.\n",
            "[76] L. Yang, Y . Wang, X. Xiong, J. Yang, and A. K. Katsaggelos, “Efficient\n",
            "video object segmentation via network modulation,” in CVPR . Salt\n",
            "Lake City, UT, USA: IEEE, 2018, pp. 6499–6507.\n",
            "[77] J. Cao, R. M. Anwer, H. Cholakkal, F. S. Khan, Y . Pang, and L. Shao,\n",
            "“Sipmask: Spatial information preservation for fast image and video\n",
            "instance segmentation,” arXiv preprint arXiv:2007.14772 , 2020.\n",
            "[78] Z. Tian, C. Shen, H. Chen, and T. He, “FCOS: Fully convolutional\n",
            "one-stage object detection,” in ICCV . Seoul, Korea: IEEE, 2019, pp.\n",
            "9627–9636.\n",
            "[79] Z. Tian, C. Shen, and H. Chen, “Conditional convolutions for instance\n",
            "segmentation,” in ECCV . Glasgow, UK: Springer, 2020.\n",
            "[80] B. Cheng, A. Choudhuri, I. Misra, A. Kirillov, R. Girdhar, and\n",
            "A. G. Schwing, “Mask2former for video instance segmentation,” arXiv\n",
            "preprint arXiv:2112.10764 , 2021.\n",
            "[81] M. Heo, S. Hwang, S. W. Oh, J.-Y . Lee, and S. J. Kim, “Vita:\n",
            "Video instance segmentation via object token association,” Advances\n",
            "in Neural Information Processing Systems , vol. 35, pp. 23 109–23 120,\n",
            "2022.14\n",
            "[82] Y . Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia,\n",
            "“End-to-end video instance segmentation with transformers,” in CVPR .\n",
            "Virtual: IEEE, 2021, pp. 8741–8750.\n",
            "[83] S. Yang, X. Wang, Y . Li, Y . Fang, J. Fang, W. Liu, X. Zhao, and\n",
            "Y . Shan, “Temporally efficient vision transformer for video instance\n",
            "segmentation,” in Proceedings of the IEEE/CVF Conference on Com-\n",
            "puter Vision and Pattern Recognition , 2022, pp. 2885–2895.\n",
            "[84] S. Hwang, M. Heo, S. W. Oh, and S. J. Kim, “Video instance seg-\n",
            "mentation using inter-frame communication transformers,” Advances\n",
            "in Neural Information Processing Systems , vol. 34, pp. 13 352–13 363,\n",
            "2021.\n",
            "[85] J. Wu, Y . Jiang, S. Bai, W. Zhang, and X. Bai, “Seqformer: Sequential\n",
            "transformer for video instance segmentation,” in European Conference\n",
            "on Computer Vision . Springer, 2022, pp. 553–569.\n",
            "[86] L. Ke, H. Ding, M. Danelljan, Y .-W. Tai, C.-K. Tang, and F. Yu,\n",
            "“Video mask transfiner for high-quality video instance segmentation,”\n",
            "inEuropean Conference on Computer Vision . Springer, 2022, pp.\n",
            "731–747.\n",
            "[87] L. Ke, M. Danelljan, X. Li, Y .-W. Tai, C.-K. Tang, and F. Yu, “Mask\n",
            "transfiner for high-quality instance segmentation,” in Proceedings of the\n",
            "IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n",
            "2022, pp. 4412–4421.\n",
            "[88] J. Wu, Q. Liu, Y . Jiang, S. Bai, A. Yuille, and X. Bai, “In defense of on-\n",
            "line models for video instance segmentation,” in European Conference\n",
            "on Computer Vision . Springer, 2022, pp. 588–605.\n",
            "[89] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\n",
            "driving? the kitti vision benchmark suite,” in 2012 IEEE Conference\n",
            "on Computer Vision and Pattern Recognition . IEEE, 2012, pp. 3354–\n",
            "3361.\n",
            "[90] L. Wen, D. Du, Z. Cai, Z. Lei, M.-C. Chang, H. Qi, J. Lim, M.-H.\n",
            "Yang, and S. Lyu, “Ua-detrac: A new benchmark and protocol for\n",
            "multi-object detection and tracking,” arXiv preprint arXiv:1511.04136 ,\n",
            "2015.\n",
            "[91] S. Manen, M. Gygli, D. Dai, and L. Van Gool, “Pathtrack: Fast\n",
            "trajectory annotation with path supervision,” in Proceedings of the\n",
            "IEEE International Conference on Computer Vision , 2017, pp. 290–\n",
            "299.\n",
            "[92] M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan,\n",
            "J. Gall, and B. Schiele, “Posetrack: A benchmark for human pose\n",
            "estimation and tracking,” in Proceedings of the IEEE Conference on\n",
            "Computer Vision and Pattern Recognition , 2018, pp. 5167–5176.\n",
            "[93] M. Cordts, M. Omran, S. Ramos, T. Scharw ¨achter, M. Enzweiler,\n",
            "R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes\n",
            "dataset,” in CVPR Workshop on the Future of Datasets in Vision , vol. 2,\n",
            "2015.\n",
            "[94] X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y . Lin,\n",
            "and R. Yang, “The apolloscape dataset for autonomous driving,” in\n",
            "Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
            "Recognition Workshops , 2018, pp. 954–960.\n",
            "[95] F. Yu, H. Chen, X. Wang, W. Xian, Y . Chen, F. Liu, V . Madhavan,\n",
            "and T. Darrell, “Bdd100k: A diverse driving dataset for heterogeneous\n",
            "multitask learning,” in Proceedings of the IEEE/CVF Conference on\n",
            "Computer Vision and Pattern Recognition , 2020, pp. 2636–2645.\n",
            "[96] J. Luiten, I. E. Zulfikar, and B. Leibe, “Unovost: Unsupervised offline\n",
            "video object segmentation and tracking,” in WACV . Snowmass Village,\n",
            "CO, USA: IEEE, 2020, pp. 2000–2009.\n",
            "[97] H. Lin, X. Qi, and J. Jia, “Agss-vos: Attention guided single-shot video\n",
            "object segmentation,” in ICCV . Seoul, Korea: IEEE, 2019, pp. 3949–\n",
            "3957.\n",
            "[98] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\n",
            "image recognition,” in CVPR . Honolulu, Hawaii, USA: IEEE, 2016,\n",
            "pp. 770–778.\n",
            "[99] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V . Golkov,\n",
            "P. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning optical\n",
            "flow with convolutional networks,” in ICCV . Santiago, Chile: IEEE,\n",
            "2015, pp. 2758–2766.\n",
            "[100] D. Liu, Y . Cui, Y . Chen, J. Zhang, and B. Fan, “Video object detection\n",
            "for autonomous driving: Motion-aid feature calibration,” Neurocomput-\n",
            "ing, pp. 1–11, 2020.\n",
            "[101] A. O ˇsep, W. Mehner, P. V oigtlaender, and B. Leibe, “Track, then\n",
            "decide: Category-agnostic vision-based multi-object tracking,” in ICRA .\n",
            "Brisbane, Australia: IEEE, 2018, pp. 1–8.\n",
            "[102] A. Osep, W. Mehner, M. Mathias, and B. Leibe, “Combined image-and\n",
            "world-space tracking in traffic scenes,” in ICRA , IEEE. Marina Bay\n",
            "Sands, Singapore: IEEE, 2017, pp. 1988–1995.[103] S. Sharma, J. A. Ansari, J. K. Murthy, and K. M. Krishna, “Beyond\n",
            "pixels: Leveraging geometry and shape cues for online multi-object\n",
            "tracking,” in ICRA . Brisbane, Australia: IEEE, 2018.\n",
            "[104] J. Ren, X. Chen, J. Liu, W. Sun, J. Pang, Q. Yan, Y .-W. Tai, and L. Xu,\n",
            "“Accurate single stage detector using recurrent rolling convolution,” in\n",
            "CVPR . Honolulu, Hawaii, USA: IEEE, 2017, pp. 5420–5428.\n",
            "[105] A. Athar, S. Mahadevan, A. O ˇsep, L. Leal-Taix ´e, and B. Leibe, “Stem-\n",
            "seg: Spatio-temporal embeddings for instance segmentation in videos,”\n",
            "arXiv preprint arXiv:2003.08429 , 2020.\n",
            "[106] T.-Y . Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays,\n",
            "P. Perona, D. Ramanan, C. L. Zitnick, and P. Doll ´ar, “Microsoft coco:\n",
            "Common objects in context,” 2014.\n",
            "[107] G. Neuhold, T. Ollmann, S. Rota Bul `o, and P. Kontschieder, “The\n",
            "mapillary vistas dataset for semantic understanding of street scenes,”\n",
            "inICCV . Venice, Italy: IEEE, 2017.\n",
            "[108] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\n",
            "2014.\n",
            "[109] L. Chen, H. Ai, Z. Zhuang, and C. Shang, “Real-time multiple\n",
            "people tracking with deeply learned candidate selection and person\n",
            "re-identification,” in ICME , 2018.\n",
            "[110] C. Kim, F. Li, A. Ciptadi, and J. M. Rehg, “Multiple hypothesis\n",
            "tracking revisited,” in ICCV . Santiago, Chile: IEEE, Dec. 2015.\n",
            "[111] M. Keuper, S. Tang, B. Andres, T. Brox, and B. Schiele, “Motion\n",
            "segmentation and multiple object tracking by correlation co-clustering,”\n",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n",
            "vol. 42, pp. 140–153, 2020.\n",
            "[112] R. Henschel, L. Leal-Taix ´e, D. Cremers, and B. Rosenhahn, “Fusion\n",
            "of head and full-body detectors for multi-object tracking,” in CVPR\n",
            "workshops . Salt Lake City, UT, USA: IEEE, 2018, pp. 1428–1437.Feature-oriented Deep Learning Framework for Pulmonary Cone-beam CT (CBCT)\n",
            "Enhancement with Multi-task Customized Perceptual Loss\n",
            "Jiarui Zhua,1, Weixing Chenb, Hongfei Sunc, Shaohua Zhia, Mayang Zhaoa, Lam Yu Lapd, Zihan Zhoue, Tao Pengf, Jing Caia,∗, Ge\n",
            "Rena,g,∗\n",
            "aDepartment of Health Technology and Informatics, The Hong Kong Polytechnic University, 999077, Hong Kong, China\n",
            "bSchool of Computer Science and Engineering, Sun Yat-Sen University, 510006, Guangzhou, China\n",
            "cDepartment of Radiation Oncology, Xijing Hospital, Fourth Military Medical University, 710032, Xian, China\n",
            "dDepartment of Clinical Oncology, Queen Mary Hospital, 999077, Hong Kong, China\n",
            "eDepartment of Radiation Oncology, Jinshazhou Hospital of Guangzhou University of Chinese Medicine, 510168, Guangzhou, China\n",
            "fSchool of Future Science and Engineering, Soochow University, 215299, Suzhou, China\n",
            "gResearch Institute for Intelligent Wearable Systems, The Hong Kong Polytechnic University, 999077, Hong Kong, China\n",
            "Abstract\n",
            "Cone-beam computed tomography (CBCT) is routinely collected during image-guided radiation therapy (IGRT) to provide up-\n",
            "dated patient anatomy information for cancer treatments. However, CBCT images often su ffer from streaking artifacts and noise\n",
            "caused by under-rate sampling projections and low-dose exposure, resulting in low clarity and information loss. While recent deep\n",
            "learning-based CBCT enhancement methods have shown promising results in suppressing artifacts, they have limited performance\n",
            "on preserving anatomical details since conventional pixel-to-pixel loss functions are incapable of describing detailed anatomy. To\n",
            "address this issue, we propose a novel feature-oriented deep learning framework that translates low-quality CBCT images into\n",
            "high-quality CT-like imaging via a multi-task customized feature-to-feature perceptual loss function. The framework comprises\n",
            "two main components: a multi-task learning feature-selection network(MTFS-Net) for customizing the perceptual loss function;\n",
            "and a CBCT-to-CT translation network guided by feature-to-feature perceptual loss, which uses advanced generative models such as\n",
            "U-Net, GAN and CycleGAN. Our experiments showed that the proposed framework can generate synthesized CT (sCT) images for\n",
            "the lung that achieved a high similarity to CT images, with an average SSIM index of 0.9869 and an average PSNR index of 39.9621.\n",
            "The sCT images also achieved visually pleasing performance with e ffective artifacts suppression, noise reduction, and distinctive\n",
            "anatomical details preservation. Functional imaging tests further demonstrated the pulmonary texture correction performance of\n",
            "the sCT images. Comparison experiments with pixel-to-pixel loss also showed that the proposed perceptual loss significantly en-\n",
            "hances the performance of involved generative models. Our experiment results indicate that the proposed framework outperforms\n",
            "the state-of-the-art models for pulmonary CBCT enhancement. This framework holds great promise for generating high-quality\n",
            "anatomical imaging from CBCT that is suitable for various clinical applications, such as CBCT-based adaptive radiation therapy,\n",
            "functional imaging synthesis, and radiomics analysis.\n",
            "Keywords: cone-beam computed tomography, image-to-image translation, perceptual loss, multi-task learning\n",
            "1. Introduction\n",
            "Cone-beam computed tomography (CBCT) images are re-\n",
            "constructed from sparse-view cone-beam projections[1]. This\n",
            "volumetric cone-beam acquisition technique enables the CBCT\n",
            "imaging system to obtain patient volume information quickly\n",
            "with low radiation exposure, making it widely used to pro-\n",
            "vide up-to-date on-board images during image-guided radia-\n",
            "tion therapy (IGRT)[2]. This on-board CBCT imaging is clini-\n",
            "cally utilized for patient setup and treatment monitoring, partic-\n",
            "ularly for cancer-based patient positioning[3]. However, CBCT\n",
            "images still su ffer from severe streaking artifacts, noise, poor\n",
            "preservation of anatomical details, and unreliable Hounsfield\n",
            "1Github link:https: //github.com /zhujiarui42 /CFP-Loss\n",
            "2Contact Email:zhujiarui42@gmail.comunit (HU) values[4], thereby limiting its potential clinical ap-\n",
            "plications, such as CT-based adaptive radiation therapy (ART)\n",
            "[5], functional imaging[6], and radiomics analysis[7]. There-\n",
            "fore, there is a pressing need for a novel technique to improve\n",
            "the quality of CBCT imaging, which would be highly valuable\n",
            "and meaningful.\n",
            "To overcome these drawbacks, considerable e fforts have been\n",
            "dedicated to enhancing the image quality of CBCT. Traditional\n",
            "methods for reducing scatter photons in two-dimensional (2D)\n",
            "projection images involve the use of anti-scatter grid hardware\n",
            "suppression and scatter deconvolution estimation[8, 9, 10].\n",
            "However, these methods only o ffer moderate artifact suppres-\n",
            "sion and incur heavy computational costs. Alternatively, iter-\n",
            "ative projection-reconstruction methods have been developed\n",
            "to enhance CBCT image quality by artifact compensation and\n",
            "information interpolation. These methods employ a range of\n",
            "spatiotemporal regularizers coupled with mathematical con-\n",
            "Preprint submitted to Elsevier November 2, 2023arXiv:2311.00412v1  [cs.CV]  1 Nov 2023straints [11], or leverage prior knowledge [12, 13, 14]. Itera-\n",
            "tive projection-reconstruction methods are e ffective in compen-\n",
            "sating for motion artifacts and suppressing streaking artifacts.\n",
            "Nevertheless, these methods may compromise the preservation\n",
            "of anatomical details and typically require access to raw projec-\n",
            "tion data.\n",
            "Recently, deep learning (DL)-based image translation mod-\n",
            "els have gained significant popularity. As the CBCT-paired CT\n",
            "is commonly collected for the treatment planning stage in ra-\n",
            "diation therapy (RT), many researchers have focused on de-\n",
            "veloping novel CBCT-to-CT translation DL models to enhance\n",
            "CBCT image quality to CT image level. U-Net was firstly ap-\n",
            "plied to minimize the pixel-to-pixel di fference between the en-\n",
            "hanced CBCT and the corresponding CT[15, 16]. Although U-\n",
            "Net models have fast and stable convergence, the detail struc-\n",
            "ture recovery is still limited. To improve the model’s tunability,\n",
            "GAN models with pixel-to-pixel adversarial loss were used to\n",
            "improve the model performance[17]. Another challenge for this\n",
            "translation is the patient position mismatch gap between CBCT\n",
            "scan and CT scan, which degraded the label accuracy during the\n",
            "learning process. To address this problem, Cycle GAN models\n",
            "with cycle-consistency loss were introduced and generated sCT\n",
            "images with satisfying artifacts suppression within pelvic[18],\n",
            "neck[19], brain[20] and prostate[21] regions. DL-based CBCT\n",
            "enhancement models have a promising future, as they o ffer su-\n",
            "perior advantages of real-time processing speed and a ffluent\n",
            "data sources for consistent updating.\n",
            "Although DL-based direct image translation methods have\n",
            "brought significant breakthroughs, there remain several un-\n",
            "solved challenges. Particularly in organ regions with abundant\n",
            "textual details, such as lung regions, the local anatomical de-\n",
            "tails are prone to loss after enhancement due to the mismatched\n",
            "nature of the corresponding CT image[22, 23]. The anatomical\n",
            "misalignments between CBCT and CT images are entangled\n",
            "with spatial displacement caused by the acquisition time inter-\n",
            "val between CBCT and CT scans, which degrades the reliance\n",
            "of the target CT images. Modern registration techniques are far\n",
            "from perfect in solving spatial displacement[24, 25] because the\n",
            "entangled spatial displacement and anatomical misalignment\n",
            "are hard to distinguish. Although unpaired CycleGAN models\n",
            "partially alleviate the misalignment by forcing a bidirectional\n",
            "mapping and achieved better performance, anatomical details\n",
            "remain poorly preserved.\n",
            "One essential reason for this issue lies in the defective op-\n",
            "timization object, as a normal pixel-to-pixel loss function is\n",
            "limited to pixel-based location and fails to describe detailed\n",
            "anatomical information from CT target. As a result, all par-\n",
            "tial mismatched anatomy is identified as unwanted informa-\n",
            "tion and eliminated from CBCT inputs during the iterative op-\n",
            "timization process. To generate enhanced CBCT images with\n",
            "well-preserved anatomy, an optimization object with sophisti-\n",
            "cated high-dimensional descriptive ability is highly demanded.\n",
            "It should be able to capture the global and subtle anatomical fea-\n",
            "tures, which can be used for detailed anatomical texture analy-\n",
            "sis.\n",
            "In this study, we proposed to develop a novel deep learn-\n",
            "ing framework that replaces the conventional pixel-to-pixel losswith a high-dimensional feature-to-feature perceptual loss. Our\n",
            "framework consists of two main components: a multi-task\n",
            "feature-selection network and a CBCT-to-CT translation net-\n",
            "work. The feature-selection network pre-trains a customized\n",
            "autoencoder that serves as a building block for the perceptual\n",
            "loss function in the translation network. The autoencoder is\n",
            "trained with three unique tasks that share the same backbone\n",
            "structure, and the loss functions from these tasks are combined\n",
            "through a gradnorm regularization technique to refine the au-\n",
            "toencoder further. The CBCT-to-CT translation network uti-\n",
            "lizes the customized autoencoder to generate high-quality syn-\n",
            "thesized CT images from CBCT scans. The translation network\n",
            "employs the perceptual loss between the synthesized CT images\n",
            "and paired CT images to guide the optimization process and to\n",
            "ensure that the fine anatomical details are well-preserved. Our\n",
            "proposed deep learning framework represents a significant im-\n",
            "provement over existing models, as it enables us to generate\n",
            "sCT images with well-preserved anatomy and high image qual-\n",
            "ity.\n",
            "The main contributions of this study can be summarized as\n",
            "follows:\n",
            "1. We propose a novel feature-oriented deep learning frame-\n",
            "work that is specifically designed for enhancing the qual-\n",
            "ity of CBCT images. Our approach leverages a multi-\n",
            "task feature-selection network for feature-to-feature per-\n",
            "ceptual loss build-up in addition to utilizing novel gener-\n",
            "ative models for image translation. Our framework e ffec-\n",
            "tively preserves fine anatomical details from CBCT im-\n",
            "ages and transfers the noise-clear trait from CT targets to\n",
            "high-quality synthesized CT (sCT) imaging results.\n",
            "2. We propose a unique multi-task neural network with\n",
            "three subtasks and gradnorm regularization to customize\n",
            "a perceptual loss function that can extract features with\n",
            "mixed desired traits from CBCT imaging. The loss func-\n",
            "tion integrates traits from three subtasks, including CT\n",
            "self-recovery task, CBCT-to-CT dual-pyramid registration\n",
            "task, and CBCT-or-CT binary classification task. Com-\n",
            "pared to the pixel-to-pixel loss function, the proposed\n",
            "feature-to-feature perceptual loss function demonstrates a\n",
            "stronger feature extraction ability for more representative\n",
            "features and further improves sCT imaging quality.\n",
            "3. We provide extensive experimental evidence to demon-\n",
            "strate the e ffectiveness of our proposed multi-task cus-\n",
            "tomized perceptual loss function. The results clearly in-\n",
            "dicate that our perceptual loss function significantly en-\n",
            "hances the performance of all existing generative models\n",
            "for CBCT-to-CT translation, including U-Net, GAN, and\n",
            "the state-of-the-art CycleGAN models.\n",
            "The remaining sections of this paper are organized as fol-\n",
            "lows: In Section II, we provide a comprehensive review of\n",
            "related studies on image-to-image translation, perceptual loss,\n",
            "and multi-task learning. Section III presents a detailed explana-\n",
            "tion of the architecture employed in our proposed framework.\n",
            "In Section IV , we discuss the experimental materials, evaluate\n",
            "our model, and present the obtained experimental results. In\n",
            "Section V , we discuss implications, limitations, future work,\n",
            "2and key contributions of our research. Finally, in Section VI,\n",
            "we summarize our work and draw a conclusion.\n",
            "2. Related Work\n",
            "2.1. Image-to-image Translation\n",
            "Image-to-image translation is a widely used technique in the\n",
            "field of medical imaging, which involves mapping an image\n",
            "from a source modality to a target modality[26]. This tech-\n",
            "nique finds applications in various areas, including noise re-\n",
            "duction, super-resolution, image synthesis, and reconstruction.\n",
            "However, a significant challenge in image-to-image translation\n",
            "is the ill-posed nature, resulting from the non-unique reverse\n",
            "mapping from the target to the source. To address this limita-\n",
            "tion, researchers have developed comprehensive loss functions\n",
            "that consider the relevant internal relationships and capture sub-\n",
            "tle image details. For example, Richardson et al. proposed a\n",
            "Context-Aware GAN with an image-gradient-di fference-based\n",
            "loss function for CT-to-MR translation[27]. Elad et al. in-\n",
            "troduced a StyleGAN with a pixel-to-latent loss function that\n",
            "achieved robust generic translation results[28]. Devavrat et\n",
            "al. employed a segmentation-to-real loss function to regular-\n",
            "ize contrastive unsupervised learning for unpaired simulated-\n",
            "ultrasound to real-ultrasound translation[29]. These studies\n",
            "highlight the significance of designing an appropriate loss func-\n",
            "tion to enhance the predictive performance of the network.\n",
            "Inspired by these works, we propose a customized feature-\n",
            "oriented perceptual loss function through a multi-task network\n",
            "that specifically considers the traits of the CBCT-to-CT transla-\n",
            "tion task.\n",
            "2.2. Perceptual Loss\n",
            "Perceptual loss is a loss function commonly utilized in\n",
            "image-to-image translation problems, which quantifies the dis-\n",
            "similarity between the high-dimensional feature maps extracted\n",
            "from the network output and the target. This is achieved by em-\n",
            "ploying a pre-trained perceptual loss network and calculating\n",
            "the di fference typically using Euclidean distance.\n",
            "The original perceptual loss network, based on VGG-16,\n",
            "was trained on the ImageNet dataset and initially applied to\n",
            "mismatched natural image super-resolution and style-transfer\n",
            "tasks[30]. Subsequently, researchers have incorporated the\n",
            "original perceptual loss function into various applications with\n",
            "impressive results. For example, Yang et al. employed the\n",
            "original perceptual loss function in WGAN for low-dose CT\n",
            "denoising, achieving remarkable performance[31]. Georgios\n",
            "et al. combined CycleGAN with the original perceptual loss\n",
            "function for spectral domain optical coherence tomography\n",
            "classification[32]. Ran et al. utilized the original perceptual\n",
            "loss along with MSE loss to train a residual encoder-decoder\n",
            "WGAN for MRI denoising [33]. Li et al. initialized the param-\n",
            "eters of a VGG-14 loss network by training a self-supervised\n",
            "NDCT recovery network, and the resulting perceptual loss\n",
            "achieved state-of-the-art performance for LDCT denoising with\n",
            "both attention U-Net and WGAN[34].Moreover, perceptual loss is not limited to image-to-image\n",
            "translation tasks and has also proven e ffective in image classifi-\n",
            "cation and segmentation. Cheng et al. constructed a perceptual\n",
            "loss function based on non-local spectral and structural simi-\n",
            "larities, which yielded excellent performance for multi-spectral\n",
            "and panchromatic image classification[35]. High-dimensional\n",
            "feature maps capture essential features and encode a manifold\n",
            "that encompasses key information. A well-designed perceptual\n",
            "loss has the potential to comprehensively describe the correla-\n",
            "tion between the network output and the target, accounting for\n",
            "anatomical di fferences and spatial misalignment.\n",
            "2.3. Multi-task Learning\n",
            "Multi-task learning (MTL) is a powerful approach in ma-\n",
            "chine learning that involves training multiple related tasks si-\n",
            "multaneously by sharing feature representations and optimizing\n",
            "a combined loss function[36]. In the field of medical imaging,\n",
            "MTL using deep learning has shown significant potential in im-\n",
            "proving model accuracy and robustness by leveraging feature\n",
            "selection and traits transfer from subtasks[37].\n",
            "Various studies in medical imaging have demonstrated the ef-\n",
            "fectiveness of MTL. For instance, Antari et al. combined tasks\n",
            "such as mammography segmentation, classification, and object\n",
            "detection[38]. Kyung et al. integrated head CT classification,\n",
            "segmentation, and reconstruction as three subtasks[39]. These\n",
            "works have shown that models trained using MTL outperform\n",
            "individually trained subtasks by benefiting from better feature\n",
            "selection.\n",
            "Furthermore, other studies have showcased the advantages\n",
            "of MTL for specific medical applications. Boutilon et al.\n",
            "combined multi-scale contrastive classification and multi-joint\n",
            "anatomical segmentation for the clinical diagnosis of the pedi-\n",
            "atric musculoskeletal system[40]. Lei et al. combined brain\n",
            "functional imaging synthesis and brain structural imaging syn-\n",
            "thesis for early diagnosis and intervention of mild cognitive\n",
            "impairment[41]. These works demonstrate that MTL enables\n",
            "the transfer of traits among various subtasks, resulting in the\n",
            "development of a more comprehensive encoder.\n",
            "In general, the MTL paradigm maximizes the utilization of\n",
            "information from subtasks, mitigating overfitting and yielding\n",
            "a more robust model with superior accuracy and generalization\n",
            "performance. By sharing the same encoder and optimizing a\n",
            "combined loss function, MTL imposes strong constraints on\n",
            "feature selection, leading to the refinement of feature maps and\n",
            "the facilitation of desired traits transfer. Building upon these\n",
            "insights, we propose a multi-task learning structure that incor-\n",
            "porates tasks containing desired traits to construct a relevant\n",
            "perceptual loss function. By leveraging MTL and designing a\n",
            "customized loss function that considers the traits of the tasks,\n",
            "we can sensitively capture subtle details and achieve improved\n",
            "performance in image-to-image translation tasks.\n",
            "3. Methodology\n",
            "The CBCT-to-CT translation problem can be formulated as a\n",
            "cross-modality translation model in the image spatial domain.\n",
            "3Figure 1: The overall architecture of our proposed framework.(a)indicates our multi-task feature-selection network (b)indicates our feature extraction network\n",
            "(c)indicates our CBCT-to-PlanCT translation network.\n",
            "LetICB∈RH∗Wdenote a CBCT imaging, where H, W represent\n",
            "height and width of the image. Let ICT∈RH∗Wdenote a CT\n",
            "imaging. The goal of the translation is to seek a function Ftrans\n",
            "that maps a CBCT image to a CT image, and a synthesized\n",
            "CT(sCT) IsCTis generated from CBCT ICBbyFtrans:\n",
            "Ftrans(ICB;Θ)=IsCT (1)\n",
            "where Θrefers to weighing parameters of Ftrans.\n",
            "And by adopting a CNN model to generate IsCTfrom ICB,\n",
            "the translation is formulated as an optimization problem, and\n",
            "the optimization object is commonly built-up as a loss function\n",
            "L:\n",
            "ˆΘ = arg min\n",
            "LL[Ftrans(ICB;Θ),ICT] (2)\n",
            "Our approach to solve the translation problem adopts a\n",
            "feature-oriented framework, which comprises two main compo-\n",
            "nents. The first component is CBCT-to-CT translation division,\n",
            "which extracts key features from CBCT( ICB) and generates an\n",
            "sCT( IsCT) that closely resembles the CT( ICT) using novel gen-\n",
            "erative CNN models designed specifically for the modality dif-\n",
            "ference. The second component is the feature-selection divi-\n",
            "sion, which customizes the modality di fference between CBCT\n",
            "and CT through multl-task feature-selection.\n",
            "To achieve this, we adopt a pre-trained autoencoder, denoted\n",
            "asFloss, to extract compressed feature maps ϕCBandϕCTfromCBCT( ICB) and CT( ICT), respectively. We assume that the fea-\n",
            "ture mapϕCBcan be decomposed as the sum of anatomical\n",
            "information ϕA1from CBCT and irrelevant information ϕnoise,\n",
            "which includes artifacts and noise caused by cone-beam recon-\n",
            "struction and low-dose exposure.\n",
            "Similarly, the feature map ϕCTrepresents the anatomical in-\n",
            "formation from CT. Despite the anatomical di fferences between\n",
            "CBCT and CT, our goal is to identify the corresponding fea-\n",
            "turesϕA0that encode the same anatomical information in both\n",
            "domains(i.e., ϕA0=ϕA1∩ϕA2). In addition to identifying ϕA0,\n",
            "we aim to transfer the noise-free trait from ϕCTtoϕCBto re-\n",
            "move the irrelevant information (i.e., ϕnoise) from the CBCT fea-\n",
            "ture map. Ultimately, we propose to customize a feature-refined\n",
            "autoencoder (FAE) as the loss net Floss, which can extract the\n",
            "desired feature map ϕA0from ICT:\n",
            "ϕA0(ICT)=Floss(ICT) (3)\n",
            "and the same loss net is used to extract ϕA0from IsCT=\n",
            "Ftrans(ICB) :\n",
            "ϕA0(IsCT)=Floss(IsCT) (4)\n",
            "And then a feature-oriented perceptual loss function\n",
            "Lperceptual (ICT,IsCT) was formulated to calculate a detailed\n",
            "high-dimensional modality di fference:\n",
            "Lperceptual (ICT,IsCT)=L(ϕ(ICT),ϕ(IsCT)) (5)\n",
            "4Fig. 1 illustrates the architecture of our proposed feature-\n",
            "oriented framework. Our scheme comprises a 3-task multi-task\n",
            "feature-selection network(MTFS-Net) to customize the FAE.\n",
            "As is shown in Fig. 1(a), the 3 tasks include a CT-to-CT self-\n",
            "recovery task 1 for extracting ϕA2, a dual-pyramid registration\n",
            "task 2 for locating feature correlations between CBCT and CT\n",
            "to locate an intersection between ϕA1+ϕnoiseandϕA2, and a bi-\n",
            "nary classification task 3 for disentangling between ϕA1+ϕnoise\n",
            "from ICBandϕA2from ICT. By jointly training task 1 and task\n",
            "3, we aim to locate ϕnoise fromϕA1+ϕnoise based onϕA2. By\n",
            "jointly training task 1 and task 2, we aim to refine the same pat-\n",
            "ternϕA0fromϕA1+ϕnoiseandϕA2. And by jointly training all 3\n",
            "tasks together we aim to enable a feature-selection on the back-\n",
            "bone autoencoder to: contain complete A2information, identify\n",
            "ϕnoise information from ϕA1+ϕnoise and locate same ϕA0style\n",
            "from bothϕA1andϕA2.\n",
            "After FAE refinement, as is shown in Fig. 1(c), we adopt a\n",
            "CBCT-to-CT translation network consisting of the parameter-\n",
            "fixed loss net and several optional generative models includ-\n",
            "ing U-Net, GAN and CycleGAN. In Fig. 1(b), we use the pre-\n",
            "trained FAE from Fig. 1(a) as the loss net to define the per-\n",
            "ceptual loss function, and use the perceptual loss to guide the\n",
            "training of the generative models for CBCT-to-CT translation.\n",
            "The MTFS-Net and CBCT-to-CT translation network are in-\n",
            "troduced in detail in the following sections.\n",
            "3.1. Multi-task Feature-Selection Network\n",
            "Fig. 2 shows the architecture of the multi-task feature-\n",
            "selection network, which is based on a “hard-sharing” MTL de-\n",
            "sign. The network has one shared backbone structure and three\n",
            "independent head structures modeling three di fferent tasks. The\n",
            "backbone structure is a deep autoencoder that creates a feature\n",
            "space shared by the three independent head structures. Task\n",
            "1 aims to extract the manifold from CT, while task 2 selects\n",
            "corresponding features between CBCT and CT domains, and\n",
            "task 3 locates noise and streaking artifacts in the CBCT do-\n",
            "main based on the di fference between CBCT and CT. These\n",
            "three tasks work together to guide the autoencoder in building\n",
            "a refined feature space with mixed desired traits, which only\n",
            "encodes features containing key information for recovering CT\n",
            "imaging, extracting the same pattern from both CBCT and CT\n",
            "domains, and excluding noise or streaking artifacts.\n",
            "3.1.1. Deep Autoencoder Backbone\n",
            "Previous research commonly used VGG-like autoencoders\n",
            "as the loss network for perceptual loss function construc-\n",
            "tion. Since that increasing the structure depth enables the\n",
            "autoencoder to further reduce information redundancy and\n",
            "extract more compressed feature maps[42], we try to in-\n",
            "crease the depth of our autoencoder backbone by using skip\n",
            "connections[43] so the shared backbone has a 101-layer deep\n",
            "ResNet architecture. The output size is cautiously set because\n",
            "over-large size may include over-redundant information, yet\n",
            "undersized compressed feature maps may be incapable of\n",
            "containing complete information. As is shown in Fig. 2, after\n",
            "fine adjustment, the autoencoder is designed to include 3maxpooling layers for downsampling, (31) two-skip resid-\n",
            "ual layers(each contains two sequential 3*3 convolutional\n",
            "layers) and 5 convolutional layers with parameters set to be\n",
            "[(1,32,256,256),(32,64,128,128),(64,128,64,64),(128,256,32,32)]\n",
            "in terms of [(input channels, output channels, H, W)].\n",
            "3.1.2. CT Self-recovery Task\n",
            "Task 1 is a single-input self-supervised decoder for CT-to-CT\n",
            "recovery with a VGG architecture. Previous works[31] have\n",
            "proved that a shallow VGG structure is well-qualified for CT\n",
            "self-recovery. As is shown in Fig. 2, we used a VGG-12 struc-\n",
            "ture consisting of 9 convolutional layers with out-channel num-\n",
            "bers:256,256,256,128,128,64,64,32,1.\n",
            "For the loss function of self-recovery task1, we utilize mean\n",
            "square error(MSE) function to describe the intensity similarity\n",
            "between the decoder output ˆ y1, ˆy3and the ground truth yCB,yCT:\n",
            "L1=MS E (ˆy,y) (6)\n",
            "3.1.3. Dual-pyramid Registration Task\n",
            "Task 2 is a dual-input self-supervised decoder generating a\n",
            "deformable vectors field(DVF) for registration between CBCT\n",
            "and CT with random rotation and o ffset. Registration net-\n",
            "works essentially learn reversible pixel-wise or voxel-wise cor-\n",
            "respondences between two input image domains in the form\n",
            "of DVF[44]. Kang et al. proved that DVF decoded from\n",
            "separate feature pyramids dominantly boosts the registration\n",
            "performance[45]. Kang et al. then interpreted that such perfor-\n",
            "mance improvement originates from the local anatomical struc-\n",
            "tural details encoded by the feature pyramids. We further in-\n",
            "fer that the dual-pyramid registration network structure decodes\n",
            "the correlations by selectively encoding strongly correlated fea-\n",
            "tures extracted from two input image domains. Therefore,\n",
            "we imply dual-pyramid registration in our multi-task feature-\n",
            "selection network, to select highly-correlated features. These\n",
            "encoded correlations were usually inferred as a structural simi-\n",
            "larity, but requires further analysis.\n",
            "We design task 2 in a dual-pyramid registration network ar-\n",
            "chitecture. Task 2 generates a deformable vector field(DVF)\n",
            "DVF =Freg(Imoved CBCT,ICT;Θ) from dual inputs, where Θis\n",
            "the weighing parameters of Freg.Imoved CBCT is warped by the\n",
            "DVF and label and becomes YCBCT reg. And the loss function\n",
            "of task 2 is the intensity similarity di fference between YCBCT reg\n",
            "andICT, vice versa:\n",
            "L2=MS E (YCBCT reg,ICT)or MS E ((YCTreg,ICBCT ) (7)\n",
            "3.1.4. Classification Task\n",
            "Task 3 is a binary classification decoder with a fully-\n",
            "connected neural network structure. Multiple-classification\n",
            "task trained on ImageNet dataset[46] was initially utilized for\n",
            "the original perceptual building-up loss net[30]. The classifica-\n",
            "tion results are closely related to the feature embedding quality\n",
            "and both shallow structural features and deep semantic features\n",
            "markedly contribute to the classification e fficacy. Given enough\n",
            "5Figure 2: The architecture of the three-task multi-task feature-selection network. The network consists of a commonly shared autoencoder as shared backbone, and\n",
            "three separate subtasks as head structure. The autoencoder is customized by the three subtasks, and the final loss function is untied by three loss functions in a\n",
            "gradnorm form.\n",
            "samples and multiple targets, a classification task will be capa-\n",
            "ble of extracting abundant and distinctive feature maps which\n",
            "may further serve as an ideal perceptual loss net. Meanwhile,\n",
            "given limited targets, a classification task is also able to dis-\n",
            "tinguish key di fferences between source and targets, in other\n",
            "words, disentangling source content from target style[29]. In\n",
            "our case, severe artifacts and noise can be a dominant di fference\n",
            "between CBCT source and CT targets. The feature distinguish-\n",
            "able trait enables a classification task to identify this dominant\n",
            "noise feature.\n",
            "We apply one fully-connected layer as the decoder for the\n",
            "binary classification task 3 which classifies CBCT and CT by\n",
            "generating the ”label-predict”. The classification task is trained\n",
            "in an supervised form. The ”label-target” set is composed of\n",
            "CBCT and CT images which are labeled as ”0” and ”1”.\n",
            "The loss function of the classification task 3 is listed as fol-\n",
            "lows:\n",
            "L3=CE(label−predict,label−target ) (8)\n",
            "To normalize the optimizing speed and magnitude of the loss\n",
            "from the three independent tasks, a gradnorm loss regulariza-\n",
            "tion strategy[47] is applied for combining di fferent task losses\n",
            "into one united loss.\n",
            "Luni=Gradnorm (L1,L2,L3) (9)\n",
            "3.2. CBCT-to-CT translation Network\n",
            "Fig. 3 shows the architecture of the CBCT-to-CT transla-\n",
            "tion network, which follows a typical “image translation net\n",
            "+loss net” design [30] , consisting of multiple optional gener-\n",
            "ative models as the translation net Ftrans(x|Θ) and a pre-trainedperceptual loss extraction network Floss(x|Θ) with fixed param-\n",
            "eters, where xis the input and Θrefers to the network parame-\n",
            "ters. Rather than targeting the pixel-to-pixel di fference between\n",
            "the pixels of the output image y=Ftrans(x) and the ground truth\n",
            "ˆy, the translation net targets the feature di fference between fea-\n",
            "ture representations extracted by the loss net from the output\n",
            "imageϕy=Floss(y) and ground truth ϕˆy=Floss(ˆy). The feature\n",
            "difference is defined as the perceptual loss Lperceptual (ϕy,ϕˆy).\n",
            "3.2.1. Translation Net\n",
            "The proposed CBCT-to-CT translation task guided by per-\n",
            "ceptual loss requires not only preserving low-level information\n",
            "such as textures and contours from CBCT, but also extracting\n",
            "semantic features for distinguishing structural streaking arti-\n",
            "facts from textures, identifying noise, and separating ϕA0from\n",
            "ϕA1. To achieve better translation results, we selected three\n",
            "commonly used generative models, including U-Net, GAN, and\n",
            "CycleGAN, with modifications to better suit our pulmonary CT\n",
            "synthesis case.\n",
            "The U-Net model is designed in a channel attention resid-\n",
            "ual form(CAR-U-Net). We adopted a U-Net[48] backbone\n",
            "for CBCT-to-CT translation because the four down-sampling\n",
            "operations allow the extraction of high-level semantic infor-\n",
            "mation and their concatenation to four up-sampled layers ef-\n",
            "fectively reserves low-level information. We added resid-\n",
            "ual blocks[43] with two-step skip connections(Res-Block) to\n",
            "increase the network depth for higher-level features extrac-\n",
            "tion. And we added classic ”squeeze-excite” [49]channel atten-\n",
            "tion blocks(SE-Block) before each concatenation for adaptive\n",
            "weighing of high-level feature channels.\n",
            "For the channel attention residual U-Net (CAR-U-Net).\n",
            "Within each sampling level of the ”U” shape backbone, we\n",
            "6Figure 3: The architecture of the CBCT-to-CT translation network.The network contains a translation net and a perceptual loss net. The translation net employs\n",
            "U-Net, Gan and Cycle-Gan models. The perceptual loss net is the parameter-fixed autoencoder which was pretrained in Fig. 2. And the CFP-loss is calculated by\n",
            "the tensor di fference between the feature maps extracted from the synthesized CT and CT by the perceptual loss net.\n",
            "added Res-Blocks with numbers of 2,4,6,8 and we added ad-\n",
            "ditional skip connection between the input and the sequential\n",
            "output. Before each concatenation, we added a SE-Block.\n",
            "The GAN and CycleGAN models apply same generator and\n",
            "discriminator structure according to previous methods. In our\n",
            "case, we specially replace all the generator loss functions with\n",
            "our proposed perceptual loss function. Especially for Cycle-\n",
            "GAN, we apply the perceptual loss function Loss perceptual ac-\n",
            "quired from the FMTL-net, calculate ϕRealfrom real image ICT\n",
            "orICBin the backward process of CycleGAN and ϕS ynfrom\n",
            "fake image generated by generator, and replace the Loss MS E\n",
            "with Loss perceptual (ϕReal,ϕS yn). We made no modification to the\n",
            "original cycle-consistency loss defined as absolute di fference\n",
            "betweenϕRealandϕS yn.\n",
            "3.2.2. Loss Net\n",
            "The pre-trained FAE from the MTFS-Net was directly used\n",
            "as the loss net for the CBCT-to-CT translation network. The\n",
            "loss net parameters remain fixed during the training of the trans-\n",
            "lation net.\n",
            "The loss net includes layers of four size-levels includ-\n",
            "ing (256*256),(128*128),(64*64) and (32*32) in terms of\n",
            "(Heights*Width). We only use the last layer in each size-level\n",
            "and Relu function is applied to regulate the output of each layer.\n",
            "LetSdenote the size-level, then the output of the loss net is for-\n",
            "mulated as :\n",
            "ϕS=Floss(I) (10)\n",
            "where Sis given a value range from 1 to 4, and each refers\n",
            "to the size-level 256,128,64,32. Each ϕSis a three-dimensional\n",
            "tensor describing the feature of the input I. AndϕSwith higher\n",
            "level describes high-level semantic feature of style, while ϕS\n",
            "with lower level describes more detailed contents.3.3. Perceptual Loss Function\n",
            "The perceptual loss function was formulated to describe the\n",
            "tensor di fference between ˆϕS(IsCT) andϕS(ICT). Since that Eu-\n",
            "clidean distance calculates an absolute matrix di fference, it’s\n",
            "commonly utilized as a ”content” loss in [30, 31, 34], which is\n",
            "formulated as:\n",
            "Lcontent (ˆϕS,ϕS)=1\n",
            "C∗H∗WCX\n",
            "c=1HX\n",
            "h=1WX\n",
            "w=1||ˆϕS−ϕS||2\n",
            "F(11)\n",
            "Using the absolute di fference above as an optimization target\n",
            "can set strong spatial constraints between corresponding pix-\n",
            "els and transfer more complete ”content” information from the\n",
            "target domain to the input domain, but this absolute di fference\n",
            "may incur an unstable training process.\n",
            "Gram matrixG(ϕ) calculates the vector inner product of a\n",
            "tensor to describe a general value distribution style:\n",
            "G(ϕ)=1\n",
            "C∗H∗WHX\n",
            "h=1WX\n",
            "w=1ϕh,w,cϕh,w,c‘ (12)\n",
            "where c‘refers to the transpose of matrix ϕh,w,c‘along ”chan-\n",
            "nel” axis. And the squared Frobenius norm between G(ˆϕ) and\n",
            "G(ϕ) is utilized as a ”style” loss[30]:\n",
            "Lstyle(ˆϕ,ϕ)=||G(ˆϕ)−G(ϕ)||2\n",
            "F (13)\n",
            "The inner vector product measures cross-relation degrees be-\n",
            "tween feature vectors, which enables a Gram matrix to describe\n",
            "a general value distribution style of a tensor. And the di ffer-\n",
            "ence between Gram matrixes calculated from di fferent tensors\n",
            "describes a ”style” di fference regardless of the specific spatial\n",
            "position of pixels.\n",
            "7To ensure stable training and transfer comprehensive in-\n",
            "formation, our proposed customed feature-to-feature percep-\n",
            "tual(CFP)loss function is further formulated as:\n",
            "LCFP(ˆϕ,ϕ)=a\n",
            "S1S1X\n",
            "1Lcontent (ˆϕS,ϕS)+b\n",
            "S2S2X\n",
            "1Lstyle(ˆϕ,ϕ) (14)\n",
            "where S1,S2refer to the size-level orders from 1 to 4(256\n",
            "to 32), and they are customized as 2 and 4 in our case. aand\n",
            "brefer to weighing factors that are set as 0.5 and 0.5 through\n",
            "comparison experiments.\n",
            "4. Experiments and Results\n",
            "4.1. Dataset\n",
            "In this study, we utilized four-dimensional thoracic CBCT\n",
            "and PCT image pairs from 100 lung cancer patients who un-\n",
            "derwent stereotactic radiotherapy on a Varian Medical Systems\n",
            "(VISION 3253) machine between 2017-2019 at Queen Mary’s\n",
            "Hospital in Hong Kong. Full-dose thin PCT slices (3.0mm)\n",
            "were acquired on a 16-row multi-detector helical CT scanner\n",
            "with a tube voltage of 120kV , variable tube current, variable\n",
            "exposure time, a gantry rotation time of 0.5s, a matrix size of\n",
            "512 by 512 on the axial plane, and a pixel size of 1.074mm\n",
            "by 1.074mm. The full-dose thin CBCT slices (1.5mm-3.0mm)\n",
            "were acquired during SBRT treatment using the onboard imag-\n",
            "ing system on VISION 3253 with a tube voltage of 125kV , a\n",
            "tube current of 40 mA /frame, and variable exposure time. For\n",
            "each scan, a total of 360 projections were acquired in a full scan.\n",
            "All patients were placed in the Headfirst-Supine position during\n",
            "CBCT and PCT collection, following QMH’s scanning proto-\n",
            "col. PCT images were acquired only once about one to two\n",
            "weeks before the course of SBRT treatment. The average inten-\n",
            "sity projection was calculated for ten phases of each patient, re-\n",
            "sulting in 100 volume pairs of 4D-average CT and 4D-average\n",
            "CBCT for each patient. These 100 patients were randomly split\n",
            "70/30 into AI-training and AI-testing groups, with the training\n",
            "dataset further split 56 /14 for training and validation.\n",
            "4.2. Preprocessing\n",
            "Fig. 4 illustrates the preprocessing procedure used to ob-\n",
            "tain the training CBCT and CT images. To align the CBCT\n",
            "and CT images into the same coordinate system with a ma-\n",
            "trix size of 256x256x50, a 3D rigid registration was performed\n",
            "using the Elastix registration toolbox [51]. In order to focus\n",
            "the deep learning model on the pulmonary textual information\n",
            "and avoid the negative e ffects of non-anatomical structures dur-\n",
            "ing the training process, binary masks were created to separate\n",
            "the pulmonary region from the non-anatomical and surrounding\n",
            "thoracic regions. These masks were generated using the deep\n",
            "learning-based R231CovidWeb auto-segmentation model [52]\n",
            "on each CT and CBCT image from the same patient. The voxel\n",
            "values outside the mask region were replaced entirely with a\n",
            "Hounsfield Unit (HU) value of 0. The masked CT and CBCT\n",
            "images were then cropped and resized to remove any irrelevant\n",
            "background, and HU values outside the range of [-1000, 200]were cut o ff. Prior to training, all pixel values of the CT and\n",
            "CBCT images were transformed from HU values to linear at-\n",
            "tenuation coe fficients (LAC) using the following formula:\n",
            "µ=HU∗µw\n",
            "1000+µw (15)\n",
            "whereµis the LAC and µwrefers to the LAC of water,which is\n",
            "set to be 0.192.\n",
            "4.3. Implement\n",
            "During the training process, we utilized the Adam optimiza-\n",
            "tion algorithm to optimize all component networks. The hyper-\n",
            "parameters were set as follows: the learning rate was set to 1.0e-\n",
            "4, the batch size was set to 16, and the exponential decay rate\n",
            "was set to 0.9. We implemented the networks using PyTorch\n",
            "1.10.1 and trained, tested, and validated them on a workstation\n",
            "equipped with an NVIDIA A6000 GPU.\n",
            "4.4. Quantitative Analysis\n",
            "We evaluated the imaging quality of our CBCT-to-CT trans-\n",
            "lation model using five common metrics: peak-to-noise ra-\n",
            "tio (PSNR), structural similarity (SSIM), visual information fi-\n",
            "delity (VIF), normalized cross-correlation (NCC), and informa-\n",
            "tion fidelity criterion (IFC). Mean values and deviations on the\n",
            "test dataset are reported in Table 2. Of the five metrics, PSNR,\n",
            "SSIM, and NCC focus more on pixel-level similarity, while VIF\n",
            "and IFC take into account psychovisual features of the human\n",
            "visual system by using natural statistics models.\n",
            "4.5. Ablation Study\n",
            "In this section, we performed model ablation studies to eval-\n",
            "uate our proposed multi-task perceptual loss customizing strat-\n",
            "egy. During the ablation studies, we gradually added subtasks\n",
            "to the multi-task perceptual loss customizing network and se-\n",
            "lected autoencoders from each trained network to build vari-\n",
            "ous perceptual loss functions. We used U-Net as the common\n",
            "CBCT-to-CT translation network and evaluated the generated\n",
            "sCT by U-Net with MSE and various perceptual loss functions\n",
            "quantitatively.The results are summarized in Table. 1, where the\n",
            "SSIM, PSNR, VIF, IFC and NCC were used to evaluate the\n",
            "similarity between CT targets and generated sCT images from\n",
            "CBCT inputs. The mean SSIM and PSNR values, along with\n",
            "deviations on the test dataset comprising 30 patients and a total\n",
            "of 3930 CBCT-CT slice pairs, were recorded in Table. 2.\n",
            "4.6. Translation Results Comparison\n",
            "In order to assess the performance of the CBCT-to-CT trans-\n",
            "lation, we selected a representative case from the test set con-\n",
            "taining a complete left lung structure, a lung cancer lesion, and\n",
            "detailed airways and vessel anatomy. We generated sCT results\n",
            "using four typical generative networks, including Unet-MSE,\n",
            "Unet-CFP, GAN, GAN-CFP, CycleGAN, and CycleGAN-CFP,\n",
            "with and without our proposed CFP loss. We set the param-\n",
            "eters of each generative model based on the settings of previ-\n",
            "ous state-of-the-art CBCT-to-CT methods, including U-Net for\n",
            "[16], GAN for [17] and CycleGAN for [20], [21] and [22]. For\n",
            "8Figure 4: Preprocessing Workflow. Ragid registration, single lung segmentation, cropping,resizing and normalization are performed on the CT and CBCT images\n",
            "before they are fed into the deep learning network.\n",
            "GAN models, the ”CFP” su ffix refers to the replacement of the\n",
            "pixel-to-pixel MSE loss with feature-to-feature CFP perceptual\n",
            "loss.\n",
            "Fig. 5 presents the global visualization results, with the en-\n",
            "larged regions of interest (ROI) marked by dashed boxes, and\n",
            "violin curves on the representative case. Fig. 6 shows the abso-\n",
            "lute maps between sCT and CBCT, and between sCT and CT.\n",
            "4.7. Functional Imaging Analysis\n",
            "To evaluate the clinical e fficacy of CBCT imaging quality\n",
            "enhancement techniques, it is crucial to assess the HU value\n",
            "distribution. In order to evaluate the HU value correction per-\n",
            "formance of our method and its ability to preserve anatomi-\n",
            "cal information, we applied a pulmonary perfusion generation\n",
            "model[6] and analyzed the prediction accuracy of functional re-\n",
            "gions between CBCT, sCT, and CT pairs. We selected four rep-\n",
            "resentative cases, two with left lungs and two with right lungs,\n",
            "and the visual results are shown in Fig. 7.\n",
            "To quantify the accuracy of functional region prediction, we\n",
            "calculated the Dice similarity coe fficient (DSC) and Pearson’s\n",
            "correlation coe fficient (R) on the predicted functional imaging\n",
            "in the test dataset. The results are summarized in Table III.\n",
            "Among the five evaluation metrics, PSNR, SSIM, and NCC fo-\n",
            "cus on pixel-level similarity, while VIF and IFC focus more on\n",
            "psychovisual features of the human visual system using natural\n",
            "statistics models.To quantify the functional regions prediction accuracy, we\n",
            "also calculate Dice similarity correlation(DSC) and Pearsman’s\n",
            "coefficient(R) on the predicted functional imaging on the test\n",
            "dataset. The results are summarized in Table. 3.\n",
            "5. Discussion\n",
            "In this study, we developed a novel deep learning framework\n",
            "for enhancing lung CBCT imaging quality to CT level, which\n",
            "consists of two main components: a multi-task feature-selection\n",
            "network and a CBCT-to-CT translation network. The multi-\n",
            "task feature-selection network is designed to pretrain a cus-\n",
            "tomized autoencoder that serves as a perceptual loss building-\n",
            "up network with fixed parameters in the CBCT-to-CT trans-\n",
            "lation network. The autoencoder is customized with mixed\n",
            "traits by jointly training three unique tasks, and the loss func-\n",
            "tions from these tasks are united through a gradnorm regulariza-\n",
            "tion technique to further refine the autoencoder. The CBCT-to-\n",
            "CT translation network generates high-quality synthesized CT\n",
            "(sCT) imaging from CBCT by using a perceptual loss between\n",
            "the sCT and paired CT images. This perceptual loss is defined\n",
            "by the di fference between level-crossing feature maps, which\n",
            "are extracted by di fferent layers from the perceptual loss net-\n",
            "work. The multi-task feature-selection network is responsible\n",
            "for building up this network.\n",
            "Thoracic CBCT can be collected conveniently during lung\n",
            "radiation therapy with real-time anatomy and is potentially\n",
            "9Figure 5: Visual performance comparison of di fferent methods on a representative case of left lung and right lung. The CT display window is [-1000,-200]HU.\n",
            ".\n",
            "10Figure 6: Absolute maps comparison of di fferent methods on a representative case(CBCT-sCT /CT-sCT) of left lung and right. The CT display window is [-1000,-\n",
            "200]HU.\n",
            ".\n",
            "11Table 1: The Quantitative test results of state-of-the-art methods with and without our proposed perceptual loss\n",
            "Method SSIM PSNR VIF IFC NCC\n",
            "Base 0 .8655±0.0435 28.6744±2.5962 0.1742±0.0645 1.1632±0.0111 0.3526±0.0967\n",
            "Unet-MSE 0 .9327±0.0624 34.1525±2.1877 0.2004±0.0179 1.1125±0.0098 0.3324±0.0217\n",
            "Unet-CFP 0 .9772±0.0227 39.9621±1.3511 0.3021±0.0108 2.0015±0.0073 0.3218±0.0189\n",
            "GAN 0 .9554±0.0525 37.1121±2.9887 0.2248±0.0493 1.4571±0.0207 0.3626±0.0913\n",
            "GAN-CFP 0 .9671±0.0326 38.8851±1.9561 0.2321±0.0366 1.7411±0.0117 0.4002±0.0897\n",
            "CycleGAN 0 .9811±0.0497 37.2215±2.1564 0.2102±0.0343 1.4436±0.0169 0.4557±0.0981\n",
            "CycleGAN-CFP 0.9869±0.0298 39.1871±1.8003 0.2574±0.2333 1.5568±0.0128 0.4971±0.0910\n",
            "Table 2: The Quantitative test results during Ablation study\n",
            "Loss Function Name SSIM PSNR VIF IFC NCC\n",
            "CBCT 0 .8655±0.0435 28.6744±2.5962 0.1742±0.0645 1.1632±0.0111 0.3526±0.0967\n",
            "MSE Loss 0 .9327±0.0624 34.1525±2.1877 0.2004±0.0179 1.1252±0.0098 0.3324±0.0217\n",
            "VGG Loss 0 .9206±0.0779 35.1616±2.2115 0.2116±0.0321 1.1108±0.0079 0.3307±0.0169\n",
            "CFP Loss\n",
            "task 1 task 2 task 3\n",
            "✓ 0.9557±0.0197 36.7461±1.1778 0.1939±0.0099 1.1539±0.0057 0.3501±0.0138\n",
            "✓ 0.9151±0.0954 32.1152±3.1152 0.0917±0.0323 1.1413±0.0109 0.2899±0.0238\n",
            "✓ 0.9254±0.0741 28.2245±2.6612 0.1067±0.0286 1.1517±0.0153 0.3105±0.0207\n",
            "✓ ✓ 0.9679±0.0256 37.1126±1.9845 0.2501±0.0207 1.1610±0.0152 0.3216±0.0303\n",
            "✓ ✓ 0.8517±0.0279 27.2215±2.7745 0.1058±0.0469 1.1208±0.0099 0.2915±0.0207\n",
            "✓ ✓ 0.9588±0.2928 38.7112±1.7511 0.2479±0.0154 1.6172±0.0162 0.3311±0.0189\n",
            "✓ ✓ ✓ 0.9772±0.0227 39.9621±1.3551 0.3021±0.0108 2.0015±0.0073 0.3218±0.0089\n",
            "Table 3: The Quantative test results of functional imaging\n",
            "Method DSC SCC R\n",
            "Base 0 .7955±0.0611 0.9064±0.1007 0.9071±0.0753\n",
            "Unet-MSE 0 .8708±0.0457 0.9093±0.0958 0.9174±0.0777\n",
            "Unet-CFP 0 .8959±0.0172 0.9465±0.0413 0.9507±0.0698\n",
            "GAN 0 .7608±0.0391 0.7991±0.0823 0.8107±0.0686\n",
            "GAN-CFP 0 .8853±0.0128 0.9284±0.0625 0.9343±0.0574\n",
            "CycleGAN 0 .8299±0.0351 0.8705±0.0653 0.8801±0.0596\n",
            "CycleGAN-CFP 0.9147±0.00860.9615±0.03220.9661±0.0399\n",
            "qualified for cancer-based positioning, adaptive treatment plan-\n",
            "ning , radionics analysis, and lung functional imaging synthesis.\n",
            "High-quality thoracic CBCT is also a precondition of real-time\n",
            "treatment planning. However, other than the severe artifacts,\n",
            "noise, low soft tissue contrast and HU value inconsistency, the\n",
            "poor reservation of tiny and detailed pulmonary anatomy can be\n",
            "the most challenging factor hampering the clinical application\n",
            "of CBCT. Our proposed feature-oriented framework provides a\n",
            "solution for pulmonary fine anatomy reservation.\n",
            "The proposed feature-oriented framework presents a signif-\n",
            "icant advance over existing CNN and GAN based models for\n",
            "CBCT-to-CT translation, which have typically focused on the\n",
            "receptive field, network structure modification, or pairs match.\n",
            "The pixel-to-pixel loss functions employed in these approachesare inadequate for comprehensively describing the di fferences\n",
            "between modalities, spatial locations, and anatomical struc-\n",
            "tures in a mismatch scenario like CBCT-to-CT translation. In\n",
            "contrast, our framework leverages a perceptual loss that com-\n",
            "prehensively captures the high-dimensional feature map di ffer-\n",
            "ences between paired CBCT and CT images, enabling the trans-\n",
            "fer of noise-free and clear anatomical styles from CT to CBCT\n",
            "at a perceptual level .\n",
            "nor limited to a simple filtering function\n",
            "The sCT images generated by our proposed feature-oriented\n",
            "framework show great imaging quality improvement compared\n",
            "to most of the current pixel-to-pixel generative models. In terms\n",
            "of statistical similarity between sCT and CT, our proposed\n",
            "method achieved the highest SSIM, PSNR, VIF, IFC and NCC\n",
            "compared to pixel-to-pixel Unet,GAN or CycleGAN models.\n",
            "The dominant quantitative results demonstrate great improve-\n",
            "ment of anatomical information preservation ability owing to\n",
            "our proposed feature-level optimization target. In terms of\n",
            "qualitative analysis, compared to other methods, sCT images\n",
            "generated by our framework show superior artifacts suppres-\n",
            "sion, noise reduction and clear anatomical details both glob-\n",
            "ally and locally. The great visual performance proves that our\n",
            "proposed multi-task feature-selection strategy has great CBCT-\n",
            "to-CT feature extraction ability and is an e ffective solution to\n",
            "the CBCT-to-CT translation scenario. Ablation experiment fur-\n",
            "ther proves that the self-recovery, registration and classification\n",
            "tasks are complementary and together build up a performant\n",
            "12Figure 7: Functional imaging of four representative cases. The CT display window is [-1000,-200]HU.\n",
            "feature-selection network. In the lung function imaging syn-\n",
            "thesis experiment, compared to other methods, our framework\n",
            "shows more precise high-function regions prediction. These re-\n",
            "sults in-depth demonstrate the HU value distribution correction\n",
            "ability of our proposed framework.\n",
            "To the best of our knowledge, our proposed framework is the\n",
            "first post-processing based CNN for CBCT refinement that op-\n",
            "erates in a feature-to-feature level. It is especially e ffective for\n",
            "enhancing pulmonary CBCT images with a high level of de-\n",
            "tail. The loss functions used in our framework set strong con-\n",
            "straints on the mapping between input and target spaces, and all\n",
            "network modification operations share the common goal of fit-\n",
            "ting an appropriate function to achieve the desired mapping pat-\n",
            "tern within the assumption space. However, further research is\n",
            "needed to explore the limitations of network architecture modi-\n",
            "fications such as attention, training strategy, or generative mod-\n",
            "els in this context:\n",
            "customize the output layers\n",
            "1. Expand our CBCT imaging quality enhancement frame-\n",
            "work to fit more body regions: Essentially, the severe\n",
            "streaking artifacts in CBCT images are caused by infor-\n",
            "mation loss that varies among di fferent tissues or organs.\n",
            "Therefore, the characteristics of CBCT streaking artifacts\n",
            "may vary across di fferent body regions. To address this,\n",
            "we plan to expand our pulmonary CBCT imaging quality\n",
            "enhancement technique to the whole thoracic region. Weaim to train separate customized perceptual loss functions\n",
            "and translation networks for enhancing the segmented pul-\n",
            "monary region and the rest of the thoracic region. Addi-\n",
            "tionally, we plan to test our framework with datasets cov-\n",
            "ering other body regions, such as the head and neck region\n",
            "or pelvic region.\n",
            "2. Find a better evaluation method: Metrics such as SSIM\n",
            "or PSNR are designed to evaluate pixel-to-pixel similarity.\n",
            "However, in the case of CBCT-to-CT translation, where\n",
            "the CBCT and CT pairs are mismatched and contain mis-\n",
            "aligned anatomical information, a pixel-to-pixel similarity\n",
            "metric may be inadequate to comprehensively describe the\n",
            "anatomy preservation performance of the synthesized CT\n",
            "(sCT) images from CBCT. Therefore, we plan to explore\n",
            "high-level correlations between CBCT and sCT and de-\n",
            "velop a more suitable evaluation method to quantify the\n",
            "anatomy preservation performance of sCT images from\n",
            "CBCT.\n",
            "3. Focus more on HU value correction: Our multi-task\n",
            "feature-selection network is designed to transfer CT-like\n",
            "traits from a CT self-recovery task to the autoencoder,\n",
            "which serves as a feature extraction loss network. The per-\n",
            "ceptual loss function is directly calculated from the feature\n",
            "maps extracted by this network. While our framework has\n",
            "demonstrated an overall ability to correct Hounsfield Unit\n",
            "(HU) value distributions, we need to provide more detailed\n",
            "13HU value distribution di fferences between CBCT and CT\n",
            "in our scheme. Firstly, a further manual evaluation of HU\n",
            "values should be included, considering a clinical HU range\n",
            "for di fferent tissues and organs. Secondly, we plan to fo-\n",
            "cus more on precise HU value correction by quantifying\n",
            "the HU distribution di fferences between CT and CBCT\n",
            "modalities.\n",
            "6. Conclusion\n",
            "In this study, we developed a novel deep learning framework\n",
            "for improving the quality of pulmonary CBCT imaging to CT\n",
            "level. The multi-task feature-selection network is designed to\n",
            "pretrain a customized autoencoder that serves as a perceptual\n",
            "loss building-up network with fixed parameters in the CBCT-to-\n",
            "CT translation network. Our proposed framework outperforms\n",
            "other state-of-the-art methods by generating sCT images with\n",
            "superior imaging quality. Quantitative and qualitative experi-\n",
            "ments show that the sCT images achieve superior fine anatom-\n",
            "ical details preservation and artifact suppression. In addition,\n",
            "functional imaging analysis further demonstrates the HU value\n",
            "correction performance and clinical e fficacy of our framework.\n",
            "Acknowledgements\n",
            "This research was partly supported by General Research\n",
            "Fund (15103520), the University Research Committee, Health\n",
            "and Medical Research Fund (07183266, 09200576), PolyU\n",
            "(UGC) Start-up Fund for RAPs under the Strategic Hiring\n",
            "Scheme (P0038378), PolyU (UGC) RI-IWEAR Seed Project\n",
            "(P0044802), the Health Bureau, The Government of the Hong\n",
            "Kong Special Administrative Region.\n",
            "References\n",
            "[1] W. C. Scarfe and A. G. Farman, “What is cone-beam ct and how does\n",
            "it work?” Dental Clinics of North America , vol. 52, no. 4, pp. 707–730,\n",
            "2008.\n",
            "[2] A. Harsolia, G. D. Hugo, L. L. Kestin, I. S. Grills, and D. Yan, “Dosimet-\n",
            "ric advantages of four-dimensional adaptive image-guided radiotherapy\n",
            "for lung tumors using online cone-beam computed tomography,” Interna-\n",
            "tional Journal of Radiation Oncology* Biology* Physics , vol. 70, no. 2,\n",
            "pp. 582–589, 2008.\n",
            "[3] X. Wang, J. Li, P. Wang, K. Yuan, G. Yin, and B. Wan, “Image guided\n",
            "radiation therapy boost in combination with high-dose-rate intracavitary\n",
            "brachytherapy for the treatment of cervical cancer,” Journal of Contem-\n",
            "porary Brachytherapy , vol. 8, no. 2, pp. 122–127, 2016.\n",
            "[4] I. Vergalasova and J. Cai, “A modern review of the uncertainties in vol-\n",
            "umetric imaging of respiratory-induced target motion in lung radiother-\n",
            "apy,” Medical physics , vol. 47, no. 10, pp. e988–e1008, 2020.\n",
            "[5] Y . Yang, E. Schreibmann, T. Li, C. Wang, and L. Xing, “Evaluation\n",
            "of on-board kv cone beam ct (cbct)-based dose calculation,” Physics in\n",
            "Medicine &Biology , vol. 52, no. 3, p. 685, 2007.\n",
            "[6] G. Ren, J. Zhang, T. Li, H. Xiao, L. Y . Cheung, W. Y . Ho, J. Qin, and\n",
            "J. Cai, “Deep learning-based computed tomography perfusion mapping\n",
            "(dl-ctpm) for pulmonary ct-to-perfusion translation,” International Jour-\n",
            "nal of Radiation Oncology* Biology* Physics , vol. 110, no. 5, pp. 1508–\n",
            "1518, 2021.\n",
            "[7] J. E. van Timmeren, R. T. Leijenaar, W. van Elmpt, B. Reymen, and\n",
            "P. Lambin, “Feature selection methodology for longitudinal cone-beam\n",
            "ct radiomics,” Acta oncologica , vol. 56, no. 11, pp. 1537–1543, 2017.[8] U. Stankovic, L. S. Ploeger, M. van Herk, and J.-J. Sonke, “Optimal com-\n",
            "bination of anti-scatter grids and software correction for cbct imaging,”\n",
            "Medical physics , vol. 44, no. 9, pp. 4437–4451, 2017.\n",
            "[9] A. Sisniega, W. Zbijewski, A. Badal, I. Kyprianou, J. Stayman, J. J. Va-\n",
            "quero, and J. Siewerdsen, “Monte carlo study of the e ffects of system ge-\n",
            "ometry and antiscatter grids on cone-beam ct scatter distributions,” Med-\n",
            "ical physics , vol. 40, no. 5, p. 051915, 2013.\n",
            "[10] M. Sun and J. Star-Lack, “Improved scatter correction using adaptive\n",
            "scatter kernel superposition,” Physics in Medicine &Biology , vol. 55,\n",
            "no. 22, p. 6695, 2010.\n",
            "[11] S. Zhi, M. Kachelrieß, and X. Mou, “High-quality initial image-guided\n",
            "4d cbct reconstruction,” Medical physics , vol. 47, no. 5, pp. 2099–2115,\n",
            "2020.\n",
            "[12] M. X. Zhi S, Kachelrieß M, “Spatiotemporal structure-aware dictionary\n",
            "learning-based 4d cbct reconstruction,” Medical physics , vol. 48, no. 10,\n",
            "pp. 6421–6436, 2021.\n",
            "[13] S. Zhi, M. Kachelrieß, F. Pan, and X. Mou, “Cycn-net: A convolutional\n",
            "neural network specialized for 4d cbct images refinement,” IEEE Trans-\n",
            "actions on Medical Imaging , vol. 40, no. 11, pp. 3054–3064, 2021.\n",
            "[14] D. Hu, Y . Zhang, J. Liu, Y . Zhang, J. L. Coatrieux, and Y . Chen, “Prior:\n",
            "Prior-regularized iterative optimization reconstruction for 4d cbct,” IEEE\n",
            "Journal of Biomedical and Health Informatics , 2022.\n",
            "[15] H. Zhang, J. Ma, Z. Bian, D. Zeng, Q. Feng, and W. Chen, “High quality\n",
            "4d cone-beam ct reconstruction using motion-compensated total variation\n",
            "regularization,” Physics in Medicine &Biology , vol. 62, no. 8, p. 3313,\n",
            "2017.\n",
            "[16] S. Kida, T. Nakamoto, M. Nakano, K. Nawa, A. Haga, J. Kotoku,\n",
            "H. Yamashita, and K. Nakagawa, “Cone beam computed tomography\n",
            "image quality improvement using a deep convolutional neural network,”\n",
            "CUREUS , vol. 10, no. 4, APR 2018.\n",
            "[17] A. Barateau, R. De Crevoisier, A. Largent, E. Mylona, N. Perichon,\n",
            "J. Castelli, E. Chajon, O. Acosta, A. Simon, J.-C. Nunes et al. , “Com-\n",
            "parison of cbct-based dose calculation methods in head and neck cancer\n",
            "radiotherapy: from hounsfield unit to density calibration curve to deep\n",
            "learning,” Medical physics , vol. 47, no. 10, pp. 4683–4693, 2020.\n",
            "[18] C. Kurz, M. Maspero, M. H. Savenije, G. Landry, F. Kamp, M. Pinto,\n",
            "M. Li, K. Parodi, C. Belka, and C. A. Van den Berg, “Cbct correction us-\n",
            "ing a cycle-consistent generative adversarial network and unpaired train-\n",
            "ing to enable photon and proton dose calculation,” Physics in Medicine &\n",
            "Biology , vol. 64, no. 22, p. 225004, 2019.\n",
            "[19] M. Eckl, L. Hoppen, G. R. Sarria, J. Boda-Heggemann, A. Simeonova-\n",
            "Chergou, V . Steil, F. A. Giordano, and J. Fleckenstein, “Evaluation of\n",
            "a cycle-generative adversarial network-based cone-beam ct to synthetic\n",
            "ct conversion algorithm for adaptive radiation therapy,” Physica Medica ,\n",
            "vol. 80, pp. 308–316, 2020.\n",
            "[20] Y . Lei, T. Wang, J. Harms, G. Shafai-Erfani, X. Dong, J. Zhou, P. Pa-\n",
            "tel, X. Tang, T. Liu, W. J. Curran et al. , “Image quality improvement in\n",
            "cone-beam ct using deep learning,” in Medical Imaging 2019: Physics of\n",
            "Medical Imaging , vol. 10948. SPIE, 2019, pp. 556–561.\n",
            "[21] S. Kida, S. Kaji, K. Nawa, T. Imae, T. Nakamoto, S. Ozaki, T. Ohta,\n",
            "Y . Nozawa, and K. Nakagawa, “Visual enhancement of cone-beam ct\n",
            "by use of cyclegan,” Medical Physics , vol. 47, no. 3, pp. 998–1010,\n",
            "2020. [Online]. Available: https: //aapm.onlinelibrary.wiley.com /doi/abs/\n",
            "10.1002 /mp.13963\n",
            "[22] R. L. Qiu, Y . Lei, J. Shelton, K. Higgins, J. D. Bradley, W. J. Curran,\n",
            "T. Liu, A. H. Kesarwala, and X. Yang, “Deep learning-based thoracic cbct\n",
            "correction with histogram matching,” Biomedical Physics &Engineering\n",
            "Express , vol. 7, no. 6, p. 065040, 2021.\n",
            "[23] L. Gao, K. Xie, J. Sun, T. Lin, J. Sui, G. Yang, and X. Ni, “Streaking\n",
            "artifact reduction for cbct-based synthetic ct generation in adaptive radio-\n",
            "therapy,” Medical Physics , 2022.\n",
            "[24] Y . Lou, T. Niu, X. Jia, P. A. Vela, L. Zhu, and A. R. Tannenbaum, “Joint\n",
            "ct/cbct deformable registration and cbct enhancement for cancer radio-\n",
            "therapy,” Medical image analysis , vol. 17, no. 3, pp. 387–400, 2013.\n",
            "[25] M. Chung, J. Lee, W. Song, Y . Song, I.-H. Yang, J. Lee, and Y .-G. Shin,\n",
            "“Automatic registration between dental cone-beam ct and scanned surface\n",
            "via deep pose regression neural networks and clustered similarities,” IEEE\n",
            "Transactions on Medical Imaging , vol. 39, no. 12, pp. 3900–3909, 2020.\n",
            "[26] S. Kaji and S. Kida, “Overview of image-to-image translation by use of\n",
            "deep neural networks: denoising, super-resolution, modality conversion,\n",
            "and reconstruction in medical imaging,” Radiological physics and tech-\n",
            "14nology , vol. 12, no. 3, pp. 235–248, 2019.\n",
            "[27] D. Nie, R. Trullo, J. Lian, C. Petitjean, S. Ruan, Q. Wang, and D. Shen,\n",
            "“Medical image synthesis with context-aware generative adversarial net-\n",
            "works,” in International conference on medical image computing and\n",
            "computer-assisted intervention . Springer, 2017, pp. 417–425.\n",
            "[28] E. Richardson, Y . Alaluf, O. Patashnik, Y . Nitzan, Y . Azar, S. Shapiro, and\n",
            "D. Cohen-Or, “Encoding in style: A stylegan encoder for image-to-image\n",
            "translation,” in Proceedings of the IEEE /CVF Conference on Computer\n",
            "Vision and Pattern Recognition (CVPR) , June 2021, pp. 2287–2296.\n",
            "[29] D. Tomar, L. Zhang, T. Portenier, and O. Goksel, “Content-preserving\n",
            "unpaired translation from simulated to realistic ultrasound images,” in\n",
            "Medical Image Computing and Computer Assisted Intervention – MIC-\n",
            "CAI 2021 , M. de Bruijne, P. C. Cattin, S. Cotin, N. Padoy, S. Speidel,\n",
            "Y . Zheng, and C. Essert, Eds. Cham: Springer International Publishing,\n",
            "2021, pp. 659–669.\n",
            "[30] J. Johnson, A. Alahi, and F. Li, “Perceptual losses for real-time style\n",
            "transfer and super-resolution,” 03 2016.\n",
            "[31] Q. Yang, P. Yan, Y . Zhang, H. Yu, Y . Shi, X. Mou, M. K. Kalra, Y . Zhang,\n",
            "L. Sun, and G. Wang, “Low-dose ct image denoising using a generative\n",
            "adversarial network with wasserstein distance and perceptual loss,” IEEE\n",
            "transactions on medical imaging , vol. 37, no. 6, pp. 1348–1357, 2018.\n",
            "[32] G. Lazaridis, M. Lorenzi, S. Ourselin, and D. Garway-Heath,\n",
            "“Improving statistical power of glaucoma clinical trials using an\n",
            "ensemble of cyclical generative adversarial networks,” Medical Image\n",
            "Analysis , vol. 68, p. 101906, 2021. [Online]. Available: https:\n",
            "//www.sciencedirect.com /science /article /pii/S136184152030270X\n",
            "[33] M. Ran, J. Hu, Y . Chen, H. Chen, H. Sun, J. Zhou, and\n",
            "Y . Zhang, “Denoising of 3d magnetic resonance images using a residual\n",
            "encoder–decoder wasserstein generative adversarial network,” Medical\n",
            "Image Analysis , vol. 55, pp. 165–180, 2019. [Online]. Available:\n",
            "https: //www.sciencedirect.com /science /article /pii/S1361841518306534\n",
            "[34] M. Li, W. Hsu, X. Xie, J. Cong, and W. Gao, “Sacnn: Self-attention con-\n",
            "volutional neural network for low-dose ct denoising with self-supervised\n",
            "perceptual loss network,” IEEE transactions on medical imaging , vol. 39,\n",
            "no. 7, pp. 2289–2301, 2020.\n",
            "[35] C. Shi and C.-M. Pun, “Adaptive multi-scale deep neural networks with\n",
            "perceptual loss for panchromatic and multispectral images classification,”\n",
            "Information Sciences , vol. 490, pp. 1–17, 2019. [Online]. Available:\n",
            "https: //www.sciencedirect.com /science /article /pii/S0020025519302683\n",
            "[36] Y . Zhang and Q. Yang, “An overview of multi-task learning,” National\n",
            "Science Review , vol. 5, no. 1, pp. 30–43, 2018.\n",
            "[37] S. Ruder, “An overview of multi-task learning in deep neural networks,”\n",
            "arXiv preprint arXiv:1706.05098 , 2017.\n",
            "[38] M. A. Al-Antari, M. A. Al-Masni, M.-T. Choi, S.-M. Han, and T.-S.\n",
            "Kim, “A fully integrated computer-aided diagnosis system for digital x-\n",
            "ray mammograms via deep learning detection, segmentation, and classifi-\n",
            "cation,” International journal of medical informatics , vol. 117, pp. 44–54,\n",
            "2018.\n",
            "[39] S. Kyung, K. Shin, H. Jeong, K. D. Kim, J. Park, K. Cho, J. H. Lee,\n",
            "G. Hong, and N. Kim, “Improved performance and robustness of multi-\n",
            "task representation learning with consistency loss between pretexts for\n",
            "intracranial hemorrhage identification in head ct,” Medical Image Analy-\n",
            "sis, vol. 81, p. 102489, 2022.\n",
            "[40] A. Boutillon, P.-H. Conze, C. Pons, V . Burdin, and B. Borotikar, “Gen-\n",
            "eralizable multi-task, multi-domain deep segmentation of sparse pediatric\n",
            "imaging datasets via multi-scale contrastive regularization and multi-joint\n",
            "anatomical priors,” Medical Image Analysis , vol. 81, p. 102556, 2022.\n",
            "[41] B. Lei, N. Cheng, A. F. Frangi, Y . Wei, B. Yu, L. Liang, W. Mai, G. Duan,\n",
            "X. Nong, C. Li, J. Su, T. Wang, L. Zhao, D. Deng, and Z. Zhang,\n",
            "“Auto-weighted centralised multi-task learning via integrating functional\n",
            "and structural connectivity for subjective cognitive decline diagnosis,”\n",
            "Medical Image Analysis , vol. 74, p. 102248, 2021. [Online]. Available:\n",
            "https: //www.sciencedirect.com /science /article /pii/S1361841521002930\n",
            "[42] K. G. Lore, A. Akintayo, and S. Sarkar, “Llnet: A deep autoencoder\n",
            "approach to natural low-light image enhancement,” Pattern Recognition ,\n",
            "vol. 61, pp. 650–662, 2017.\n",
            "[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\n",
            "recognition,” in Proceedings of the IEEE conference on computer vision\n",
            "and pattern recognition , 2016, pp. 770–778.\n",
            "[44] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V . Dalca,\n",
            "“V oxelmorph: a learning framework for deformable medical image regis-tration,” IEEE transactions on medical imaging , vol. 38, no. 8, pp. 1788–\n",
            "1800, 2019.\n",
            "[45] X. Hu, M. Kang, W. Huang, M. R. Scott, R. Wiest, and M. Reyes,\n",
            "“Dual-stream pyramid registration network,” in International Confer-\n",
            "ence on Medical Image Computing and Computer-Assisted Intervention .\n",
            "Springer, 2019, pp. 382–390.\n",
            "[46] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\n",
            "A large-scale hierarchical image database,” in 2009 IEEE Conference on\n",
            "Computer Vision and Pattern Recognition , 2009, pp. 248–255.\n",
            "[47] Z. Chen, V . Badrinarayanan, C.-Y . Lee, and A. Rabinovich, “Gradnorm:\n",
            "Gradient normalization for adaptive loss balancing in deep multitask net-\n",
            "works,” in International conference on machine learning . PMLR, 2018,\n",
            "pp. 794–803.\n",
            "[48] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\n",
            "for biomedical image segmentation,” in International Conference on\n",
            "Medical image computing and computer-assisted intervention . Springer,\n",
            "2015, pp. 234–241.\n",
            "[49] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in Pro-\n",
            "ceedings of the IEEE conference on computer vision and pattern recogni-\n",
            "tion, 2018, pp. 7132–7141.\n",
            "15Multi-task Representation Learning for Pure\n",
            "Exploration in Bilinear Bandits\n",
            "Subhojyoti Mukherjee\n",
            "ECE Department\n",
            "UW-Madison\n",
            "Wisconsin, Madison\n",
            "smukherjee27@wisc.eduQiaomin Xie\n",
            "ISyE Department\n",
            "UW-Madison\n",
            "Wisconsin, MadisonJosiah P. Hanna\n",
            "CS Department\n",
            "UW-Madison\n",
            "Wisconsin, Madison\n",
            "Robert Nowak\n",
            "ECE Department\n",
            "UW-Madison\n",
            "Wisconsin, Madison\n",
            "Abstract\n",
            "We study multi-task representation learning for the problem of pure exploration\n",
            "in bilinear bandits. In bilinear bandits, an action takes the form of a pair of arms\n",
            "from two different entity types and the reward is a bilinear function of the known\n",
            "feature vectors of the arms. In the multi-task bilinear bandit problem , we aim\n",
            "to find optimal actions for multiple tasks that share a common low-dimensional\n",
            "linear representation. The objective is to leverage this characteristic to expedite the\n",
            "process of identifying the best pair of arms for all tasks. We propose the algorithm\n",
            "GOBLIN that uses an experimental design approach to optimize sample allocations\n",
            "for learning the global representation as well as minimize the number of samples\n",
            "needed to identify the optimal pair of arms in individual tasks. To the best of\n",
            "our knowledge, this is the first study to give sample complexity analysis for pure\n",
            "exploration in bilinear bandits with shared representation. Our results demonstrate\n",
            "that by learning the shared representation across tasks, we achieve significantly\n",
            "improved sample complexity compared to the traditional approach of solving tasks\n",
            "independently.\n",
            "1 Introduction\n",
            "Bilinear bandits (Jun et al., 2019; Lu et al., 2021; Kang et al., 2022) are an important class of\n",
            "sequential decision-making problems. In bilinear bandits (as opposed to the standard linear bandit\n",
            "setting) we are given a pair of arms xt∈Rd1andzt∈Rd2at every round tand the interaction\n",
            "of this pair of arms with a low-rank hidden parameter, Θ∗∈Rd1×d2generates the noisy feedback\n",
            "(reward) rt=x⊤\n",
            "tΘ∗zt+ηt. The ηtis random 1-subGaussian noise.\n",
            "A lot of real-world applications exhibit the above bilinear feedback structure, particularly applications\n",
            "that involve selecting pairs of items and evaluating their compatibility. For example, in a drug\n",
            "discovery application, scientists may want to determine whether a particular (drug, protein) pair\n",
            "interacts in the desired way (Luo et al., 2017). Likewise, an online dating service might match a pair\n",
            "of people and gather feedback about their compatibility (Shen et al., 2023). A clothing website’s\n",
            "recommendation system may suggest a pair of items (top, bottom) for a customer based on their\n",
            "likelihood of matching (Reyes et al., 2021). In all of these scenarios, the two items are considered as\n",
            "a single unit, and the system must utilize available feature vectors ( xt,zt) to learn which features of\n",
            "the pairs are most indicative of positive feedback in order to make effective recommendations. All\n",
            "the previous works in this setting (Jun et al., 2019; Lu et al., 2021; Kang et al., 2022) exclusively\n",
            "37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2311.00327v1  [cs.LG]  1 Nov 2023focused on maximizing the number of pairs with desired interactions discovered over time (regret\n",
            "minimization). However, in many real-world applications where obtaining a sample is expensive\n",
            "and time-consuming, e.g., clinical trials (Zhao et al., 2009; Zhang et al., 2012), it is often desirable\n",
            "to identify the optimal option using as few samples as possible, i.e., we face the pure exploration\n",
            "scenario (Fiez et al., 2019; Katz-Samuels et al., 2020) rather than regret minimization.\n",
            "Moreover, in various decision-making scenarios, we may encounter multiple interrelated tasks such\n",
            "as treatment planning for different diseases (Bragman et al., 2018) and content optimization for\n",
            "multiple websites (Agarwal et al., 2009). Often, there exists a shared representation among these\n",
            "tasks, such as the features of drugs or the representations of website items. Therefore, we can\n",
            "leverage this shared representation to accelerate learning. This area of research is called multi-task\n",
            "representation learning and has recently generated a lot of attention in machine learning (Bengio\n",
            "et al., 2013; Li et al., 2014; Maurer et al., 2016; Du et al., 2020; Tripuraneni et al., 2021). There\n",
            "are many applications of this multi-task representation learning in real-world settings. For instance,\n",
            "in clinical treatment planning, we seek to determine the optimal treatments for multiple diseases,\n",
            "and there may exist a low-dimensional representation common to multiple diseases. To avoid the\n",
            "time-consuming process of conducting clinical trials for individual tasks and collecting samples, we\n",
            "utilize the shared representation and decrease the number of required samples.\n",
            "The above multi-task representation learning naturally shows up in bilinear bandit setting as follows:\n",
            "Let there be Mtasks indexed as m= 1,2, . . . , M with each task having its own hidden parameter\n",
            "Θm,∗∈Rd1×d2. Let each Θm,∗has a decomposition of Θm,∗=B1Sm,∗B⊤\n",
            "2, where B1∈Rd1×k1\n",
            "andB2∈Rd2×k2are shared across tasks, but Sm,∗∈Rk1×k2is specific for task m. We assume\n",
            "thatk1, k2≪d1, d2andM≫d1, d2. Thus, B1andB2provide a means of dimensionality\n",
            "reduction. Furthermore, we assume that each Sm,∗has rank r≪min{k1, k2}. In the terminology of\n",
            "multi-task representation learning B1,B2are called feature extractors andxm,t,zm,tare called rich\n",
            "observations (Yang et al., 2020, 2022; Du et al., 2023). The reward for the task m∈ {1,2, . . . , M }\n",
            "at round tis\n",
            "rm,t=x⊤\n",
            "m,tΘm,∗zm,t+ηm,t=x⊤\n",
            "m,tB1|{z}\n",
            "g⊤\n",
            "m,tSm,∗B⊤\n",
            "2zm,t|{z}\n",
            "vm,t+ηm,t=g⊤\n",
            "m,tSm,∗vm,t+ηm,t.(1)\n",
            "Observe that similar to the learning procedure in Yang et al. (2020, 2022), at each round t= 1,2,···,\n",
            "for each task m∈[M], the learner selects a left and right action xm,t∈ X andzm,t∈ Z. After the\n",
            "player commits the batch of actions for each task {xm,t,zm,t:m∈[M]}, it receives the batch of\n",
            "rewards {rm,t:m∈[M]}. Also note that in (1)we define the egm,t∈Rk1,evm,t∈Rk2as the latent\n",
            "features, and both egm,t,evm,tare unknown to the learner and needs to be learned for each task m\n",
            "(hence the name multi-task representation learning).\n",
            "In this paper, we focus on pure exploration for multi-task representation learning in bilinear bandits\n",
            "where the goal is to find the optimal left arm xm,∗and right arm zm,∗for each task mwith a minimum\n",
            "number of samples (fixed confidence setting). First, consider a single-task setting and let Θ∗have\n",
            "low rank r. Let the SVD of the Θ∗=UDV⊤. Prima-facie, if UandVare known then one might\n",
            "want to project all the left and right arms in the r×rsubspace of UandVand reduce the bilinear\n",
            "bandit problem into a r2dimension linear bandit setting. Then one can apply one of the algorithms\n",
            "from Soare et al. (2014); Fiez et al. (2019); Katz-Samuels et al. (2020) to solve this r2dimensional\n",
            "linear bandit pure exploration problem. Following the analysis of this line of work (in linear bandits)\n",
            "(Mason et al., 2021; Mukherjee et al., 2022, 2023) one might conjecture that a sample complexity\n",
            "bound of eO(r2/∆2)is possible where ∆is the minimum reward gap and eO(·)hides log factors.\n",
            "Similarly, for the multi-task setting one might be tempted to use the linear bandit analysis of Du et al.\n",
            "(2023) to convert this problem into Mconcurrent r2dimensional linear bandit problems with shared\n",
            "representation and achieve a sample complexity bound of eO(Mr2/∆2). However, these matrices\n",
            "(subspaces) are unknown and so there is a model mismatch as noted in the regret analysis of bilinear\n",
            "bandits (Jun et al., 2019; Lu et al., 2021; Kang et al., 2022). Thus it is difficult to apply the r2\n",
            "dimensional linear bandit sample complexity analysis. Following the regret analysis of bilinear bandit\n",
            "setting by Jun et al. (2019); Lu et al. (2021); Kang et al. (2022) we know that the effective dimension\n",
            "is actually (d1+d2)r. Similarly for the multi-task representation learning the effective dimension\n",
            "should scale with the learned latent features (k1+k2)r. Hence the natural questions to ask are these:\n",
            "1) Can we design a single-task pure exploration bilinear bandit algorithm whose\n",
            "sample complexity scales as eO((d1+d2)r/∆2)?\n",
            "22) Can we design an algorithm for multi-task pure exploration bilinear ban-\n",
            "dit problem that can learn the latent features and has sample complexity that scales as\n",
            "eO(M(k1+k2)r/∆2)?\n",
            "In this paper, we answer both these questions affirmatively. In doing so, we make the following novel\n",
            "contributions to the growing literature of multi-task representation learning in online settings:\n",
            "1)We formulate the multi-task bilinear representation learning problem. To our knowledge, this is\n",
            "the first work that explores pure exploration in a multi-task bilinear representation learning setting.\n",
            "2)We proposed the algorithm GOBLIN for a single-task pure exploration bilinear bandit setting\n",
            "whose sample complexity scales as eO((d1+d2)r/∆2). This improves over RAGE (Fiez et al., 2019)\n",
            "whose sample complexity scales as eO((d1d2)/∆2).\n",
            "3)Our algorithm GOBLIN for multi-task pure exploration bilinear bandit problem learns the latent\n",
            "features and has sample complexity that scales as eO(M(k1+k2)r/∆2). This improves over\n",
            "DouExpDes (Du et al., 2023) whose samples complexity scales as eO(M(k1k2)/∆2).\n",
            "Preliminaries: We assume that ∥x∥2≤1,∥z∥2≤1,∥Θ∗∥F≤S0and the r-th largest\n",
            "singular value of Θ∗∈Rd1×d2isSr. Let p:=d1d2denote the ambient dimension, and\n",
            "k= (d1+d2)rdenote the effective dimension. Let [n]:={1,2, . . . , n }. Let x∗,z∗:=\n",
            "arg maxx,zx⊤Θ∗z. For any x,zdefine the gap ∆(x,z):=x⊤\n",
            "∗Θ∗z∗−x⊤Θ∗zand further-\n",
            "more ∆ = min x̸=x∗,z̸=z∗∆(x,z). Similarly, for any arbitrary vector w∈ W define the gap\n",
            "ofw∈Rpas∆(w):= (w∗−w)⊤θ∗, for some θ∗∈Rpand furthermore, ∆ = min w̸=w∗∆(w).\n",
            "IfA∈Rd×d\n",
            "≥0is a positive semidefinite matrix, and w∈Rpis a vector, let ∥w∥2\n",
            "A:=w⊤Aw\n",
            "denote the induced semi-norm. Given any vector b∈R|W|we denote the w-th component as\n",
            "bw. Let ∆W:=\b\n",
            "b∈R|W|:bw≥0,P\n",
            "w∈Wbw= 1\t\n",
            "denote the set of probability distributions\n",
            "onW. We define Y(W) ={w−w′:∀w,w′∈ W,w̸=w′}as the directions obtained from the\n",
            "differences between each pair of arms and Y∗(W) ={w∗−w:∀w∈ W\\ w∗}as the directions\n",
            "obtained from the differences between the optimal arm and each suboptimal arm.\n",
            "2 Pure Exploration in Single-Task Bilinear Bandits\n",
            "In this section, we consider pure exploration in a single-task bilinear bandit setting as a warm-up to\n",
            "the main goal of learning representations for the multi-task bilinear bandit. To our knowledge, this\n",
            "is the first study of pure exploration in single-task bilinear bandits. We first recall the single-task\n",
            "bilinear bandit setting as follows: At every round t= 1,2, . . . the learner observes the reward\n",
            "rt=x⊤\n",
            "tΘ∗zt+ηtwhere the low rank hidden parameter Θ∗∈Rd1×d2is unknown to the learner,\n",
            "xt∈Rd1,zt∈Rd2are visible to the learner, and ηtis a1-sub-Gaussian noise. We assume that the\n",
            "matrix Θ∗has a low rank rwhich is known to the learner and d1, d2≫r. Finally recall that the goal\n",
            "is to identify the optimal left and right arms x∗,z∗with a minimum number of samples.\n",
            "We propose a phase-based, two-stage arm elimination algorithm called G-Optimal Design for Bilinear\n",
            "Bandits (abbreviated as GOBLIN). GOBLIN proceeds in phases indexed by ℓ= 1,2, . . . As this\n",
            "is a pure-exploration problem, the total number of samples is controlled by the total phases which\n",
            "depends on the intrinsic problem complexity. Each phase ℓof GOBLIN consists of two stages; the\n",
            "estimation of Θ∗stage, which runs for τE\n",
            "ℓrounds, and pure exploration in rotated arms stage that\n",
            "runs for τG\n",
            "ℓrounds. We will define τE\n",
            "ℓin Section 2.1, while rotated arms and τG\n",
            "ℓare defined in\n",
            "Section 2.2. At the end of every phase, GOBLIN eliminates sub-optimal arms to build the active\n",
            "set for the next phase and stops when only the optimal left and right arms are remaining. Now we\n",
            "discuss the individual stages that occur at every phase ℓof GOBLIN.\n",
            "2.1 Estimating Subspaces of Θ∗(Stage 1 of the ℓ-th phase)\n",
            "In the first stage of phase ℓ, GOBLIN estimates the row and column sub-spaces Θ∗. Then GOBLIN\n",
            "uses these estimates to reduce the bilinear bandit problem in the original ambient dimension p:=\n",
            "d1d2to a lower effective dimension k:= (d1+d2)r. To do this, GOBLIN first vectorizes the\n",
            "x∈Rd1,z∈Rd2into a new vector w∈Rpand then solves the E-optimal design in Step 3of\n",
            "3Algorithm 1 (Pukelsheim, 2006; Jun et al., 2019; Du et al., 2023). Let the solution to the E-optimal\n",
            "design problem at the stage 1ofℓ-th phase be denoted by bE\n",
            "ℓ. Then GOBLIN samples each wfor\n",
            "⌈τE\n",
            "ℓbE\n",
            "ℓ,w⌉times, where τE\n",
            "ℓ=eO(√d1d2r/Sr)(step 7of Algorithm 1). In this paper, we sample an\n",
            "arm⌈τE\n",
            "ℓbE\n",
            "ℓ,w⌉number of times. However, this may lead to over-sampling of an arm than what the\n",
            "design ( GorE-optimal) is actually suggesting. However, we can match the number of allocations\n",
            "of an arm to the design using an efficcient Rounding Procedures (see Pukelsheim (2006); Fiez et al.\n",
            "(2019)). Let bΘℓbe estimate of Θ∗in stage 1of phase ℓ. GOBLIN estimates this by solving the\n",
            "following well-defined regularized minimization problem with nuclear norm penalty:\n",
            "bΘℓ= arg min\n",
            "Θ∈Rd1×d2Lℓ(Θ) +γℓ∥Θ∥nuc, L ℓ(Θ) =⟨Θ,Θ⟩ −2\n",
            "τE\n",
            "ℓτE\n",
            "ℓX\n",
            "s=1⟨eψν(rs·Q(xsz⊤\n",
            "s)),Θ⟩(2)\n",
            "where Q(·),eψν(·), are appropriate functions stated in Definition 1, 3 respectively in Appendix A.3.\n",
            "TheQ(·)function takes as input the rank-one matrix xsz⊤\n",
            "swhich is obtained after reshaping ws.\n",
            "Note that xs, andzsare the observed vectors in d1andd2dimension and bΘℓ∈Rd1×d2Finally, set\n",
            "the regularization parameter γℓ:= 4r\n",
            "2(4+S2\n",
            "0)Cd1d2log(2( d1+d2)/δ)\n",
            "τE\n",
            "ℓ. This is in step 8of Algorithm 1.\n",
            "2.2 Optimal Design for Rotated Arms (Stage 2 of ℓ-th phase)\n",
            "In stage 2of phase ℓ, GOBLIN leverages the information about the learned sub-space of Θ∗to rotate\n",
            "the arm set and then run the optimal design on the rotated arm set. Once we recover bΘℓ, one might\n",
            "be tempted to run a pure exploration algorithm (Soare et al., 2014; Fiez et al., 2019; Katz-Samuels\n",
            "et al., 2020; Zhu et al., 2021) to identify x∗andz∗. However, then the sample complexity will scale\n",
            "withd1d2. In contrast GOBLIN uses the information about the learned sub-space of Θ∗to reduce the\n",
            "problem from ambient dimension d1d2to effective dimension (d1+d2)r. This reduction is done as\n",
            "follows: Let bΘℓ=bUℓbDℓbV⊤\n",
            "ℓbe the SVD of bΘℓin the ℓ-th phase. Let bUℓ\n",
            "⊥andbVℓ\n",
            "⊥be orthonormal\n",
            "bases of the complementary subspaces of bUℓandbVℓrespectively. Let XℓandZℓbe the active set of\n",
            "arms in the stage 2of phase ℓ. Then rotate the arm sets such that new rotated arm sets are as follows:\n",
            "Xℓ={x= [bUℓbU⊥\n",
            "ℓ]⊤x|x∈ Xℓ},Zℓ={z= [bVℓbV⊥\n",
            "ℓ]⊤z|z∈ Zℓ}. (3)\n",
            "LetbHℓ= [bUℓbU⊥\n",
            "ℓ]⊤bΘℓ[bVℓbV⊥\n",
            "ℓ]. Then define vectorized arm set so that the last (d1−r)·(d2−r)\n",
            "components are from the complementary subspaces as follows:\n",
            "Wℓ=\b\u0002\n",
            "vec\u0000\n",
            "x1:rz⊤\n",
            "1:r\u0001\n",
            ";vec\u0000\n",
            "xr+1:d1z⊤\n",
            "1:r\u0001\n",
            ";vec\u0000\n",
            "x1:rz⊤\n",
            "r+1:d2\u0001\n",
            ";\n",
            "vec\u0000\n",
            "xr+1:d1z⊤\n",
            "r+1:d2\u0001\u0003\n",
            "∈Rd1d2:x∈ Xℓ,z∈ Zℓ\t\n",
            "bθℓ,1:k= [vec(bHℓ,1:r,1:r);vec(bHℓ,r+1:d1,1:r);vec(bHℓ,1:r,r+1:d2)],\n",
            "bθℓ,k+1:p=vec(bHℓ,r+1:d1,r+1:d2). (4)\n",
            "which implies ∥bθk+1:p∥2=O\u0000\n",
            "d1d2r/τE\n",
            "ℓ\u0001\n",
            "by Lemma 3 in Appendix A.1. So the last p−k\n",
            "components of bθℓare very small compared to the first kcomponents. Hence, GOBLIN has now\n",
            "reduced the d1d2dimensional linear bandit to (d1+d2)rdimensional linear bandit using (3),(4).\n",
            "This is shown in step 10of Algorithm 1.\n",
            "Now in stage 2of phase ℓ, GOBLIN implements G-optimal design (Pukelsheim, 2006; Fiez et al.,\n",
            "2019) in the rotated arm set Xℓ,Zℓdefined in (3). To do this, first GOBLIN defines the rotated vector\n",
            "w= [x1:d1;z1:d2]∈Rpthat belong to the set Wℓ. Then GOBLIN solves the G-optimal design\n",
            "(Pukelsheim, 2006) as follows:\n",
            "bbG\n",
            "ℓ= arg min\n",
            "bwmax\n",
            "w,w′∈Wℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λℓ/n)−1. (5)\n",
            "This is shown in step 11of Algorithm 1 and Λℓis defined in (6). It can be shown that sampling ac-\n",
            "cording to bbG\n",
            "ℓleads to the optimal sample complexity. This is discussed in Remark 1 in Appendix A.2.\n",
            "The key point to note from (5)is that due to the estimation in the rotated arm space Wℓwe are\n",
            "guaranteed that the support of supp (bbG\n",
            "ℓ)≤eO(k(k+ 1)/2)(Pukelsheim, 2006). On the other hand,\n",
            "4if the G-optimal design of Fiez et al. (2019); Katz-Samuels et al. (2020) are run in d1d2dimension\n",
            "then the support of bbG\n",
            "ℓwill scale with d1d2which will lead to higher sample complexity. Then\n",
            "GOBLIN samples each w∈ Wℓfor⌈τG\n",
            "ℓbG\n",
            "ℓ,w⌉times, where τG\n",
            "ℓ:=⌈8Bℓ\n",
            "∗ρG(Y(Wℓ)) log(4 ℓ2|W|/δ)\n",
            "ϵ2\n",
            "ℓ⌉.\n",
            "Note that the total length of phase ℓ, combining stages 1and2is(τE\n",
            "ℓ+τG\n",
            "ℓ)rounds. Observe that the\n",
            "stage 1design is on the whole arm set Wwhereas stage 2design is on the refined active set Wℓ.\n",
            "Let the observed features in stage 2of phase ℓbe denoted by Wℓ∈RτG\n",
            "ℓ×p, andrℓ∈RτG\n",
            "ℓbe the\n",
            "observed rewards. Define the diagonal matrix Λℓas\n",
            "Λℓ=diag[λ, . . . , λ|{z}\n",
            "k, λ⊥\n",
            "ℓ, . . . , λ⊥\n",
            "ℓ|{z}\n",
            "p−k] (6)\n",
            "where, λ⊥\n",
            "ℓ:=τG\n",
            "ℓ−1/8klog(1 + τG\n",
            "ℓ−1/λ)≫λ. Deviating from Soare et al. (2014); Fiez et al. (2019)\n",
            "GOBLIN constructs a regularized least square estimator at phase ℓas follows\n",
            "bθℓ= arg min\n",
            "θ∈Rp1\n",
            "2∥Wℓθ−rℓ∥2\n",
            "2+1\n",
            "2∥θ∥2\n",
            "Λℓ. (7)\n",
            "This regularized least square estimator in (7)forces the last p−kcomponents of bθℓto be very small\n",
            "compared to the first kcomponents. Then GOBLIN builds the estimate bθℓfrom (7)only from the\n",
            "observations from this phase (step 13in Algorithm 1) and eliminates sub-optimal actions in step 14\n",
            "in Algorithm 1 using the estimator bθℓ. Finally GOBLIN eliminates sub-optimal arms to build the\n",
            "next phase active set Wℓand stops when |Wℓ|= 1. GOBLIN outputs the arm in Wℓand reshapes it\n",
            "to get the bx∗andbz∗. The full pseudocode is presented in Algorithm 1.\n",
            "Algorithm 1 G-Optimal Design for Bilinear Bandits (GOBLIN) for single-task setting\n",
            "1:Input: arm set X,Z, confidence δ, rank rofΘ∗, spectral bound SrofΘ∗,S, S⊥\n",
            "ℓ:=\n",
            "8d1d2r\n",
            "τE\n",
            "ℓS2rlog\u0010\n",
            "d1+d2\n",
            "δℓ\u0011\n",
            ", λ, λ⊥\n",
            "ℓ:=τG\n",
            "ℓ−1/8(d1+d2)rlog(1+τG\n",
            "ℓ−1\n",
            "λ). Letp:=d1d2,k:= (d1+d2)r.\n",
            "2:LetW1←W, ℓ←1,τG\n",
            "0:= log(4 ℓ2|X|/δ). Define Λℓas in (6), Bℓ\n",
            "∗:= (8√\n",
            "λS+q\n",
            "λ⊥\n",
            "ℓS⊥\n",
            "ℓ).\n",
            "3:Define a vectorized arm w:= [x1:d1;z1:d2]andw∈W. LetτE\n",
            "ℓ:=√\n",
            "8d1d2rlog(4ℓ2|W|/δℓ)\n",
            "Sr. Let\n",
            "theE-optimal design be bE\n",
            "ℓ:= arg minb∈△W\r\r\u0000P\n",
            "w∈Wbwww⊤\u0001−1\r\r.\n",
            "4:while|Wℓ|>1do\n",
            "5: ϵℓ= 2−ℓ,δℓ=δ/ℓ2.\n",
            "6: (Stage 1:) Explore the Low-Rank Subspace\n",
            "7: Pull arm w∈Wexactlyl\n",
            "bbE\n",
            "ℓ,wτE\n",
            "ℓm\n",
            "times and observe rewards rt, fort= 1, . . . , τE\n",
            "ℓ.\n",
            "8: Compute bΘℓusing (2).\n",
            "9: (Stage 2:) Reduction to low dimensional linear bandits\n",
            "10: Let the SVD of bΘℓ=bUℓbDℓbV⊤\n",
            "ℓ. Rotate arms in active set Wℓ−1to build Wℓfollowing (4).\n",
            "11: LetbbG\n",
            "ℓ:= arg minbwmaxw,w′∈Wℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λℓ/n)−1.\n",
            "12: Define ρG(Y(Wℓ)):= min bwmaxw,w′∈Wℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λℓ/n)−1.\n",
            "13: SetτG\n",
            "ℓ:=⌈64Bℓ\n",
            "∗ρG(Y(Wℓ)) log(4 ℓ2|W|/δℓ)\n",
            "ϵ2\n",
            "ℓ⌉. Then pull arm w∈ W exactlyl\n",
            "bbG\n",
            "ℓ,wτG\n",
            "ℓm\n",
            "times\n",
            "and construct the least squares estimator bθℓusing only the observations of this phase where\n",
            "bθℓis defined in (7). Note that bθℓis also rotated following (4).\n",
            "14: Eliminate arms such that Wℓ+1← Wℓ\\{w∈ Wℓ: max w′∈Wℓ⟨w′−w,bθℓ⟩>2ϵℓ}\n",
            "15: ℓ←ℓ+ 1\n",
            "16:Output the arm in Wℓand reshape to get the bx∗andbz∗\n",
            "2.3 Sample Complexity Analysis of Single-Task GOBLIN\n",
            "We now analyze the sample complexity of GOBLIN in the single-task setting through the following\n",
            "theorem.\n",
            "5Theorem 1. (informal) With probability at least 1−δ, GOBLIN returns the best arms x∗,z∗, and\n",
            "the number of samples used is bounded by eO\u0010\n",
            "(d1+d2)r\n",
            "∆2+√d1d2r\n",
            "Sr\u0011\n",
            ".\n",
            "Discussion 1. In Theorem 1 the first quantity is the number of samples needed to identify the best\n",
            "armsx∗,z∗while the second quantity is the number of samples to learn Θ∗(which is required to\n",
            "find the best arms). Note that the magnitude of Srwould be free of d1, d2sinceΘ∗contains only\n",
            "rnonzero singular values and ∥Θ∗∥ ≤1, and hence we assume that Sr= Θ(1 /√r)(Kang et al.,\n",
            "2022). So the sample complexity of single-task GOBLIN scales as eO((d1+d2)r\n",
            "∆2). However, if one\n",
            "runs RAGE (Fiez et al., 2019) on the arms in X,Zthen the sample complexity will scale as eO(d1d2\n",
            "∆2).\n",
            "Proof (Overview) of Theorem 1: Step 1 (Subspace estimation in high dimension): We denote\n",
            "the vectorized arms in high dimension as w∈W. We run the E-optimal design to sample the\n",
            "arms in W. Note that this E-optimal design satisfies the distribution assumption of Kang et al.\n",
            "(2022) which enables us to apply the Lemma 3 in Appendix A.1. This leads to ∥bΘℓ−Θ∗∥2\n",
            "F≤\n",
            "C1d1d2rlog(2( d1+d2)/δ)\n",
            "τE\n",
            "ℓfor some C1>0. Also, note that in the first stage of the ℓ-th phase by\n",
            "setting τE\n",
            "ℓ=√\n",
            "8d1d2rlog(4ℓ2|W|/δℓ)\n",
            "Srand sampling each arm w∈Wexactly ⌈bbE\n",
            "ℓ,wτE\n",
            "ℓ⌉times we are\n",
            "guaranteed that ∥θ∗\n",
            "k+1:p∥2=O(d1d2r/τE\n",
            "ℓ). Summing up over ℓ= 1to\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "we get that\n",
            "the total sample complexity of the first stage is bounded by eO(√d1d2r/Sr).\n",
            "Step 2 (Effective dimension for rotated arms): We rotate the arms w∈Win high dimension to get\n",
            "the rotated arms w∈ Wℓin step 10of Algorithm 1. Then we show that the effective dimension of w\n",
            "scales 8klog\u0000\n",
            "1 +τG\n",
            "ℓ−1/λ\u0001\n",
            "when λ⊥\n",
            "ℓ=τG\n",
            "ℓ−1\n",
            "8klog(1+τG\n",
            "ℓ−1/λ)in Lemma 7 of Appendix A.4. Note that\n",
            "this requires a different proof technique than Valko et al. (2014) where the budget nis given apriori\n",
            "and effective dimension scales with log(n). This step also diverges from the pure exploration proof\n",
            "technique of Fiez et al. (2019); Katz-Samuels et al. (2020) as there is no parameter λ⊥\n",
            "ℓto control\n",
            "during phase ℓ, and the effective dimensions in those papers do not depend on phase length.\n",
            "Step 3 (Bounded Support): For any phase ℓ, we can show that 1≤ρG(Y(Wℓ))≤p/γ2\n",
            "Ywhere,\n",
            "γY= max {c >0 :cY ⊂ conv(W∪ −W )}is the gauge norm of Y(Rockafellar, 2015). Note\n",
            "that this is a worst-case dependence when ρG(Y(Wℓ))scales with p. Substituting this value of\n",
            "ρG(Y(Wℓ))in the definition of λ⊥\n",
            "ℓwe can show that Λℓdoes not depend on wory=w−w′.\n",
            "Then following Theorem 21.1 in Lattimore and Szepesvári (2020) we can show that the G-optimal\n",
            "designbbG\n",
            "ℓis equivalent to D-optimal design bbD\n",
            "ℓ= arg maxblog\f\f\fP\n",
            "w∈Wℓbwww⊤+Λℓ\f\f\f\n",
            "|Λℓ|. Then using\n",
            "Frank-Wolfe algorithm (Jamieson and Jain, 2022) we can show the support bbG\n",
            "ℓor equivalently bbD\n",
            "ℓis\n",
            "bounded by at most8klog(1+ τG\n",
            "ℓ−1/λ)(8klog(1+ τG\n",
            "ℓ−1/λ)+1)\n",
            "2. This is shown in Lemma 9 (Appendix A.4).\n",
            "Step 4 (Phase length and Elimination): Using the Lemma 9, concentration Lemma 5, and using\n",
            "the log determinant inequality in Lemma 7 and Proposition 1 (Appendix A.4) we show that the\n",
            "phase length in the second stage is given by τG\n",
            "ℓ=⌈8Bℓ\n",
            "∗ρ(Y(Wℓ)) log(2 |W|/δ)\n",
            "(x⊤(bθℓ−θ∗))2⌉. This is discussed in\n",
            "Discussion 3 (Appendix A.4). We show in Lemma 10 (Appendix A.4) that setting this phase length\n",
            "and sampling each active arm in Wℓexactly ⌈bbℓ,wτG\n",
            "ℓ⌉times results in the elimination of sub-optimal\n",
            "actions with high probability.\n",
            "Step 5 (Total Samples): We first show that the total samples in the second phase are bounded by\n",
            "O(k\n",
            "γ2\n",
            "Ylog(klog2(∆−1)|W|\n",
            "δ)⌈log2(∆−1)⌉)where the effective dimension k= (d1+d2)r. Finally,\n",
            "we combine the total samples of phase ℓas(τE\n",
            "ℓ+τG\n",
            "ℓ). The final sample complexity is given by\n",
            "summing over all phases from ℓ= 1to\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            ". The claim of the theorem follows by noting\n",
            "eO(k/γ2\n",
            "Y)≤eO(k/∆2).\n",
            "3 Multi-task Representation Learning\n",
            "In this section, we extend GOBLIN to multi-task representation learning for the bilinear bandit setting.\n",
            "In the multi-task setting, we now have Mtasks, where each task m∈[M]has a reward model stated\n",
            "6in(1). The learning proceeds as follows: At each round t= 1,2,···, for each task m∈[M], the\n",
            "learner selects a left and right action xm,t∈ X andzm,t∈ Z. After the player commits the batch\n",
            "of actions for each task {xm,t,zm,t:m∈[M]}, it receives the batch of rewards {rm,t:m∈[M]}.\n",
            "Finally recall that the goal is to identify the optimal left and right arms xm,∗,zm,∗for each task m\n",
            "with a minimum number of samples. We now state the following assumptions to enable representation\n",
            "learning across tasks.\n",
            "Assumption 1. (Low-rank Tasks) We assume that the hidden parameter Θm,∗for all the m∈[M]\n",
            "have a decomposition Θm,∗=B1Sm,∗B⊤\n",
            "2and each Sm,∗has rank r.\n",
            "This is similar to the assumptions in Yang et al. (2020, 2022); Du et al. (2023) ensuring the feature\n",
            "extractors are shared across tasks in the bilinear bandit setting.\n",
            "Assumption 2. (Diverse Tasks) We assume that σmin(1\n",
            "MPM\n",
            "m=1Θm,∗)≥c0\n",
            "Sr, for some c0>0,Sr\n",
            "is the r-th largest singular value of Θm,∗andσmin(A)denotes the minimum eigenvalue of matrix A.\n",
            "This assumption is similar to the diverse tasks assumption of Yang et al. (2020, 2022); Tripuraneni\n",
            "et al. (2021); Du et al. (2023) and ensures the possibility of recovering the feature extractors B1and\n",
            "B2shared across tasks.\n",
            "Our extension of GOBLIN to the multi-task setting is now a phase-based, three-stage arm elimination\n",
            "algorithm. In GOBLIN each phase ℓ= 1,2, . . .consists of three stages; the stage for estimation of\n",
            "feature extractors B1,B2, which runs for τE\n",
            "ℓrounds, the stage for estimation of Sm,∗which runs forP\n",
            "meτE\n",
            "m,ℓrounds, and a stage of pure exploration with rotated arms that runs forP\n",
            "mτG\n",
            "m,ℓrounds.\n",
            "We will define τE\n",
            "m,ℓin Section 3.1, eτE\n",
            "m,ℓin Section 3.2, while the rotated arms and τG\n",
            "m,ℓare defined\n",
            "in Section 3.3. At the end of every phase, GOBLIN eliminates sub-optimal arms to build the active\n",
            "set for the next phase and stops when only the optimal left and right arms are remaining. Now we\n",
            "discuss the individual stages that occur at every phase ℓ= 1,2, . . .for multi-task GOBLIN.\n",
            "3.1 Estimating Feature Extractors B1andB2(Stage 1 of Phase ℓ)\n",
            "In the first stage of phase ℓ, GOBLIN leverages the batch of rewards {rm,t:m∈[M]}at every\n",
            "round tfromMtasks to learn the feature extractors B1andB2. To do this, GOBLIN first vectorizes\n",
            "thex∈ X,z∈ Z into a new vector w= [x1:d1;z1:d2]∈Wmand then solves the E-optimal\n",
            "design in step 3of Algorithm 2. Similar to the single-task setting (Section 2) GOBLIN samples\n",
            "eachw∈Wmfor⌈τE\n",
            "ℓbE\n",
            "ℓ,w⌉times for each task m, where τE\n",
            "ℓ=eO(√d1d2r/Sr)andbE\n",
            "ℓ,wis the\n",
            "solution to E-optimal design on w. Let the sampled arms for each task mat round sbe denoted by\n",
            "xm,s,zm,swhich is obtained after reshaping ws. Then it builds the estimator bZℓas follows:\n",
            "bZℓ= arg min\n",
            "Θ∈Rd1×d2Lℓ(Θ) +γℓ∥Θ∥nuc,\n",
            "Lℓ(Θ) =⟨Θ,Θ⟩ −2\n",
            "MτE\n",
            "ℓMX\n",
            "m=1τE\n",
            "ℓX\n",
            "s=1⟨eψν(rm,s·Q(xm,sz⊤\n",
            "m,s)),Θ⟩ (8)\n",
            "Then it performs SVD decomposition on bZℓ, and let bB1,bB2be the top- k1and top- k2left and right\n",
            "singular vectors of bZℓrespectively. These are the estimation of the feature extractors B1andB2.\n",
            "3.2 Estimating Hidden Parameter Sm,∗per Task (Stage 2 of phase ℓ)\n",
            "In the second stage of phase ℓ, the goal is to recover the hidden parameter Sm,∗for each task m.\n",
            "GOBLIN proceeds as follows: First, let egm=x⊤bB1,ℓandevm=z⊤bB2,ℓbe the latent left and right\n",
            "arm respectively for each m. Then GOBLIN defines the vector ew= [egm;evm]∈fWmand then\n",
            "solves the E-optimal design in step 11of Algorithm 2. It then samples for each task m, the latent\n",
            "armew∈fWmfor⌈eτE\n",
            "m,ℓebE\n",
            "m,ℓ,ew⌉times, where eτE\n",
            "m,ℓ:=eO(√k1k2r/Sr)andebE\n",
            "m,ℓ,ewis the solution to\n",
            "7E-optimal design on ew. Then it builds estimator bSm,ℓfor each task min step 12as follows:\n",
            "bSm,ℓ= arg min\n",
            "Θ∈Rk1×k2L′\n",
            "ℓ(Θ) +γℓ∥Θ∥nuc,\n",
            "L′\n",
            "ℓ(Θ) =⟨Θ,Θ⟩ −2\n",
            "eτE\n",
            "m,ℓeτE\n",
            "m,ℓX\n",
            "s=1⟨eψν(rm,s·Q(egm,sev⊤\n",
            "m,s)),Θ⟩ (9)\n",
            "Once GOBLIN recovers the bSm,ℓfor each task mit has reduced the d1d2bilinear bandit to a k1k2\n",
            "dimension bilinear bandit where the left and right arms are egm∈ Gm,evm∈ Vmrespectively.\n",
            "3.3 Optimal Design for Rotated Arms per Task (Stage 3 of phase ℓ)\n",
            "In the third stage of phase ℓ, similar to Algorithm 1, the multi-task GOBLIN defines the rotated arm\n",
            "setGm,Vmfor each task mfor these k1k2dimensional bilinear bandits. Let the SVD of bSm,ℓ=\n",
            "bUm,ℓbDm,ℓbV⊤\n",
            "m,ℓ. Define bHm,ℓ= [bUm,ℓbU⊥\n",
            "m,ℓ]⊤bSm,ℓ[bVm,ℓbV⊥\n",
            "m,ℓ]. Then define the vectorized arm\n",
            "set so that the last (k1−r)·(k2−r)components are from the complementary subspaces as follows:\n",
            "Wm,ℓ=\b\u0002\n",
            "vec\u0000egm,1:rev⊤\n",
            "m,1:r\u0001\n",
            ";vec\u0000egm,r+1:k1ev⊤\n",
            "m,1:r\u0001\n",
            ";vec\u0000egm,1:rev⊤\n",
            "m,r+1:k2\u0001\n",
            ";\n",
            "vec\u0000egm,r+1:k1ev⊤\n",
            "m,r+1:k2\u0001\u0003\t\n",
            "bθm,ℓ,1:k= [vec(bHm,ℓ,1:r,1:r);vec(bHm,ℓ,r +1:k1,1:r);vec(bHm,ℓ,1:r,r+1:k2)],\n",
            "θℓ,k+1:p=vec(bHm,ℓ,r +1:k1,r+1:k2). (10)\n",
            "This is shown in step 14of Algorithm 2. Now we proceed similarly to Section 2.2. We construct a\n",
            "per-task optimal design for the rotated arm set Vm,Gmand define the w= [egm,1:d1;evm,1:d2]and\n",
            "ew∈fWmwhereegm∈ Gmandevm∈ Vmrespectively. Following (5)we know that to minimize the\n",
            "sample complexity for the m-th bilinear bandit we need to sample according to G-optimal design\n",
            "bbG\n",
            "m,ℓ= arg min\n",
            "bm,wmax\n",
            "w,w′∈Wm,ℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wmbm,www⊤+Λm,ℓ/n)−1 (11)\n",
            "Then GOBLIN runs G-optimal design on the arm set Wℓfollowing the (11) and then samples each\n",
            "w∈ Wm,ℓfor⌈τG\n",
            "m,ℓbbG\n",
            "m,ℓ,w⌉times where bbG\n",
            "m,ℓ,wis the solution to the G-optimal design, and τG\n",
            "ℓ\n",
            "is defined in step 17of Algorithm 2. So the total length of phase ℓ, combining stages 1,2and3is\n",
            "(τE\n",
            "ℓ+P\n",
            "meτE\n",
            "m,ℓ+P\n",
            "mτG\n",
            "m,ℓ)rounds. Observe that the stage 1 and 2 design is on the whole arm\n",
            "setW,fWmwhereas the stage 3design is on the refined active set Wm,ℓ. Let at the stage 3ofℓ-th\n",
            "phase the actions sampled be denoted by the matrix Wm,ℓ∈RτG\n",
            "m,ℓ×k1k2and observed rewards\n",
            "rm∈RτG\n",
            "m,ℓ×k1k2. Define the positive diagonal matrix Λm,ℓaccording to (6)but set p=k1k2and\n",
            "k= (k1+k2)r. Then similar to Section 2.2 we can build for each task monly from the observations\n",
            "from this phase\n",
            "bθm,ℓ= arg min\n",
            "θ1\n",
            "2∥Wm,ℓθ−rm∥2\n",
            "2+1\n",
            "2∥θ∥2\n",
            "Λm,ℓ(12)\n",
            "Finally GOBLIN eliminates the sub-optimal arms using the estimator bθm,ℓto build the next phase\n",
            "active set Wm,ℓand stops when |Wm,ℓ|= 1. The full pseudo-code is given in Algorithm 2.\n",
            "3.4 Sample Complexity analysis of Multi-task GOBLIN\n",
            "We now present the sample complexity of GOBLIN for the multi-task setting.\n",
            "Theorem 2. (informal) With probability at least 1−δ, GOBLIN returns the best arms xm,∗,zm,∗for\n",
            "each task m, and the total number of samples is bounded by eO\u0010\n",
            "M(k1+k2)r\n",
            "∆2 +M√k1k2r\n",
            "Sr+√d1d2r\n",
            "Sr\u0011\n",
            ".\n",
            "Discussion 2. In Theorem 2 the first quantity is the sample complexity to identify the best arms xm,∗,\n",
            "zm,∗and the second quantity is the number of samples to learn Sm,∗for each task m. This is required\n",
            "to rotate the arms to reach the effective dimension of (k1+k2)r. Finally, the third quantity is the\n",
            "number of samples needed to learn Θm,∗(which in turn is used to estimate the feature extractors\n",
            "B1andB2to learn the Sm,∗). Again we assume that Sr= Θ(1 /√r)(Kang et al., 2022). So the\n",
            "sample complexity of multi-task GOBLIN scales as eO(M(k1+k2)r/∆2). However, if one runs\n",
            "DouExpDes (Du et al., 2023) then the sample complexity will scale as eO(M(k1k2)/∆2)which is\n",
            "worse than GOBLINwhen r≪k1ork2.\n",
            "8Algorithm 2 G-Optimal Design for Bilinear Bandits (GOBLIN) for multi-task setting\n",
            "1:Input: arm set X,Z, confidence δ, rank rofΘ∗, spectral bound SrofΘ∗,S, S⊥\n",
            "m,ℓ=\n",
            "8k1k2r\n",
            "eτE\n",
            "m,ℓS2rlog(k1+k2\n",
            "δℓ), λ, λ⊥\n",
            "m,ℓ=τG\n",
            "m,ℓ−1\n",
            "(8(k1+k2)rlog(1+ τG\n",
            "m,ℓ−1/λ)). Letp=k1k2,k= (k1+k2)r.\n",
            "2:LetWm,1← Wm, ℓ←1,τG\n",
            "0= log(4 ℓ2|X|/δ). Define Λm,ℓas in (6),Bℓ\n",
            "m,∗:= (8√\n",
            "λS+q\n",
            "λ⊥\n",
            "m,ℓS⊥\n",
            "m,ℓ)\n",
            "3:Define arm w= [x1:d1;z1:d2]andw∈Wm. Let τE\n",
            "ℓ=√\n",
            "8d1d2rlog(4ℓ2|W|/δℓ)\n",
            "Sr. Let E-optimal\n",
            "design be bE\n",
            "ℓ=arg minb∈△W\r\r(P\n",
            "w∈Wbwww⊤)−1\r\r.\n",
            "4:while∃m∈[M],\f\fWm,ℓ\f\f>1do\n",
            "5: ϵℓ= 2−ℓ,δℓ=δ/ℓ2.\n",
            "6: (Stage 1:) Explore the Low-Rank Subspace\n",
            "7: Pull arm w∈Wexactly ⌈bbE\n",
            "ℓ,wτE\n",
            "ℓ⌉times for each task mand observe rewards {rm,t}τE\n",
            "ℓ\n",
            "t=1.\n",
            "8: Compute bZℓusing (8).\n",
            "9: (Stage 2:) Build bSm,ℓfor each task m\n",
            "10: LetbB1,ℓ,bB2,ℓbe the top- k1left and top- k2right singular vectors of bZℓrespectively. Build\n",
            "egm=x⊤bB1,ℓandevm=z⊤bB2,ℓfor all x∈ X andz∈ Z for each m.\n",
            "11: Define a vectorized arm ew= [egm,1:k1;evm,1:k2]andew∈fWmfor each m. LeteτE\n",
            "m,ℓ=√\n",
            "8k1k2rlog(4ℓ2|W|/δℓ)\n",
            "Sr, andebE\n",
            "m,ℓ= arg minbm∈△fWm\r\r\u0000P\n",
            "ew∈fWmbm,ewewew⊤\u0001−1\r\r.\n",
            "12: Pull arm ew∈fWmexactlyl\n",
            "ebE\n",
            "m,ℓ,eweτE\n",
            "m,ℓm\n",
            "times and observe rewards rm,t, for t=\n",
            "1, . . . ,eτE\n",
            "m,ℓ, for each task m. Then compute bSm,ℓusing (9) for each m.\n",
            "13: (Stage 3:) Reduction to low dimensional linear bandits for each task m\n",
            "14: SVD of bSm,ℓ=bUm,ℓbDm,ℓbV⊤\n",
            "m,ℓ. Rotate arms in active set Wm,ℓ−1to build Wm,ℓusing (10).\n",
            "15: LetbbG\n",
            "m,ℓ=arg minbm,wmaxw,w′∈Wm,ℓ∥w−w′∥2\n",
            "(P\n",
            "wm∈Wmbm,wwmw⊤\n",
            "m+Λm,ℓ/n)−1.\n",
            "16: Define ρG(Y(Wm,ℓ))=min\n",
            "bm,wmax\n",
            "w,w′∈Wm,ℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wmbm,www⊤+Λm,ℓ\n",
            "n)−1.\n",
            "17: SetτG\n",
            "m,ℓ=64Bℓ\n",
            "m,∗ρG(Y(Wm,ℓ)) log(4 ℓ2|Wm|/δℓ)\n",
            "ϵ2\n",
            "ℓ. Then pull arm w∈ Wmfor each task m\n",
            "exactly ⌈bbm,ℓ,wτG\n",
            "m,ℓ⌉times and construct the least squares estimator bθm,ℓusing only the\n",
            "observations of this phase where bθm,ℓis defined in (12).\n",
            "18: Eliminate arms such that Wm,ℓ+1← Wm,ℓ\\n\n",
            "wm∈ Wm,ℓ: max w′\n",
            "m∈Wm,ℓD\n",
            "w′\n",
            "m−wm,bθm,ℓE\n",
            ">2ϵm,ℓo\n",
            "19: ℓ←ℓ+ 1\n",
            "20:Output the arm in Wm,ℓand reshape to get the bxm,∗andbzm,∗for each task m.\n",
            "Proof (Overview) of Theorem 2: Step 1 (Subspace estimation in high dimension): The first steps\n",
            "diverge from the proof technique of Theorem 1. We now build the average estimator bZℓto estimate the\n",
            "quantity Z∗=1\n",
            "MPM\n",
            "m=1Θ∗,musing (8). This requires us to modify the Lemma 3 in Appendix A.1\n",
            "and apply Stein’s lemma (Lemma 1) to get a bound of ∥bZℓ−Z∗∥2\n",
            "F≤C1d1d2rlog(2( d1+d2)/δ)\n",
            "τE\n",
            "ℓfor some\n",
            "C1>0. This is shown in Lemma 12 in Appendix A.6. Summing up over ℓ= 1to\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "we get that the total samples complexity of the first stage is bounded by eO(√d1d2r/Sr).\n",
            "Step 2 (Estimation of left and right feature extractors): Now using the estimator in (8)we get a\n",
            "good estimation of the feature extractors B1andB2. LetbB1,ℓ,bB2,ℓbe the top- k1left and top- k2\n",
            "right singular vectors of bZℓrespectively. Then using the Davis-Kahan sinθTheorem (Bhatia, 2013)\n",
            "in Lemma 14, 15 (Appendix A.6) we have ∥(bB⊥\n",
            "1,ℓ)⊤B1∥,∥(bB⊥\n",
            "2,ℓ)⊤B2∥ ≤eO(q\n",
            "(d1+d2)r/MτE\n",
            "ℓ).\n",
            "Step 3 (Estimation of bSm,ℓin low dimension): Now we estimate the quantity bSm,ℓ∈Rk1×k2for\n",
            "each task m. To do this we first build the latent arms egm=x⊤bUℓandevm=z⊤bVℓfor all x∈ X\n",
            "9andz∈ Z for each m, and sample them following the E-optimal design in step 12of Algorithm 2.\n",
            "We also show in Lemma 16 (Appendix A.6) that σmin(P\n",
            "ew∈fWbewewew⊤)>0which enables us to\n",
            "sample following E-optimal design. Then use the estimator in (9). Then in Lemma 19 we show that\n",
            "∥bSm,ℓ−µ∗Sm,∗∥2\n",
            "F≤C1k1k2rlog\u0010\n",
            "2(k1+k2)\n",
            "δℓ\u0011\n",
            "/τE\n",
            "m,ℓholds with probability greater than (1−δ).\n",
            "Also, note that in the second phase by setting eτE\n",
            "m,ℓ=p\n",
            "8k1k2rlog(4ℓ2|W|/δℓ)/Srand sampling\n",
            "each arm w∈Wexactly ⌈bbE\n",
            "ℓ,weτE\n",
            "m,ℓ⌉times we are guaranteed that ∥θ∗\n",
            "k+1:p∥2=O(k1k2r/eτE\n",
            "m,ℓ)in\n",
            "theℓ-th phase. Summing up over ℓ= 1to\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "across each task Mwe get that the total\n",
            "samples complexity of the second stage is bounded by eO(M√k1k2r/Sr).\n",
            "Step 4 (Convert to k1k2bilinear bandits): Once GOBLIN recovers bSm,τE\n",
            "ℓit rotates the arm set\n",
            "following (10) to build Wmto get the k1k2bilinear bandits. The rest of the steps follow the same\n",
            "way as in steps 2,3and4of proof of Theorem 1.\n",
            "Step 5 (Total Samples): We show the total samples in the third phase are bounded by\n",
            "O(k\n",
            "γ2\n",
            "Ylog(klog2(∆−1)|W|\n",
            "δ)⌈log2(∆−1)⌉)where the effective dimension k= (k1+k2)r. The to-\n",
            "tal samples of phase ℓis given by τE\n",
            "ℓ+P\n",
            "m(eτE\n",
            "m,ℓ+τG\n",
            "m,ℓ). Finally, we get the total sample complexity\n",
            "by summing over all phases from ℓ= 1to⌈log2\u0000\n",
            "4∆−1\u0001\n",
            "⌉. The claim of the theorem follows by\n",
            "noting eO(k/γ2\n",
            "Y)≤eO(k/∆2).\n",
            "4 Experiments\n",
            "In this section, we conduct proof-of-concept experiments on both single and multi-task bilinear\n",
            "bandits. In the single-task experiment, we compare against the state-of-the-art RAGE algorithm (Fiez\n",
            "et al., 2019). We show in Figure 1 (left) that GOBLIN requires fewer samples than the RAGE with\n",
            "an increasing number of arms. In the multi-task experiment, we compare against the state-of-the-art\n",
            "DouExpDes algorithm (Du et al., 2023). We show in Figure 1 (right) that GOBLIN requires fewer\n",
            "samples than DouExpDes with an increasing number of tasks. As experiments are not a central\n",
            "contribution, we defer a fuller description of the experimental set-up to Appendix A.8.\n",
            "Figure 1: (Left) Single-task experiment: results show the number of samples required to identify the\n",
            "optimal action pair for differing numbers of actions. (Right) Multi-task experiment: results show the\n",
            "number of samples required to identify the optimal action pair for varying numbers of tasks.\n",
            "5 Conclusions and Future Directions\n",
            "In this paper, we formulated the first pure exploration multi-task representation learning problem. We\n",
            "introduce an algorithm, GOBLIN that achieves a sample complexity bound of eO((d1+d2)r/∆2)\n",
            "which improves upon the eO((d1d2)/∆2)sample complexity of RAGE (Fiez et al., 2019) in a single-\n",
            "task setting. We then extend GOBLIN for multi-task pure exploration bilinear bandit problems\n",
            "by learning latent features which enables sample complexity that scales as eO(M(k1+k2)r/∆2)\n",
            "which improves over the eO(M(k1k2)/∆2)sample complexity of DouExpDes (Du et al., 2023). Our\n",
            "analysis opens an exciting opportunity to analyze representation learning in the kernel and neural\n",
            "bandits (Zhu et al., 2021; Mason et al., 2021). We can leverage the fact that this type of optimal\n",
            "design does not require the arm set to be an ellipsoid (Du et al., 2023) which enables us to extend our\n",
            "analysis to non-linear representations.\n",
            "10References\n",
            "Agarwal, D., Chen, B.-C., and Elango, P. (2009). Explore/exploit schemes for web content optimiza-\n",
            "tion. In 2009 Ninth IEEE International Conference on Data Mining , pages 1–10. IEEE.\n",
            "Bengio, Y ., Courville, A., and Vincent, P. (2013). Representation learning: A review and new\n",
            "perspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):1798–1828.\n",
            "Bhatia, R. (2013). Matrix analysis , volume 169. Springer Science & Business Media.\n",
            "Bragman, F. J., Tanno, R., Eaton-Rosen, Z., Li, W., Hawkes, D. J., Ourselin, S., Alexander, D. C.,\n",
            "McClelland, J. R., and Cardoso, M. J. (2018). Uncertainty in multitask learning: joint representa-\n",
            "tions for probabilistic mr-only radiotherapy planning. In Medical Image Computing and Computer\n",
            "Assisted Intervention–MICCAI 2018: 21st International Conference, Granada, Spain, September\n",
            "16-20, 2018, Proceedings, Part IV 11 , pages 3–11. Springer.\n",
            "Du, S. S., Hu, W., Kakade, S. M., Lee, J. D., and Lei, Q. (2020). Few-shot learning via learning the\n",
            "representation, provably. arXiv preprint arXiv:2002.09434 .\n",
            "Du, Y ., Huang, L., and Sun, W. (2023). Multi-task representation learning for pure exploration in\n",
            "linear bandits. arXiv preprint arXiv:2302.04441 .\n",
            "Fiez, T., Jain, L., Jamieson, K. G., and Ratliff, L. (2019). Sequential experimental design for\n",
            "transductive linear bandits. Advances in neural information processing systems , 32.\n",
            "Jamieson, K. and Jain, L. (2022). Interactive machine learning.\n",
            "Jun, K.-S., Willett, R., Wright, S., and Nowak, R. (2019). Bilinear bandits with low-rank structure.\n",
            "InInternational Conference on Machine Learning , pages 3163–3172. PMLR.\n",
            "Kang, Y ., Hsieh, C.-J., and Lee, T. C. M. (2022). Efficient frameworks for generalized low-rank\n",
            "matrix bandit problems. Advances in Neural Information Processing Systems , 35:19971–19983.\n",
            "Katz-Samuels, J., Jain, L., Jamieson, K. G., et al. (2020). An empirical process approach to the union\n",
            "bound: Practical algorithms for combinatorial and linear bandits. Advances in Neural Information\n",
            "Processing Systems , 33:10371–10382.\n",
            "Kiefer, J. and Wolfowitz, J. (1960). The equivalence of two extremum problems. Canadian Journal\n",
            "of Mathematics , 12:363–366.\n",
            "Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms . Cambridge University Press.\n",
            "Li, J., Zhang, H., Zhang, L., Huang, X., and Zhang, L. (2014). Joint collaborative representation with\n",
            "multitask learning for hyperspectral image classification. IEEE Transactions on Geoscience and\n",
            "Remote Sensing , 52(9):5923–5936.\n",
            "Lu, Y ., Meisami, A., and Tewari, A. (2021). Low-rank generalized linear bandit problems. In\n",
            "International Conference on Artificial Intelligence and Statistics , pages 460–468. PMLR.\n",
            "Luo, Y ., Zhao, X., Zhou, J., Yang, J., Zhang, Y ., Kuang, W., Peng, J., Chen, L., and Zeng, J. (2017).\n",
            "A network integration approach for drug-target interaction prediction and computational drug\n",
            "repositioning from heterogeneous information. Nature communications , 8(1):573.\n",
            "Mason, B., Camilleri, R., Mukherjee, S., Jamieson, K., Nowak, R., and Jain, L. (2021). Nearly\n",
            "optimal algorithms for level set estimation. arXiv preprint arXiv:2111.01768 .\n",
            "Maurer, A., Pontil, M., and Romera-Paredes, B. (2016). The benefit of multitask representation\n",
            "learning. Journal of Machine Learning Research , 17(81):1–32.\n",
            "Minsker, S. (2018). Sub-gaussian estimators of the mean of a random matrix with heavy-tailed entries.\n",
            "The Annals of Statistics , 46(6A):2871–2903.\n",
            "Mukherjee, S., Tripathy, A. S., and Nowak, R. (2022). Chernoff sampling for active testing and\n",
            "extension to active regression. In International Conference on Artificial Intelligence and Statistics ,\n",
            "pages 7384–7432. PMLR.\n",
            "11Mukherjee, S., Xie, Q., Hanna, J., and Nowak, R. (2023). Speed: Experimental design for policy\n",
            "evaluation in linear heteroscedastic bandits. arXiv preprint arXiv:2301.12357 .\n",
            "Pukelsheim, F. (2006). Optimal design of experiments . SIAM.\n",
            "Reyes, L. J. P., Oviedo, N. B., Camacho, E. C., and Calderon, J. M. (2021). Adaptable recommenda-\n",
            "tion system for outfit selection with deep learning approach. IFAC-PapersOnLine , 54(13):605–610.\n",
            "Rockafellar, R. (2015). Convex analysis. princeton landmarks in mathematics and physics.\n",
            "Shamir, O. (2011). A variant of azuma’s inequality for martingales with subgaussian tails. arXiv\n",
            "preprint arXiv:1110.2392 .\n",
            "Shen, Q., Han, S., Han, Y ., and Chen, X. (2023). User review analysis of dating apps based on text\n",
            "mining. Plos one , 18(4):e0283896.\n",
            "Soare, M., Lazaric, A., and Munos, R. (2014). Best-arm identification in linear bandits. Advances in\n",
            "Neural Information Processing Systems , 27.\n",
            "Stein, C., Diaconis, P., Holmes, S., and Reinert, G. (2004). Use of exchangeable pairs in the analysis\n",
            "of simulations. Lecture Notes-Monograph Series , pages 1–26.\n",
            "Tripuraneni, N., Jin, C., and Jordan, M. (2021). Provable meta-learning of linear representations. In\n",
            "International Conference on Machine Learning , pages 10434–10443. PMLR.\n",
            "Valko, M., Munos, R., Kveton, B., and Kocák, T. (2014). Spectral bandits for smooth graph functions.\n",
            "InInternational Conference on Machine Learning , pages 46–54. PMLR.\n",
            "Yang, J., Hu, W., Lee, J. D., and Du, S. S. (2020). Impact of representation learning in linear bandits.\n",
            "arXiv preprint arXiv:2010.06531 .\n",
            "Yang, J., Lei, Q., Lee, J. D., and Du, S. S. (2022). Nearly minimax algorithms for linear bandits with\n",
            "shared representation. arXiv preprint arXiv:2203.15664 .\n",
            "Zhang, D., Shen, D., Initiative, A. D. N., et al. (2012). Multi-modal multi-task learning for joint\n",
            "prediction of multiple regression and classification variables in alzheimer’s disease. NeuroImage ,\n",
            "59(2):895–907.\n",
            "Zhao, Y ., Kosorok, M. R., and Zeng, D. (2009). Reinforcement learning design for cancer clinical\n",
            "trials. Statistics in medicine , 28(26):3294–3315.\n",
            "Zhu, Y ., Zhou, D., Jiang, R., Gu, Q., Willett, R., and Nowak, R. (2021). Pure exploration in kernel\n",
            "and neural bandits. Advances in neural information processing systems , 34:11618–11630.\n",
            "12A Appendix\n",
            "A.1 Probability Tools and Previous Results\n",
            "In this section, we state useful lemmas we use in our proofs and previous results.\n",
            "Lemma 1. (Generalized Stein’s Lemma, (Stein et al., 2004)) For a random variable Xwith con-\n",
            "tinuously differentiable density function p:Rd→R, and any continuously differentiable function\n",
            "f:Rd→R. LetQ(·)be a scoring function defined in Definition 1. If the expected values of both\n",
            "∇f(X)andf(X)·Q(X)regarding the density pexist, then they are identical, i.e.\n",
            "E[f(X)·Q(X)] =E[∇f(X)].\n",
            "Lemma 2. (Minsker, 2018) Define ∥A∥opas the operator norm of A. LetY1, . . . ,Yn∈Rd1×d2\n",
            "be a sequence of independent real random matrices, and assume that\n",
            "σ2\n",
            "n≥max\n",
            "\r\r\r\r\r\rnX\n",
            "j=1E\u0000\n",
            "YjY⊤\n",
            "j\u0001\r\r\r\r\r\r\n",
            "op,\r\r\r\r\r\rnX\n",
            "j=1E\u0000\n",
            "Y⊤\n",
            "jYj\u0001\r\r\r\r\r\r\n",
            "op\n",
            ".\n",
            "Then for any t∈R+andν∈R+, it holds that,\n",
            "P\n",
            "\r\r\r\r\r\rnX\n",
            "j=1eψν(Yj)−nX\n",
            "j=1E(Yj)\r\r\r\r\r\r\n",
            "op≥t√n\n",
            "≤2 (d1+d2) exp\u0012\n",
            "νt√n+ν2σ2\n",
            "n\n",
            "2\u0013\n",
            "Lemma 3. (Restatement of Theorem 4.1 in Kang et al. (2022)) For any low-rank linear model with\n",
            "samples X1. . . ,Xn1drawn from Xaccording to Dthen for the optimal solution to the nuclear norm\n",
            "regularization problem in (2)withν=p\n",
            "2 log (2 ( d1+d2)/δ)/((4 + S2\n",
            "0)Mn1d1d2)and\n",
            "γn1= 4s\n",
            "2 (4 + S2\n",
            "0)Cd1d2log (2 ( d1+d2)/δ)\n",
            "n1,\n",
            "with probability at least 1−δit holds that:\n",
            "\r\r\rbΘ−µ∗Θ∗\r\r\r2\n",
            "F≤C1d1d2rlog\u0010\n",
            "2(d1+d2)\n",
            "δ\u0011\n",
            "n1,\n",
            "forC1= 36\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "C,∥X∥F,∥Θ∗∥F≤S0, some nonzero constant µ∗, andEh\n",
            "(Sp(X))2\n",
            "iji\n",
            "≤\n",
            "C,∀i, j.\n",
            "A.2 G-optimal design on rotated arms\n",
            "Remark 1. ( G-optimal design on rotated arms:) Using the concentration inequality in Proposition 1\n",
            "we can show that for any arbitrary vector y∈Rp:\n",
            "|y⊤(bθℓ−θ∗)| ≤ ∥y∥V−1\n",
            "ℓ2p\n",
            "14 log(2 /δ) +∥θ∗∥Λℓ≤ ∥y∥V−1\n",
            "ℓq\n",
            "8Bℓ∗log(2/δ)\n",
            "where the co-variance matrix Vℓ:=PτE\n",
            "ℓ\n",
            "s=1wsw⊤\n",
            "s+ΛℓandBℓ\n",
            "∗is defined in Proposition 1. Now we\n",
            "want this to hold for all y∈ Y∗(Wℓ), and so we need to union bound over W ⊇ Wℓreplacing δwith\n",
            "δ/|W|. Set the phase length τG\n",
            "ℓ:=l\n",
            "64Bℓ\n",
            "∗ρG(Y(Wℓ)) log(4 ℓ2|W|/δ)\n",
            "ϵ2\n",
            "ℓm\n",
            "where ρG(Y(Wℓ))is defined in\n",
            "step14of Algorithm 1.\n",
            "Then for the allocation 2⌊bG\n",
            "wτG\n",
            "ℓ⌋for each bw∈ Wℓ, we have for each w∈ Wℓ\\w∗that with\n",
            "probability at least 1−δ,\n",
            "(w∗−w)⊤bθℓ≥(w∗−w)⊤θ∗− ∥w∗−w∥(P\n",
            "w∈W⌈2τG\n",
            "ℓb∗w⌉ww⊤+2τG\n",
            "ℓΛℓ/τG\n",
            "ℓ)−1q\n",
            "8Bℓ∗log(2|W|/δ)\n",
            "13since for every w∗−w∈ Y∗(W)we have\n",
            "(w∗−w)⊤\u0000\n",
            "2X\n",
            "w∈W⌈τG\n",
            "ℓb∗\n",
            "w⌉ww⊤+ 2τG\n",
            "ℓΛℓ\n",
            "τG\n",
            "ℓ\u0001−1(w∗−w)≤1\n",
            "τG\n",
            "ℓ∥w∗−w∥2\n",
            "(P\n",
            "w∈Wℓb∗www⊤+Λℓ\n",
            "τG\n",
            "ℓ)−1\n",
            "≤((w∗−w)⊤θ∗)2√\n",
            "8Bℓ∗log(2|W|/δ).\n",
            "The last inequality follows by plugging in the value of τG\n",
            "ℓandρG(Y(Wℓ)). Hence to minimize the\n",
            "number of samples τG\n",
            "ℓin phase ℓwe can re-arrange the above equation to show that\n",
            "τG\n",
            "ℓ≥q\n",
            "8Bℓ∗log(2|W|/δ) max\n",
            "w∈Wℓ\\w∗∥w∗−w∥2\n",
            "(P\n",
            "w∈Wℓb∗www⊤+Λ/n)−1\n",
            "(w∗−w)⊤θ∗\n",
            "Hence, to minimize the sample complexity for the bilinear setting we need to sample according to\n",
            "bG\n",
            "ℓ= arg min\n",
            "bmax\n",
            "w∥w∗−w∥2\n",
            "(P\n",
            "w∈Wℓbwww⊤+Λ/n)−1\n",
            "(w∗−w)⊤θ∗(13)\n",
            "However, note that we do know the identity of w∗or the gaps (w∗−w)⊤θ∗. So we replace the gaps\n",
            "with a lower bound of ϵ= 2−tand compare against every pair of arms wandw′as follows:\n",
            "bG\n",
            "ℓ= arg min\n",
            "bwmax\n",
            "w,w′∈Wℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λℓ/n)−1 (14)\n",
            "This is shown in step 12of Algorithm 1.\n",
            "A.3 Application of Stein’s Lemma\n",
            "We also present the following two definitions from Kang et al. (2022) to facilitate analysis via Stein’s\n",
            "method:\n",
            "Definition 1. (Score Function) Letp:R→Rbe a univariate probability density function defined\n",
            "onR. The score function Qp:R→Rregarding density p(·)is defined as:\n",
            "Qp(x) =−∇xlog(p(x)) =−∇xp(x)/p(x), x∈R.\n",
            "In particular, for a random matrix with its entrywise probability density p= (pij) :Rd1×d2→\n",
            "Rd1×d2, we define its score function Qp=\u0000\n",
            "Qp\n",
            "ij\u0001\n",
            ":Rd1×d2→Rd1×d2asQp\n",
            "ij(x) =Qpij(x)by\n",
            "applying the univariate score function to each entry of pindependently.\n",
            "Assumption 3. The norm of true parameter Θ∗and feature matrices in Xis bounded: there exists\n",
            "S∈R+such that for all arms X∈ X,∥X∥F,∥Θ∗∥F≤S0\n",
            "Assumption 4. (Finite second-moment score) There exists a sampling distribution DoverXsuch\n",
            "that for the random matrix Xdrawn from Dwith its associated density p:Rd1×d2→Rd1×d2, we\n",
            "haveEh\n",
            "(Qp(X))2\n",
            "iji\n",
            "≤C,∀i, j\n",
            "Definition 2. Given a rectangular matrix A∈Rd1×d2, the (Hermitian) dilation H:Rd1×d2→\n",
            "R(d1+d2)×(d1+d2)is defined as:\n",
            "H(A) =\u00120A\n",
            "A⊤0\u0013\n",
            "Definition 3. (The function eψν)To explore the valid subspace of the parameter matrix Θ∗, we\n",
            "define a function ψ:R→Rin(15). Let H(·)be as defined in Definition 2. Then define\n",
            "eψν:Rd1×d2→Rd1×d2aseψν(A) =ψ(νH(A))1:d1,(d1+1):( d1+d2)/νfor some parameter ν∈R+\n",
            "ψ(x) =\u001alog\u0000\n",
            "1 +x+x2/2\u0001\n",
            ", x ≥0\n",
            "−log\u0000\n",
            "1−x+x2/2\u0001\n",
            ", x < 0(15)\n",
            "14A.4 Single-task Pure Exploration Proofs\n",
            "Good Event: Define the good event Fℓin phase ℓthat GOBLIN has a good estimate of Θ∗as\n",
            "follows:\n",
            "Fℓ=\r\r\rbΘℓ−µ∗Θ∗\r\r\r2\n",
            "F≤C1d1d2rlog\u0010\n",
            "2(d1+d2)\n",
            "δℓ\u0011\n",
            "τE\n",
            "ℓ, (16)\n",
            "where, C1= 36\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "C,∥X∥F,∥Θ∗∥F≤S0, some nonzero constant µ∗,Eh\n",
            "(Sp(X))2\n",
            "iji\n",
            "≤\n",
            "C,∀i, j, andbΘℓis the estimate from (2). Then define the good event\n",
            "F:=∞\\\n",
            "ℓ=1Fℓ. (17)\n",
            "Lemma 4. The event Fholds with probability greater than (1−δ/2).\n",
            "Proof. From Lemma 3 we know the event Fℓin(16) holds with probability (1−δℓ). Taking a union\n",
            "bound over all phases ℓ≥1and recalling δℓ:=δ\n",
            "2ℓ2, we obtain\n",
            "P(F)≥1−∞X\n",
            "ℓ=1P(Fc\n",
            "ℓ)\n",
            "≥1−∞X\n",
            "ℓ=1δℓ\n",
            "2\n",
            "= 1−∞X\n",
            "ℓ=1δ\n",
            "4ℓ2\n",
            "≥1−δ\n",
            "2.\n",
            "This concludes our proof.\n",
            "Now we move to the second stage for the rotated arm set w∈ W and prove the following concentra-\n",
            "tion event.\n",
            "Lemma 5. For any fixed w∈Rpand any δ >0, we have that if β(θ∗, δ) = 2p\n",
            "14 log(2 /δ)+∥θ∗∥Λ,\n",
            "then at time τℓ−1+ 1(beginning of phase ℓ):\n",
            "P\u0010\f\f\fw⊤\u0010\n",
            "bθℓ−θ∗\u0011\f\f\f≤ ∥w∥V−1\n",
            "ℓβ(θ∗, δ)\u0011\n",
            "≥1−δ\n",
            "where, Vℓ:=Pτℓ\n",
            "s=τℓ−1+1wsw⊤\n",
            "s+Λℓ.\n",
            "Proof. We follow the proof technique of Lemma 7 of Valko et al. (2014). Defining ξℓ=Pτℓ\n",
            "s=τℓ−1+1wsηs, we have:\n",
            "\f\f\fw⊤\u0010\n",
            "bθℓ−θ∗\u0011\f\f\f(a=\f\fw⊤\u0000\n",
            "−V−1\n",
            "ℓΛθ∗+V−1\n",
            "ℓξℓ\u0001\f\f\n",
            "(b)\n",
            "≤\f\fw⊤V−1\n",
            "ℓΛℓθ∗\f\f+\f\fw⊤V−1\n",
            "ℓξℓ\f\f (18)\n",
            "where (a)follows from Woodbury matrix identity and rearranging the terms, and (b)follows from\n",
            "the triangle inequality.\n",
            "The first term in the right-hand side of (18) is bounded as:\n",
            "\f\fw⊤V−1\n",
            "ℓΛℓθ∗\f\f≤\r\r\rw⊤V−1\n",
            "ℓΛ1/2\n",
            "ℓ\r\r\r\r\r\rΛ1/2\n",
            "ℓθ∗\r\r\r\n",
            "(a)=∥θ∗∥Λℓq\n",
            "w⊤V−1\n",
            "ℓΛℓV−1\n",
            "ℓw\n",
            "≤ ∥θ∗∥Λℓq\n",
            "w⊤V−1\n",
            "ℓw=∥θ∗∥Λℓ∥w∥V−1\n",
            "ℓ\n",
            "15where, (a)follows as ∥θ∗∥Λℓ=p\n",
            "θ⊤∗Λℓθ∗=∥Λ1/2\n",
            "ℓθ∗∥and similarly for\r\r\rw⊤V−1\n",
            "ℓΛ1/2\n",
            "ℓ\r\r\r. Now\n",
            "consider the second term in the r.h.s. of (18). We have:\n",
            "\f\fw⊤V−1\n",
            "ℓξℓ\f\f=\f\f\f\f\f\fτℓX\n",
            "s=τℓ−1+1\u0000\n",
            "w⊤V−1\n",
            "ℓws\u0001\n",
            "ηs\f\f\f\f\f\f.\n",
            "Now note that the arms (ws)selected by the algorithm during phase ℓonly depend on the proportion\n",
            "bG\n",
            "∗(the G-optimal design) and do not depend on the rewards received during the phase ℓ−1. Thus,\n",
            "givenFj−2, the sequence (ws)τℓ−1+1≤s<τℓis deterministic. Consequently, one may use a variant of\n",
            "Azuma’s inequality (Shamir (2011)) with a 1-sub Gaussian assumption:\n",
            "P\n",
            "\f\fw⊤V−1\n",
            "ℓξℓ\f\f2≤28×2 log(2 /δ)×w⊤V−1\n",
            "ℓ\n",
            "τℓX\n",
            "s=τℓ−1+1wsw⊤\n",
            "s\n",
            "V−1\n",
            "ℓw| Fℓ−2\n",
            "≥1−δ,\n",
            "from which we deduce:\n",
            "P\u0010\f\fw⊤V−1\n",
            "ℓξℓ\f\f2≤56w⊤V−1\n",
            "ℓwlog(2/δ)| Fℓ−2\u0011\n",
            "≥1−δ,\n",
            "sincePτℓ\n",
            "s=τℓ−1+1wsw⊤\n",
            "s≺Vℓ. Thus:\n",
            "P\u0010\f\fw⊤V−1\n",
            "ℓξℓ\f\f≤2∥w∥V−1\n",
            "ℓp\n",
            "14 log(2 /δ)\u0011\n",
            "≥1−δ\n",
            "Combining everything we get that\n",
            "P\u0010\f\f\fw⊤\u0010\n",
            "bθℓ−θ∗\u0011\f\f\f≤2p\n",
            "14 log(2 /δ) +∥θ∗∥Λℓ\u0011\n",
            "≤1−δ.\n",
            "We need to change Lemma 6 of Valko et al. (2014) in the following way so that the dependence\n",
            "on horizon nis replaced by τG\n",
            "ℓ−1. Note that τG\n",
            "ℓ−1is the phase length in the ℓ−1-th phase and is\n",
            "determined before the start of phase τG\n",
            "ℓ−1. Also, note that using the standard analysis of phase-based\n",
            "algorithms in Fiez et al. (2019); Lattimore and Szepesvári (2020) we do not re-use data between\n",
            "phases. First, we need the following support lemma from Valko et al. (2014).\n",
            "Lemma 6. (Restatement of Lemma 4 from Valko et al. (2014)) LetΛℓ= diag\u0000\n",
            "λ1, . . . , λ⊥\n",
            "ℓ\u0001\n",
            "be\n",
            "any diagonal matrix with strictly positive entries. Define Vℓ=Λℓ+PτG\n",
            "ℓ\n",
            "s=τG\n",
            "ℓ−1+1wsw⊤\n",
            "s. Then for\n",
            "any vectors (ws)τG\n",
            "ℓ−1+1≤s≤τG\n",
            "ℓ, such that ∥ws∥2≤1for all rounds ssuch that τG\n",
            "ℓ−1+ 1≤s≤τG\n",
            "ℓ,\n",
            "we have that the determinant |Vℓ|is maximized when all wsare aligned with the axes.\n",
            "Lemma 7. Letkbe the effective dimension. Then\n",
            "log|Vℓ|\n",
            "|Λℓ|≤8klog \n",
            "1 +τG\n",
            "ℓ−1\n",
            "λ!\n",
            "when λ⊥\n",
            "ℓ=τG\n",
            "ℓ−1\n",
            "klog(1+τG\n",
            "ℓ−1/λ).\n",
            "Proof. We want to bound the determinant |Vℓ|under the coordinate constraints ∥wt∥2≤1. Let:\n",
            "M(w1, . . . ,wt) =\f\f\f\f\f\fΛℓ+τG\n",
            "ℓX\n",
            "s=τG\n",
            "ℓ−1+1wsw⊤\n",
            "s\f\f\f\f\f\f\n",
            "From Lemma 6 we deduce that the maximum of Mis reached when all wsare aligned with the\n",
            "axes. Let the number of samples of these axes-aligned ws’s during the ℓ-th phase be denoted as\n",
            "16tℓ\n",
            "1, tℓ\n",
            "2, . . . , tℓ\n",
            "psuch thatPp\n",
            "i=1tℓ\n",
            "i=τG\n",
            "ℓ. Then we can show that\n",
            "M(a)= max\n",
            "w1,...,wt;ws∈{e1,...,ep}\f\f\f\f\f\fΛℓ+τG\n",
            "ℓX\n",
            "s=τG\n",
            "ℓ−1+1wsw⊤\n",
            "s\f\f\f\f\f\f\n",
            "(b)= max\n",
            "tℓ\n",
            "1,...,tℓp,positive integers,Pp\n",
            "i=1tℓ\n",
            "i=τG\n",
            "ℓ\f\fdiag\u0000\n",
            "λi+tℓ\n",
            "i\u0001\f\f\n",
            "(c)\n",
            "≤ max\n",
            "tℓ\n",
            "1,...,tℓp,positive integers,Pp\n",
            "i=1tℓ\n",
            "i=τG\n",
            "ℓpY\n",
            "i=1\u0000\n",
            "λi+tℓ\n",
            "i\u0001\n",
            "where, (a)follows from Lemma 6, (b)follows as the diag\u0000\n",
            "λi+tℓ\n",
            "i\u0001\n",
            "contains the number of times\n",
            "axis-aligned ws∈ {e1, . . . ,ep}are observed, and (c)follows as the determinant of a diagonal matrix\n",
            "is the product of the diagonal elements. Now we can show that\n",
            "log|Vℓ|\n",
            "|Λℓ|≤kX\n",
            "i=1log\u0012\n",
            "1 +tℓ\n",
            "i\n",
            "λ\u0013\n",
            "+pX\n",
            "i=k+1log\u0012\n",
            "1 +tℓ\n",
            "i\n",
            "λi\u0013\n",
            "(a)\n",
            "≤klog\u0012\n",
            "1 +tℓ\n",
            "i\n",
            "λ\u0013\n",
            "+pX\n",
            "i=1tℓ−1\n",
            "i\n",
            "λ⊥\n",
            "ℓ\n",
            "(b)\n",
            "≤klog\u0012\n",
            "1 +tℓ\n",
            "i\n",
            "λ\u0013\n",
            "+τG\n",
            "ℓ−1\n",
            "λ⊥\n",
            "ℓ\n",
            "(c)\n",
            "≤klog\u0012\n",
            "1 +tℓ\n",
            "i\n",
            "λ\u0013\n",
            "+klog \n",
            "1 +τG\n",
            "ℓ−1\n",
            "λ!\n",
            "(d)\n",
            "≤8klog \n",
            "1 +τG\n",
            "ℓ−1\n",
            "λ!\n",
            "where, (a)follows as log(1 + tℓ\n",
            "i/λi)≤tℓ−1\n",
            "i/λ⊥\n",
            "ℓ,(b)follows asPp\n",
            "i=1tℓ−1\n",
            "i=τG\n",
            "ℓ−1, and (c)follows\n",
            "forλ⊥\n",
            "ℓ=τG\n",
            "ℓ−1\n",
            "klog(1+τG\n",
            "ℓ−1/λ)and(d)follows from Lemma 8.\n",
            "Lemma 8. LetρG(Y(Wℓ)) = min bwmaxw,w′∈Wℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λℓ/n)−1. Recall\n",
            "thatτG\n",
            "ℓ=8Bℓ\n",
            "∗ρG(Y(Wℓ)) log(4 ℓ2|W|/δ)\n",
            "ϵ2\n",
            "ℓ. Assume log(p)≤k. Then we can show that\n",
            "log\u0012\n",
            "1 +τG\n",
            "ℓ\n",
            "λ\u0013\n",
            "≤8klog \n",
            "1 +τG\n",
            "ℓ−1\n",
            "λ!\n",
            ".\n",
            "Proof. We start by first recalling the definition of\n",
            "τG\n",
            "ℓ= 2ϵ−2\n",
            "ℓ8klog(1 +τG\n",
            "ℓ−1\n",
            "λ)ρG(Y(Wℓ)) log\u0000\n",
            "4ℓ2|W|/δℓ\u0001\n",
            ".\n",
            "Then we can show the following\n",
            "τG\n",
            "ℓ\n",
            "τG\n",
            "ℓ−1=2ϵ−2\n",
            "ℓBℓ\n",
            "∗ρG(Y(Wℓ)) log\u0000\n",
            "4ℓ2|W|/δℓ\u0001\n",
            "2ϵ−2\n",
            "ℓ−1Bℓ−1\n",
            "∗ρG(Y(Wℓ−1)) log (4( ℓ−1)2|W|/δℓ)≤4ρG(Y(Wℓ))\u0010\n",
            "64(λS2+λ⊥\n",
            "ℓS(2),ℓ\n",
            "⊥)\u0011\n",
            "ρG(Y(Wℓ−1))\u0010\n",
            "64(λS2+λ⊥\n",
            "ℓ−1S(2),ℓ−1\n",
            "⊥)\u0011\n",
            "≤4ρG(Y(Wℓ))τG\n",
            "ℓ−1/log(1 +τG\n",
            "ℓ−1\n",
            "λ)\n",
            "ρG(Y(Wℓ−1))τG\n",
            "ℓ−2/log(1 +τG\n",
            "ℓ−2\n",
            "λ)\n",
            "(a)\n",
            "≤4ρG(Y(Wℓ))τG\n",
            "ℓ−1log(1 +τG\n",
            "ℓ−1\n",
            "λ)\n",
            "ρG(Y(Wℓ−1))\n",
            "(b)\n",
            "≤4p\n",
            "γ2\n",
            "Ymaxw∈W∥w∥2\n",
            "maxy∈Y(Wℓ)∥y∥2\n",
            "2τG\n",
            "ℓ−1log(1 +τG\n",
            "ℓ−1\n",
            "λ) =4p\n",
            "Cγ2\n",
            "Y\n",
            "17where, (a)follows as log(1 +τG\n",
            "ℓ−2\n",
            "λ)≥1andlog(1 +τG\n",
            "ℓ−1\n",
            "λ)≥log(1 +τG\n",
            "ℓ−2\n",
            "λ). The (b)follows using\n",
            "Lemma 1 from Fiez et al. (2019) such that\n",
            "max\n",
            "y∈Y(Wℓ)∥y∥2\n",
            "2/\u0012\n",
            "max\n",
            "w∈W∥w∥2\u0013\n",
            "≤ρG(Y(Wℓ))≤p/γ2\n",
            "Y(a1)=⇒1≤ρG(Y(Wℓ))≤p/γ2\n",
            "Y.\n",
            "where, (a1)follows as ∥x∥ ≤1,∥z∥ ≤1. This implies that for a constant C >0\n",
            "τG\n",
            "ℓ≤4p\n",
            "Cγ2\n",
            "Y(τG\n",
            "ℓ−1)2log(1 +τG\n",
            "ℓ−1\n",
            "λ) =⇒log\u0012\n",
            "1 +τG\n",
            "ℓ\n",
            "λ\u0013\n",
            "≤log \n",
            "1 +4p\n",
            "Cγ2\n",
            "Y(τG\n",
            "ℓ−1)2log(1 +τG\n",
            "ℓ−1\n",
            "λ)!\n",
            "(a)=⇒log\u0012\n",
            "1 +τG\n",
            "ℓ\n",
            "λ\u0013\n",
            "≤4klog \n",
            "1 +τG\n",
            "ℓ−1\n",
            "Cγ2\n",
            "Yλ!\n",
            "(b)=⇒log\u0012\n",
            "1 +τG\n",
            "ℓ\n",
            "λ\u0013\n",
            "≤8klog \n",
            "1 +τG\n",
            "ℓ−1\n",
            "λ!\n",
            "where, in (a)follows for log(p)≤k,log(a2log(a))≤4 log( a). The (b)follows as\n",
            "4klog \n",
            "1 +τG\n",
            "ℓ−1\n",
            "Cγ2\n",
            "Yλ!\n",
            "≤8klog \n",
            "1 +τG\n",
            "ℓ−1\n",
            "λ!\n",
            ".\n",
            "Lemma 9. TheG-optimal design in (5)is equivalent to solving the D-optimal design\n",
            "bD\n",
            "ℓ= arg max\n",
            "blog\f\f\fP\n",
            "w∈Wℓbwww⊤+Λℓ\f\f\f\n",
            "|Λℓ|.\n",
            "Furthermore, the support of |bD\n",
            "ℓ| ≤8klog(1+τG\n",
            "ℓ−1\n",
            "λ)(8klog(1+τG\n",
            "ℓ−1\n",
            "λ)+1)\n",
            "2, where k= (d1+d2)r.\n",
            "Proof. To prove the equivalence between bG\n",
            "∗andbD\n",
            "∗we need to first show that the regularization\n",
            "matrix Λℓdoes not depend on wory=w−w′, where w∈ Wℓ. Define conv(X ∪ −X )as the\n",
            "convex hull of X ∪ −X . Now recall we have from Lemma 1 of Fiez et al. (2019) that\n",
            "1≤ρG(Y(Wℓ))≤p/γ2\n",
            "Y (19)\n",
            "where, γY= max {c >0 :cY ⊂conv(Wℓ∪ −Wℓ)}as the gauge norm of Y(Rockafellar, 2015).\n",
            "We can consider the gauge norm γYas a problem-dependent constant. Now recall that\n",
            "λℓ\n",
            "⊥=τG\n",
            "ℓ−1\n",
            "8klog(1 +τG\n",
            "ℓ−1\n",
            "λ)≤2τG\n",
            "ℓ−1≤64Bℓ−1\n",
            "∗ρG(Y(Wℓ−1)) log(4( ℓ−1)2|W|/δℓ)\n",
            "ϵ2\n",
            "ℓ−1\n",
            "(a)\n",
            "≤64(Bℓ−1\n",
            "∗)2plog(4( ℓ−1)2|W|/δℓ)\n",
            "γ2\n",
            "Yϵ2\n",
            "ℓ−1\n",
            "(b)\n",
            "≤\u0000\n",
            "256(λS2+ 8p2r)\u0001\n",
            "log(4( ℓ−1)2p|W|/δℓ)\n",
            "Srγ2\n",
            "Yϵ2\n",
            "ℓ−1\n",
            "where, (a)follows from (19) and noting that Bℓ−1\n",
            "∗≤(Bℓ−1\n",
            "∗)2. The (b)follows as S⊥\n",
            "ℓ:=\n",
            "8pr\n",
            "τE\n",
            "ℓS2rlog\u0010\n",
            "d1+d2\n",
            "δℓ\u0011\n",
            ",p=d1d2and substituting this value and λℓ\n",
            "⊥inBℓ\n",
            "∗we get\n",
            "Bℓ\n",
            "∗≤(Bℓ\n",
            "∗)2≤\u0010\n",
            "256(λS2+λℓ\n",
            "⊥S(2),ℓ\n",
            "⊥)\u0011\n",
            "≤ \n",
            "256 \n",
            "λS2+8pr\n",
            "S2rτG\n",
            "ℓ−1\n",
            "τE\n",
            "ℓ·log(4( ℓ−1)2|W|/δℓ) log( d1+d2/δℓ)!!\n",
            "≤\u0012\n",
            "256\u0012\n",
            "λS2+8pr\n",
            "S2rρG\n",
            "ℓ−1(W) log( p|W|/δℓ)\u0013\u0013(a)\n",
            "≤\u0000\n",
            "512\u0000\n",
            "λS2+ 8p2r\u0001\n",
            "/Sr\u0001\n",
            ".\n",
            "HereSris the r-th larget eigenvalue of matrix Θ∗. Substituting this value of λ⊥\n",
            "ℓwe can show that Λℓ\n",
            "does not depend on wory=w−w′. The rest of the proof to show equivalence follows the same\n",
            "way as in Theorem 21.1 in Lattimore and Szepesvári (2020).\n",
            "18To bound the support of bD\n",
            "∗we proceed as follows: Define the set Y(Wℓ)as the set of all arms\n",
            "containing y=w−w′∈Rp. Then we can use Lemma 7 of Soare et al. (2014) to show that the\n",
            "solution to\n",
            "max\n",
            "y∈Y(Wℓ)∥y∥2\u0010P\n",
            "w∈Wℓbwww⊤+Λℓ\u0011−1= max\n",
            "w,w′∥w−w′∥2\u0010P\n",
            "w∈Wℓbwww⊤+Λℓ\u0011−1\n",
            "has a support of atmost (k1+ 1)k1/2where k1= 8klog(1 + τG\n",
            "ℓ−1/λ). The proof follows from the\n",
            "fact that for any pair (w,w′)we can show that\n",
            "∥w−w′∥\u0010P\n",
            "w∈Wℓbwww⊤+Λ\u0011−1≤2 max\n",
            "w′′∈Wℓ∥w′′∥(P\n",
            "w∈Wbwww⊤+Λℓ)−1.\n",
            "Then following the work of Jamieson and Jain (2022) Frank-Wolfe algorithm (in section 2.3.1) with\n",
            "the\n",
            "g(b) = log\f\f\fP\n",
            "w∈Wℓbwww⊤+Λℓ\f\f\f\n",
            "|Λℓ|= log\f\f\f\f\f\fX\n",
            "w∈Wℓbwww⊤+Λℓ\f\f\f\f\f\f−log|Λℓ|,\n",
            "and setting for the j-th iteration of the Frank-Wolfe the\n",
            "Ij= arg max\n",
            "y∈Y(Wℓ)∥y∥2\u0010P\n",
            "w∈Wℓbj\n",
            "www⊤+Λℓ\u0011−1,\n",
            "and stopping condition\n",
            "max\n",
            "y∈Y(Wℓ)∥y∥2\u0010P\n",
            "w∈Wℓbj\n",
            "www⊤+Λℓ\u0011−1≤8klog(1 +τG\n",
            "ℓ−1\n",
            "λ) (20)\n",
            "This can be done because note that for any b∈ △Wℓwe have by Kiefer-Wolfowitz Theorem\n",
            "(Kiefer and Wolfowitz, 1960) that [∇g(b)]y=∥y∥2\n",
            "(P\n",
            "x∈Xbxxx⊤+Λ)−1≥8klog(1 +τG\n",
            "ℓ−1\n",
            "λ). This\n",
            "is because Λℓdoes not depend on woryby the same logic as discussed before. The rest of the\n",
            "proof follows by the same way as in section 2.3.1 in Jamieson and Jain (2022). This will result\n",
            "in a support size of bat most 8klog(1 +τG\n",
            "ℓ−1\n",
            "λ)(8klog(1 +τG\n",
            "ℓ−1\n",
            "λ) + 1) /2following Lemma 7 of\n",
            "Soare et al. (2014). Hence, it follows that solving the Equation (5) will result in a support of\n",
            "|bD\n",
            "ℓ| ≤8klog(1+τG\n",
            "ℓ−1\n",
            "λ)(8klog(1+τG\n",
            "ℓ−1\n",
            "λ)+1)\n",
            "2.\n",
            "Proposition 1. IfbG\n",
            "ℓis the G-optimal design for Wℓthen if we pull arm w∈ Wℓexactly\u0006\n",
            "τGbG\n",
            "ℓ\u0007\n",
            "times for some τG\n",
            "ℓ>0and compute the least squares estimator bθℓ. Then for each w∈ Wℓwe have\n",
            "with probability at least 1−δ\n",
            "P\u0012[\n",
            "w∈Wℓ\u001a\f\f\fD\n",
            "w,bθℓ−θ∗E\f\f\f≤vuut64Bℓ\n",
            "∗klog(1 +τG\n",
            "ℓ−1\n",
            "λ) log (2 |W|/δℓ)\n",
            "τG\n",
            "ℓ\u001b\u0013\n",
            "≥1−δℓ.\n",
            "where, ∥θ∗∥Λ≤q\n",
            "λ∥θ1:k∥2\n",
            "2+λ⊥\n",
            "ℓ∥θk+1:p∥2\n",
            "2≤√\n",
            "λS+q\n",
            "λ⊥\n",
            "ℓSℓ\n",
            "⊥,Bℓ\n",
            "∗=√\n",
            "λS+q\n",
            "λ⊥\n",
            "ℓS⊥\n",
            "ℓ.\n",
            "Proof. From Woodbury Matrix Identity we know that for any arbitrary matrix AandB, we have the\n",
            "following identity (A+B)−1=A−1−\u0000\n",
            "A+AB−1A\u0001−1. It follows then that\n",
            "w⊤(A+B)−1w=w⊤\u0010\n",
            "A−1−\u0000\n",
            "A+AB−1A\u0001−1\u0011\n",
            "w≤w⊤A−1w=∥w∥A−1.\n",
            "Hence we can show that,\n",
            "∥w∥\u0010P\n",
            "w∈Wℓ⌈τG\n",
            "ℓbG\n",
            "ℓ⌉ww⊤+Λℓ\u0011−1≤w⊤\n",
            "X\n",
            "w∈Wℓ⌈τG\n",
            "ℓbG\n",
            "ℓ⌉ww⊤\n",
            "−1\n",
            "w. (21)\n",
            "From Lemma 9 we know the support of bG\n",
            "ℓis less than\n",
            "8klog(1 +τG\n",
            "ℓ−1\n",
            "λ)(8klog(1 +τG\n",
            "ℓ−1\n",
            "λ) + 1)\n",
            "2≤(8klog(1 +τG\n",
            "ℓ−1\n",
            "λ))2.\n",
            "19Also note that ∥θ∗∥Λℓ≤q\n",
            "λ∥θ1:k∥2\n",
            "2+λℓ\n",
            "⊥∥θk+1:p∥2\n",
            "2≤√\n",
            "λS+q\n",
            "λℓ\n",
            "⊥Sℓ\n",
            "⊥. Then we can show that\n",
            "D\n",
            "w,bθℓ−θ∗E\n",
            "≤ ∥w∥\u0010P\n",
            "w∈Wℓ⌈τG\n",
            "ℓbG\n",
            "ℓ⌉ww⊤+Λ\u0011−1\u0012\n",
            "2p\n",
            "14 log(2 /δℓ) +√\n",
            "λS+q\n",
            "λℓ\n",
            "⊥Sℓ\n",
            "⊥\u0013\n",
            "≤1q\n",
            "τG\n",
            "ℓ∥w∥\u0010P\n",
            "w∈Wℓbℓ∗ww⊤+Λ\u0011−1\u0012\n",
            "2p\n",
            "14 log(2 /δℓ) +√\n",
            "λS+q\n",
            "λℓ\n",
            "⊥Sℓ\n",
            "⊥\u0013\n",
            "(a)\n",
            "≤vuut56×8klog(1 +τG\n",
            "ℓ−1\n",
            "λ) log(2 /δℓ)\n",
            "τG\n",
            "ℓ+vuut28klog(1 +τG\n",
            "ℓ−1\n",
            "λ) log(2 /δℓ)\n",
            "τG\n",
            "ℓ(√\n",
            "λS+q\n",
            "λℓ\n",
            "⊥Sℓ\n",
            "⊥)\n",
            "=vuut8klog(1 +τG\n",
            "ℓ−1\n",
            "λ) log(2 /δℓ)\n",
            "τG\n",
            "ℓ\n",
            "√\n",
            "56 +√\n",
            "λS+q\n",
            "λℓ\n",
            "⊥Sℓ\n",
            "⊥| {z }\n",
            "Bℓ∗\n",
            "\n",
            "≤vuut64Bℓ∗klog(1 +τG\n",
            "ℓ−1\n",
            "λ) log(2 /δℓ)\n",
            "τG\n",
            "ℓ\n",
            "where, (a)follows as ∥w∥\u0010P\n",
            "w∈WℓbG\n",
            "ℓ,www⊤+Λℓ\u0011−1≤(8klog(1 + τG\n",
            "ℓ−1/λ))2. Thus we have taken\n",
            "at most τG\n",
            "ℓ+8klog(1+τG\n",
            "ℓ−1\n",
            "λ)(8klog(1+τG\n",
            "ℓ−1\n",
            "λ)+1)\n",
            "2pulls. Thus, for any δ∈(0,1)we have\n",
            "P\u0012[\n",
            "w∈Wℓ\u001a\f\f\fD\n",
            "w,bθℓ−θ∗E\f\f\f≥vuut64kBℓ\n",
            "∗log(1 +τG\n",
            "ℓ−1\n",
            "λ) log (2 |W|/δℓ)\n",
            "τG\n",
            "ℓ\u001b\u0013\n",
            "≤δℓ.\n",
            "The claim of the lemma follows.\n",
            "Discussion 3. (Phase Length) It follows from Proposition 1 that if the gaps are known, one can set\n",
            "the phase length as\n",
            "τG\n",
            "ℓ=64Bℓ\n",
            "∗ρ(Y(Wℓ)) log (2 |W|/δ)\n",
            "(w⊤(bθℓ−θ∗))2\n",
            "since 8klog(1 +τG\n",
            "ℓ−1\n",
            "λ)≤ρ(Y(Wℓ)):= 8 klog(1 +τG\n",
            "ℓ\n",
            "λ)to guarantee that the event\n",
            "S\n",
            "w∈Wℓ\u001a\f\f\fD\n",
            "w,bθℓ−θ∗E\f\f\fholds with probability greater than 1−δ.\n",
            "However, since in practice, the gaps are not known, for an agnostic algorithm that does not know the\n",
            "gaps, one can set a proxy for the gap as ϵℓ(for some ϵℓ>0) and get the phase length as follows:\n",
            "τG\n",
            "ℓ=64Bℓ\n",
            "∗ρ(Y(Wℓ)) log (2 |W|/δ)\n",
            "ϵ2\n",
            "ℓ.\n",
            "This gives us the desired phase length so that the eventS\n",
            "w∈Wℓ\u001a\f\f\fD\n",
            "w,bθℓ−θ∗E\f\f\fholds with proba-\n",
            "bility greater than 1−δ.\n",
            "Lemma 10. Assume that maxw∈W⟨w∗−w,θ∗⟩ ≤2. With probability at least 1−δ, we have\n",
            "w∗∈ Wℓandmaxw∈Wℓ⟨w∗−w,θ∗⟩ ≤4ϵℓfor all ℓ∈N.\n",
            "Proof. For any V ⊆ Wℓbe the active set and w∈ V define\n",
            "Ew,ℓ(V) =n\f\f\fD\n",
            "w−w∗,bθℓ(V)−θ∗E\f\f\f≤ϵℓo\n",
            "(22)\n",
            "where it is implicit that bθℓ:=bθℓ(V)is the design constructed in the algorithm at stage ℓwith respect\n",
            "toWℓ=V. Also note that δℓ=δ\n",
            "4ℓ2. Given Wℓ, with probability at least 1−2·δ\n",
            "4ℓ2|W|\n",
            "20\f\f\fD\n",
            "w−w∗,bθℓ−θ∗E\f\f\f(a)\n",
            "≤s\n",
            "64Bℓ∗klog(1 +τG\n",
            "ℓ−1\n",
            "λ) log (4 ℓ2|W|/δ)\n",
            "≤(klog(1 +τG\n",
            "ℓ−1\n",
            "λ))2\n",
            "q\n",
            "τG\n",
            "ℓq\n",
            "64Bℓ∗log (4 ℓ2|W|/δ)\n",
            "(b)\n",
            "≤vuut∥w−w∗∥2\n",
            "(P\n",
            "w∈VbG\n",
            "ℓ,w(V)ww⊤+Λℓ)−1\n",
            "64Bℓ∗ϵ−2\n",
            "ℓρ(Y(Wℓ)) log (4 ℓ2|W|/δ)q\n",
            "64Bℓ∗log (4 ℓ2|W|/δ)\n",
            "≤vuut∥w−w∗∥2\n",
            "(P\n",
            "w∈VbG\n",
            "ℓ,w(V)ww⊤+Λℓ)−1\n",
            "ϵ−2\n",
            "ℓρ(Y(Wℓ)) log (4 ℓ2|W|/δ)p\n",
            "log (4 ℓ2|W|/δ)\n",
            "(c)=ϵℓ\n",
            "where, (a)follows Proposition 1. The (b)follows as\n",
            "(klog(1 +τG\n",
            "ℓ−1\n",
            "λ))2≤(klog(1 +τG\n",
            "ℓ\n",
            "λ))2:=ρ(Y(Wℓ)) =∥w−w∗∥2\n",
            "(P\n",
            "w∈VbG\n",
            "ℓ,w(V)ww⊤+Λℓ)−1.\n",
            "The(c)follows as ρ(Y(Wℓ)) =∥w−w∗∥2\n",
            "(P\n",
            "w∈VbG\n",
            "ℓ,w(V)ww⊤+Λℓ)−1. By exactly the same se-\n",
            "quence of steps as above, we have\n",
            "P\n",
            "∞\\\n",
            "ℓ=1\\\n",
            "w∈Wℓn\f\f\fD\n",
            "w−w∗,bθt−θ∗E\f\f\f> ϵto\n",
            "=P\n",
            "\\\n",
            "w∈Wℓ∞\\\n",
            "ℓ=1Ew,ℓ(Wℓ)\n",
            "≥1−δ,\n",
            "so assume these events hold. Consequently, for any w′∈ Wℓ\n",
            "D\n",
            "w′−w∗,bθℓE\n",
            "=D\n",
            "w′−w∗,bθℓ−θ∗E\n",
            "+⟨w′−w∗,θ∗⟩\n",
            "≤D\n",
            "w′−w∗,bθℓ−θ∗E\n",
            "≤ϵℓ\n",
            "so that w∗would survive to round ℓ+ 1. And for any w∈ Wℓsuch that ⟨w∗−w,θ∗⟩>2ϵℓwe\n",
            "have\n",
            "max\n",
            "w′∈WℓD\n",
            "w′−w,bθℓE\n",
            "≥D\n",
            "w∗−w,bθℓE\n",
            "=D\n",
            "w∗−w,bθℓ−θ∗E\n",
            "+⟨w∗−w,θ∗⟩\n",
            ">−ϵℓ+ 2ϵℓ\n",
            "=ϵℓ\n",
            "which implies this wwould be eliminated. Note that this implies that maxw∈Wℓ+1⟨w∗−w,θ∗⟩ ≤\n",
            "2ϵℓ= 4ϵℓ+1. Hence, the claim of the lemma follows.\n",
            "A.5 Final Sample Complexity Bound for Single Task Setting\n",
            "Theorem 1. (Restatement) With probability at least 1−δ, GOBLIN returns the best arms x∗,z∗,\n",
            "and the number of samples used is bounded by\n",
            "eO\u0012(d1+d2)r\n",
            "∆2+√d1d2r\n",
            "Sr\u0013\n",
            "where, ∆ = min x∈X\\{ x∗},z∈Z\\{ z∗}(x⊤\n",
            "∗Θ∗z∗−x⊤Θ∗z), and Sris the r-th largest singular value\n",
            "ofΘ∗.\n",
            "21Proof. For the rest of the proof we have that the good events FℓTEw,ℓ(Wℓ)holds true for each\n",
            "phase ℓwith probability greater than (1−δ). The two events are defined in (16) and (22).\n",
            "Second Stage: Define Aℓ={w∈ Wℓ:⟨w∗−w,θ∗⟩ ≤4ϵℓ}. Note that by assumption W=\n",
            "W1=S1. The above lemma implies that with probability at least 1−δwe haveT∞\n",
            "ℓ=1{Wℓ⊆ Aℓ}.\n",
            "This implies that\n",
            "ρG(Wℓ) = min\n",
            "b∈∆Wmax\n",
            "w,w′∈Wℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "≤min\n",
            "b∈∆Wmax\n",
            "w,w′∈Aℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "=ρG(Aℓ).\n",
            "Define kℓ\n",
            "1= 8klog(1+ τG\n",
            "ℓ−1/λ). Forℓ≥\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "we have that Aℓ={w∗}, thus, the sample\n",
            "complexity to identify w∗is equal to\n",
            "⌈log2(4∆−1)⌉X\n",
            "ℓ=1X\n",
            "w∈Wl\n",
            "τG\n",
            "ℓbbG\n",
            "ℓ,wm\n",
            "=⌈log2(4∆−1)⌉X\n",
            "ℓ=1\u0012(kℓ\n",
            "1+ 1)kℓ\n",
            "1\n",
            "2+τG\n",
            "ℓ\u0013\n",
            "=⌈log2(4∆−1)⌉X\n",
            "ℓ=1\u0012(kℓ\n",
            "1+ 1)kℓ\n",
            "1\n",
            "2+ 2ϵ−2\n",
            "ℓρG(Wℓ)Bℓ\n",
            "∗log\u0000\n",
            "4kℓ\n",
            "1ℓ2|W|/δ\u0001\u0013\n",
            "(a)\n",
            "≤2⌈log2(4∆−1)⌉X\n",
            "ℓ=1\u0012(k+ 1)k\n",
            "2log2(1 +τG\n",
            "ℓ−1) + 2ϵ−2\n",
            "ℓρG(Wℓ)Bℓ\n",
            "∗log\u0000\n",
            "4kℓ\n",
            "1ℓ2|W|/δ\u0001\u0013\n",
            "(b)\n",
            "≤2(k+ 1)k\n",
            "2⌈log2(4∆−1)⌉X\n",
            "ℓ=1\u0000\n",
            "log2(1 +τG\n",
            "ℓ−1) + 8ϵ−2\n",
            "ℓρG(Wℓ)Bℓ\n",
            "∗log\u0000\n",
            "4kℓ\n",
            "1ℓ2|W|/δ\u0001\u0001\n",
            "(c)\n",
            "≤(k+ 1)k⌈log2(4∆−1)⌉X\n",
            "ℓ=1\u0000\n",
            "1 + 16 ϵ−2\n",
            "ℓρG(Wℓ)Bℓ\n",
            "∗log2\u0000\n",
            "4kℓ\n",
            "1ℓ2|W|/δ\u0001\u0001\n",
            "(d)\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+⌈log2(4∆−1)⌉X\n",
            "ℓ=132ϵ−2\n",
            "ℓf(Aℓ)Bℓ\n",
            "∗log\u0000\n",
            "4kℓ2|W|/δ\u0001\n",
            "(e)\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+⌈log2(4∆−1)⌉X\n",
            "ℓ=132ϵ−2\n",
            "ℓf(Aℓ) (64λS2+ 64τG\n",
            "ℓ−1) log\u0000\n",
            "4kℓ2|W|/δ\u0001\n",
            "= (k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+⌈log2(4∆−1)⌉X\n",
            "ℓ=132ϵ−2\n",
            "ℓf(Aℓ) (64λS2) log\u0000\n",
            "4kℓ2|W|/δ\u0001\n",
            "+ (k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+⌈log2(4∆−1)⌉X\n",
            "ℓ=132ϵ−2\n",
            "ℓf(Aℓ) (64τG\n",
            "ℓ−1) log\u0000\n",
            "4kℓ2|W|/δ\u0001\n",
            "(f)\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+⌈log2(4∆−1)⌉X\n",
            "ℓ=164ϵ−2\n",
            "ℓf(Aℓ) (64λS2) log\u0000\n",
            "4kℓ2|W|/δ\u0001\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+ 2048 λS2log \n",
            "4klog2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!⌈log2(4∆−1)⌉X\n",
            "ℓ=122ℓf(Aℓ)\n",
            "where, (a)follows as log2(1 +τG\n",
            "ℓ−1/λ)≤log2(1 +τG\n",
            "ℓ−1),(b)follows by noting that log(xlog(1 +\n",
            "x))≤2 log( x)for any x >1. The (c)follows by subsuming the log2(1 +τG\n",
            "ℓ−1)into2τG\n",
            "ℓ. The (d)\n",
            "follows as log(1 + τG\n",
            "ℓ−1)< τG\n",
            "ℓwhich enables us to replace the kℓ\n",
            "1inside the logwith an additional\n",
            "22factor of 2. The (e)follows by noting that\n",
            "Bℓ\n",
            "∗≤64(√\n",
            "λS+q\n",
            "λ⊥\n",
            "ℓS⊥\n",
            "ℓ)\n",
            "≤64λS2+\n",
            "64τG\n",
            "ℓ−1\n",
            "8(d1+d2)rlog(1 +τG\n",
            "ℓ−1\n",
            "λ)\n",
            "·\u00128d1d2r\n",
            "τE\n",
            "ℓS2rlog\u0012d1+d2\n",
            "δℓ\u0013\u0013\n",
            "(a1)\n",
            "≤64λS2+ 64τG\n",
            "ℓ−1. (23)\n",
            "where, (a1)follows by first substituting the value of τE\n",
            "ℓ:=√\n",
            "8d1d2rlog(4ℓ2|W|/δℓ)\n",
            "Srand noting thatp\n",
            "(d1d2r)≤(d1+d2)rand cancelling out the other terms. Finally the (f)follows by subsuming\n",
            "theτG\n",
            "ℓ−1with a factor of 2into the quantity of τG\n",
            "ℓ. Then it follows that\n",
            "ρG\n",
            "∗= inf\n",
            "b∈△Wmax\n",
            "w∈W∥w−w∗∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "(⟨w−w∗,θ∗⟩)2\n",
            "= inf\n",
            "b∈△Wmax\n",
            "ℓ≤⌈log2(4∆−1)⌉max\n",
            "w∈Aℓ∥w−w∗∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "(⟨w−w∗,θ∗⟩)2\n",
            "≥1\n",
            "⌈log2(4∆−1)⌉inf\n",
            "b∈△W⌈log2(4∆−1)⌉X\n",
            "ℓ=1max\n",
            "w∈Aℓ∥w−w∗∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "(⟨w−w∗,θ∗⟩)2\n",
            "≥1\n",
            "16⌈log2(4∆−1)⌉⌈log2(4∆−1)⌉X\n",
            "ℓ=122ℓinf\n",
            "b∈△Wmax\n",
            "w∈Aℓ∥w−w∗∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "≥1\n",
            "64⌈log2(4∆−1)⌉⌈log2(4∆−1)⌉X\n",
            "ℓ=122ℓinf\n",
            "b∈△Wmax\n",
            "w,w′∈Aℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "≥1\n",
            "64⌈log2(4∆−1)⌉⌈log2(4∆−1)⌉X\n",
            "ℓ=122ℓf(Aℓ).\n",
            "This implies that\n",
            "⌈log2(4∆−1)⌉X\n",
            "ℓ=122ℓf(Aℓ)≤ρG\n",
            "∗64\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "Plugging this back we get\n",
            "⌈log2(4∆−1)⌉X\n",
            "ℓ=1X\n",
            "w∈Wl\n",
            "τG\n",
            "ℓbbℓ,wm\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+ 2048 λS2log \n",
            "8klog2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!\n",
            "ρG\n",
            "∗64\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+C2λS2log \n",
            "8klog2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!\n",
            "ρG\n",
            "∗\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "for some constant C2>0. Now to understand the bound we need the following: Let conv(W∪−W )\n",
            "denote the convex hull of W∪ −W , and for any set Y ⊂Rpdefine the gauge of Y\n",
            "γY= max {c >0 :cY ⊂conv(W∪ −W )} (24)\n",
            "23In the case where Yis a singleton Y={y}, γ(y) := γYis the gauge norm of ywith respect to\n",
            "conv(W∪−W ). We can provide a natural upper bound for ρ(Y)in terms of the gauge. Observe that\n",
            "ρG\n",
            "∗= inf\n",
            "b∈△Wmax\n",
            "y∈Y∥y∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "=1\n",
            "γ2\n",
            "Yinf\n",
            "b∈△Wmax\n",
            "y∈Y∥yγY∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "≤1\n",
            "γ2\n",
            "Yinf\n",
            "b∈△Wmax\n",
            "w∈conv(WS−W)∥w∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "(a)=1\n",
            "γ2\n",
            "Yinf\n",
            "b∈△Wmax\n",
            "w∈W∥w∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1(b)\n",
            "≤k3/2\n",
            "≤k\n",
            "γ2\n",
            "YO \n",
            "B∗log \n",
            "klog2\u0000\n",
            "∆−1\u0001\n",
            "|W|\n",
            "δ!\n",
            "\u0006\n",
            "log2\u0000\n",
            "∆−1\u0001\u0007!\n",
            "The(a)follows from the fact that the maximum value of a convex function on a convex set\n",
            "must occur at a vertex. The (b)follows from Kiefer-wolfowitz theorem for w∈Rpsuch that\n",
            "infb∈△Wmaxw∈W∥w∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1≤klog(1 +τG\n",
            "ℓ−1\n",
            "λ). The simplified sample complex-\n",
            "ity for the second stage is given by\n",
            "N2≤O \n",
            "k\n",
            "∆2log \n",
            "klog2\u0000\n",
            "∆−1\u0001\n",
            "|W|\n",
            "δ!!\n",
            "=eO\u0012(d1+d2)r\n",
            "∆2\u0013\n",
            "where ∆ = min w∈W(w∗−w)⊤θ∗(a1)= min x∈X\\{ x∗},z∈Z\\{ z∗}(x⊤\n",
            "∗Θ∗z∗−x⊤Θ∗z). The (a1)\n",
            "follows by reshaping the arms in Wto recover the arms in XandZ.\n",
            "1st Stage: First recall that the E-optimal design in step 3of Algorithm 1 satisfies the Assumption 4\n",
            "as the sample distribution Dhas finite second order moments. For the first stage first observe that by\n",
            "plugging in the definition of τE\n",
            "ℓ=√\n",
            "8d1d2rlog(4ℓ2|W|/δ)\n",
            "Srwe get\n",
            "\r\rθ∗\n",
            "k+1:p\r\r2\n",
            "2=X\n",
            "i>r∧j>rH2\n",
            "ij=\r\r\r(bU⊥\n",
            "ℓ)⊤\u0000\n",
            "U∗S∗V∗⊤\u0001bV⊥\n",
            "ℓ\r\r\r2\n",
            "F\n",
            "≤\r\r\r(bU⊥\n",
            "ℓ)⊤U∗\r\r\r2\n",
            "F∥S∗∥2\n",
            "2\r\r\r(bV⊥\n",
            "ℓ)⊤V∗\r\r\r2\n",
            "F≤O\u0012d1d2r\n",
            "τE\n",
            "ℓS2rlog\u0012d1+d2\n",
            "δ\u0013\u0013\n",
            "=O\u0012√d1d2r\n",
            "Srlog\u0012d1+d2\n",
            "δ\u0013\u0013\n",
            "which implies\r\r\rθ∗\n",
            "k+1:p\r\r\r\n",
            "2=eO\u0000√d1d2r/Sr\u0001\n",
            ". Now we bound the sample complexity from the first\n",
            "stage. From the first stage we can show that we have for the arm set W\n",
            "N1=⌈log2(4∆−1)⌉X\n",
            "ℓ=1X\n",
            "w∈Wl\n",
            "τE\n",
            "ℓbbE\n",
            "ℓ,wm\n",
            "=⌈log2(4∆−1)⌉X\n",
            "ℓ=1\u0012(p+ 1)p\n",
            "2+τE\n",
            "ℓ\u0013\n",
            "=⌈log2(4∆−1)⌉X\n",
            "ℓ=1 \n",
            "(p+ 1)p\n",
            "2+p\n",
            "8d1d2rlog (4 ℓ2|W|/δ)\n",
            "Sr!\n",
            "≤(p+ 1)p\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+ 32√d1d2r\n",
            "Srlog \n",
            "4 log2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!\n",
            "\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "(a)=O √d1d2r\n",
            "Srlog \n",
            "4 log2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!!\n",
            "=eO\u0012√d1d2r\n",
            "Sr\u0013\n",
            "where, (a)follows as p=d1d2. Combining N1andN2gives the claim of the theorem.\n",
            "24A.6 Multi-Task Pure Exploration Proofs\n",
            "Remark 2. (Comparison with Du et al. (2023)) In this remark, we discuss a key comparison of\n",
            "DouExpDes (Du et al., 2023) with GOBLIN. Note that DouExpDes does not implement the second\n",
            "stage of finding the Sm,∗∈Rk1×k2for each of the mbilinear bandits. Hence, DouExpDes does\n",
            "not rotate the arms so that the last (k1−r)·(k2−r)components are from the complementary\n",
            "subspaces of the left and right eigenvectors of Sm,∗. This results in DouExpDes suffering a sample\n",
            "complexity of eO(k1k2/∆2)even though it learns the common feature extractors shared across the\n",
            "tasks. In contrast GOBLIN uses the second stage to learn Sm,∗∈Rk1×k2and reduces the latent\n",
            "bilinear bandits problem of k1k2dimension to (k1+k2)rdimension by rotating the arms so that\n",
            "the last (k1−r)·(k2−r)components are from the complementary subspaces of the left and right\n",
            "eigenvectors of Sm,∗. Hence, GOBLIN suffers a sample complexity of eO((k1+k2)r/∆2).\n",
            "Remark 3. (Arm set) The observable left and right arm sets XandZare common across the M\n",
            "tasks. This leads to each task estimating the same E-optimal design in line 3of Algorithm 2 of stage\n",
            "1. Note that Du et al. (2023) also uses a similar idea of the same arm set Xshared across tasks in the\n",
            "linear bandit setting. Observe that if each task has access to its own separate arm sets XmandZm,\n",
            "then each of the m-tasks has to estimate a separate E-optimal design for the stage 1. This will lead to\n",
            "the sample complexity of the first stage scaling as eO(M√d1d2/Sr)instead of eO(√d1d2r/Sr).\n",
            "Good Event: We first recall the total stage 1length as\n",
            "τE\n",
            "ℓ:=p\n",
            "8d1d2rlog(4ℓ2|W|/δℓ)\n",
            "Sr.\n",
            "Then define the good event Fℓin phase ℓthat GOBLIN has a good estimate of Z∗=1\n",
            "MPM\n",
            "m=1Θm\n",
            "as follows: For any phase ℓ >0\n",
            "Fℓ:=\n",
            "\n",
            "\r\r\rbZℓ−µ∗Z∗\r\r\r2\n",
            "F≤C1d1d2rlog\u0010\n",
            "2(d1+d2)\n",
            "δℓ\u0011\n",
            "τE\n",
            "ℓ\n",
            "\n",
            ", (25)\n",
            "where, C1= 36\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "C,∥X∥F,∥Θ∗∥F≤S0, some nonzero constant µ∗,Eh\n",
            "(Sp(X))2\n",
            "iji\n",
            "≤\n",
            "C,∀i, j, andbΘℓis the estimate from (8). Then define the event\n",
            "F:=∞\\\n",
            "ℓ=1Fℓ (26)\n",
            "Then we start by modifying Lemma 3 for the multi-task setting. We first prove this support lemma\n",
            "for the loss function defined in (8).\n",
            "Lemma 11. LetL:Rd1×d2→Ris the loss function defined in (8). Then by setting\n",
            "t=s\n",
            "2d1d2C(4 +S2\n",
            "0) log\u00122 (d1+d2)\n",
            "δℓ\u0013\n",
            ",\n",
            "ν=t\n",
            "(4 +S0)Cd1d2q\n",
            "MτE\n",
            "ℓ=vuut2 log\u0010\n",
            "2(d1+d2)\n",
            "δℓ\u0011\n",
            "MτE\n",
            "ℓd1d2C(4 +S2\n",
            "0),\n",
            "we have with probability at least 1−δℓ, it holds that\n",
            "P\n",
            "∥∇L(µ∗Z∗)∥op≥2tq\n",
            "MτE\n",
            "ℓ\n",
            "≤δℓ,\n",
            "where µ∗=2\n",
            "ME[⟨Xm,Z∗⟩]>0, andXm=xmz⊤\n",
            "m.\n",
            "25Proof. LetZ∗=1\n",
            "MPM\n",
            "m=1Θm,∗. LetXm,i=xm,iz⊤\n",
            "m,ifori∈[τE\n",
            "ℓ]. Based on the definition of\n",
            "our loss function L(·)in (8), we have that\n",
            "∇xmL(Z∗) =µ∗Z∗−2\n",
            "MτE\n",
            "ℓMX\n",
            "m=1τE\n",
            "ℓX\n",
            "i=1eψν(rm,i·Q(xm))\n",
            "=2\n",
            "ME[⟨Xm,1,Z∗⟩]Z∗−2\n",
            "MτE\n",
            "ℓMX\n",
            "m=1τE\n",
            "ℓX\n",
            "i=1eψν(rm,i·Q(Xm,i))\n",
            "(a)=2\n",
            "ME[⟨Xm,1,Z∗⟩Q(Xm,1)]−2\n",
            "MτE\n",
            "ℓMX\n",
            "m=1τE\n",
            "ℓX\n",
            "i=1eψν(rm,i·Q(Xm,i))\n",
            "(b)=2\n",
            "M\n",
            "E(rm,1·Q(Xm,1))−1\n",
            "MτE\n",
            "ℓMX\n",
            "m=1τE\n",
            "ℓX\n",
            "i=1eψν(rm,i·Q(Xm,i))\n",
            "\n",
            "where we have (a) due to the generalized Stein’s Lemma stated in Lemma 1, and (b) comes from the\n",
            "fact that the random noise η1=y1− ⟨X1,Z∗⟩is zero-mean and independent from X1. Therefore,\n",
            "in order to implement the Lemma 2, we can see that it suffices to get σ2defined as:\n",
            "σ2= max\n",
            "\r\r\r\r\r\r2\n",
            "MMX\n",
            "m=1τE\n",
            "ℓX\n",
            "j=1Eh\n",
            "r2\n",
            "m,jQ(Xm,j)Q(Xm,j)⊤i\r\r\r\r\r\r\n",
            "op,\r\r\r\r\r\r2\n",
            "MMX\n",
            "m=1τE\n",
            "ℓX\n",
            "j=1Eh\n",
            "r2\n",
            "m,jQ(Xm,j)⊤Q(Xm,j)i\r\r\r\r\r\r\n",
            "op\n",
            ".\n",
            "\r\r\r\r\r\r2\n",
            "MMX\n",
            "m=1τE\n",
            "ℓX\n",
            "j=1Eh\n",
            "r2\n",
            "m,jQ(Xm,j)Q(Xm,j)⊤i\r\r\r\r\r\r\n",
            "op≤τE\n",
            "ℓ×\r\r\rEh\n",
            "r2\n",
            "m,1Q(Xm,1)Q(Xm,1)⊤i\r\r\r\n",
            "op\n",
            "(a)=τE\n",
            "ℓ×\r\r\rEh\n",
            "(ηm,1+⟨Xm,1,Z∗⟩)2Q(Xm,1)Q(Xm,1)⊤i\r\r\r\n",
            "op\n",
            "(b)=τE\n",
            "ℓ× ∥Eh\n",
            "η2\n",
            "m,1Q(Xm,1)Q(Xm,1)⊤i\n",
            "+E[⟨Xm,1,Z∗⟩)2Q(Xm,1)Q(Xm,1)⊤\u0015\n",
            "∥op\n",
            "(c)=τE\n",
            "ℓ× ∥E\u0000\n",
            "η2\n",
            "m,1\u0001\n",
            "Eh\n",
            "Q(Xm,1)Q(Xm,1)⊤i\n",
            "+E[⟨Xm,1,Z∗⟩)2Q(Xm,1)Q(Xm,1)⊤\u0015\n",
            "∥op\n",
            "(d)\n",
            "≤τE\n",
            "ℓ×\r\r\r4Eh\n",
            "Q(Xm,1)Q(Xm,1)⊤i\n",
            "+S2\n",
            "0Eh\n",
            "Q(Xm,1)Q(Xm,1)⊤i\r\r\r\n",
            "op\n",
            "=\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "τE\n",
            "ℓ×\r\r\rEh\n",
            "Q(Xm,1)Q(Xm,1)⊤i\r\r\r\n",
            "op\n",
            "where the (a)follows by plugging in the definition for reward, (b)follows by the linearity of\n",
            "expectation, (c)follows as noises are independent, and the inequality (d)comes from the fact that\n",
            "|⟨Xm,1,Z∗⟩| ≤S0, andQ(Xm,1)Q(Xm,1)⊤is always positive semidefinite. Next, since we know\n",
            "thatEh\n",
            "Q(Xm,1)Q(Xm,1)⊤i\n",
            "is always symmetric and positive semidefinite, and hence we have\n",
            "\r\r\rEh\n",
            "Q(Xm,1)Q(Xm,1)⊤i\r\r\r\n",
            "op(a)\n",
            "≤\r\r\rEh\n",
            "Q(Xm,1)Q(Xm,1)⊤i\r\r\r\n",
            "nuc= trace\u0010\n",
            "Eh\n",
            "Q(Xm,1)Q(Xm,1)⊤i\u0011\n",
            "=Eh\n",
            "trace\u0010\n",
            "Q(Xm,1)Q(Xm,1)⊤\u0011i\n",
            "=E\n",
            "2\n",
            "MMX\n",
            "m=1d1X\n",
            "i=1d2X\n",
            "j=1Qij(Xm,1)2\n",
            "\n",
            "≤d1d2C.\n",
            "where, in (a)∥ · ∥ nucdenotes the nuclear norm. Therefore, we have that under 1-subGaussian\n",
            "assumption\n",
            "\r\r\r\r\r\r2\n",
            "MMX\n",
            "m=1τE\n",
            "ℓX\n",
            "j=1Eh\n",
            "r2\n",
            "m,jQ(Xm,j)Q(Xm,j)⊤i\r\r\r\r\r\r\n",
            "op≤\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "d1d2τE\n",
            "ℓC.\n",
            "26And similarly, we can prove that\n",
            "\r\r\r\r\r\r2\n",
            "MMX\n",
            "m=1τE\n",
            "ℓX\n",
            "j=1Eh\n",
            "r2\n",
            "m,jQ(Xm,j)⊤Q(Xm,j)i\r\r\r\r\r\r\n",
            "op≤\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "d1d2τE\n",
            "ℓC.\n",
            "Therefore, we can take σ2=\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "d1d2τE\n",
            "ℓCconsequently. By using Lemma 2, we have\n",
            "P\n",
            "∥∇L(µ∗Z∗)∥op≥2tq\n",
            "MτE\n",
            "ℓ\n",
            "≤2 (d1+d2) exp \n",
            "−νtq\n",
            "MτE\n",
            "ℓ+ν2\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "Cd1d2τE\n",
            "ℓ\n",
            "2!\n",
            "By plugging the values of tandνin Lemma 11, we finish the proof.\n",
            "Lemma 12. For any low-rank linear model with samples X1. . . ,XτE\n",
            "ℓdrawn from Xaccording\n",
            "toDthen for the optimal solution to the nuclear norm regularization problem in (2)withν=q\n",
            "2 log (2 ( d1+d2)/δℓ)/\u0000\n",
            "(4 +S2\n",
            "0)MτE\n",
            "ℓd1d2\u0001\n",
            "and\n",
            "γℓ= 4s\n",
            "2 (4 + S2\n",
            "0)Cd1d2log (2 ( d1+d2)/δℓ)\n",
            "MτE\n",
            "ℓ,\n",
            "with probability at least 1−δℓit holds that:\n",
            "\r\r\rbZℓ−µ∗Z∗\r\r\r2\n",
            "F≤C1d1d2rlog\u0010\n",
            "2(d1+d2)\n",
            "δℓ\u0011\n",
            "MτE\n",
            "ℓ,\n",
            "forC1= 36\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "C,∥X∥F,∥Z∗∥F≤S0, some nonzero constant µ∗, andEh\n",
            "(Sp(X))2\n",
            "iji\n",
            "≤\n",
            "C,∀i, j. Summing over all phases ℓ≥1it follows that P(F)≥1−δ/2.\n",
            "Proof. Since the estimator bZℓminimizes the regularized loss function defined in (8), we have\n",
            "L(bZℓ) +γℓ∥bZℓ∥nuc≤L(µ∗Z∗) +γℓ∥µ∗Z∗∥nuc\n",
            "And due to the fact that L(·)is a quadratic function, we have the following expression based on\n",
            "multivariate Taylor’s expansion:\n",
            "L(bZℓ)−L(µ∗Z∗) =⟨∇L(µ∗Z∗),Θ⟩+ 2∥Θ∥2\n",
            "F,where Θ=bZℓ−µ∗Z∗\n",
            "By rearranging the above two results, we can deduce that\n",
            "2∥Θ∥2\n",
            "F≤ −⟨∇ L(µ∗Z∗),Θ⟩+γℓ∥µ∗Z∗∥nuc−γℓ∥bZℓ∥nuc\n",
            "(a)\n",
            "≤ ∥∇ L(µ∗Z∗)∥op∥Θ∥nuc+γℓ∥µ∗Z∗∥nuc−γℓ∥bZℓ∥nuc, (27)\n",
            "where (a)comes from the duality between matrix operator norm and nuclear norm. Next, we\n",
            "represent the saturated SVD of Z∗in the main paper as Z∗=UDV⊤where U∈Rd1×rand\n",
            "V∈Rd2×r, and here we would work on its full version, i.e.\n",
            "Z∗= (U,U⊥)\u0012\n",
            "D0\n",
            "0 0\u0013\n",
            "(V,V⊥)⊤= (U,U⊥)D∗(V,V⊥)⊤\n",
            "where we have U⊥∈Rd1×(d1−r),D∗∈Rd1×d2andV⊥∈Rd2×(d2−r). Furthermore, we define\n",
            "Λ= (U,U⊥)⊤Θ(V,V⊥) =\u0012\n",
            "U⊤ΘV U⊤ΘV⊥\n",
            "U⊤\n",
            "⊥ΘV U⊤\n",
            "⊥ΘV⊥\u0013\n",
            "=Λ1+Λ2\n",
            "where we write\n",
            "Λ1=\u00120 0\n",
            "0U⊤\n",
            "⊥ΘV⊥\u0013\n",
            ",Λ2=\u0012\n",
            "U⊤ΘV U⊤ΘV⊥\n",
            "U⊤\n",
            "⊥ΘV 0\u0013\n",
            ".\n",
            "27Afterward, it holds that\n",
            "∥bZℓ∥nuc=∥µ∗Z∗+Θ∥nuc(a)=\r\r\r(U,U⊥) (µ∗D∗+Λ) (V,V⊥)⊤\r\r\r\n",
            "nuc\n",
            "(b)=∥µ∗D∗+Λ∥nuc+∥µ∗D∗+Λ1+Λ2∥nuc\n",
            "≥ ∥µ∗D∗+Λ1∥nuc− ∥Λ2∥nuc\n",
            "=∥µ∗D∥nuc+∥Λ1∥nuc− ∥Λ2∥nuc\n",
            "=∥µ∗Z∗∥nuc+∥Λ1∥nuc− ∥Λ2∥nuc, (28)\n",
            "where, (a)follows from the definition of Z∗, and (b)follows from the definition of Λ. This implies\n",
            "that\n",
            "∥µ∗Z∗∥nuc− ∥bZℓ∥nuc≤ ∥Λ2∥nuc− ∥Λ1∥nuc.\n",
            "Combining (27) and (28), we have that\n",
            "2∥Θ∥2\n",
            "F≤\u0010\n",
            "∥∇L(µ∗Z∗)∥op+γℓ\u0011\n",
            "∥Λ2∥nuc+\u0010\n",
            "∥∇L(µ∗Z∗)∥op−γℓ\u0011\n",
            "∥Λ1∥nuc.\n",
            "Then, we refer to the setting in our Lemma 11, and we choose γℓ= 4t/q\n",
            "MτE\n",
            "ℓwhere the value of t\n",
            "is determined in Lemma 11, i.e.\n",
            "γℓ= 4s\n",
            "2 (4 + S2\n",
            "0)Cd1d2log (2 ( d1+d2)/δℓ)\n",
            "MτE\n",
            "ℓ,\n",
            "we know that λT−1≥2∥∇L(µ∗Z∗)∥opwith probability at least 1−δℓfor any δℓ∈(0,1). Therefore,\n",
            "with a probability at least 1−δℓ, we have\n",
            "2∥Θ∥2\n",
            "F≤3\n",
            "2γℓ∥Λ2∥nuc−1\n",
            "2γℓ∥Λ1∥nuc≤3\n",
            "2γℓ∥Λ2∥nuc.\n",
            "Since we can easily verify that the rank of Λ2is at most 2r, and by using Cauchy-Schwarz Inequality\n",
            "we have that\n",
            "2∥Θ∥2\n",
            "F≤3\n",
            "2γℓ√\n",
            "2r∥Λ2∥F≤3\n",
            "2γℓ√\n",
            "2r∥Λ∥F=3\n",
            "2γℓ√\n",
            "2r∥Θ∥F\n",
            "which implies that\n",
            "∥Θ∥F≤3\n",
            "4√\n",
            "2rγℓ= 6vuut(4 +S2\n",
            "0)Cd1d2rlog\u0010\n",
            "2(d1+d2)\n",
            "δℓ\u0011\n",
            "MτE\n",
            "ℓ.\n",
            "This implies that P(Fℓ)≥1−δℓ. Taking a union bound over all phases ℓ≥1and recalling δℓ:=δ\n",
            "2ℓ2,\n",
            "we obtain\n",
            "P(F)≥1−∞X\n",
            "ℓ=1P(Fc\n",
            "ℓ)\n",
            "≥1−∞X\n",
            "ℓ=1δℓ\n",
            "2\n",
            "= 1−∞X\n",
            "ℓ=1δ\n",
            "4ℓ2\n",
            "≥1−δ\n",
            "2.\n",
            "This concludes our proof.\n",
            "Define X+\n",
            "batch:=\u0000\n",
            "X⊤\n",
            "batchXbatch\u0001−1X⊤\n",
            "batch where X+\n",
            "batch is constructed through the E-optimal design.\n",
            "Using Lemma C.1 from Du et al. (2023) it holds that\n",
            "\r\rX+\n",
            "batch\r\r≤s\n",
            "(1 +β)ρE\n",
            "ℓ\n",
            "p.\n",
            "where p= 180 d1d2/β2is the batch size to control the rounding procedure and ρE\n",
            "ℓ=\n",
            "minb∈△W\r\r(P\n",
            "w∈Wbwww⊤)−1\r\r. It follows then that\r\rX+\n",
            "batch\r\r2≤4ρE\n",
            "ℓ.\n",
            "28Lemma 13. (Expectation of bZℓ). It holds that Eh\n",
            "bZℓi\n",
            "=Z=1\n",
            "MPM\n",
            "m=1Θm.\n",
            "Proof. Note that we can re-write\n",
            "bZℓ= arg min\n",
            "Θ∈Rd1×d2Lℓ(Θ) +γℓ∥Θ∥nuc, Lℓ(Θ)=⟨Θ,Θ⟩ −2\n",
            "MτE\n",
            "ℓMX\n",
            "m=1τE\n",
            "ℓX\n",
            "s=1⟨eψν(rm,s·Q(xm,sz⊤\n",
            "m,s)),Θ⟩\n",
            "such that\n",
            "bZℓ=2\n",
            "MτE\n",
            "ℓMX\n",
            "m=1τE\n",
            "ℓX\n",
            "s=1bΘm,s−X+\n",
            "batch\u0000\n",
            "X+\n",
            "batch\u0001⊤\n",
            "wherebΘm,s=⟨Θ,Θ⟩ −2\n",
            "MsPM\n",
            "m=1⟨eψν(rm,s·Q(xm,sz⊤\n",
            "m,s)),Θ⟩. Now using Lemma C.2 from\n",
            "Du et al. (2023) we can prove the result of the lemma.\n",
            "Lemma 14. (Concentration of bB1,ℓ). Suppose that event Fℓholds. Then, for any phase ℓ >0,\n",
            "\r\r\r(bB⊥\n",
            "1,ℓ)⊤B1\r\r\r≤c′ρE\n",
            "ℓp\n",
            "(d1+d2)r\n",
            "Srq\n",
            "MτE\n",
            "ℓlog\u001216(d1+d2)rMτE\n",
            "ℓ\n",
            "δℓ\u0013\n",
            ",\n",
            "for some constant c′>0andρE\n",
            "ℓ=min b∈△W\r\r(P\n",
            "w∈Wbwww⊤)−1\r\r.\n",
            "Proof. Using the Davis-Kahan sinθTheorem (Bhatia, 2013) and letting τE\n",
            "ℓbe large enough to satisfy\n",
            "\r\r\rbZℓ−µ∗Z∗\r\r\r2\n",
            "F≤C1d1d2rlog\u00102(d1+d2)\n",
            "δℓ\u0011\n",
            "MτE\n",
            "ℓ, we have\n",
            "\r\r\r(bB⊥\n",
            "1,ℓ)⊤B1\r\r\r≤\r\r\rbZℓ−Eh\n",
            "bZℓi\r\r\r\n",
            "σr\u0010\n",
            "Eh\n",
            "bZℓi\u0011\n",
            "−σr+1\u0010\n",
            "Eh\n",
            "bZℓi\u0011\n",
            "−\r\r\rbZℓ−Eh\n",
            "bZℓi\r\r\r\n",
            "(a)\n",
            "≤c0\n",
            "Sr\r\r\rbZℓ−Eh\n",
            "bZℓi\r\r\r\n",
            "(b)\n",
            "≤cc0\r\rX+\n",
            "batch\r\r2p\n",
            "(d1+d2)r\n",
            "Srq\n",
            "MτE\n",
            "ℓlog\u001216(d1+d2)rMτE\n",
            "ℓ\n",
            "δℓ\u0013\n",
            "(c)\n",
            "≤c′ρE\n",
            "ℓp\n",
            "(d1+d2)r\n",
            "Srq\n",
            "MτE\n",
            "ℓlog\u001216(d1+d2)rMτE\n",
            "ℓ\n",
            "δℓ\u0013\n",
            ".\n",
            "where, (a)follows from Assumption 2, the (b)follows from event Fℓand(b)follows as\r\rX+\n",
            "batch\r\r2≤\n",
            "4ρE\n",
            "ℓ. The claim of the lemma follows.\n",
            "Lemma 15. (Concentration of bB2,ℓ). Suppose that event Fℓholds. Then, for any phase ℓ >0,\n",
            "\r\r\r(bB⊥\n",
            "2,ℓ)⊤B2\r\r\r≤cρE\n",
            "ℓp\n",
            "(d1+d2)r\n",
            "Srq\n",
            "MτE\n",
            "ℓlog\u001216(d1+d2)rMτE\n",
            "ℓ\n",
            "δℓ\u0013\n",
            ",\n",
            "for some constant c′>0andρE\n",
            "ℓ=min b∈△W\r\r(P\n",
            "w∈Wbwww⊤)−1\r\r.\n",
            "Proof. The proof follows the same way as Lemma 14 and using the Davis-Kahan sinθTheorem\n",
            "(Bhatia, 2013)\n",
            "\r\r\r(bB⊥\n",
            "2,ℓ)⊤B2\r\r\r≤cρE\n",
            "ℓp\n",
            "(d1+d2)r\n",
            "Srq\n",
            "MτE\n",
            "ℓlog\u001216(d1+d2)rMτE\n",
            "ℓ\n",
            "δℓ\u0013\n",
            ".\n",
            "The claim of the lemma follows.\n",
            "29Good Event per Task: We now define the good event F′\n",
            "ℓin phase ℓthat GOBLIN has a good\n",
            "estimate of Sm,∗as follows: For any phase ℓ >0\n",
            "F′\n",
            "ℓ:=\n",
            "\n",
            "\r\r\rbSm,ℓ−µ∗Sm,∗\r\r\r2\n",
            "F≤C1k1k2rlog\u0010\n",
            "2(k1+k2)\n",
            "δℓ\u0011\n",
            "τE\n",
            "m,ℓ\n",
            "\n",
            ", (29)\n",
            "where, C, µ∗>0are constants and bSm,ℓis the estimate from (9). Then define the event\n",
            "F′:=∞\\\n",
            "ℓ=1F′\n",
            "ℓ. (30)\n",
            "We now prove the following lemmas to show the good event F′\n",
            "ℓholds with probability (1−δℓ).\n",
            "Before proving the concentration of bSm,ℓwe first need to show that σmin(P\n",
            "ew∈fWbewewew⊤)>0. If\n",
            "this holds true then we can sample following E-optimal design.\n",
            "Lemma 16. For any phase ℓ >0and task m∈[M], let\r\r\rbB⊤\n",
            "1,ℓB⊥\n",
            "1\r\r\r≤c1and\r\r\rbB⊤\n",
            "2,ℓB⊥\n",
            "2\r\r\r≤c2, for\n",
            "some c1, c2>0. Then we have\n",
            "σmin(X\n",
            "ew∈fWbewewew⊤)>0\n",
            "Proof. We can show that\n",
            "X\n",
            "ew∈fWbewewew⊤(a)=X\n",
            "x∈Xm,z∈Zmbx,zbU⊤\n",
            "ℓxm|{z}\n",
            "egmzmbV⊤\n",
            "ℓ|{z}\n",
            "ev⊤m\n",
            "where, in (a)thebx,zis the sampling proportion for the arms xandz(they are allocated the same\n",
            "proportion, as they are pulled the same number of times). Also note that from Lemma 12 we know\n",
            "that\r\r\rbB⊤\n",
            "1,ℓB⊥\n",
            "1\r\r\r≤c1and\r\r\rbB⊤\n",
            "2,ℓB⊥\n",
            "2\r\r\r≤c2for some c1, c2>0holds with high probability. This\n",
            "helps us to apply Lemma 17 to get the claim of the lemma.\n",
            "Lemma 17. (Restatement of Lemma C.5 from Du et al. (2023)) For any phase ℓ >0and task\n",
            "m∈[M], if\r\r\rbU⊤\n",
            "ℓU⊥\r\r\r≤cfor some c >0, then we have\n",
            "σmin nX\n",
            "i=1b∗\n",
            "m(xi)bU⊤\n",
            "ℓxix⊤\n",
            "ibUℓ!\n",
            ">0\n",
            "where b∗\n",
            "mis a sampling proportion on x.\n",
            "Lemma 18. LetL′:Rk1×k2→Ris the loss function defined in (9). Then by setting\n",
            "t=s\n",
            "2k1k2C(4 +S2\n",
            "0) log\u00122 (k1+k2)\n",
            "δℓ\u0013\n",
            ",\n",
            "ν=t\n",
            "(4 +S0)Ck1k2q\n",
            "τE\n",
            "ℓ=vuut2 log\u0010\n",
            "2(k1+k2)\n",
            "δℓ\u0011\n",
            "τE\n",
            "ℓk1k2C(4 +S2\n",
            "0),\n",
            "we have with probability at least 1−δℓ, it holds that\n",
            "P\n",
            "∥∇L′(µ∗Sm,∗)∥op≥2tq\n",
            "τE\n",
            "ℓ\n",
            "≤δℓ,\n",
            "where µ∗=E[⟨Xm,Sm,∗⟩]>0, andXm=egmev⊤\n",
            "m.\n",
            "30Proof. LetXm,i=egm,iev⊤\n",
            "m,iBased on the definition of our loss function L′(·)in (9), we have that\n",
            "∇xmL′(Sm,∗) =µ∗Sm,∗−2\n",
            "τE\n",
            "ℓτE\n",
            "ℓX\n",
            "i=1eψν(rm,i·Q(xm))\n",
            "(a)=\n",
            "E(rm,1·Q(Xm,1))−1\n",
            "τE\n",
            "ℓτE\n",
            "ℓX\n",
            "i=1eψν(rm,i·Q(Xm,i))\n",
            "\n",
            "where (a)follows using the same steps as in Lemma 11. Similarly, using the same steps for a single\n",
            "task as in Lemma 11 we have\n",
            "P\n",
            "∥∇L′(µ∗Sm,∗)∥op≥2tq\n",
            "τE\n",
            "ℓ\n",
            "≤2 (k1+k2) exp \n",
            "−νtq\n",
            "τE\n",
            "ℓ+ν2\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "Ck1k2τE\n",
            "ℓ\n",
            "2!\n",
            "By plugging the values of tandνin Lemma 11, we finish the proof.\n",
            "Lemma 19. (Concentration of bSm,ℓ)For any low-rank linear model with samples X1. . . ,XτE\n",
            "ℓdrawn from Xaccording to Dthen for the optimal solution to the nuclear norm regularization\n",
            "problem in (2)withν=q\n",
            "2 log (2 ( k1+k2)/δℓ)/\u0000\n",
            "(4 +S2\n",
            "0)τE\n",
            "ℓk1k2\u0001\n",
            "and\n",
            "γm,ℓ= 4s\n",
            "2 (4 + S2\n",
            "0)Ck1k2log (2 ( k1+k2)/δℓ)\n",
            "τE\n",
            "m,ℓ,\n",
            "with probability at least 1−δℓit holds that:\n",
            "\r\r\rbSm,ℓ−µ∗Sm,∗\r\r\r2\n",
            "F≤C1k1k2rlog\u0010\n",
            "2(k1+k2)\n",
            "δℓ\u0011\n",
            "τE\n",
            "m,ℓ,\n",
            "forC1= 36\u0000\n",
            "4 +S2\n",
            "0\u0001\n",
            "C,∥X∥F,∥Sm,∗∥F≤S0, some nonzero constant µ∗, andEh\n",
            "(Sp(X))2\n",
            "iji\n",
            "≤\n",
            "C,∀i, j. Summing over all phases ℓ≥1it follows that P(F′)≥1−δ/2.\n",
            "Proof. Since the estimator bSm,ℓminimizes the regularized loss function defined in Eqn. (6), we have\n",
            "L(bSm,ℓ) +γℓ∥bSm,ℓ∥nuc≤L(µ∗Sm,∗) +γℓ∥µ∗Sm,∗∥nuc.\n",
            "And due to the fact that L′(·)is a quadratic function, we have the following expression based on\n",
            "multivariate Taylor’s expansion:\n",
            "L′(bSm,ℓ)−L′(µ∗Sm,∗) =⟨∇L′(µ∗Sm,∗),Θ⟩+ 2∥Θ∥2\n",
            "F,where Θ=bSm,ℓ−µ∗Sm,∗.\n",
            "By rearranging the above two results, we can deduce that\n",
            "2∥Θ∥2\n",
            "F≤ −⟨∇ L′(µ∗Sm,∗),Θ⟩+γℓ∥µ∗Sm,∗∥nuc−γℓ∥bSm,∗∥nuc\n",
            "(i)\n",
            "≤ ∥∇ L′(µ∗Sm,∗)∥op∥Θ∥nuc+γℓ∥µ∗Sm,∗∥nuc−γℓ∥bSm,ℓ∥nuc, (31)\n",
            "where (i) comes from the duality between matrix operator norm and nuclear norm. Next, we represent\n",
            "the saturated SVD of Sm,∗asSm,∗=UDV⊤where U∈Rk1×randV∈Rk2×r, and here we\n",
            "would work on its full version, i.e.\n",
            "Sm,∗= (U,U⊥)\u0012\n",
            "D0\n",
            "0 0\u0013\n",
            "(V,V⊥)⊤= (U,U⊥)D∗(V,V⊥)⊤\n",
            "where we have U⊥∈Rk1×(k1−r),D∗∈Rk1×k2andV⊥∈Rk2×(k2−r). Furthermore, we define\n",
            "Λ= (U,U⊥)⊤Θ(V,V⊥) =\u0012\n",
            "U⊤ΘV U⊤ΘV⊥\n",
            "U⊤\n",
            "⊥ΘV U⊤\n",
            "⊥ΘV⊥\u0013\n",
            "=Λ1+Λ2\n",
            "31where we write\n",
            "Λ1=\u00120 0\n",
            "0U⊤\n",
            "⊥ΘV⊥\u0013\n",
            ",Λ2=\u0012\n",
            "U⊤ΘV U⊤ΘV⊥\n",
            "U⊤\n",
            "⊥ΘV 0\u0013\n",
            ".\n",
            "Afterward, it holds that\n",
            "∥bSℓ∥nuc=∥µ∗Sm,∗+Θ∥nuc(a)=\r\r\r(U,U⊥) (µ∗D∗+Λ) (V,V⊥)⊤\r\r\r\n",
            "nuc\n",
            "(b)=∥µ∗D∗+Λ∥nuc+∥µ∗D∗+Λ1+Λ2∥nuc\n",
            "≥ ∥µ∗D∗+Λ1∥nuc− ∥Λ2∥nuc\n",
            "=∥µ∗D∥nuc+∥Λ1∥nuc− ∥Λ2∥nuc\n",
            "=∥µ∗Sm,∗∥nuc+∥Λ1∥nuc− ∥Λ2∥nuc, (32)\n",
            "where, (a)follows from the definition of Sm,∗, and (b)follows the definition of Λ. This implies that\n",
            "∥µ∗Sm,∗∥nuc− ∥bSm,ℓ∥nuc≤ ∥Λ2∥nuc− ∥Λ1∥nuc.\n",
            "Combining (31) and (32), we have that\n",
            "2∥Θ∥2\n",
            "F≤\u0010\n",
            "∥∇L(µ∗B)∥op+γℓ\u0011\n",
            "∥Λ2∥nuc+\u0010\n",
            "∥∇L(µ∗B)∥op−γℓ\u0011\n",
            "∥Λ1∥nuc.\n",
            "Then, we refer to the setting in our Lemma 11, and we choose γℓ= 4t/q\n",
            "MτE\n",
            "ℓwhere the value of t\n",
            "is determined in Lemma 11, i.e.\n",
            "γℓ= 4s\n",
            "2 (4 + S2\n",
            "0)Ck1k2log (2 ( d1+d2)/δℓ)\n",
            "MτE\n",
            "ℓ,\n",
            "we know that λT−1≥2∥∇L(µ∗Sm,∗)∥opwith probability at least 1−δℓfor any δℓ∈(0,1).\n",
            "Therefore, with a probability at least 1−δℓ, we have\n",
            "2∥Θ∥2\n",
            "F≤3\n",
            "2γℓ∥Λ2∥nuc−1\n",
            "2γℓ∥Λ1∥nuc≤3\n",
            "2γℓ∥Λ2∥nuc.\n",
            "Since we can easily verify that the rank of Λ2is at most 2r, and by using Cauchy-Schwarz Inequality\n",
            "we have that\n",
            "2∥Θ∥2\n",
            "F≤3\n",
            "2γℓ√\n",
            "2r∥Λ2∥F≤3\n",
            "2γℓ√\n",
            "2r∥Λ∥F=3\n",
            "2γℓ√\n",
            "2r∥Θ∥F\n",
            "which implies that\n",
            "∥Θ∥F≤3\n",
            "4√\n",
            "2rγℓ= 6vuut(4 +S2\n",
            "0)Ck1k2rlog\u0010\n",
            "2(k1+k2)\n",
            "δℓ\u0011\n",
            "τE\n",
            "ℓ\n",
            "This implies that P(F′\n",
            "ℓ)≥1−δℓ. Taking a union bound over all phases ℓ≥1and recalling δℓ:=δ\n",
            "2ℓ2,\n",
            "we obtain\n",
            "P(F′)≥1−∞X\n",
            "ℓ=1P((F′)c\n",
            "ℓ)\n",
            "≥1−∞X\n",
            "ℓ=1δℓ\n",
            "2\n",
            "= 1−∞X\n",
            "ℓ=1δ\n",
            "4ℓ2\n",
            "≥1−δ\n",
            "2.\n",
            "This concludes our proof.\n",
            "32A.7 Final Sample Complexity Bound\n",
            "We first define the arm elimination event similar to Theorem 1. For any V ⊆ W be the active set and\n",
            "w∈ V define\n",
            "Ew,ℓ(V) =n\f\f\fD\n",
            "w−w⋆,bθℓ(V)−θ∗E\f\f\f≤ϵℓo\n",
            "(33)\n",
            "where it is implicit that bθℓ:=bθℓ(V)is the design constructed in the algorithm at stage ℓwith respect\n",
            "toWℓ=V.\n",
            "Theorem 2. (Restatement) With probability at least 1−δ, multi-task GOBLIN returns the best arms\n",
            "x∗,z∗, and the number of samples used is bounded by\n",
            "eO\u0012M(k1+k2)r\n",
            "∆2+M√k1k2r\n",
            "Sr+√d1d2r\n",
            "Sr\u0013\n",
            ".\n",
            "Proof. For the rest of the proof we have that the good events FℓTF′\n",
            "ℓTEw,ℓ(Wℓ)holds true for\n",
            "each phase ℓwith probability greater than (1−δ). The three events are defined in (25),(29) and(33).\n",
            "Third Stage: Define Am,ℓ={w∈ Wℓ:⟨w⋆−w,θ∗⟩ ≤4ϵm,ℓ}. Note that by assumption\n",
            "W=W1=A1. The above lemma implies that with probability at least 1−δwe haveT∞\n",
            "ℓ=1\b\n",
            "Wm,ℓ⊆Sm,ℓ\t\n",
            ". This implies that\n",
            "ρG\u0000\n",
            "Wm,ℓ\u0001\n",
            "= min\n",
            "b∈∆Wmmax\n",
            "w,w′∈Wm,ℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "≤min\n",
            "b∈∆Wmmax\n",
            "w,w′∈Sm,ℓ∥w−w′∥2\n",
            "(P\n",
            "w∈Wbwww⊤+Λ)−1\n",
            "=ρG\u0000\n",
            "Am,ℓ\u0001\n",
            ".\n",
            "Let the effective dimension be k= (k1+k2)r. Define kℓ\n",
            "1= 8klog(1 + τG\n",
            "m,ℓ−1/λ). For ℓ≥\u0006\n",
            "log2\u0000\n",
            "4∆−1\n",
            "m\u0001\u0007\n",
            "we have that Sm,ℓ={w⋆}, thus, the sample complexity to identify w⋆\n",
            "mis equal to\n",
            "⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=1X\n",
            "w∈Wml\n",
            "τG\n",
            "m,ℓbbG\n",
            "m,ℓ,wm\n",
            "=⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=1\u0012(kℓ\n",
            "1+ 1)kℓ\n",
            "1\n",
            "2+τG\n",
            "m,ℓ\u0013\n",
            "=⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=1\u0012(kℓ\n",
            "1+ 1)kℓ\n",
            "1\n",
            "2+ 2ϵ−2\n",
            "m,ℓρG(Wm,ℓ)Bℓ\n",
            "m,∗log\u0000\n",
            "4kℓ\n",
            "1ℓ2|Wm|/δ\u0001\u0013\n",
            "(a)\n",
            "≤2⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=1\u0012(k+ 1)k\n",
            "2log2(1 +τG\n",
            "m,ℓ−1) + 2ϵ−2\n",
            "m,ℓρG(Wm,ℓ)Bℓ\n",
            "m,∗log\u0000\n",
            "4kℓ\n",
            "1ℓ2|Wm|/δ\u0001\u0013\n",
            "(b)\n",
            "≤2(k+ 1)k\n",
            "2⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=1\u0010\n",
            "log2(1 +τG\n",
            "m,ℓ−1) + 8ϵ−2\n",
            "m,ℓρG(Wm,ℓ)Bℓ\n",
            "m,∗log\u0000\n",
            "4kℓ\n",
            "1ℓ2|Wm|/δ\u0001\u0011\n",
            "(c)\n",
            "≤(k+ 1)k⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=1\u0010\n",
            "1 + 16 ϵ−2\n",
            "m,ℓρG(Wm,ℓ)Bℓ\n",
            "m,∗log2\u0000\n",
            "4kℓ\n",
            "1ℓ2|Wm|/δ\u0001\u0011\n",
            "(d)\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\n",
            "m\u0001\u0007\n",
            "+⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=132ϵ−2\n",
            "m,ℓf\u0000\n",
            "Am,ℓ\u0001\n",
            "Bℓ\n",
            "m,∗log\u0000\n",
            "4kℓ2|Wm|/δ\u0001\n",
            "(e)\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+⌈log2(4∆−1)⌉X\n",
            "ℓ=132ϵ−2\n",
            "ℓf\u0000\n",
            "Am,ℓ\u0001\n",
            "(64λS2+ 64τG\n",
            "m,ℓ−1) log\u0000\n",
            "4kℓ2|W|/δ\u0001\n",
            "33= (k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+⌈log2(4∆−1)⌉X\n",
            "ℓ=132ϵ−2\n",
            "ℓf\u0000\n",
            "Am,ℓ\u0001\n",
            "(64λS2) log\u0000\n",
            "4kℓ2|W|/δ\u0001\n",
            "+ (k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+⌈log2(4∆−1)⌉X\n",
            "ℓ=132ϵ−2\n",
            "ℓf\u0000\n",
            "Am,ℓ\u0001\n",
            "(64τG\n",
            "m,ℓ−1) log\u0000\n",
            "4kℓ2|W|/δ\u0001\n",
            "(f)\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\n",
            "m\u0001\u0007\n",
            "+ 2048 λS2log \n",
            "4klog2\n",
            "2\u0000\n",
            "8∆−1\n",
            "m\u0001\n",
            "|Wm|\n",
            "δ!⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=122ℓf\u0000\n",
            "Am,ℓ\u0001\n",
            ".\n",
            "where, (a)follows as log2(1+τG\n",
            "m,ℓ−1/λ)≤log2(1+τG\n",
            "m,ℓ−1),(b)follows by noting that log(xlog(1+\n",
            "x))≤2 log( x)for any x >1. The (c)follows by subsuming the log2(1 +τG\n",
            "m,ℓ−1)into2τG\n",
            "ℓ. The (d)\n",
            "follows as log(1 + τG\n",
            "m,ℓ−1)< τG\n",
            "ℓwhich enables us to replace the kℓ\n",
            "1inside the logwith an addtional\n",
            "factor of 2. The (e)follows similarly to (23) by noting that\n",
            "Bℓ\n",
            "m,∗≤64(√\n",
            "λS+q\n",
            "λ⊥\n",
            "m,ℓS⊥\n",
            "m,ℓ)\n",
            "≤64λS2+\n",
            "64τG\n",
            "m,ℓ−1\n",
            "8(d1+d2)rlog(1 +τG\n",
            "m,ℓ−1\n",
            "λ)\n",
            "·\u00128d1d2r\n",
            "τE\n",
            "ℓS2rlog\u0012d1+d2\n",
            "δℓ\u0013\u0013\n",
            "(a1)\n",
            "≤64λS2+ 64τG\n",
            "m,ℓ−1.\n",
            "Finally the (f)follows by subsuming the τG\n",
            "ℓ−1with a factor of 2into the quantity of τG\n",
            "ℓ. Then it\n",
            "follows that\n",
            "ρG\n",
            "m,∗= inf\n",
            "b∈△Wmmax\n",
            "w∈Wm∥w−w⋆∥2\u0010P\n",
            "w∈Wmbwww⊤+Λ\u0011−1\n",
            "(⟨w−w⋆,θ∗⟩)2\n",
            "= inf\n",
            "b∈△Wmmax\n",
            "ℓ≤⌈log2(4∆−1\n",
            "m)⌉max\n",
            "w∈Sm,ℓ∥w−w⋆∥2\u0010P\n",
            "w∈Wmbwww⊤+Λ\u0011−1\n",
            "(⟨w−w⋆,θ∗⟩)2\n",
            "≥1\u0006\n",
            "log2\u0000\n",
            "4∆−1m\u0001\u0007 inf\n",
            "b∈△Wm⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=1max\n",
            "w∈Am,ℓ∥w−w⋆∥2\u0010P\n",
            "w∈Wmbwww⊤+Λ\u0011−1\n",
            "(⟨w−w⋆,θ∗⟩)2\n",
            "≥1\n",
            "16\u0006\n",
            "log2\u0000\n",
            "4∆−1m\u0001\u0007⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=122ℓinf\n",
            "b∈△Wmmax\n",
            "w∈Am,ℓ∥w−w⋆∥2\u0010P\n",
            "w∈Wmbwww⊤+Λ\u0011−1\n",
            "≥1\n",
            "64\u0006\n",
            "log2\u0000\n",
            "4∆−1m\u0001\u0007⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=122ℓinf\n",
            "b∈△Wmmax\n",
            "w,w′∈Am,ℓ∥w−w′∥2\u0010P\n",
            "w∈Wmbwww⊤+Λ\u0011−1\n",
            "≥1\n",
            "64\u0006\n",
            "log2\u0000\n",
            "4∆−1m\u0001\u0007⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=122ℓf\u0000\n",
            "Am,ℓ\u0001\n",
            ".\n",
            "This implies that\n",
            "⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=122ℓf\u0000\n",
            "Am,ℓ\u0001\n",
            "≤ρG\n",
            "m,∗64\u0006\n",
            "log2\u0000\n",
            "4∆−1\n",
            "m\u0001\u0007\n",
            ".\n",
            "34Plugging this back we get\n",
            "⌈log2(4∆−1\n",
            "m)⌉X\n",
            "ℓ=1X\n",
            "w∈Wml\n",
            "τG\n",
            "m,ℓbbℓ,wm\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\n",
            "m\u0001\u0007\n",
            "+ 2048 λS2log \n",
            "8klog2\n",
            "2\u0000\n",
            "8∆−1\n",
            "m\u0001\n",
            "|Wm|\n",
            "δ!\n",
            "64ρG\n",
            "m,∗\u0006\n",
            "log2\u0000\n",
            "4∆−1\n",
            "m\u0001\u0007\n",
            "≤(k+ 1)k\u0006\n",
            "log2\u0000\n",
            "4∆−1\n",
            "m\u0001\u0007\n",
            "+C2λS2log \n",
            "8klog2\n",
            "2\u0000\n",
            "8∆−1\n",
            "m\u0001\n",
            "|Wm|\n",
            "δ!\n",
            "ρG\n",
            "m,∗\u0006\n",
            "log2\u0000\n",
            "4∆−1\n",
            "m\u0001\u0007\n",
            "where, C2>0is a constant. Summing over each task m, the simplified sample complexity for the\n",
            "third stage is given by\n",
            "N3≤O \n",
            "Mk\n",
            "∆2log \n",
            "klog2\u0000\n",
            "∆−1\u0001\n",
            "|Wm|\n",
            "δ!!\n",
            "=eO\u0012M(k1+k2)r\n",
            "∆2\u0013\n",
            "where ∆ = min w∈W(w∗−w)⊤θ∗(a1)= min x∈X\\{ x∗},z∈Z\\{ z∗}(x⊤\n",
            "∗Θ∗z∗−x⊤Θ∗z). The (a1)\n",
            "follows by reshaping the arms in Wto recover the arms in XandZ.\n",
            "2nd Stage: Again recall that the E-optimal design in stage 2 of Algorithm 2 satisfies the Assumption 4\n",
            "as the sample distribution Dhas finite second order moments.\n",
            "For the second stage first observe that by plugging in the definition of eτℓ\n",
            "Ewe get\n",
            "\r\rθ∗\n",
            "m,k+1:p\r\r2\n",
            "2=X\n",
            "i>r∧j>rH2\n",
            "ij=\r\r\r(bU⊥\n",
            "ℓ)⊤\u0000\n",
            "U∗S∗V∗⊤\u0001bV⊥\n",
            "ℓ\r\r\r2\n",
            "F\n",
            "≤\r\r\r(bU⊥\n",
            "ℓ)⊤U∗\r\r\r2\n",
            "F∥S∗∥2\n",
            "2\r\r\r(bV⊥\n",
            "ℓ)⊤V∗\r\r\r2\n",
            "F≤O\u0012k1k2r\n",
            "τE\n",
            "ℓlog\u0012k1+k2\n",
            "δ\u0013\u0013\n",
            "=O\u0012√d1d2r\n",
            "Srlog\u0012d1+d2\n",
            "δℓ\u0013\u0013\n",
            ",\n",
            "which implies\r\r\rθ∗\n",
            "k+1:p\r\r\r\n",
            "2=eO\u0000√k1k2r/Sr\u0001\n",
            ". We also set8k1k2r\n",
            "τE\n",
            "m,ℓS2rlog\u0010\n",
            "k1+k2\n",
            "δℓ\u0011\n",
            ":=S⊥\n",
            "m,ℓ. Now we\n",
            "bound the sample complexity from the second stage. From the second stage we can show that we\n",
            "have for the arm set fWm\n",
            "N2=⌈log2(4∆−1)⌉X\n",
            "ℓ=1X\n",
            "ew∈fWml\n",
            "eτE\n",
            "m,ℓbbE\n",
            "m,ℓ,ewm\n",
            "=⌈log2(4∆−1)⌉X\n",
            "ℓ=1\u0012(p+ 1)p\n",
            "2+eτE\n",
            "m,ℓ\u0013\n",
            "=⌈log2(4∆−1)⌉X\n",
            "ℓ=1 \n",
            "(p+ 1)p\n",
            "2+p\n",
            "8k1k2rlog (4 ℓ2|W|/δ)\n",
            "Sr!\n",
            "≤(p+ 1)p\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+ 32√k1k2r\n",
            "Srlog \n",
            "4 log2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!\n",
            "\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "=O √k1k2r\n",
            "Srlog \n",
            "4 log2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!!\n",
            "(a)=eO\u0012√k1k2r\n",
            "Sr\u0013\n",
            "1st Stage: Finally we also use the E-optimal design in first stage of Algorithm 2. Note that this\n",
            "design satisfies the Assumption 4 as the sample distribution Dhas finite second-order moments. Now\n",
            "35we bound the sample complexity from the first stage. From the first stage we can show that we have\n",
            "for the arm set W\n",
            "N1=⌈log2(4∆−1)⌉X\n",
            "ℓ=1MX\n",
            "m=1X\n",
            "w∈Wml\n",
            "τE\n",
            "m,ℓbbE\n",
            "m,ℓ,wm\n",
            "=⌈log2(4∆−1)⌉X\n",
            "ℓ=1 \n",
            "M(p+ 1)p\n",
            "2+X\n",
            "mτE\n",
            "m,ℓ!\n",
            "=⌈log2(4∆−1)⌉X\n",
            "ℓ=1 \n",
            "M(p+ 1)p\n",
            "2+p\n",
            "8d1d2rlog (4 ℓ2|W|/δ)\n",
            "Sr!\n",
            "≤M(p+ 1)p\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "+ 32√d1d2r\n",
            "Srlog \n",
            "4 log2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!\n",
            "\u0006\n",
            "log2\u0000\n",
            "4∆−1\u0001\u0007\n",
            "(a)=O √d1d2r\n",
            "Srlog \n",
            "4 log2\n",
            "2\u0000\n",
            "8∆−1\u0001\n",
            "|W|\n",
            "δ!!\n",
            "(a)=eO\u0012√d1d2r\n",
            "Sr\u0013\n",
            "where, (a)follows as p=d1d2. Combining N1, N2andN3gives the claim of the theorem.\n",
            "A.8 Additional Experimental Details\n",
            "Single Task Unit Ball: This experiment consists of a set of {6,10,14}left and right arms that are\n",
            "arranged in a unit ball in R6, and∥x∥= 1,∥z∥= 1 for all x∈ X andz∈ Z. Hence, we have\n",
            "d1∈R6andd2∈R6. We choose a random Θ∗∈Rd1×d2which has rank r= 2. We set δ= 0.1.\n",
            "We compare against RAGE (Fiez et al., 2019) that treats this d1d2bilinear bandit as a linear bandit\n",
            "setting and suffers a sample complexity that scales as eO(d1d2/∆2). We do a continuous relaxation\n",
            "of the algorithm when implementing it to make this more tractable.\n",
            "Multi-task Unit Ball: This experiment consists of a set of {5,10,15,20,25,30}tasks. For each\n",
            "task, we choose left and right arms that are arranged in a unit ball in R8, and∥x∥= 1,∥z∥= 1\n",
            "for all x∈ X andz∈ Z. Hence, we have d1∈R8andd2∈R8. We choose k1=k2= 4, and\n",
            "feature extractors B1∈Rd1×k1,B2∈Rd2×k2shared across tasks. We choose a random matrix\n",
            "Sm,∗∈Rk1×k2for each task msuch that Sm,∗has rank r= 2. We set δ= 0.1. We compare against\n",
            "DouExpDes (Du et al., 2023) that treats this setting as M k 1k2bilinear bandits (after learning the\n",
            "feature extractors) and suffers a sample complexity that scales as eO(Mk1k2/∆2)(see Remark 2).\n",
            "Again we do a continuous relaxation of the algorithm when implementing it to make this more\n",
            "tractable.\n",
            "36B Table of Notations\n",
            "Notations Definition\n",
            "X Left arm set\n",
            "Z Right arm set\n",
            "M Number of tasks\n",
            "ℓ Phase number\n",
            "Θm,∗ Hidden parameter matrix for\n",
            "bE\n",
            "ℓ E-optimal design at the ℓ-th phase\n",
            "bG\n",
            "m,ℓ G-optimal design at the ℓ-th phase for the m-th task\n",
            "S⊥\n",
            "m,ℓ8d1d2r\n",
            "τE\n",
            "ℓS2\n",
            "rlog\u0010\n",
            "d1+d2\n",
            "δℓ\u0011\n",
            "λ⊥\n",
            "m,ℓ τG\n",
            "m,ℓ−1/8(d1+d2)rlog(1 +τG\n",
            "m,ℓ−1\n",
            "λ)\n",
            "Bℓ\n",
            "m,∗ (8√\n",
            "λS+q\n",
            "λ⊥\n",
            "m,ℓS⊥\n",
            "m,ℓ)\n",
            "B1 Left feature extractor\n",
            "B2 Right feature extractor\n",
            "Sr r-th largest singular value of Θ∗\n",
            "∆(x,z) x⊤\n",
            "∗Θ∗z∗−x⊤Θ∗z\n",
            "∆ minx̸=x∗,z̸=z∗∆(x,z)\n",
            "Y(W) {w−w′:∀w,w′∈ W,w̸=w′}\n",
            "Y∗(W) {w∗−w:∀w∈ W\\ w∗}\n",
            "δ confidence level\n",
            "Table 1: Table of Notations\n",
            "37arXiv:2311.00201v1  [cs.LG]  1 Nov 2023Federated Natural Policy Gradient Methods\n",
            "for Multi-task Reinforcement Learning\n",
            "Tong Yang∗\n",
            "CMUShicong Cen†\n",
            "CMUYuting Wei‡\n",
            "UPennYuxin Chen§\n",
            "UPennYuejie Chi¶\n",
            "CMU\n",
            "November 2, 2023\n",
            "Abstract\n",
            "Federated reinforcement learning (RL) enables collaborat ive decision making of multiple distributed\n",
            "agents without sharing local data trajectories. In this wor k, we consider a multi-task setting, in which\n",
            "each agent has its own private reward function correspondin g to diﬀerent tasks, while sharing the same\n",
            "transition kernel of the environment. Focusing on inﬁnite- horizon tabular Markov decision processes, the\n",
            "goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all\n",
            "the agents in a decentralized manner, where each agent only c ommunicates with its neighbors over some\n",
            "prescribed graph topology.\n",
            "We develop federated vanilla and entropy-regularized natu ral policy gradient (NPG) methods under\n",
            "softmax parameterization, where gradient tracking is appl ied to the global Q-function to mitigate the\n",
            "impact of imperfect information sharing. We establish non- asymptotic global convergence guarantees\n",
            "under exact policy evaluation, which are nearly independen t of the size of the state-action space and\n",
            "illuminate the impacts of network size and connectivity. To the best of our knowledge, this is the\n",
            "ﬁrst time that global convergence is established for federa ted multi-task RL using policy optimization.\n",
            "Moreover, the convergence behavior of the proposed algorit hms is robust against inexactness of policy\n",
            "evaluation.\n",
            "Keywords: federated reinforcement learning, multi-task reinforcement lear ning, natural policy gradient\n",
            "methods, entropy regularization, global convergence\n",
            "Contents\n",
            "1 Introduction 2\n",
            "1.1 Our contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
            "1.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n",
            "2 Model and backgrounds 4\n",
            "2.1 Markov decision processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n",
            "2.2 Entropy-regularized RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
            "2.3 Natural policy gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
            "3 Federated NPG methods for multi-task RL 7\n",
            "3.1 Federated multi-task RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "3.2 Proposed federated NPG algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
            "∗Department of Electrical and Computer Engineering, Carneg ie Mellon University; email: tongyang@andrew.cmu.edu .\n",
            "†Department of Electrical and Computer Engineering, Carneg ie Mellon University; email: shicongc@andrew.cmu.edu .\n",
            "‡Department of Statistics and Data Science, Wharton School, University of Pennsylvania; email: ytwei@wharton.upenn.edu .\n",
            "§Department ofStatistics and Data Science, Wharton School, UniversityofPennsylvania; email: yuxinc@wharton.upenn.edu .\n",
            "¶Department of Electrical and Computer Engineering, Carneg ie Mellon University; email: yuejiechi@cmu.edu .\n",
            "14 Theoretical guarantees 10\n",
            "4.1 Global convergence of FedNPG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
            "4.2 Global convergence of FedNPG with entropy regularization . . . . . . . . . . . . . . . . . . . 11\n",
            "5 Conclusions 12\n",
            "A Convergence analysis 16\n",
            "A.1 Analysis of entropy-regularized FedNPG with exact policy evaluat ion. . . . . . . . . . . . . . 16\n",
            "A.2 Analysis of entropy-regularized FedNPG with inexact policy evalua tion. . . . . . . . . . . . . 18\n",
            "A.3 Analysis of FedNPG with exact policy evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
            "A.4 Analysis of FedNPG with inexact policy evaluation . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
            "B Proof of key lemmas 25\n",
            "B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n",
            "B.2 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n",
            "B.3 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n",
            "B.4 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n",
            "B.5 Proof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n",
            "C Proof of auxiliary lemmas 36\n",
            "C.1 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n",
            "C.2 Proof of Lemma 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n",
            "C.3 Proof of Lemma 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n",
            "C.4 Proof of Lemma 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n",
            "1 Introduction\n",
            "Federated reinforcement learning (FRL) is an emerging paradigm th at combines the advantages of federated\n",
            "learning (FL) and reinforcement learning (RL) ( Qi et al.,2021;Zhuo et al. ,2019), allowing multiple agents\n",
            "to learn a shared policy from local experiences, without exposing th eir private data to a central server nor\n",
            "other agents. FRL is poised to enable collaborative and eﬃcient decis ion making in scenarios where data\n",
            "is distributed, heterogeneous, and sensitive, which arise frequen tly in applications such as edge computing,\n",
            "smart cities, and healthcare ( Wang et al. ,2023,2020;Zhuo et al. ,2019), to name just a few. As has been\n",
            "observed ( Lian et al. ,2017), decentralized training can lead to performance improvements in F L by avoiding\n",
            "communication congestions at busy nodes such as the server, esp ecially under high-latency scenarios. This\n",
            "motivates us to design algorithms for the fully decentralized setting , a scenario where the agents can only\n",
            "communicate with their local neighbors over a prescribed network t opology.\n",
            "Inthiswork,westudytheproblemof federated multi-taskreinforcement learning (Anwar and Raychowdhury ,\n",
            "2021;Qi et al.,2021;Yu et al. ,2020), where each agent collects its own reward — possibly unknown to ot her\n",
            "agents — corresponding to the local task at hand, while having acce ss to the same dynamics (i.e., transition\n",
            "kernel) of the environment. The collective goal is to learn a shared p olicy that maximizes the total rewards\n",
            "accumulated from all the agents; in other words, one seeks a policy that performs well in terms of overall\n",
            "beneﬁts, rather than biasing towards any individual task, achievin g the Pareto frontier in a multi-objective\n",
            "context. There is no shortage of application scenarios where fede rated multi-task RL becomes highly rel-\n",
            "evant. For instance, in healthcare ( Zerka et al. ,2020), diﬀerent hospitals may be interested in ﬁnding an\n",
            "optimal treatment for all patients without disclosing private data, where the eﬀectiveness of the treatment\n",
            "can vary across diﬀerent hospitals due to demographical diﬀerenc es. As another potential application, to\n",
            "enhance ChatGPT’s performance across diﬀerent tasks or domain s (M Alshater ,2022;Rahman et al. ,2023),\n",
            "one might consult domain experts to chat and rate ChatGPT’s outpu ts for solving diﬀerent tasks, and train\n",
            "ChatGPT in a federated manner without exposing private data or fe edback of each expert.\n",
            "Nonetheless, despite the promise, provably eﬃcient algorithms for federated multi-task RL remain sub-\n",
            "stantially under-explored, especially in the fully decentralized settin g. The heterogeneity of local tasks leads\n",
            "to a higher degree of disagreements between the global value func tion and local value functions of individual\n",
            "agents. Due to the lack of global information sharing, care needs t o be taken to judiciously balance the use\n",
            "2of neighboring information (to facilitate consensus) and local data (to facilitate learning) when updating the\n",
            "policy. To the best of our knowledge, no algorithms are currently av ailable to ﬁnd the global optimal policy\n",
            "with non-asymptotic convergence guarantees even for tabular in ﬁnite-horizon Markov decision processes.\n",
            "Motivated by the connection with decentralized optimization, it is tem pting to take a policy optimization\n",
            "perspective to tackle this challenge. Policy gradient (PG) methods, which seek to learn the policy of interest\n",
            "via ﬁrst-order optimization methods, play an eminent role in RL due to their simplicity and scalability.\n",
            "In particular, natural policy gradient (NPG) methods ( Amari,1998;Kakade,2001) are among the most\n",
            "popular variants of PG methods, underpinning default methods use d in practice such as trust region policy\n",
            "optimization (TRPO) ( Schulman et al. ,2015) and proximal policy optimization (PPO) ( Schulman et al. ,\n",
            "2017). On the theoretical side, it has also been established recently tha t the NPG algorithm enjoys fast\n",
            "globalconvergencetotheoptimalpolicyinanalmostdimension-free manner( Agarwal et al. ,2021;Cen et al. ,\n",
            "2021), where the iteration complexity is nearly independent of the size of the state-action space. Inspired by\n",
            "the eﬃcacy of NPG methods, it is natural to ask:\n",
            "Can we develop federated variants of NPG methods that are easy to implement in the full y decentralized\n",
            "setting with non-asymptotic global convergence guarantees for multi-task RL?\n",
            "1.1 Our contributions\n",
            "Focusing on inﬁnite-horizon Markov decision processes (MDPs), we provide an aﬃrmative answer to the\n",
            "above question, by developing federated NPG (FedNPG) methods f or solving both the vanilla and entropy-\n",
            "regularized multi-task RL problems with ﬁnite-time global convergen ce guarantees. While entropy regu-\n",
            "larization is often incorporated as an eﬀective strategy to encour age exploration during policy learning,\n",
            "solving the entropy-regularized RL problem is of interest in its own rig ht, as the optimal regularized policy\n",
            "possesses desirable robust properties with respect to reward pe rturbations ( Eysenbach and Levine ,2021;\n",
            "McKelvey and Palfrey ,1995).\n",
            "Due to the multiplicative update nature of NPG methods under softm ax parameterization, it is more\n",
            "convenient to work with the logarithms of local policies in the decentr alized setting. In each iteration of the\n",
            "proposed FedNPG method, the logarithms of local policies are updat ed by a weighted linear combination\n",
            "of two terms (up to normalization): a gossip mixing ( Nedic and Ozdaglar ,2009) of the logarithms of neigh-\n",
            "boring local policies, and a local estimate of the global Q-function tr acked via the technique of dynamic\n",
            "average consensus ( Zhu and Mart´ ınez ,2010), a prevalent idea in decentralized optimization that allows for\n",
            "the use of large constant learning rates ( Di Lorenzo and Scutari ,2016;Nedic et al. ,2017;Qu and Li ,2017)\n",
            "to accelerate convergence. Our contributions are as follows.\n",
            "•We propose FedNPG methods for both the vanilla and entropy-regu larized multi-task RL problems,\n",
            "where each agent only communicates with its neighbors and perform s local computation using its own\n",
            "reward or task information.\n",
            "•Assuming access to exact policy evaluation, we establish that the av erage iterate of vanilla FedNPG\n",
            "converges globally at a rate of O(1/T2/3) in terms of the sub-optimality gap for the multi-task RL\n",
            "problem, and that the last iterate of entropy-regularized FedNPG converges globally at a linear rate\n",
            "to the regularized optimal policy. Our convergence theory highlight s the impacts of all salient problem\n",
            "parameters (see Table 1for details), such as the size and connectivity of the communication network.\n",
            "In particular, the iteration complexities of FedNPG are again almost in dependent of the size of the\n",
            "state-action space, which recover prior results on the centralize d NPG methods when the network is\n",
            "fully connected.\n",
            "•We further demonstrate the stability of the proposed FedNPG met hods when policy evaluations are\n",
            "only available in an inexact manner. To be speciﬁc, we prove that their convergence rates remain\n",
            "unchanged as long as the approximation errors are suﬃciently small in theℓ∞sense.\n",
            "To the best of our knowledge, the proposed federated NPG metho ds are the ﬁrst policy optimization\n",
            "methods for multi-task RL that achieve explicit non-asymptotic glob al convergence guarantees, allowing for\n",
            "fully decentralized communication without any need to share local re ward/task information.\n",
            "31.2 Related work\n",
            "Global convergence of NPG methods for tabular MDPs. Agarwal et al. (2021) ﬁrst establishes a\n",
            "O(1/T) last-iterate convergencerate of the NPG method under softma x parameterization with constant step\n",
            "size, assuming access to exact policy evaluation. When entropy reg ularization is in place, Cen et al. (2021)\n",
            "establishes a global linear convergence to the optimal regularized p olicy for the entire range of admissible\n",
            "constant learning rates using softmax parameterization and exac t policy evaluation, which is further shown\n",
            "to be stable in the presence of ℓ∞policy evaluation errors. The iteration complexity of NPG methods is\n",
            "nearly independent with the size of the state-action space, which is in sharp contrast to softmax policy\n",
            "gradient methods that may take exponential time to converge ( Li et al.,2023c;Mei et al. ,2020).Lan\n",
            "(2023) proposed a more general framework through the lens of mirror d escent for regularized RL with\n",
            "global linear convergence guarantees, which is further generalize d inZhan et al. (2023);Lan et al. (2023).\n",
            "Earlier analysis of regularized MDPs can be found in Shani et al. (2020). Besides, Xiao(2022) proves that\n",
            "vanilla NPG also achieves linear convergence when geometrically increa sing learning rates are used; see also\n",
            "Khodadadian et al. (2021);Bhandari and Russo (2021).Zhou et al. (2022) developed an anchor-changing\n",
            "NPG method for multi-task RL under various optimality criteria in the c entralized setting.\n",
            "Distributed and federated RL. There have been a variety of settings being set forth for distribut ed\n",
            "andfederatedRL. Mnih et al. (2016);Espeholt et al. (2018);Assran et al. (2019);Khodadadian et al. (2022);\n",
            "Woo et al. (2023) focused on developing federated versions of RL algorithms to acc elerate training, assuming\n",
            "all agents share the same transition kernel and reward function; in particular, Khodadadian et al. (2022);\n",
            "Woo et al. (2023) established the provable beneﬁts of federated learning in terms o f linear speedup. More\n",
            "pertinent to our work, Zhao et al. (2023);Anwar and Raychowdhury (2021) considered the federated multi-\n",
            "task framework, allowing diﬀerent agents having private reward fu nctions. Zhao et al. (2023) proposed an\n",
            "empirically probabilistic algorithm that can seek an optimal policy under the server-client setting, while\n",
            "Anwar and Raychowdhury (2021) developed new attack methods in the presence of adversarial ag ents. Dif-\n",
            "ferent from the FRL framework, Chen et al. (2021,2022b);Omidshaﬁei et al. (2017);Kar et al. (2012);\n",
            "Chen et al. (2022a);Zeng et al. (2021) considered the distributed multi-agent RL setting where the agen ts\n",
            "interact with a dynamic environment through a multi-agent Markov d ecision process, where each agent can\n",
            "have their own state or action spaces. Zeng et al. (2021) developed a decentralized policy gradient method\n",
            "where diﬀerent agents have diﬀerent MDPs.\n",
            "Decentralized ﬁrst-order optimization algorithms. Early work of consensus-based ﬁrst-order opti-\n",
            "mization algorithms for the fully decentralized setting include but are not limited to Lobel and Ozdaglar\n",
            "(2008);Nedic and Ozdaglar (2009);Duchi et al. (2011). Gradient tracking, which leverages the idea of dy-\n",
            "namic averageconsensus ( Zhu and Mart´ ınez ,2010) to track the gradient of the global objective function, is a\n",
            "popularmethodtoimprovetheconvergencespeed( Qu and Li ,2017;Nedic et al. ,2017;Di Lorenzo and Scutari ,\n",
            "2016;Pu and Nedi´ c ,2021;Li et al.,2020).\n",
            "Notation. Boldface small and capital letters denote vectors and matrices, r espectively. Sets are denoted\n",
            "with curlycapitalletters, e.g., S,A. We let ( Rd,∝bardbl·∝bardbl)denote the d-dimensionalrealcoordinatespaceequipped\n",
            "with norm ∝bardbl·∝bardbl. Theℓp-norm of vis denoted by ∝bardblv∝bardblp, where 1 ≤p≤ ∞, and the spectral norm of a matrix\n",
            "Mis denoted by ∝bardblM∝bardbl2. We let [ N] denote {1,...,N}, use1Nto represent the all-one vector of length N,\n",
            "and denote by 0a vector or a matrix consisting of all 0’s. We allow the application of fun ctions such as log( ·)\n",
            "and exp( ·) to vectors or matrices, with the understanding that they are ap plied in an element-wise manner.\n",
            "2 Model and backgrounds\n",
            "2.1 Markov decision processes\n",
            "Markov decision processes. We consider an inﬁnite-horizon discounted Markov decision process (MDP)\n",
            "denoted by M= (S,A,P,r,γ), where SandAdenote the state space and the action space, respectively,\n",
            "γ∈[0,1) indicates the discount factor, P:S ×A → ∆(S) is the transition kernel, and r:S ×A → [0,1]\n",
            "stands for the reward function. To be more speciﬁc, for each sta te-action pair ( s,a)∈ S ×A and any state\n",
            "4setting algorithms iteration complexity optimality criteria\n",
            "unregularizedNPG\n",
            "(Agarwal et al. ,2021)O/parenleftig\n",
            "1\n",
            "(1−γ)2ε+log|A|\n",
            "ηε/parenrightig\n",
            "V⋆−Vπ(t)≤ε\n",
            "FedNPG\n",
            "(ours)O/parenleftbigg√\n",
            "σNlog|A|\n",
            "(1−γ)9\n",
            "2(1−σ)ε3\n",
            "2+1\n",
            "(1−γ)2ε/parenrightbigg\n",
            "1\n",
            "T/summationtextT−1\n",
            "t=0/parenleftbig\n",
            "V⋆−Vπ(t)/parenrightbig\n",
            "≤ε\n",
            "regularizedNPG\n",
            "(Cen et al. ,2021)O/parenleftig\n",
            "1\n",
            "τηlog/parenleftbig1\n",
            "ε/parenrightbig/parenrightig\n",
            "V⋆\n",
            "τ−Vπ(t)\n",
            "τ≤ε\n",
            "FedNPG\n",
            "(ours)O/parenleftig\n",
            "max/braceleftig\n",
            "1\n",
            "τη,1\n",
            "1−σ/bracerightig\n",
            "log/parenleftbig1\n",
            "ε/parenrightbig/parenrightig\n",
            "V⋆\n",
            "τ−Vπ(t)\n",
            "τ≤ε\n",
            "Table 1: Iteration complexities of NPG and FedNPG (ours) methods t o reachε-accuracy of the vanilla and\n",
            "entropy-regularizedproblems, where we assume exact gradient e valuation, and only keep the dominant terms\n",
            "w.r.t.ε. The policy estimates in the t-iterationare π(t)and ¯π(t)for NPG and FedNPG, respectively, where T\n",
            "is the number of iterations. Here, Nis the number of agents, τ≤1 is the regularization parameter, σ∈[0,1]\n",
            "is the spectral radius of the network, γ∈[0,1) is the discount factor, |A|is the size of the action space, and\n",
            "η >0is the learningrate. ForvanillaFedNPG, the learningrateisset as η=η1=O/parenleftig\n",
            "(1−γ)9(1−σ)2log|A|\n",
            "TNσ/parenrightig1/3\n",
            ";\n",
            "for entropy-regularized FedNPG, the learning rate satisﬁes 0 < η < η 0=O/parenleftig\n",
            "(1−γ)7(1−σ)2τ\n",
            "σN/parenrightig\n",
            ". The iteration\n",
            "complexities of FedNPG reduce to their centralized counterparts w henσ= 0.\n",
            "s′∈ S, we denote by P(s′|s,a) the transition probability from state sto state s′when action ais taken,\n",
            "andr(s,a) the instantaneous reward received in state swhen action ais taken. Furthermore, a policy\n",
            "π:S →∆(A) speciﬁes an action selection rule, where π(a|s) speciﬁes the probability of taking action ain\n",
            "statesfor each ( s,a)∈ S ×A.\n",
            "For any given policy π, we denote by Vπ:S ∝ma√sto→Rthe corresponding value function, which is the expected\n",
            "discounted cumulative reward with an initial state s0=s, given by\n",
            "∀s∈ S:Vπ(s):=E/bracketleftigg∞/summationdisplay\n",
            "t=0γtr(st,at)|s0=s/bracketrightigg\n",
            ", (1)\n",
            "where the randomness is over the trajectory generated following the policy at∼π(·|st) and the MDP\n",
            "dynamic st+1∼P(·|st,at). We also overload the notation Vπ(ρ) to indicate the expected value function of\n",
            "policyπwhen the initial state follows a distribution ρoverS, namely, Vπ(ρ):=Es∼ρ[Vπ(s)]. Similarly, the\n",
            "Q-function Qπ:S ×A ∝ma√sto→ Rof policy πis deﬁned by\n",
            "∀(s,a)∈ S ×A :Qπ(s,a):=E/bracketleftigg∞/summationdisplay\n",
            "t=0γtr(st,at)|s0=s,a0=a/bracketrightigg\n",
            ", (2)\n",
            "which measures the expected discounted cumulative reward with an initial state s0=sand an initial action\n",
            "a0=a, with expectation taken over the randomness of the trajectory . The optimal policy π⋆refers to the\n",
            "policy that maximizes the value function Vπ(s) for all states s∈ S, which is guaranteed to exist ( Puterman ,\n",
            "2014). The corresponding optimal value function and Q-function are de noted as V⋆andQ⋆, respectively.\n",
            "2.2 Entropy-regularized RL\n",
            "Entropy regularization ( Williams and Peng ,1991;Ahmed et al. ,2019) is a popular technique in practice\n",
            "that encourages stochasticity of the policy to promote exploratio n, as well as robustness against reward\n",
            "uncertainties. Mathematically, this can be viewed as adjusting the in stantaneous reward based the current\n",
            "policy in use as\n",
            "∀(s,a)∈ S ×A :rτ(s,a):=r(s,a)−τlogπ(a|s), (3)\n",
            "5whereτ≥0 denotes the regularization parameter. Typically, τshould not be too large to outweigh the\n",
            "actual rewards; for ease of presentation, we assume τ≤min/braceleftig\n",
            "1,1\n",
            "log|A|/bracerightig\n",
            "(Cen et al. ,2022b). Equivalently,\n",
            "this amounts to the entropy-regularized (also known as “soft”) v alue function, deﬁned as\n",
            "∀s∈ S:Vπ\n",
            "τ(s):=Vπ(s)+τH(s,π). (4)\n",
            "Here, we deﬁne\n",
            "H(s,π):=E/bracketleftigg∞/summationdisplay\n",
            "t=0−γtlogπ(at|st)/vextendsingle/vextendsingles0=s/bracketrightigg\n",
            "=1\n",
            "1−γEs′∼dπs/bracketleftigg\n",
            "−/summationdisplay\n",
            "a∈Aπ(a|s′)logπ(a|s′)/bracketrightigg\n",
            ", (5)\n",
            "wheredπ\n",
            "s0is the discounted state visitation distribution of policy πgiven an initial state s0∈ S, denoted by\n",
            "∀s∈ S:dπ\n",
            "s0(s):= (1−γ)∞/summationdisplay\n",
            "t=0γtP(st=s|s0), (6)\n",
            "with the trajectory generated by following policy πin the MDP Mstarting from state s0. Analogously, the\n",
            "regularized (or soft) Q-function Qπ\n",
            "τof policy πis related to the soft value function Vπ\n",
            "τ(s) as\n",
            "∀(s,a)∈ S ×A :Qπ\n",
            "τ(s,a) =r(s,a)+γEs′∈P(·|s,a)[Vπ\n",
            "τ(s′)], (7a)\n",
            "∀s∈ S:Vπ\n",
            "τ(s) =Ea∼π(·|s)[−τπ(a|s)+Qπ\n",
            "τ(s,a)]. (7b)\n",
            "The optimal regularized policy, the optimal regularized value functio n, and the Q-function are denoted by\n",
            "π⋆\n",
            "τ,V⋆\n",
            "τ, andQ⋆\n",
            "τ, respectively.\n",
            "2.3 Natural policy gradient methods\n",
            "Natural policy gradient (NPG) methods lie at the heart of policy optim ization, serving as the backbone\n",
            "of popular heuristics such as TRPO ( Schulman et al. ,2015) and PPO ( Schulman et al. ,2017). Instead of\n",
            "directly optimizing the policy over the probability simplex, one often ad opts the softmax parameterization,\n",
            "which parameterizes the policy as\n",
            "πθ:= softmax( θ) or∀(s,a)∈ S ×A :πθ(a|s):=expθ(s,a)/summationtext\n",
            "a′∈Aexpθ(s,a′)(8)\n",
            "for anyθ:S ×A → R.\n",
            "Vanilla NPG method. In the tabular setting, the update rule of vanilla NPG at the t-th iteration can\n",
            "be concisely represented as\n",
            "∀(s,a)∈ S ×A :π(t+1)(a|s)∝π(t)(a|s)exp/parenleftbiggηQ(t)(s,a)\n",
            "1−γ/parenrightbigg\n",
            ", (9)\n",
            "whereη >0 denotes the learning rate, and Q(t)=Qπ(t)is the Q-function under policy π(t).Agarwal et al.\n",
            "(2021) shows that: in order to ﬁnd an ε-optimal policy, NPG takes at most O/parenleftig\n",
            "1\n",
            "(1−γ)2ε/parenrightig\n",
            "iterations, assuming\n",
            "exact policy evaluation.\n",
            "Entropy-regularized NPG method. Turning to the regularized problem, we note that the update rule\n",
            "of entropy-regularized NPG becomes\n",
            "∀(s,a)∈ S ×A :π(t+1)(a|s)∝(π(t)(a|s))1−ητ\n",
            "1−γexp/parenleftigg\n",
            "ηQ(t)\n",
            "τ(s,a)\n",
            "1−γ/parenrightigg\n",
            ", (10)\n",
            "whereη∈(0,1−γ\n",
            "τ] is the learning rate, and Q(t)\n",
            "τ=Qπ(t)\n",
            "τis the soft Q-function of policy π(t).Cen et al.\n",
            "(2022a) provesthat entropy-regularizedNPG enjoys fast global linear c onvergenceto the optimal regularized\n",
            "policy: to ﬁnd an ε-optimal regularized policy, entropy-regularized NPG takes no mor e thanO/parenleftig\n",
            "1\n",
            "ητlog/parenleftbig1\n",
            "ε/parenrightbig/parenrightig\n",
            "iterations.\n",
            "63 Federated NPG methods for multi-task RL\n",
            "3.1 Federated multi-task RL\n",
            "In this paper, we consider the federated multi-task RL setting, wh ere a set of agents learn collaboratively\n",
            "a single policy that maximizes its average performance over all the ta sks using only local computation and\n",
            "communication.\n",
            "Multi-task RL. Each agent n∈[N] has its own private reward function rn(s,a) — corresponding to\n",
            "diﬀerent tasks — while sharing the same transition kernel of the env ironment. The goal is to collectively\n",
            "learn a single policy πthat maximizes the global value function given by\n",
            "Vπ(s) =1\n",
            "NN/summationdisplay\n",
            "n=1Vπ\n",
            "n(s), (11)\n",
            "whereVπ\n",
            "nis the value function of agent n∈[N], deﬁned by\n",
            "∀s∈ S:Vπ\n",
            "n(s):=E/bracketleftigg∞/summationdisplay\n",
            "t=0γtrn(st,at)|s0=s/bracketrightigg\n",
            ". (12)\n",
            "Clearly, the global value function ( 11) corresponds to using the average reward of all agents\n",
            "r(s,a) =1\n",
            "NN/summationdisplay\n",
            "n=1rn(s,a). (13)\n",
            "TheglobalQ-function Qπ(s,a)andtheagentQ-functions Qπ\n",
            "n(s,a)canbedeﬁnedinasimilarmannerobeying\n",
            "Qπ(s,a) =1\n",
            "N/summationtextN\n",
            "n=1Qπ\n",
            "n(s,a).\n",
            "In parallel, we are interested in the entropy-regularized setting, w here each agent n∈[N] is equipped\n",
            "with a regularized reward function given by\n",
            "rτ,n(s,a):=rn(s,a)−τlogπ(a|s), (14)\n",
            "and we deﬁne similarly the regularized value function and the global re gularized value function as\n",
            "∀s∈ S:Vπ\n",
            "τ,n(s):=E/bracketleftigg∞/summationdisplay\n",
            "t=0γtrτ,n(st,at)|s0=s/bracketrightigg\n",
            ",andVπ\n",
            "τ(s) =1\n",
            "NN/summationdisplay\n",
            "n=1Vπ\n",
            "τ,n(s). (15)\n",
            "The soft Q-function of agent nis given by\n",
            "Qπ\n",
            "τ,n(s,a) =rn(s,a)+γEs′∈P(·|s,a)/bracketleftbig\n",
            "Vπ\n",
            "τ,n(s′)/bracketrightbig\n",
            ", (16)\n",
            "and the global soft Q-function is given by Qπ\n",
            "τ(s,a) =1\n",
            "N/summationtextN\n",
            "n=1Qπ\n",
            "τ,n(s,a).\n",
            "Federated policy optimization in the fully decentralized s etting. We consider a federated setting\n",
            "with fully decentralized communication, that is, all the agents are sy nchronized to perform information\n",
            "exchange over some prescribed network topology denoted by an u ndirected weighted graph G([N],E). Here,\n",
            "Estands for the edge set of the graph with Nnodes — each correspondingto an agent — and two agentscan\n",
            "communicatewitheachotherifandonlyifthereisanedgeconnecting them. Theinformationsharingoverthe\n",
            "graph is best described by a mixing matrix ( Nedic and Ozdaglar ,2009), denoted by W= [wij]∈[0,1]N×N,\n",
            "wherewijisapositivenumberif( i,j)∈Eand0otherwise. Wealsomakethefollowingstandardassumptions\n",
            "on the mixing matrix.\n",
            "Assumption 1 (double stochasticity) .The mixing matrix W= [wij]∈[0,1]N×Nis symmetric (i.e.,\n",
            "W⊤=W) and doubly stochastic (i.e., W1N=1N,1⊤\n",
            "NW=1⊤\n",
            "N).\n",
            "7The following standard metric measures how fast information propa gates over the graph.\n",
            "Deﬁnition 1 (spectral radius) .The spectral radius of Wis deﬁned as\n",
            "σ:=/vextenddouble/vextenddouble/vextenddoubleW−1\n",
            "N1N1⊤\n",
            "N/vextenddouble/vextenddouble/vextenddouble\n",
            "2∈[0,1). (17)\n",
            "The spectral radius σdetermines how fast information propagate over the network. Fo r instance, in a\n",
            "fully-connected network, we can achieve σ= 0 by setting W=1\n",
            "N1N1⊤\n",
            "N. For control of 1 /(1−σ) regarding\n",
            "diﬀerent graphs, we refer the readers to paper Nedi´ c et al. (2018). In an Erd¨ os-R´ enyi random graph, as long\n",
            "as the graph is connected, one has with high probability σ≍1. Another immediate consequence is that for\n",
            "anyx∈RN, letting x=1\n",
            "N1⊤\n",
            "Nxbe its average, we have\n",
            "∝bardblWx−x1N∝bardbl2≤σ∝bardblx−x1N∝bardbl2, (18)\n",
            "where the consensus error contracts by a factor of σ.\n",
            "3.2 Proposed federated NPG algorithms\n",
            "Assuming softmax parameterization, the problem can be formulate d as decentralized optimization,\n",
            "(unregularized) max\n",
            "θVπθ(s) =1\n",
            "NN/summationdisplay\n",
            "n=1Vπθn(s), (19)\n",
            "(regularized) max\n",
            "θVπθτ(s) =1\n",
            "NN/summationdisplay\n",
            "n=1Vπθτ,n(s), (20)\n",
            "whereπθ:= softmax( θ) subject to communication constraints. Motivated by the succes s of NPG methods,\n",
            "we aim to develop federated NPG methods to achieve our goal. For no tational convenience, let π(t):=/parenleftbig\n",
            "π(t)\n",
            "1,···,π(t)\n",
            "N/parenrightbig⊤be the collection of policy estimates at all agents in the t-th iteration. Let\n",
            "π(t):= softmax/parenleftigg\n",
            "1\n",
            "NN/summationdisplay\n",
            "n=1logπ(t)\n",
            "n/parenrightigg\n",
            ", (21)\n",
            "which satisﬁes that π(t)(a|s)∝/parenleftig/producttextN\n",
            "n=1π(t)\n",
            "n(a|s)/parenrightig1/N\n",
            "for each ( s,a)∈ S ×A. Therefore, π(t)could be seen\n",
            "as the normalized geometric mean of {π(t)\n",
            "n}n∈[N]. Deﬁne the collection of Q-function estimates as\n",
            "Q(t):=/parenleftig\n",
            "Qπ(t)\n",
            "1\n",
            "1,···,Qπ(t)\n",
            "N\n",
            "N/parenrightig⊤\n",
            ",Q(t)\n",
            "τ:=/parenleftig\n",
            "Qπ(t)\n",
            "1\n",
            "τ,1,···,Qπ(t)\n",
            "N\n",
            "τ,N/parenrightig⊤\n",
            ".\n",
            "We shall often abuse the notation and treat π(t),Q(t)\n",
            "τas matrices in RN×|S||A|, and treat π(t)(a|s),Q(t)\n",
            "τ(a|s)\n",
            "as vectors in RN, for all ( s,a)∈ S ×A.\n",
            "Vanilla federated NPG methods. To motivate the algorithm development, observe that the NPG\n",
            "method (cf. ( 9)) applied to ( 19) adopts the update rule\n",
            "π(t+1)(a|s)∝π(t)(a|s)exp/parenleftigg\n",
            "ηQπ(t)(s,a)\n",
            "1−γ/parenrightigg\n",
            "=π(t)(a|s)exp/parenleftigg\n",
            "η/summationtextN\n",
            "n=1Qπ(t)\n",
            "n(s,a)\n",
            "N(1−γ)/parenrightigg\n",
            "for all (s,a)∈ S × A . Two challenges arise when executing this update rule: the policy est imates are\n",
            "maintained locally without consensus, and the global Q-function are unavailable in the decentralized setting.\n",
            "To address these challenges, we apply the idea of dynamic average c onsensus ( Zhu and Mart´ ınez ,2010),\n",
            "where each agent maintains its own estimate T(t)\n",
            "n(s,a) of the global Q-function, which are collected as vector\n",
            "T(t)=/parenleftbig\n",
            "T(t)\n",
            "1,···,T(t)\n",
            "N/parenrightbig⊤.\n",
            "8Algorithm 1 Federated NPG (FedNPG)\n",
            "1:Input:learning rate η >0, iteration number T∈N+, mixing matrix W∈RN×N.\n",
            "2:Initialize: π(0),T(0)=Q(0).\n",
            "3:fort= 0,1,···T−1do\n",
            "4:Update the policy for each ( s,a)∈ S ×A:\n",
            "logπ(t+1)(a|s) =Wlogπ(t)(a|s)+η\n",
            "1−γT(t)(s,a)−logz(t)(s), (22)\n",
            "wherez(t)(s) =/summationtext\n",
            "a′∈Aexp/braceleftig\n",
            "Wlogπ(t)(a′|s)+η\n",
            "1−γT(t)(s,a′)/bracerightig\n",
            ".\n",
            "5:Evaluate Q(t+1).\n",
            "6:Update the global Q-function estimate for each ( s,a)∈ S ×A:\n",
            "T(t+1)(s,a) =W/parenleftig\n",
            "T(t)(s,a)+Q(t+1)(s,a)−Q(t)(s,a)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\n",
            "Q-tracking/parenrightig\n",
            ". (23)\n",
            "7:end for\n",
            "At each iteration, each agent updates its policy estimates based on its neighbors’ information via gossip\n",
            "mixing, in addition to a correction term that tracks the diﬀerence Qπ(t+1)\n",
            "nn(s,a)−Qπ(t)\n",
            "nn(s,a) of the local Q-\n",
            "functions between consecutive policy updates. Note that the mixin g is applied linearly to the logarithms of\n",
            "local policies, which translates into a multiplicative mixing of the local po licies. Algorithm 1summarizes the\n",
            "detailed procedure of the proposed algorithm written in a compact m atrix form, which we dub as federated\n",
            "NPG (FedNPG). Note that the agents do not need to share their re ward functions with others, and agent\n",
            "n∈[N] will only be responsible to evaluate the local policy π(t)\n",
            "nusing the local reward rn.\n",
            "Entropy-regularized federated NPG methods. Moving onto the entropy regularized case, we adopt\n",
            "similar algorithmic ideas to decentralize ( 10), and propose the federated NPG (FedNPG) method with en-\n",
            "tropy regularization, summarized in Algorithm 2. Clearly, the entropy-regularized FedNPG method reduces\n",
            "to the vanilla FedNPG in the absence of the regularization (i.e., when τ= 0).\n",
            "Algorithm 2 Federated NPG (FedNPG) with entropy regularization\n",
            "1:Input: learning rate η >0, iteration number T∈N+, mixing matrix W∈RN×N, regularization\n",
            "coeﬃcient τ >0.\n",
            "2:Initialize: π(0),T(0)=Q(0)\n",
            "τ.\n",
            "3:fort= 0,1,···do\n",
            "4:Update the policy for each ( s,a)∈ S ×A:\n",
            "logπ(t+1)(a|s) =/parenleftbigg\n",
            "1−ητ\n",
            "1−γ/parenrightbigg\n",
            "Wlogπ(t)(a|s)+η\n",
            "1−γT(t)(s,a)−logz(t)(s),(24)\n",
            "wherez(t)(s) =/summationtext\n",
            "a′∈Aexp/braceleftig/parenleftig\n",
            "1−ητ\n",
            "1−γ/parenrightig\n",
            "Wlogπ(t)(a′|s)+η\n",
            "1−γT(t)(s,a′)/bracerightig\n",
            ".\n",
            "5:Evaluate Q(t+1)\n",
            "τ.\n",
            "6:Update the global Q-function estimate for each ( s,a)∈ S ×A:\n",
            "T(t+1)(s,a) =W/parenleftig\n",
            "T(t)(s,a)+Q(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\n",
            "Q-tracking/parenrightig\n",
            ". (25)\n",
            "7:end for\n",
            "94 Theoretical guarantees\n",
            "4.1 Global convergence of FedNPG\n",
            "Convergence with exact policy evaluation. We begin with the global convergence of FedNPG (cf. Al-\n",
            "gorithm 1), stated in the following theorem. The formal statement and proo f of this result can be found in\n",
            "SectionA.3.\n",
            "Theorem 1 (Global sublinear convergence of exact FedNPG (informal)) .Suppose π(0)\n",
            "n,n∈[N]are set as\n",
            "the uniform distribution. Then for 0< η≤η1:=(1−σ)2(1−γ)3\n",
            "16√\n",
            "Nσ, we have\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftig\n",
            "V⋆(ρ)−Vπ(t)(ρ)/parenrightig\n",
            "≤V⋆(dπ⋆\n",
            "ρ)\n",
            "(1−γ)T+log|A|\n",
            "ηT+32Nση2\n",
            "(1−γ)9(1−σ)2. (26)\n",
            "Theorem 1characterizes the average-iterate convergence of the averag e policy π(t)(cf. (21)) across the\n",
            "agents, which depends logarithmically on the size of the action space , and independently on the size of\n",
            "the state space. When T≥128√\n",
            "Nlog|A|σ2\n",
            "(1−σ)4, by optimizing the learning rate η=/parenleftig\n",
            "(1−γ)9(1−σ)2log|A|\n",
            "32TNσ/parenrightig1/3\n",
            "to\n",
            "balance the latter two terms, we arrive at\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftig\n",
            "V⋆(ρ)−Vπ(t)(ρ)/parenrightig\n",
            "/lessorsimilarV⋆(dπ⋆\n",
            "ρ)\n",
            "(1−γ)T+N1/3σ1/3\n",
            "(1−γ)3(1−σ)2/3/parenleftbigglog|A|\n",
            "T/parenrightbigg2/3\n",
            ". (27)\n",
            "A few comments are in order.\n",
            "•Server-client setting. When the network is fully connected, i.e., σ= 0, the convergencerate of FedNPG\n",
            "recovers the O(1/T) rate, matching that of the centralized NPG established in Agarwal et al. (2021).\n",
            "•Well-connected networks. When the network is relatively well-connected in the sense ofσ\n",
            "(1−σ)2/lessorsimilar1−γ\n",
            "N1/2,\n",
            "FedNPG ﬁrst converges at the rate of O(1/T), and then at the slower O(1/T2/3) rate after T/greaterorsimilar\n",
            "(1−γ)3(1−σ)2\n",
            "Nσ.\n",
            "•Poorly-connected networks. In addition, when the network is poorly connected in the sense ofσ\n",
            "(1−σ)2/greaterorsimilar\n",
            "1−γ\n",
            "N1/2, we see that FedNPG converges at the slower O(1/T2/3) rate.\n",
            "We state the iteration complexity in Corollary 1.\n",
            "Corollary 1 (Iteration complexity of exact FedNPG) .To reach\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftbig\n",
            "V⋆(ρ)−Vπ(t)(ρ)/parenrightbig\n",
            "≤ε,\n",
            "the iteration complexity of FedNPG is at most O/parenleftig/parenleftig\n",
            "σ1/2\n",
            "(1−γ)9/2(1−σ)ε3/2+σ2\n",
            "(1−σ)4/parenrightig√\n",
            "Nlog|A|+1\n",
            "ε(1−γ)2/parenrightig\n",
            ".\n",
            "Convergence with inexact policy evaluation. In practice, the policies need to be evaluated using\n",
            "samplescollectedbytheagents,wheretheQ-functionsareonlyes timatedapproximately. Weareinterestedin\n",
            "gauging how the approximation error impacts the performance of F edNPG, as demonstrated in the following\n",
            "theorem.\n",
            "Theorem 2 (Global sublinear convergence of inexact FedNPG (informal)) .Suppose that an estimate qπ(t)\n",
            "nn\n",
            "are used in replace of Qπ(t)\n",
            "nnin Algorithm 1. Under the assumptions of Theorem 1, we have\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftig\n",
            "V⋆(ρ)−Vπ(t)(ρ)/parenrightig\n",
            "≤V⋆(dπ⋆\n",
            "ρ)\n",
            "(1−γ)T+log|A|\n",
            "ηT+32Nση2\n",
            "(1−γ)9(1−σ)2+C3max\n",
            "n∈[N],t∈[T]/vextenddouble/vextenddouble/vextenddoubleQπ(t)\n",
            "nn−qπ(t)\n",
            "nn/vextenddouble/vextenddouble/vextenddouble\n",
            "∞,(28)\n",
            "whereC3:=32√\n",
            "Nση\n",
            "(1−γ)5(1−σ)2/parenleftig\n",
            "η√\n",
            "N\n",
            "(1−γ)3+1/parenrightig\n",
            "+2\n",
            "(1−γ)2.\n",
            "10The formal statement and proof of this result is given in Section A.4.\n",
            "As long as max n∈[N],t∈[T]/vextenddouble/vextenddoubleQπ(t)\n",
            "nn−qπ(t)\n",
            "nn/vextenddouble/vextenddouble\n",
            "∞≤ε\n",
            "C3, inexact FedNPG reaches1\n",
            "T/summationtextT−1\n",
            "t=0/parenleftbig\n",
            "V⋆(ρ)−Vπ(t)(ρ)/parenrightbig\n",
            "≤\n",
            "2εat the same iteration complexity as predicted in Corollary 1. Equipped with existing sample complexity\n",
            "bounds on policy evaluation, e.g. using a simulator as in Li et al. (2023b) andLi et al. (2023a), this imme-\n",
            "diate leads to a sample complexity bound for a federated actor-crit ic type algorithm for multi-task RL. We\n",
            "detail this in the following remark.\n",
            "Remark 1 (sample complexity bound of inexact FedNPG) .Recall that Li et al. (2023b) shows that for\n",
            "any ﬁxed policy π, model-based policy evaluation achieves ∝bardblqπ\n",
            "τ−Qπ\n",
            "τ∝bardbl∞≤εevalwith high probability if the\n",
            "number of samples per state-action pair exceeds the order of /tildewideO/parenleftig\n",
            "1\n",
            "(1−γ)3ε2\n",
            "eval/parenrightig\n",
            ". When T/greaterorsimilar√\n",
            "Nlog|A|σ2\n",
            "(1−σ)4and\n",
            "η=/parenleftig\n",
            "(1−γ)9(1−σ)2log|A|\n",
            "32TNσ/parenrightig1/3\n",
            ", we have C3≍1/(1−γ)2. By employing fresh samples for the policy evaluation\n",
            "of each agent at every iteration, we can set εeval:= max n∈[N],t∈[T]/vextenddouble/vextenddouble/vextenddoubleQπ(t)\n",
            "nn−qπ(t)\n",
            "nn/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≍ε\n",
            "C3≍(1−γ)2ε, and\n",
            "invoke the union bound over all iterations to give a (very loo se) upper bound of sample complexity of FedNPG\n",
            "per state-action pair at each agent as follows:\n",
            "/tildewideO/parenleftbigg/parenleftbiggσ1/2\n",
            "(1−γ)9/2(1−σ)ε3/2+σ2\n",
            "(1−σ)4/parenrightbigg√\n",
            "N+1\n",
            "ε(1−γ)2/parenrightbigg\n",
            "/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "iteration complexity·/tildewideO/parenleftbigg1\n",
            "(1−γ)7ε2/parenrightbigg\n",
            "/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\n",
            "sample complexity per iteration\n",
            "=/tildewideO/parenleftbigg1\n",
            "(1−γ)7ε2·/bracketleftbigg/parenleftbiggσ1/2\n",
            "(1−γ)9/2(1−σ)ε3/2+σ2\n",
            "(1−σ)4/parenrightbigg√\n",
            "N+1\n",
            "ε(1−γ)2/bracketrightbigg/parenrightbigg\n",
            ".\n",
            "Hence, the total sample complexity scales linearly with res pect to the size of the state-action space up to\n",
            "logarithmic factors. When σis close to 1, which corresponds to the case where the network exhibits a high\n",
            "degree of locality, the above sample complexity becomes\n",
            "/tildewideO/parenleftigg√\n",
            "N\n",
            "(1−γ)7ε2·/bracketleftbigg/parenleftbigg1\n",
            "(1−γ)9/2(1−σ)ε3/2+1\n",
            "(1−σ)4/parenrightbigg/bracketrightbigg/parenrightigg\n",
            ",\n",
            "which further simpliﬁes to /tildewideO/parenleftig √\n",
            "N\n",
            "(1−γ)11.5(1−σ)ε3.5/parenrightig\n",
            "for suﬃciently small ε.\n",
            "4.2 Global convergence of FedNPG with entropy regularizati on\n",
            "Convergence with exact policy evaluation. Next, we present our global convergence guarantee of\n",
            "entropy-regularized FedNPG with exact policy evaluation (cf. Algor ithm2).\n",
            "Theorem 3 (Global linear convergenceof exact entropy-regularizedFedNPG (informal)) .For anyγ∈(0,1)\n",
            "and0< τ≤1, there exists η0= min/braceleftig\n",
            "1−γ\n",
            "τ,O/parenleftig\n",
            "(1−γ)7(1−σ)2τ\n",
            "σN/parenrightig/bracerightig\n",
            ", such that if 0< η≤η0, then we have\n",
            "/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞≤2γC1ρ(η)t,/vextenddouble/vextenddoublelogπ⋆\n",
            "τ−logπ(t)/vextenddouble/vextenddouble\n",
            "∞≤2C1\n",
            "τρ(η)t, (29)\n",
            "whereQ(t)\n",
            "τ:=Qπ(t)\n",
            "τ,ρ(η)≤max{1−τη\n",
            "2,3+σ\n",
            "4}<1, andC1is some problem-dependent constant.\n",
            "The exact expressions of C1andη0are speciﬁed in Appendix A.1. Theorem 3conﬁrms that entropy-\n",
            "regularized FedNPG convergesat a linear rate to the optimal regula rizedpolicy, which is almost independent\n",
            "ofthe size ofthe state-actionspace, highlighting the positive roleo f entropyregularizationin federated policy\n",
            "optimization. Whenthenetworkisfullyconnected, i.e. σ= 0,theiterationcomplexityofentropy-regularized\n",
            "FedNPG reduces to O/parenleftig\n",
            "1\n",
            "ητlog1\n",
            "ε/parenrightig\n",
            ", matching that of the centralized entropy-regularized NPG estab lished in\n",
            "Cen et al. (2021). When the network is less connected, one needs to be more conse rvative in the choice of\n",
            "learning rates, leading to a higher iteration complexity, as described in the following corollary.\n",
            "11Corollary 2 (Iteration complexity of exact entropy-regularized FedNPG) .To reach/vextenddouble/vextenddoublelogπ⋆\n",
            "τ−logπ(t)/vextenddouble/vextenddouble\n",
            "∞≤\n",
            "ε, the iteration complexity of entropy-regularized FedNPG i s at most\n",
            "/tildewideO/parenleftbigg\n",
            "max/braceleftbigg2\n",
            "τη,4\n",
            "1−σ/bracerightbigg\n",
            "log1\n",
            "ε/parenrightbigg\n",
            "(30)\n",
            "up to logarithmic factors. Especially, when η=η0, the best iteration complexity becomes\n",
            "/tildewideO/parenleftbigg/parenleftbiggNσ\n",
            "(1−γ)7(1−σ)2τ2+1\n",
            "1−γ/parenrightbigg\n",
            "log1\n",
            "τε/parenrightbigg\n",
            ".\n",
            "Convergence with inexact policy evaluation. Last but not the least, we present the informal conver-\n",
            "gence results of entropy-regularized FedNPG with inexact policy ev aluation, whose formal version can be\n",
            "found in Appendix A.2.\n",
            "Theorem 4 (Global linear convergence of inexact entropy-regularized FedNP G (informal)) .Suppose that\n",
            "an estimate qπ(t)\n",
            "nτ,nare used in replace of Qπ(t)\n",
            "nτ,nin Algorithm 2. Under the assumptions of Theorem 3, we have\n",
            "/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞≤2γ/parenleftig\n",
            "C1ρ(η)t+C2max\n",
            "n∈[N],t∈[T]/vextenddouble/vextenddoubleQπ(t)\n",
            "nτ,n−qπ(t)\n",
            "nτ,n/vextenddouble/vextenddouble\n",
            "∞/parenrightig\n",
            ",\n",
            "/vextenddouble/vextenddoublelogπ⋆\n",
            "τ−logπ(t)/vextenddouble/vextenddouble\n",
            "∞≤2\n",
            "τ/parenleftig\n",
            "C1ρ(η)t+C2max\n",
            "n∈[N],t∈[T]/vextenddouble/vextenddoubleQπ(t)\n",
            "nτ,n−qπ(t)\n",
            "nτ,n/vextenddouble/vextenddouble\n",
            "∞/parenrightig\n",
            ",(31)\n",
            "whereQ(t)\n",
            "τ:=Qπ(t)\n",
            "τ,ρ(η)≤max{1−τη\n",
            "2,3+σ\n",
            "4}<1, andC1,C2are problem-dependent constants.\n",
            "5 Conclusions\n",
            "This work proposes the ﬁrst provably eﬃcient federated NPG (Fed NPG) methods for solving vanilla and\n",
            "entropy-regularized multi-task RL problems in the fully decentralize d setting. The established ﬁnite-time\n",
            "global convergence guarantees are almost independent of the siz e of the state-action space up to some\n",
            "logarithmic factor, and illuminate the impacts of the size and connect ivity of the network. Furthermore,\n",
            "the proposed FedNPG methods are robust vis-a-vis inexactness o f local policy evaluations, leading to a\n",
            "ﬁnite-sample complexity bound of a federated actor-critic method for multi-task RL. When it comes to\n",
            "future directions, it would be of great interest to further explore sample-eﬃcient algorithms and examine\n",
            "if it is possible to go beyond the entrywise approximation error assum ption in policy evaluation. Another\n",
            "interesting direction is to extend the analysis of FedNPG to incorpor ate function approximations.\n",
            "Acknowledgments\n",
            "The work of T. Yang, S. Cen and Y. Chi are supported in part by the grants ONR N00014-19-1-2404, NSF\n",
            "CCF-1901199, CCF-2106778, AFRL FA8750-20-2-0504, and a CM U Cylab seed grant. The work of Y. Wei\n",
            "is supported in part by the the NSF grants DMS-2147546/2015447 , CAREER award DMS-2143215, CCF-\n",
            "2106778, and the Google Research Scholar Award. The work of Y. C hen is supported in part by the Alfred\n",
            "P. Sloan Research Fellowship, the Google Research Scholar Award, t he AFOSR grant FA9550-22-1-0198,the\n",
            "ONR grant N00014-22-1-2354,and the NSF grants CCF-2221009 and CCF-1907661. S. Cen is also gratefully\n",
            "supported by Wei Shen and Xuehong Zhang Presidential Fellowship, Boeing Scholarship, and JP Morgan\n",
            "Chase PhD Fellowship.\n",
            "References\n",
            "Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G. (2021). On t he theory of policy gradient methods:\n",
            "Optimality, approximation,anddistributionshift. The Journal of Machine Learning Research , 22(1):4431–\n",
            "4506.\n",
            "12Ahmed, Z., Le Roux, N., Norouzi, M., and Schuurmans, D. (2019). Un derstanding the impact of entropy on\n",
            "policy optimization. In International Conference on Machine Learning , pages 151–160.\n",
            "Amari, S.-I. (1998). Natural gradient works eﬃciently in learning. Neural computation , 10(2):251–276.\n",
            "Anwar, A. and Raychowdhury, A. (2021). Multi-task federated r einforcement learning with adversaries.\n",
            "arXiv preprint arXiv:2103.06473 .\n",
            "Assran, M., Romoﬀ, J., Ballas, N., Pineau, J., and Rabbat, M. (2019). Gossip-based actor-learner architec-\n",
            "tures for deep reinforcement learning. Advances in Neural Information Processing Systems , 32.\n",
            "Bhandari, J. and Russo, D. (2021). On the linear convergence of p olicy gradient methods for ﬁnite MDPs.\n",
            "InInternational Conference on Artiﬁcial Intelligence and St atistics, pages 2386–2394. PMLR.\n",
            "Cen, S., Cheng, C., Chen, Y., Wei, Y., and Chi, Y. (2022a). Fast global convergence of natural policy\n",
            "gradient methods with entropy regularization. Operations Research , 70(4):2563–2578.\n",
            "Cen, S., Chi, Y., Du, S. S., and Xiao, L. (2022b). Faster last-iterate convergence of policy optimization in\n",
            "zero-sum Markov games. In The Eleventh International Conference on Learning Represe ntations.\n",
            "Cen, S., Wei, Y., and Chi, Y. (2021). Fast policy extragradient metho ds for competitive games with entropy\n",
            "regularization. Advances in Neural Information Processing Systems , 34:27952–27964.\n",
            "Chen, J., Feng, J., Gao, W., and Wei, K. (2022a). Decentralized natu ral policy gradient with variance\n",
            "reduction for collaborative multi-agent reinforcement learning. arXiv preprint arXiv:2209.02179 .\n",
            "Chen, T., Zhang, K., Giannakis, G. B., and Ba¸ sar, T. (2021). Commu nication-eﬃcient policy gradient meth-\n",
            "ods for distributed reinforcement learning. IEEE Transactions on Control of Network Systems , 9(2):917–\n",
            "929.\n",
            "Chen, Z., Zhou, Y., and Chen, R.-R. (2022b). Multi-agent oﬀ-policy t dc with near-optimal sample and\n",
            "communication complexities.\n",
            "Di Lorenzo, P. and Scutari, G. (2016). Next: In-network nonco nvex optimization. IEEE Transactions on\n",
            "Signal and Information Processing over Networks , 2(2):120–136.\n",
            "Duchi, J. C., Agarwal, A., and Wainwright, M. J. (2011). Dual averag ing for distributed optimization:\n",
            "Convergence analysis and network scaling. IEEE Transactions on Automatic control , 57(3):592–606.\n",
            "Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Do ron, Y., Firoiu, V., Harley, T.,\n",
            "Dunning, I., et al. (2018). Impala: Scalable distributed deep-rl with importance weighted actor-learner\n",
            "architectures. In International conference on machine learning , pages 1407–1416. PMLR.\n",
            "Eysenbach, B. and Levine, S. (2021). Maximum entropy RL (prova bly) solves some robust RL problems. In\n",
            "International Conference on Learning Representations .\n",
            "Horn, R. A. and Johnson, C. R. (2012). Matrix analysis . Cambridge university press.\n",
            "Kakade, S. M. (2001). A natural policy gradient. Advances in neural information processing systems , 14.\n",
            "Kar, S., Moura, J. M., and Poor, H. V. (2012). Qd-learning: A collabo rative distributed strategy for\n",
            "multi-agent reinforcement learning through consensus. arXiv preprint arXiv:1205.0047 .\n",
            "Khodadadian, S., Jhunjhunwala, P. R., Varma, S. M., and Maguluri, S. T. (2021). On the linear convergence\n",
            "of natural policy gradient algorithm. In 2021 60th IEEE Conference on Decision and Control (CDC) ,\n",
            "pages 3794–3799. IEEE.\n",
            "Khodadadian, S., Sharma, P., Joshi, G., and Maguluri, S. T. (2022). F ederated reinforcement learning:\n",
            "Linear speedup under Markovian sampling. In International Conference on Machine Learning , pages\n",
            "10997–11057. PMLR.\n",
            "13Lan, G. (2023). Policy mirror descent for reinforcement learning: Linear convergence, new sampling com-\n",
            "plexity, and generalized problem classes. Mathematical programming , 198(1):1059–1106.\n",
            "Lan, G., Li, Y., and Zhao, T. (2023). Block policy mirror descent. SIAM Journal on Optimization ,\n",
            "33(3):2341–2378.\n",
            "Li, B., Cen, S., Chen, Y., and Chi, Y. (2020). Communication-eﬃcient d istributed optimization in networks\n",
            "with gradient tracking and variance reduction. The Journal of Machine Learning Research , 21(1):7331–\n",
            "7381.\n",
            "Li, G., Cai, C., Chen, Y., Wei, Y., and Chi, Y. (2023a). Is q-learning minima x optimal? a tight sample\n",
            "complexity analysis. Operations Research .\n",
            "Li, G., Wei, Y., Chi, Y., andChen, Y.(2023b). Breakingthesamplesizeb arrierinmodel-basedreinforcement\n",
            "learning with a generative model. Operations Research .\n",
            "Li, G., Wei, Y., Chi, Y., and Chen, Y. (2023c). Softmax policy gradient m ethods can take exponential time\n",
            "to converge. Mathematical Programming , pages 1–96.\n",
            "Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J. (201 7). Can decentralized algorithms\n",
            "outperform centralized algorithms? a case study for decentralize d parallel stochastic gradient descent.\n",
            "Advances in neural information processing systems , 30.\n",
            "Lobel, I. and Ozdaglar, A. (2008). Convergence analysis of distrib uted subgradient methods over random\n",
            "networks. In 2008 46th Annual Allerton Conference on Communication, Con trol, and Computing , pages\n",
            "353–360. IEEE.\n",
            "M Alshater, M. (2022). Exploring the role of artiﬁcial intelligence in en hancing academic performance: A\n",
            "case study of chatgpt. Available at SSRN .\n",
            "McKelvey, R. D. and Palfrey, T. R. (1995). Quantal response equ ilibria for normal form games. Games and\n",
            "economic behavior , 10(1):6–38.\n",
            "Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D. (2020). On the global convergence rates of softmax\n",
            "policy gradient methods. In International Conference on Machine Learning , pages 6820–6829. PMLR.\n",
            "Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver , D., and Kavukcuoglu, K.\n",
            "(2016). Asynchronous methods for deep reinforcement learning . InInternational conference on machine\n",
            "learning, pages 1928–1937.\n",
            "Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridgin g the gap between value and policy\n",
            "based reinforcement learning. In Advances in Neural Information Processing Systems , pages 2775–2785.\n",
            "Nedi´ c, A., Olshevsky, A., and Rabbat, M. G. (2018). Network top ology and communication-computation\n",
            "tradeoﬀs in decentralized optimization. Proceedings of the IEEE , 106(5):953–976.\n",
            "Nedic, A., Olshevsky, A., and Shi, W. (2017). Achieving geometric con vergence for distributed optimization\n",
            "over time-varying graphs. SIAM Journal on Optimization , 27(4):2597–2633.\n",
            "Nedic, A. and Ozdaglar, A. (2009). Distributed subgradient metho ds for multi-agent optimization. IEEE\n",
            "Transactions on Automatic Control , 54(1):48–61.\n",
            "Omidshaﬁei, S., Pazis, J., Amato, C., How, J. P., and Vian, J. (2017). D eep decentralized multi-task\n",
            "multi-agent reinforcement learning under partial observability. In International Conference on Machine\n",
            "Learning , pages 2681–2690. PMLR.\n",
            "Petersen, K. B. and Pedersen, M. S. (2008). The matrix cookboo k.Technical University of Denmark ,\n",
            "7(15):510.\n",
            "Pu, S. and Nedi´ c, A. (2021). Distributed stochastic gradient tr acking methods. Mathematical Programming ,\n",
            "187:409–457.\n",
            "14Puterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic pr ogramming . John Wiley\n",
            "& Sons.\n",
            "Qi, J., Zhou, Q., Lei, L., and Zheng, K. (2021). Federated reinforce ment learning: Techniques, applications,\n",
            "and open challenges. arXiv preprint arXiv:2108.11887 .\n",
            "Qu, G. and Li, N. (2017). Harnessing smoothness to accelerate dis tributed optimization. IEEE Transactions\n",
            "on Control of Network Systems , 5(3):1245–1260.\n",
            "Rahman, M. M., Terano, H. J., Rahman, M. N., Salamzadeh, A., and Rah aman, M. S. (2023). Chatgpt and\n",
            "academic research: a review and recommendations based on pract ical examples. Rahman, M., Terano,\n",
            "HJR, Rahman, N., Salamzadeh, A., Rahaman, S.(2023). ChatGP T and Academic Research: A Review\n",
            "and Recommendations Based on Practical Examples. Journal o f Education, Management and Development\n",
            "Studies, 3(1):1–12.\n",
            "Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015 ). Trust region policy optimization.\n",
            "InInternational conference on machine learning , pages 1889–1897.\n",
            "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017 ). Proximal policy optimization\n",
            "algorithms. arXiv preprint arXiv:1707.06347 .\n",
            "Shani, L., Efroni, Y., and Mannor, S. (2020). Adaptive trust region policy optimization: Global convergence\n",
            "and faster rates for regularized MDPs. In Proceedings of the AAAI Conference on Artiﬁcial Intelligen ce,\n",
            "volume 34, pages 5668–5675.\n",
            "Wang, H., Kaplan, Z., Niu, D., and Li, B. (2020). Optimizing federated le arning on non-iid data with\n",
            "reinforcement learning. In IEEE INFOCOM 2020-IEEE Conference on Computer Communicati ons, pages\n",
            "1698–1707. IEEE.\n",
            "Wang, J., Hu, J., Mills, J., Min, G., Xia, M., and Georgalas, N. (2023). Fed erated ensemble model-based\n",
            "reinforcement learning in edge computing. IEEE Transactions on Parallel and Distributed Systems .\n",
            "Williams, R. J. and Peng, J. (1991). Function optimization using conne ctionist reinforcement learning\n",
            "algorithms. Connection Science , 3(3):241–268.\n",
            "Woo, J., Joshi, G., and Chi, Y. (2023). The blessing of heterogeneity in federated q-learning: Linear speedup\n",
            "and beyond. arXiv preprint arXiv:2305.10697 .\n",
            "Xiao, L. (2022). On the convergence rates of policy gradient meth ods.The Journal of Machine Learning\n",
            "Research , 23(1):12887–12922.\n",
            "Yu, T., Li, T., Sun, Y., Nanda, S., Smith, V., Sekar, V., and Seshan, S. ( 2020). Learning context-aware poli-\n",
            "cies from multiple smart homes via federated multi-task learning. In 2020 IEEE/ACM Fifth International\n",
            "Conference on Internet-of-Things Design and Implementati on (IoTDI) , pages 104–115. IEEE.\n",
            "Zeng, S., Anwar, M. A., Doan, T. T., Raychowdhury, A., and Romberg , J. (2021). A decentralized policy\n",
            "gradient approach to multi-task reinforcement learning. In Uncertainty in Artiﬁcial Intelligence , pages\n",
            "1002–1012. PMLR.\n",
            "Zerka, F., Barakat, S., Walsh, S., Bogowicz, M., Leijenaar, R. T., Joc hems, A., Miraglio, B., Townend,\n",
            "D., and Lambin, P. (2020). Systematic review of privacy-preservin g distributed machine learning from\n",
            "federated databases in health care. JCO clinical cancer informatics , 4:184–200.\n",
            "Zhan, W., Cen, S., Huang, B., Chen, Y., Lee, J. D., and Chi, Y. (2023). Policy mirror descent for regularized\n",
            "reinforcement learning: A generalized frameworkwith linear conver gence.SIAM Journal on Optimization ,\n",
            "33(2):1061–1091.\n",
            "Zhao, F., Ren, X., Yang, S., Zhao, P., Zhang, R., and Xu, X. (2023). F ederated multi-objective reinforcement\n",
            "learning. Information Sciences , 624:811–832.\n",
            "15Zhou, R., Liu, T., Kalathil, D., Kumar, P., and Tian, C. (2022). Anchor- changing regularized natural policy\n",
            "gradient for multi-objective reinforcement learning. Advances in Neural Information Processing Systems ,\n",
            "35:13584–13596.\n",
            "Zhu, M. and Mart´ ınez, S. (2010). Discrete-time dynamic average consensus. Automatica , 46(2):322–329.\n",
            "Zhuo, H. H., Feng, W., Lin, Y., Xu, Q., and Yang, Q. (2019). Federate d deep reinforcement learning. arXiv\n",
            "preprint arXiv:1901.08277 .\n",
            "A Convergence analysis\n",
            "For technical convenience, we present ﬁrst the analysis for entr opy-regularized FedNPG and then for vanilla\n",
            "FedNPG.\n",
            "A.1 Analysis of entropy-regularized FedNPG with exact poli cy evaluation\n",
            "To facilitate analysis, we introduce several notation below. For all t≥0, we recall π(t)as the normalized\n",
            "geometric mean of {π(t)\n",
            "n}n∈[N]:\n",
            "π(t):= softmax/parenleftigg\n",
            "1\n",
            "NN/summationdisplay\n",
            "n=1logπ(t)\n",
            "n/parenrightigg\n",
            ", (32)\n",
            "from which we can easily see that for each ( s,a)∈ S × A ,π(t)(a|s)∝/parenleftig/producttextN\n",
            "n=1π(t)\n",
            "n(a|s)/parenrightig1\n",
            "N. We denote the\n",
            "softQ-functions of π(t)byQ(t)\n",
            "τ:\n",
            "Q(t)\n",
            "τ:=\n",
            "Qπ(t)\n",
            "τ,1\n",
            "...\n",
            "Qπ(t)\n",
            "τ,N\n",
            ". (33)\n",
            "In addition, we deﬁne /hatwideQ(t)\n",
            "τ,Q(t)\n",
            "τ∈R|S||A|andV(t)\n",
            "τ∈R|S|as follows\n",
            "/hatwideQ(t)\n",
            "τ:=1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "nτ,n, (34a)\n",
            "Q(t)\n",
            "τ:=Qπ(t)\n",
            "τ=1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "τ,n. (34b)\n",
            "V(t)\n",
            "τ:=Vπ(t)\n",
            "τ=1\n",
            "NN/summationdisplay\n",
            "n=1Vπ(t)\n",
            "τ,n. (34c)\n",
            "For notational convenience, we also denote\n",
            "α:= 1−ητ\n",
            "1−γ. (35)\n",
            "Following Cen et al. (2022a), we introduce the following auxiliary sequence {ξ(t)= (ξ(t)\n",
            "1,···,ξ(t)\n",
            "N)⊤∈\n",
            "RN×|S||A|}t=0,1,···, each recursively deﬁned as\n",
            "∀(s,a)∈ S ×A :ξ(0)(s,a):=∝bardblexp(Q⋆\n",
            "τ(s,·)/τ)∝bardbl1 /vextenddouble/vextenddouble/vextenddoubleexp/parenleftig\n",
            "1\n",
            "N/summationtextN\n",
            "n=1logπ(0)\n",
            "n(·|s)/parenrightig/vextenddouble/vextenddouble/vextenddouble\n",
            "1·π(0)(a|s), (36a)\n",
            "logξ(t+1)(s,a) =αWlogξ(t)(s,a)+(1−α)T(t)(s,a)/τ , (36b)\n",
            "16whereT(t)(s,a) is updated via ( 23). Similarly, we introduce an averaged auxiliary sequence {ξ(t)∈R|S||A|}\n",
            "given by\n",
            "∀(s,a)∈ S ×A :ξ(0)(s,a):=∝bardblexp(Q⋆\n",
            "τ(s,·)/τ)∝bardbl1·π(0)(a|s), (37a)\n",
            "logξ(t+1)(s,a) =αlogξ(t)(s,a)+(1−α)/hatwideQ(t)\n",
            "τ(s,a)/τ. (37b)\n",
            "We introduces four error metrics deﬁned as\n",
            "Ω(t)\n",
            "1:=/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞, (38a)\n",
            "Ω(t)\n",
            "2:=/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞, (38b)\n",
            "Ω(t)\n",
            "3:=/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞, (38c)\n",
            "Ω(t)\n",
            "4:= max/braceleftbigg\n",
            "0,−min\n",
            "s,a/parenleftig\n",
            "Q(t)\n",
            "τ(s,a)−τlogξ(t)(s,a)/parenrightig/bracerightbigg\n",
            ", (38d)\n",
            "whereu(t),v(t)∈R|S||A|are deﬁned as\n",
            "u(t)(s,a):=/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2, (39)\n",
            "v(t)(s,a):=/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2. (40)\n",
            "We collect the error metrics above in a vector Ω(t)∈R4:\n",
            "Ω(t):=/parenleftig\n",
            "Ω(t)\n",
            "1,Ω(t)\n",
            "2,Ω(t)\n",
            "3,Ω(t)\n",
            "4/parenrightig⊤\n",
            ". (41)\n",
            "With the abovepreparation, wearereadytostate the convergen ceguaranteeofAlgorithm 2in Theorem 5\n",
            "below, which is the formal version of Theorem 3.\n",
            "Theorem 5. For any N∈N+,τ >0,γ∈(0,1), there exists η0>0which depends only on N,γ,τ,σ, |A|,\n",
            "such that if 0< η≤η0and1−σ >0, then the updates of Algorithm 2satisfy\n",
            "/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞≤2γρ(η)t/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble\n",
            "2, (42)\n",
            "/vextenddouble/vextenddoublelogπ⋆\n",
            "τ−logπ(t)/vextenddouble/vextenddouble\n",
            "∞≤2\n",
            "τρ(η)t/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble\n",
            "2, (43)\n",
            "where\n",
            "ρ(η)≤max/braceleftig\n",
            "1−τη\n",
            "2,3+σ\n",
            "4/bracerightig\n",
            "<1.\n",
            "The dependency of η0onN,γ,τ,σ, |A|is made clear in Lemma 2that will be presented momentarily in\n",
            "this section. The rest of this section is dedicated to the proof of Th eorem5. We ﬁrst state a key lemma that\n",
            "tracks the error recursion of Algorithm 2.\n",
            "Lemma 1. The following linear system holds for all t≥0:\n",
            "Ω(t+1)≤\n",
            "σαη\n",
            "1−γ0 0\n",
            "Sσ/parenleftig\n",
            "1+ηM√\n",
            "N\n",
            "1−γ/parenrightig\n",
            "σ(2+γ)ηMN\n",
            "1−γσγηMN\n",
            "1−γσ\n",
            "(1−α)M 0 (1 −α)γ+α(1−α)γ\n",
            "2γ+ητ\n",
            "1−γM 0 0 α\n",
            "\n",
            "/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=:A(η)Ω(t), (44)\n",
            "where we let\n",
            "S:=M√\n",
            "N/parenleftbigg\n",
            "2α+(1−α)·√\n",
            "2N+1−α\n",
            "τ·√\n",
            "NM/parenrightbigg\n",
            ", (45)\n",
            "17and\n",
            "M:=1+γ+2τ(1−γ)log|A|\n",
            "(1−γ)2·γ.\n",
            "In addition, it holds for all t≥0that\n",
            "/vextenddouble/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤γΩ(t)\n",
            "3+γΩ(t)\n",
            "4, (46)\n",
            "/vextenddouble/vextenddoublelogπ(t)−logπ⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞≤2\n",
            "τΩ(t)\n",
            "3. (47)\n",
            "Proof.See Appendix B.1.\n",
            "Letρ(η) denote the spectral norm of A(η). AsΩ(t)≥0, it is immediate from ( 44) that\n",
            "/vextenddouble/vextenddoubleΩ(t)/vextenddouble/vextenddouble\n",
            "2≤ρ(η)t/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble\n",
            "2,\n",
            "and therefore we have /vextenddouble/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤2γ/vextenddouble/vextenddoubleΩ(t)/vextenddouble/vextenddouble\n",
            "∞≤2γρ(η)t/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble\n",
            "2,\n",
            "and/vextenddouble/vextenddoublelogπ(t)−logπ⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞≤2\n",
            "τ/vextenddouble/vextenddoubleΩ(t)/vextenddouble/vextenddouble\n",
            "∞≤2\n",
            "τρ(η)t/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble\n",
            "2.\n",
            "It remains to bound the spectral radius ρ(η), which is achieved by the following lemma.\n",
            "Lemma 2 (Bounding the spectral norm of A(η)).Let\n",
            "ζ:=(1−γ)(1−σ)2τ\n",
            "8(τS0σ+10Mcσ/(1−γ)+(1−σ)2τ2/16), (48)\n",
            "whereS0:=M√\n",
            "N/parenleftig\n",
            "2+√\n",
            "2N+M√\n",
            "N\n",
            "τ/parenrightig\n",
            ". For any N∈N+,τ >0,γ∈(0,1), if\n",
            "0< η≤η0:= min/braceleftig1−γ\n",
            "τ,ζ/bracerightig\n",
            ", (49)\n",
            "then we have\n",
            "ρ(η)≤max/braceleftig3+σ\n",
            "4,1+(1−α)γ+α\n",
            "2/bracerightig\n",
            "<1. (50)\n",
            "Proof.See Appendix B.2.\n",
            "A.2 Analysis of entropy-regularized FedNPG with inexact po licy evaluation\n",
            "We deﬁne the collection of inexactQ-function estimates as\n",
            "q(t)\n",
            "τ:=/parenleftig\n",
            "qπ(t)\n",
            "1\n",
            "τ,1,···,qπ(t)\n",
            "N\n",
            "τ,N/parenrightig⊤\n",
            ",\n",
            "and then the update rule ( 25) should be understood as\n",
            "T(t+1)(s,a) =W/parenleftig\n",
            "T(t)(s,a)+q(t+1)\n",
            "τ(s,a)−q(t)\n",
            "τ(s,a)/parenrightig\n",
            "(51)\n",
            "in the inexact setting. For notational simplicity, we deﬁne en∈Ras\n",
            "en:= max\n",
            "t∈[T]/vextenddouble/vextenddouble/vextenddoubleQπ(t)\n",
            "nτ,n−qπ(t)\n",
            "nτ,n/vextenddouble/vextenddouble/vextenddouble\n",
            "∞, n∈[N], (52)\n",
            "and lete= (e1,···,en)⊤. Deﬁne/hatwideq(t)\n",
            "τ, the approximation of /hatwideQ(t)\n",
            "τas\n",
            "/hatwideq(t)\n",
            "τ:=1\n",
            "NN/summationdisplay\n",
            "n=1qπ(t)\n",
            "nτ,n. (53)\n",
            "18With slight abuse of notation, we adapt the auxiliary sequence {ξ(t)}t=0,···to the inexact updates as\n",
            "ξ(0)(s,a):=∝bardblexp(Q⋆\n",
            "τ(s,·)/τ)∝bardbl1·π(0)(a|s), (54a)\n",
            "ξ(t+1)(s,a):=/bracketleftig\n",
            "ξ(t)(s,a)/bracketrightigα\n",
            "exp/parenleftigg\n",
            "(1−α)/hatwideq(t)\n",
            "τ(s,a)\n",
            "τ/parenrightigg\n",
            ",∀(s,a)∈ S ×A, t≥0. (54b)\n",
            "In addition, we deﬁne\n",
            "Ω(t)\n",
            "1:=/vextenddouble/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞, (55a)\n",
            "Ω(t)\n",
            "2:=/vextenddouble/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞, (55b)\n",
            "Ω(t)\n",
            "3:=/vextenddouble/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞, (55c)\n",
            "Ω(t)\n",
            "4:= max/braceleftbigg\n",
            "0,−min\n",
            "s,a/parenleftig\n",
            "q(t)\n",
            "τ(s,a)−τlogξ(t)(s,a)/parenrightig/bracerightbigg\n",
            ", (55d)\n",
            "where\n",
            "u(t)(s,a):=/vextenddouble/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2, (56)\n",
            "v(t)(s,a):=/vextenddouble/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2. (57)\n",
            "We letΩ(t)be\n",
            "Ω(t):=/parenleftig\n",
            "Ω(t)\n",
            "1,Ω(t)\n",
            "2,Ω(t)\n",
            "3,Ω(t)\n",
            "4/parenrightig⊤\n",
            ". (58)\n",
            "With the above preparation, we are ready to state the inexact con vergence guarantee of Algorithm 2in\n",
            "Theorem 6below, which is the formal version of Theorem 4.\n",
            "Theorem 6. Suppose that qπ(t)\n",
            "nτ,nare used in replace of Qπ(t)\n",
            "nτ,nin Algorithm 2. For any N∈N+,τ >0,γ∈\n",
            "(0,1), there exists η0>0which depends only on N,γ,τ,σ, |A|, such that if 0< η≤η0and1−σ >0, we\n",
            "have\n",
            "/vextenddouble/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤2γ/parenleftbigg\n",
            "ρ(η)t/vextenddouble/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble/vextenddouble\n",
            "2+C2max\n",
            "n∈[N],t∈[T]/vextenddouble/vextenddouble/vextenddoubleQπ(t)\n",
            "nτ,n−qπ(t)\n",
            "nτ,n/vextenddouble/vextenddouble/vextenddouble\n",
            "∞/parenrightbigg\n",
            ", (59)\n",
            "/vextenddouble/vextenddouble/vextenddoublelogπ⋆\n",
            "τ−logπ(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤2\n",
            "τ/parenleftbigg\n",
            "ρ(η)t/vextenddouble/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble/vextenddouble\n",
            "2+C2max\n",
            "n∈[N],t∈[T]/vextenddouble/vextenddouble/vextenddoubleQπ(t)\n",
            "nτ,n−qπ(t)\n",
            "nτ,n/vextenddouble/vextenddouble/vextenddouble\n",
            "∞/parenrightbigg\n",
            ", (60)\n",
            "whereρ(η)≤max{1−τη\n",
            "2,3+σ\n",
            "4}<1is the same as in Theorem 5, andC2:=σ√\n",
            "N(2(1−γ)+M√\n",
            "Nη)+2γ2+ητ\n",
            "(1−γ)(1−ρ(η)).\n",
            "From Theorem 6, we can conclude that if\n",
            "max\n",
            "n∈[N],t∈[T]/vextenddouble/vextenddouble/vextenddoubleQπ(t)\n",
            "nτ,n−qπ(t)\n",
            "nτ,n/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤(1−γ)(1−ρ(η))ε\n",
            "2γ/parenleftig\n",
            "σ√\n",
            "N(2(1−γ)+M√\n",
            "Nη)+2γ2+ητ/parenrightig, (61)\n",
            "then inexact entropy-regularized FedNPG could still achieve 2 ε-accuracy (i.e./vextenddouble/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤2ε) within\n",
            "max/braceleftig\n",
            "2\n",
            "τη,4\n",
            "1−σ/bracerightig\n",
            "log2γ∝bardblΩ(0)∝bardbl2\n",
            "εiterations.\n",
            "Remark 2. Whenη=η0(cf.(49)and(48)) andτ≤1, the RHS of (61)is of the order\n",
            "O/parenleftbigg(1−γ)τη0ε\n",
            "γ(γ2+σ√\n",
            "N(1−γ))/parenrightbigg\n",
            "=O/parenleftbigg(1−γ)8τ2(1−σ)2ε\n",
            "γ(γ2+σ√\n",
            "N(1−γ))(γ2Nσ+(1−σ)2τ2(1−γ)6)/parenrightbigg\n",
            ",\n",
            "which can be translated into a crude sample complexity bound when using fresh samples to estimate the soft\n",
            "Q-functions in each iteration.\n",
            "19The rest of this section outlines the proof of Theorem 6. We ﬁrst state a key lemma that tracks the error\n",
            "recursion of Algorithm 2with inexact policy evaluation, which is a modiﬁed version of Lemma 1.\n",
            "Lemma 3. The following linear system holds for all t≥0:\n",
            "Ω(t+1)≤A(η)Ω(t)+\n",
            "0\n",
            "σ√\n",
            "N/parenleftig\n",
            "2+M√\n",
            "Nη\n",
            "1−γ/parenrightig\n",
            "ητ\n",
            "1−γ\n",
            "2γ2\n",
            "1−γ\n",
            "∝bardble∝bardbl∞\n",
            "/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=:b(η), (62)\n",
            "whereA(η)is provided in Lemma 1. In addition, it holds for all t≥0that\n",
            "/vextenddouble/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤γΩ(t)\n",
            "3+γΩ(t)\n",
            "4, (63)\n",
            "/vextenddouble/vextenddoublelogπ(t)−logπ⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞≤2\n",
            "τΩ(t)\n",
            "3. (64)\n",
            "Proof.See Appendix B.3.\n",
            "By (62), we have\n",
            "∀t∈N+:Ω(t)≤A(η)tΩ(0)+t/summationdisplay\n",
            "s=1A(η)t−sb(η),\n",
            "which gives\n",
            "/vextenddouble/vextenddouble/vextenddoubleΩ(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "2≤ρ(η)t/vextenddouble/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble/vextenddouble\n",
            "2+t/summationdisplay\n",
            "s=1ρ(η)t−s∝bardblb(η)∝bardbl2∝bardble∝bardbl∞\n",
            "≤ρ(η)t/vextenddouble/vextenddouble/vextenddoubleΩ(0)/vextenddouble/vextenddouble/vextenddouble\n",
            "2+σ√\n",
            "N(2(1−γ)+M√\n",
            "Nη)+2γ2+ητ\n",
            "(1−γ)(1−ρ(η))∝bardble∝bardbl∞. (65)\n",
            "Here, (65) follows from ∝bardblb(η)∝bardbl2≤ ∝bardblb(η)∝bardbl1=σ√\n",
            "N(2(1−γ)+M√\n",
            "Nη)+2γ2+ητ\n",
            "1−γ∝bardble∝bardbl∞and/summationtextt\n",
            "s=1ρ(η)t−s≤1/(1−\n",
            "ρ(η)). Recall that the bound on ρ(η) has already been established in Lemma 2. Therefore we complete the\n",
            "proof of Theorem 6by combining the above inequality with ( 63) and (64) in a similar fashion as before. We\n",
            "omit further details for conciseness.\n",
            "A.3 Analysis of FedNPG with exact policy evaluation\n",
            "We state the formal version of Theorem 1below.\n",
            "Theorem 7. Suppose all π(0)\n",
            "nin Algorithm 1are initialized as uniform distribution. When\n",
            "0< η≤η1:=(1−σ)2(1−γ)3\n",
            "8(1+γ)γ√\n",
            "Nσ,\n",
            "we have\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftig\n",
            "V⋆(ρ)−Vπ(t)(ρ)/parenrightig\n",
            "≤V⋆(dπ⋆\n",
            "ρ)\n",
            "(1−γ)T+log|A|\n",
            "ηT+8(1+γ)2γ2Nσ\n",
            "(1−γ)9(1−σ)2η2(66)\n",
            "for any ﬁxed state distribution ρ.\n",
            "The rest of this section is dedicated to prove Theorem 7. Similar to ( 33), we denote the Q-functions of\n",
            "π(t)byQ(t):\n",
            "Q(t):=\n",
            "Qπ(t)\n",
            "1\n",
            "...\n",
            "Qπ(t)\n",
            "N\n",
            ". (67)\n",
            "20In addition, similar to ( 34), we deﬁne/hatwideQ(t),Q(t)∈R|S||A|andV(t)∈R|S|as follows\n",
            "/hatwideQ(t):=1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "nn, (68a)\n",
            "Q(t):=Qπ(t)=1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "n. (68b)\n",
            "V(t):=Vπ(t)=1\n",
            "NN/summationdisplay\n",
            "n=1Vπ(t)\n",
            "n. (68c)\n",
            "Following the same strategy in the analysis of entropy-regularized F edNPG, we introduce the auxiliary\n",
            "sequence {ξ(t)= (ξ(t)\n",
            "1,···,ξ(t)\n",
            "N)⊤∈RN×|S||A|}recursively:\n",
            "ξ(0)(s,a):=1/vextenddouble/vextenddouble/vextenddoubleexp/parenleftig\n",
            "1\n",
            "N/summationtextN\n",
            "n=1logπ(0)\n",
            "n(·|s)/parenrightig/vextenddouble/vextenddouble/vextenddouble\n",
            "1·π(0)(a|s), (69a)\n",
            "logξ(t+1)(s,a) =Wlogξ(t)(s,a)+η\n",
            "1−γT(t)(s,a), (69b)\n",
            "as well as the averaged auxiliary sequence {ξ(t)∈R|S||A|}:\n",
            "ξ(0)(s,a):=π(0)(a|s), (70a)\n",
            "logξ(t+1)(s,a):= logξ(t)(s,a)+η\n",
            "1−γ/hatwideQ(t)(s,a),∀(s,a)∈ S ×A, t≥0. (70b)\n",
            "As usual, we collect the consensus errors in a vector Ω(t)= (/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞,/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞)⊤, whereu(t),v(t)∈R|S||A|\n",
            "are deﬁned as:\n",
            "u(t)(s,a):=/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2, (71)\n",
            "v(t)(s,a):=/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2. (72)\n",
            "Step 1: establishing the error recursion. The next key lemma establishes the error recursion of\n",
            "Algorithm 1.\n",
            "Lemma 4. The updates of FedNPG satisfy\n",
            "Ω(t+1)≤/parenleftiggση\n",
            "1−γ\n",
            "Jσ σ/parenleftig\n",
            "1+(1+γ)γ√\n",
            "Nη\n",
            "(1−γ)3/parenrightig/parenrightigg\n",
            "/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=:B(η)Ω(t)+/parenleftigg\n",
            "0\n",
            "(1+γ)γNσ\n",
            "(1−γ)4η/parenrightigg\n",
            "/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\n",
            "=:d(η)(73)\n",
            "for allt≥0, where\n",
            "J:=2(1+γ)γ\n",
            "(1−γ)2√\n",
            "N . (74)\n",
            "In addition, we have\n",
            "φ(t+1)(η)≤φ(t)(η)+2(1+γ)γ\n",
            "(1−γ)4η/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            ", (75)\n",
            "where\n",
            "φ(t)(η):=Es∼dπ⋆\n",
            "ρ/bracketleftig\n",
            "KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(t)(·|s)/parenrightbig/bracketrightig\n",
            "−η\n",
            "1−γV(t)(dπ⋆\n",
            "ρ),∀t≥0. (76)\n",
            "Proof.See Appendix B.4.\n",
            "21Step 2: bounding the value functions. Letp∈R2be deﬁned as:\n",
            "p(η) =/parenleftbigg\n",
            "p1(η)\n",
            "p2(η)/parenrightbigg\n",
            ":=2(1+γ)γ\n",
            "(1−γ)4\n",
            "(1−γ)(1−σ−(1+γ)γ√\n",
            "Nση/(1−γ)3)η\n",
            "(1−γ)(1−σ−(1+γ)γ√\n",
            "Nση/(1−γ)3)(1−σ)−Jση\n",
            "η2\n",
            "(1−γ)(1−σ−(1+γ)γ√\n",
            "Nση/(1−γ)3)(1−σ)−Jση\n",
            "; (77)\n",
            "the rationale for this choice will be made clear momentarily. We deﬁne t he following Lyapunov function\n",
            "Φ(t)(η) =φ(t)(η)+p(η)⊤Ω(t),∀t≥0, (78)\n",
            "which satisﬁes\n",
            "Φ(t+1)(η) =φ(t+1)(η)+p(η)⊤Ω(t+1)\n",
            "≤φ(t)(η)+2(1+γ)γ\n",
            "(1−γ)4η/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            "+p(η)⊤/parenleftig\n",
            "B(η)Ω(t)+d(η)/parenrightig\n",
            "= Φ(t)(η)+/bracketleftbigg\n",
            "p(η)⊤(B(η)−I)+/parenleftbigg2(1+γ)γ\n",
            "(1−γ)4η,0/parenrightbigg/bracketrightbigg\n",
            "Ω(t)−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            "+p2(η)(1+γ)γNσ\n",
            "(1−γ)4η. (79)\n",
            "Here, the second inequality follows from ( 75). One can verify that the second term vanishes due to the choice\n",
            "ofp(η):\n",
            "p(η)⊤(B(η)−I)+/parenleftbigg2(1+γ)γ\n",
            "(1−γ)4η,0/parenrightbigg\n",
            "= (0,0). (80)\n",
            "Therefore, we conclude that\n",
            "V⋆(ρ)−V(t)(ρ)≤Φ(t)(η)−Φ(t+1)(η)\n",
            "η+p2(η)(1+γ)γNσ\n",
            "(1−γ)4.\n",
            "Averaging over t= 0,···,T−1,\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            "≤Φ(0)(η)−Φ(T)(η)\n",
            "ηT+2(1+γ)2γ2\n",
            "(1−γ)8·Nση2\n",
            "(1−γ)(1−σ−(1+γ)γ√\n",
            "Nση/(1−γ)3)(1−σ)−σJη.(81)\n",
            "Step 3: simplifying the expression. We ﬁrst upper bound the ﬁrst term in the RHS of ( 81). Assuming\n",
            "uniform initialization for all π(0)\n",
            "nin Algorithm 1, we have/vextenddouble/vextenddoubleu(0)/vextenddouble/vextenddouble\n",
            "∞=/vextenddouble/vextenddoublev(0)/vextenddouble/vextenddouble\n",
            "∞= 0, and\n",
            "Es∼dπ⋆\n",
            "ρ/bracketleftig\n",
            "KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(0)(·|s)/parenrightbig/bracketrightig\n",
            "≤log|A|.\n",
            "Therefore, putting together relations ( 78) and (158) we have\n",
            "Φ(0)(η)−Φ(T)(η)\n",
            "ηT≤log|A|\n",
            "Tη+1\n",
            "T/parenleftigg\n",
            "p(η)⊤Ω(0)/η+V⋆(dπ⋆\n",
            "ρ)\n",
            "1−γ/parenrightigg\n",
            "=log|A|\n",
            "Tη+V⋆(dπ⋆\n",
            "ρ)\n",
            "T(1−γ),(82)\n",
            "To continue, we upper bound the second term in the RHS of ( 81). Note that\n",
            "η≤η1≤(1−σ)(1−γ)3\n",
            "2(1+γ)γ√\n",
            "Nσ,\n",
            "which gives\n",
            "(1+γ)γ√\n",
            "Nσ\n",
            "(1−γ)3η≤1−σ\n",
            "2. (83)\n",
            "22Thus we have\n",
            "(1−γ)(1−σ−(1+γ)γ√\n",
            "Nση/(1−γ)3)(1−σ)−Jση\n",
            "≥(1−γ)(1−σ)2/2−Jση1\n",
            "≥(1−γ)(1−σ)2/4, (84)\n",
            "where the ﬁrst inequality follows from ( 83) and the second inequality follows from the deﬁnition of η1and\n",
            "J. By (84), we deduce\n",
            "2(1+γ)2γ2\n",
            "(1−γ)8·Nση2\n",
            "(1−γ)(1−σ−(1+γ)γ√\n",
            "Nση/(1−γ)3)(1−σ)−Jση≤8(1+γ)2γ2Nσ\n",
            "(1−γ)9(1−σ)2η2,(85)\n",
            "and our advertised bound ( 66) thus follows from plugging ( 82) and (85) into (81).\n",
            "A.4 Analysis of FedNPG with inexact policy evaluation\n",
            "We state the formal version of Theorem 2below.\n",
            "Theorem 8. Suppose that qπ(t)\n",
            "nnare used in replace of Qπ(t)\n",
            "nnin Algorithm 1. Suppose all π(0)\n",
            "nin Algorithm 1\n",
            "set to uniform distribution. Let\n",
            "0< η≤η1:=(1−σ)2(1−γ)3\n",
            "8(1+γ)γ√\n",
            "Nσ,\n",
            "we have\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftig\n",
            "V⋆(ρ)−Vπ(t)(ρ)/parenrightig\n",
            "≤V⋆(dπ⋆\n",
            "ρ)\n",
            "(1−γ)T+log|A|\n",
            "ηT+8(1+γ)2γ2Nσ\n",
            "(1−γ)9(1−σ)2η2\n",
            "+/bracketleftigg\n",
            "8(1+γ)γ\n",
            "(1−γ)5(1−σ)2√\n",
            "Nση/parenleftigg\n",
            "(1+γ)γη√\n",
            "N\n",
            "(1−γ)3+2/parenrightigg\n",
            "+2\n",
            "(1−γ)2/bracketrightigg\n",
            "max\n",
            "n∈[N],t∈[T]/vextenddouble/vextenddouble/vextenddoubleQπ(t)\n",
            "nn−qπ(t)\n",
            "nn/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "for any ﬁxed state distribution ρ.\n",
            "We next outline the proof of Theorem 8. With slight abuse of notation, we again deﬁne en∈Ras\n",
            "en:= max\n",
            "t∈[T]/vextenddouble/vextenddouble/vextenddoubleQπ(t)\n",
            "nn−qπ(t)\n",
            "nn/vextenddouble/vextenddouble/vextenddouble\n",
            "∞, n∈[N], (86)\n",
            "and lete= (e1,···,en)⊤. We deﬁne the collection of inexactQ-function estimates as\n",
            "q(t):=/parenleftig\n",
            "qπ(t)\n",
            "1\n",
            "1,···,qπ(t)\n",
            "N\n",
            "N/parenrightig⊤\n",
            ",\n",
            "and then the update rule ( 23) should be understood as\n",
            "T(t+1)(s,a) =W/parenleftig\n",
            "T(t)(s,a)+q(t+1)(s,a)−q(t)(s,a)/parenrightig\n",
            "(87)\n",
            "in the inexact setting. Deﬁne /hatwideq(t), the approximation of /hatwideQ(t)as\n",
            "/hatwideq(t):=1\n",
            "NN/summationdisplay\n",
            "n=1qπ(t)\n",
            "nn, (88)\n",
            "we adapt the averaged auxiliary sequence {ξ(t)∈R|S||A|}to the inexact updates as follows:\n",
            "ξ(0)(s,a):=π(0)(a|s), (89a)\n",
            "23ξ(t+1)(s,a):=ξ(t)(s,a)exp/parenleftbiggη\n",
            "1−γ/hatwideq(t)(s,a)/parenrightbigg\n",
            ",∀(s,a)∈ S ×A, t≥0. (89b)\n",
            "As usual, we deﬁne the consensus error vector as Ω(t)= (/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞,/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞)⊤, whereu(t),v(t)∈R|S||A|\n",
            "are given by\n",
            "u(t)(s,a):=/vextenddouble/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2, (90)\n",
            "v(t)(s,a):=/vextenddouble/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2. (91)\n",
            "The following lemma characterizes the dynamics of the error vector Ω(t), perturbed by additional approxi-\n",
            "mation error.\n",
            "Lemma 5. The updates of inexact FedNPG satisfy\n",
            "Ω(t+1)≤B(η)Ω(t)+d(η)+/parenleftigg\n",
            "0√\n",
            "Nσ/parenleftig\n",
            "(1+γ)γη√\n",
            "N\n",
            "(1−γ)3+2/parenrightig/parenrightigg\n",
            "/vextenddouble/vextenddoublee/vextenddouble/vextenddouble\n",
            "∞\n",
            "/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=:c(η). (92)\n",
            "In addition, we have\n",
            "φ(t+1)(η)≤φ(t)(η)+2(1+γ)γ\n",
            "(1−γ)4η/vextenddouble/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+2η\n",
            "(1−γ)2∝bardble∝bardbl∞−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            ", (93)\n",
            "whereφ(t)(η)is deﬁned in (76).\n",
            "Proof.See Appendix B.5.\n",
            "Similar to ( 79), we can recursively bound Φ(t)(η) (deﬁned in ( 78)) as\n",
            "Φ(t+1)(η) =φ(t+1)(η)+p(η)⊤Ω(t+1)\n",
            "(93)\n",
            "≤φ(t)(η)+2(1+γ)γ\n",
            "(1−γ)4η/vextenddouble/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+2η\n",
            "(1−γ)2∝bardble∝bardbl∞−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            "+p(η)⊤/parenleftig\n",
            "B(η)Ω(t)+d(η)+c(η)/parenrightig\n",
            "= Φ(t)(η)+/bracketleftbigg\n",
            "p(η)⊤(B(η)−I)+/parenleftbigg2(1+γ)γ\n",
            "(1−γ)4η,0/parenrightbigg/bracketrightbigg\n",
            "/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=(0,0)via(80)Ω(t)−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            "+p2(η)(1+γ)γNσ\n",
            "(1−γ)4η+/bracketleftigg\n",
            "p2(η)√\n",
            "Nσ/parenleftigg\n",
            "(1+γ)γη√\n",
            "N\n",
            "(1−γ)3+2/parenrightigg\n",
            "+2η\n",
            "(1−γ)2/bracketrightigg\n",
            "∝bardble∝bardbl∞.(94)\n",
            "From the above expression we know that\n",
            "V⋆(ρ)−V(t)(ρ)≤Φ(t)(η)−Φ(t+1)(η)\n",
            "η+p2(η)(1+γ)γNσ\n",
            "(1−γ)4+/bracketleftigg\n",
            "p2(η)√\n",
            "Nσ/parenleftigg\n",
            "(1+γ)γ√\n",
            "N\n",
            "(1−γ)3+2\n",
            "η/parenrightigg\n",
            "+2\n",
            "(1−γ)2/bracketrightigg\n",
            "∝bardble∝bardbl∞,\n",
            "which gives\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            "≤Φ(0)(η)−Φ(T)(η)\n",
            "ηT+p2(η)(1+γ)γNσ\n",
            "(1−γ)4\n",
            "+/bracketleftigg\n",
            "p2(η)√\n",
            "Nσ/parenleftigg\n",
            "(1+γ)γ√\n",
            "N\n",
            "(1−γ)3+2\n",
            "η/parenrightigg\n",
            "+2\n",
            "(1−γ)2/bracketrightigg\n",
            "∝bardble∝bardbl∞ (95)\n",
            "24via telescoping. Combining the above expression with ( 82), (84) and (85), we have\n",
            "1\n",
            "TT−1/summationdisplay\n",
            "t=0/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            "≤log|A|\n",
            "Tη+V⋆(dπ⋆\n",
            "ρ)\n",
            "T(1−γ)+8(1+γ)2γ2Nσ\n",
            "(1−γ)9(1−σ)2η2\n",
            "+/bracketleftigg\n",
            "8(1+γ)γ\n",
            "(1−γ)5(1−σ)2√\n",
            "Nση/parenleftigg\n",
            "(1+γ)γη√\n",
            "N\n",
            "(1−γ)3+2/parenrightigg\n",
            "+2\n",
            "(1−γ)2/bracketrightigg\n",
            "∝bardble∝bardbl∞,(96)\n",
            "which establishes ( 86).\n",
            "B Proof of key lemmas\n",
            "B.1 Proof of Lemma 1\n",
            "Before proceeding, we summarize several useful properties of t he auxiliary sequences (cf. ( 36) and (37)),\n",
            "whose proof is postponed to Appendix C.1.\n",
            "Lemma 6 (Properties of auxiliary sequences {ξ(t)}and{ξ(t)}).{ξ(t)}and{ξ(t)}have the following prop-\n",
            "erties:\n",
            "1.ξ(t)can be viewed as an unnormalized version of π(t), i.e.,\n",
            "π(t)\n",
            "n(·|s) =ξ(t)\n",
            "n(s,·)/vextenddouble/vextenddoubleξ(t)\n",
            "n(s,·)/vextenddouble/vextenddouble\n",
            "1,∀n∈[N], s∈ S. (97)\n",
            "2. For any t≥0,logξ(t)keeps track of the average of logξ(t), i.e.,\n",
            "1\n",
            "N1⊤\n",
            "Nlogξ(t)= logξ(t). (98)\n",
            "It follows that\n",
            "∀s∈ S, t≥0 :π(t)(·|s) =ξ(t)(s,·)\n",
            "/vextenddouble/vextenddoubleξ(t)(s,·)/vextenddouble/vextenddouble\n",
            "1. (99)\n",
            "Lemma 7 ((Cen et al. ,2022a, Appendix. A.2)) .For any vector θ= [θa]a∈A∈R|A|, we denote by πθ∈R|A|\n",
            "the softmax transform of θsuch that\n",
            "πθ(a) =exp(θa)/summationtext\n",
            "a′∈Aexp(θa′), a∈ A. (100)\n",
            "For any θ1,θ2∈R|A|, we have\n",
            "/vextendsingle/vextendsinglelog(∝bardblexp(θ1)∝bardbl1)−log(∝bardblexp(θ2)∝bardbl1)/vextendsingle/vextendsingle≤ ∝bardblθ1−θ2∝bardbl∞, (101)\n",
            "∝bardbllogπθ1−logπθ2∝bardbl∞≤2∝bardblθ1−θ2∝bardbl∞. (102)\n",
            "Step 1: bound u(t+1)(s,a) =/vextenddouble/vextenddoublelogξ(t+1)(s,a)−logξ(t+1)(s,a)1N/vextenddouble/vextenddouble\n",
            "2.By (36b) and (37b) we have\n",
            "u(t+1)(s,a) =/vextenddouble/vextenddoublelogξ(t+1)(s,a)−logξ(t+1)(s,a)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddoubleα/parenleftig\n",
            "Wlogξ(t)(s,a)−logξ(t)(s,a)1N/parenrightig\n",
            "+(1−α)/parenleftig\n",
            "T(t)(s,a)−/hatwideQ(t)\n",
            "τ(s,a)1N/parenrightig\n",
            "/τ/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σα/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+1−α\n",
            "τ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σα/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+1−α\n",
            "τ/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞, (103)\n",
            "where the penultimate step results from the averaging property o fW(property ( 18)). Taking maximum\n",
            "over (s,a)∈ S ×A establishes the bound on Ω(t+1)\n",
            "1in (44).\n",
            "25Step 2: bound v(t+1)(s,a) =/vextenddouble/vextenddoubleT(t+1)(s,a)−/hatwideQ(t+1)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2.By (25) we have\n",
            "/vextenddouble/vextenddoubleT(t+1)(s,a)−/hatwideQ(t+1)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddoubleW/parenleftig\n",
            "T(t)(s,a)+Q(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/parenrightig\n",
            "−/hatwideQ(t+1)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddouble/parenleftig\n",
            "WT(t)(s,a)−/hatwideQ(t)\n",
            "τ(s,a)1N/parenrightig\n",
            "+W/parenleftig\n",
            "Q(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/parenrightig\n",
            "+/parenleftig\n",
            "/hatwideQ(t)\n",
            "τ(s,a)−/hatwideQ(t+1)\n",
            "τ(s,a)/parenrightig\n",
            "1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+σ/vextenddouble/vextenddouble/vextenddouble/parenleftig\n",
            "Q(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/parenrightig\n",
            "+/parenleftig\n",
            "/hatwideQ(t)\n",
            "τ(s,a)−/hatwideQ(t+1)\n",
            "τ(s,a)/parenrightig\n",
            "1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+σ/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble\n",
            "2, (104)\n",
            "where the penultimate step uses property ( 18), and the last step is due to\n",
            "/vextenddouble/vextenddouble/vextenddouble/parenleftig\n",
            "Q(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/parenrightig\n",
            "+/parenleftig\n",
            "/hatwideQ(t)\n",
            "τ(s,a)−/hatwideQ(t+1)\n",
            "τ(s,a)/parenrightig\n",
            "1N/vextenddouble/vextenddouble/vextenddouble2\n",
            "2\n",
            "=/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble2\n",
            "2+N/parenleftbig/hatwideQ(t)\n",
            "τ(s,a)−/hatwideQ(t+1)\n",
            "τ(s,a)/parenrightbig2\n",
            "−2N/summationdisplay\n",
            "n=1/parenleftig\n",
            "Qπ(t+1)\n",
            "nτ,n(s,a)−Qπ(t)\n",
            "nτ,n(s,a)/parenrightig/parenleftig\n",
            "/hatwideQ(t+1)\n",
            "τ(s,a)−/hatwideQ(t)\n",
            "τ(s,a)/parenrightig\n",
            "=/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble2\n",
            "2−N/parenleftbig/hatwideQ(t)\n",
            "τ(s,a)−/hatwideQ(t+1)\n",
            "τ(s,a)/parenrightbig2\n",
            "≤/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble2\n",
            "2.\n",
            "Step 3: bound/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞.We decompose the term of interest as\n",
            "Q⋆\n",
            "τ−τlogξ(t+1)=Q⋆\n",
            "τ−ταlogξ(t)−(1−α)/hatwideQ(t)\n",
            "τ\n",
            "=α(Q⋆\n",
            "τ−τlogξ(t))+(1−α)(Q⋆\n",
            "τ−Q(t)\n",
            "τ)+(1−α)(Q(t)\n",
            "τ−/hatwideQ(t)\n",
            "τ),\n",
            "which gives\n",
            "/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞≤α/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞+(1−α)/vextenddouble/vextenddoubleQ⋆\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+(1−α)/vextenddouble/vextenddoubleQ(t)\n",
            "τ−/hatwideQ(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞.(105)\n",
            "Note that we can upper bound/vextenddouble/vextenddoubleQ(t)\n",
            "τ−/hatwideQ(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞by\n",
            "/vextenddouble/vextenddoubleQ(t)\n",
            "τ−/hatwideQ(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "nτ,n−1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "τ,n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "≤1\n",
            "NN/summationdisplay\n",
            "n=1/vextenddouble/vextenddoubleQπ(t)\n",
            "nτ,n−Qπ(t)\n",
            "τ,n/vextenddouble/vextenddouble\n",
            "∞\n",
            "≤M\n",
            "NN/summationdisplay\n",
            "n=1/vextenddouble/vextenddoublelogξ(t)\n",
            "n−logξ(t)/vextenddouble/vextenddouble\n",
            "∞/lessorequalslantM/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞. (106)\n",
            "The last step is due to/vextendsingle/vextendsinglelogξ(t)\n",
            "n(s,a)−logξ(t)(s,a)/vextendsingle/vextendsingle≤u(t)(s,a), while the penultimate step results from\n",
            "writing\n",
            "π(t)(·|s) = softmax/parenleftig\n",
            "logξ(t)(s,·)/parenrightig\n",
            ",\n",
            "π(t)\n",
            "n(·|s) = softmax/parenleftig\n",
            "logξ(t)\n",
            "n(s,·)/parenrightig\n",
            ",\n",
            "and applying the following lemma.\n",
            "26Lemma 8 (Lipschitz constant of soft Q-function) .Assume that r(s,a)∈[0,1],∀(s,a)∈ S ×A andτ≥0.\n",
            "For any θ,θ′∈R|S||A|, we have\n",
            "∝bardblQπθ′\n",
            "τ−Qπθτ∝bardbl∞≤1+γ+2τ(1−γ)log|A|\n",
            "(1−γ)2·γ\n",
            "/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=:M∝bardblθ′−θ∝bardbl∞. (107)\n",
            "Plugging ( 106) into (105) gives\n",
            "/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞≤α/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞+(1−α)/vextenddouble/vextenddoubleQ⋆\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+(1−α)M/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞.(108)\n",
            "Step 4: bound/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble\n",
            "2.Letw(t):S ×A → Rbe deﬁned as\n",
            "∀(s,a)∈ S ×A :w(t)(s,a):=/vextenddouble/vextenddoublelogξ(t+1)(s,a)−logξ(t)(s,a)−(1−α)V⋆\n",
            "τ(s)1N/τ/vextenddouble/vextenddouble\n",
            "2.(109)\n",
            "Again, we treat w(t)as vectors in R|S||A|whenever it is clear from context. For any ( s,a)∈ S × A and\n",
            "n∈[N], by Lemma 8it follows that\n",
            "/vextendsingle/vextendsingle/vextendsingleQπ(t+1)\n",
            "nτ,n(s,a)−Qπ(t)\n",
            "nτ,n(s,a)/vextendsingle/vextendsingle/vextendsingle≤Mmax\n",
            "s∈S/vextenddouble/vextenddoublelogξ(t+1)\n",
            "n(s,·)−logξ(t)\n",
            "n(s,·)−(1−α)V⋆\n",
            "τ(s)1|A|/τ/vextenddouble/vextenddouble\n",
            "∞\n",
            "≤Mmax\n",
            "s∈Smax\n",
            "a∈Aw(t)(s,a)≤M/vextenddouble/vextenddoublew(t)/vextenddouble/vextenddouble\n",
            "∞, (110)\n",
            "and consequently/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble\n",
            "2≤M√\n",
            "N/vextenddouble/vextenddoublew(t)/vextenddouble/vextenddouble\n",
            "∞. (111)\n",
            "It boils down to control/vextenddouble/vextenddoublew(t)/vextenddouble/vextenddouble\n",
            "∞. To do so, we ﬁrst note that for each ( s,a)∈ S ×A, we have\n",
            "w(t)(s,a)\n",
            "=/vextenddouble/vextenddoubleαWlogξ(t)(s,a)+(1−α)T(t)(s,a)/τ−logξ(t)(s,a)−(1−α)V⋆\n",
            "τ(s)1N/τ/vextenddouble/vextenddouble\n",
            "2\n",
            "(a)=/vextenddouble/vextenddouble/vextenddoubleα(W−IN)/parenleftig\n",
            "logξ(t)(s,a)−logξ(t)(s,a)1N/parenrightig\n",
            "+(1−α)/parenleftig\n",
            "T(t)(s,a)/τ−logξ(t)(s,a)−V⋆\n",
            "τ(s)1N/τ/parenrightig/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "(b)\n",
            "≤2α/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+1−α\n",
            "τ/vextenddouble/vextenddoubleT(t)(s,a)−τlogξ(t)(s,a)−V⋆\n",
            "τ(s)1N/vextenddouble/vextenddouble\n",
            "2(112)\n",
            "where (a) is due to the doubly stochasticity property of Wand (b) is from the fact ∝bardblW−IN∝bardbl2≤2. We\n",
            "further bound the second term as follows:\n",
            "/vextenddouble/vextenddouble/vextenddoubleT(t)(s,a)−τlogξ(t)(s,a)−V⋆\n",
            "τ(s)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddoubleT(t)(s,a)−τlogξ(t)(s,a)−/parenleftbig\n",
            "Q⋆\n",
            "τ(s,a)−τlogπ⋆\n",
            "τ(a|s)/parenrightbig\n",
            "1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤/vextenddouble/vextenddoubleT(t)(s,a)−Q⋆\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+τ/vextenddouble/vextenddoublelogξ(t)(s,a)−logπ⋆\n",
            "τ(a|s)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "≤/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQτ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+/vextenddouble/vextenddouble/hatwideQτ(s,a)1N−Q⋆\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "+τ/vextenddouble/vextenddoublelogξ(t)(s,a)−logπ(t)(a|s)1N/vextenddouble/vextenddouble\n",
            "2+τ/vextenddouble/vextenddoublelogπ(t)(a|s)1N−logπ⋆\n",
            "τ(a|s)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+√\n",
            "N/vextendsingle/vextendsingle/hatwideQ(t)\n",
            "τ(s,a)−Q⋆\n",
            "τ(s,a)/vextendsingle/vextendsingle\n",
            "+τ/vextenddouble/vextenddoublelogξ(t)(s,a)−logπ(t)(a|s)1N/vextenddouble/vextenddouble\n",
            "2+τ√\n",
            "N/vextendsingle/vextendsinglelogπ(t)(a|s)−logπ⋆\n",
            "τ(a|s)/vextendsingle/vextendsingle. (113)\n",
            "Here, the ﬁrst step results from the following relation established in Nachum et al. (2017):\n",
            "∀(s,a)∈ S ×A :V⋆\n",
            "τ(s) =−τlogπ⋆\n",
            "τ(a|s)+Q⋆\n",
            "τ(s,a), (114)\n",
            "which also leads to\n",
            "/vextenddouble/vextenddoublelogπ(t)−logπ⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞≤2\n",
            "τ/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞(115)\n",
            "27by Lemma 7. For the remaining terms in ( 113), we have\n",
            "/vextendsingle/vextendsingle/hatwideQ(t)\n",
            "τ(s,a)−Q⋆\n",
            "τ(s,a)/vextendsingle/vextendsingle≤/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞, (116)\n",
            "and\n",
            "/vextenddouble/vextenddoublelogξ(t)(s,a)−logπ(t)(a|s)1N/vextenddouble/vextenddouble\n",
            "2=/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay\n",
            "n=1/parenleftig\n",
            "logξ(t)\n",
            "n(s,a)−logπ(t)(a|s)/parenrightig2\n",
            "≤/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay\n",
            "n=12/vextenddouble/vextenddoublelogξ(t)\n",
            "n−logξ(t)/vextenddouble/vextenddouble2\n",
            "∞\n",
            "≤/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay\n",
            "n=12/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble2\n",
            "∞=√\n",
            "2N/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞, (117)\n",
            "where the ﬁrst inequality again results from Lemma 7. Plugging ( 115), (116), (117) into (113) and using the\n",
            "deﬁnition of u(t),v(t), we arrive at\n",
            "w(t)(s,a)≤/parenleftig\n",
            "2α+(1−α)·√\n",
            "2N/parenrightig/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+1−α\n",
            "τ/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞+1−α\n",
            "τ·√\n",
            "N/parenleftig/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞/parenrightig\n",
            "+1−α\n",
            "τ·2√\n",
            "N/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞.\n",
            "Using previous display, we can write ( 111) as\n",
            "/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble\n",
            "2\n",
            "≤M√\n",
            "N/braceleftbigg/parenleftig\n",
            "2α+(1−α)·√\n",
            "2N/parenrightig/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+1−α\n",
            "τ/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞\n",
            "+1−α\n",
            "τ·√\n",
            "N/parenleftig\n",
            "M/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞/parenrightig\n",
            "+1−α\n",
            "τ·2√\n",
            "N/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞/bracerightbigg\n",
            ".(118)\n",
            "Combining ( 104) with the above expression ( 118), we get\n",
            "/vextenddouble/vextenddoublev(t+1)/vextenddouble/vextenddouble\n",
            "∞≤σ/parenleftigg\n",
            "1+ηM√\n",
            "N\n",
            "1−γ/parenrightigg\n",
            "/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞+σM√\n",
            "N/braceleftigg/parenleftbigg\n",
            "2α+(1−α)·√\n",
            "2N+1−α\n",
            "τ·√\n",
            "NM/parenrightbigg/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞\n",
            "+1−α\n",
            "τ·√\n",
            "N/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+1−α\n",
            "τ·2√\n",
            "N/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞/bracerightigg\n",
            ". (119)\n",
            "Step 5: bound/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞.For any state-action pair ( s,a)∈ S ×A, we observe that\n",
            "Q⋆\n",
            "τ(s,a)−Q(t+1)\n",
            "τ(s,a)\n",
            "=r(s,a)+γE\n",
            "s′∼P(·|s,a)[V⋆\n",
            "τ(s′)]−/parenleftbigg\n",
            "r(s,a)+γE\n",
            "s′∼P(·|s,a)/bracketleftig\n",
            "Vπ(t+1)\n",
            "τ(s′)/bracketrightig/parenrightbigg\n",
            "=γE\n",
            "s′∼P(·|s,a)/bracketleftbigg\n",
            "τlog/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleexp/parenleftbiggQ⋆\n",
            "τ(s′,·)\n",
            "τ/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "1/parenrightbigg/bracketrightbigg\n",
            "−γE\n",
            "s′∼P(·|s,a),\n",
            "a′∼π(t+1)(·|s′)/bracketleftig\n",
            "Q(t+1)\n",
            "τ(s′,a′)−τlogπ(t+1)(a′|s′)/bracketrightig\n",
            ",(120)\n",
            "where the ﬁrst step invokes the deﬁnition of Qτ(cf. (7a)), and the second step is due to the following\n",
            "expression of V⋆\n",
            "τestablished in Nachum et al. (2017):\n",
            "V⋆\n",
            "τ(s) =τlog/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleexp/parenleftbiggQ⋆\n",
            "τ(s,·)\n",
            "τ/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "1/parenrightbigg\n",
            ". (121)\n",
            "28To continue, note that by ( 99) and (37b) we have\n",
            "logπ(t+1)(a|s) = logξ(t+1)(s,a)−log/parenleftig/vextenddouble/vextenddoubleξ(t+1)(s,·)/vextenddouble/vextenddouble\n",
            "1/parenrightig\n",
            "=αlogξ(t)(s,a)+(1−α)/hatwideQ(t)\n",
            "τ(s,a)\n",
            "τ−log/parenleftig/vextenddouble/vextenddoubleξ(t+1)(s,·)/vextenddouble/vextenddouble\n",
            "1/parenrightig\n",
            ". (122)\n",
            "Plugging ( 122) into (120) and (118) establishes the bounds on\n",
            "Q⋆\n",
            "τ(s,a)−Q(t+1)\n",
            "τ(s,a) =γE\n",
            "s′∼P(·|s,a)/bracketleftbigg\n",
            "τlog/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleexp/parenleftbiggQ⋆\n",
            "τ(s′,·)\n",
            "τ/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "1/parenrightbigg\n",
            "−τlog/parenleftig/vextenddouble/vextenddouble/vextenddoubleξ(t+1)(s′,·)/vextenddouble/vextenddouble/vextenddouble\n",
            "1/parenrightig/bracketrightbigg\n",
            "−γE\n",
            "s′∼P(·|s,a),\n",
            "a′∼π(t+1)(·|s′)/bracketleftigg\n",
            "Q(t+1)\n",
            "τ(s′,a′)−τ/parenleftigg\n",
            "αlogξ(t)(s′,a′)+(1−α)/hatwideQ(t)\n",
            "τ(s′,a′)\n",
            "τ/parenrightigg\n",
            "/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=logξ(t+1)(s′,a′)/bracketrightigg\n",
            "(123)\n",
            "for any ( s,a)∈ S × A . In view of property ( 101), the ﬁrst term on the right-hand side of ( 123) can be\n",
            "bounded by\n",
            "τlog/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleexp/parenleftbiggQ⋆\n",
            "τ(s′,·)\n",
            "τ/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "1/parenrightbigg\n",
            "−τlog/parenleftig/vextenddouble/vextenddoubleξ(t+1)(s′,·)/vextenddouble/vextenddouble\n",
            "1/parenrightig\n",
            "≤/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞.\n",
            "Plugging the above expression into ( 123), we have\n",
            "0≤Q⋆\n",
            "τ(s,a)−Q(t+1)\n",
            "τ(s,a)≤γ/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞−γmin\n",
            "s,a/parenleftig\n",
            "Q(t+1)\n",
            "τ(s,a)−τlogξ(t+1)(s,a)/parenrightig\n",
            ",\n",
            "which gives\n",
            "/vextenddouble/vextenddoubleQ⋆\n",
            "τ−Q(t+1)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞≤γ/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞+γmax/braceleftig\n",
            "0,−min\n",
            "s,a/parenleftig\n",
            "Q(t+1)\n",
            "τ(s,a)−τlogξ(t+1)(s,a)/parenrightig/bracerightig\n",
            ".(124)\n",
            "Plugging the above inequality into ( 108) and (119) establishes the bounds on Ω(t+1)\n",
            "3and Ω(t+1)\n",
            "2in (44),\n",
            "respectively.\n",
            "Step 6: bound −mins,a/parenleftbig\n",
            "Q(t+1)\n",
            "τ(s,a)−τlogξ(t+1)(s,a)/parenrightbig\n",
            ".We need the following lemma which is adapted\n",
            "from Lemma 1 in Cen et al. (2022a):\n",
            "Lemma 9 (Performance improvement of FedNPG with entropy regularization ).Suppose 0< η≤(1−γ)/τ.\n",
            "For any state-action pair (s0,a0)∈ S ×A, one has\n",
            "V(t+1)\n",
            "τ(s0)−V(t)\n",
            "τ(s0)≥1\n",
            "ηE\n",
            "s∼dπ(t+1)\n",
            "s0/bracketleftig\n",
            "αKL/parenleftbig\n",
            "π(t+1)(·|s0)∝bardblπ(t)(·|s0)/parenrightbig\n",
            "+KL/parenleftbig\n",
            "π(t)(·|s0)∝bardblπ(t+1)(·|s0)/parenrightbig/bracketrightig\n",
            "−2\n",
            "1−γ/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞, (125)\n",
            "Q(t+1)\n",
            "τ(s0,a0)−Q(t)\n",
            "τ(s0,a0)≥ −2γ\n",
            "1−γ/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞. (126)\n",
            "Proof.See Appendix C.3.\n",
            "Using (126), we have\n",
            "Q(t+1)\n",
            "τ(s,a)−τ/parenleftigg\n",
            "αlogξ(t)(s,a)+(1−α)/hatwideQ(t)\n",
            "τ(s,a)\n",
            "τ/parenrightigg\n",
            "29≥Q(t)\n",
            "τ(s,a)−τ/parenleftigg\n",
            "αlogξ(t)(s,a)+(1−α)/hatwideQ(t)\n",
            "τ(s,a)\n",
            "τ/parenrightigg\n",
            "−2γ\n",
            "1−γ/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞\n",
            "≥α/parenleftig\n",
            "Q(t)\n",
            "τ(s,a)−τlogξ(t)(s,a)/parenrightig\n",
            "−2γ+ητ\n",
            "1−γ/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞, (127)\n",
            "which gives\n",
            "−min\n",
            "s,a/parenleftig\n",
            "Q(t+1)\n",
            "τ(s,a)−τlogξ(t+1)(s,a)/parenrightig\n",
            "≤ −αmin\n",
            "s,a/parenleftig\n",
            "Q(t)\n",
            "τ(s,a)−τlogξ(t)(s,a)/parenrightig\n",
            "+2γ+ητ\n",
            "1−γM/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞\n",
            "≤αmax/braceleftig\n",
            "0,min\n",
            "s,a/parenleftig\n",
            "Q(t)\n",
            "τ(s,a)−τlogξ(t)(s,a)/parenrightig/bracerightig\n",
            "+2γ+ητ\n",
            "1−γM/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞. (128)\n",
            "This establishes the bounds on Ω(t+1)\n",
            "4in (44).\n",
            "B.2 Proof of Lemma 2\n",
            "Letf(λ) denote the characteristic function. In view of some direct calcula tions, we obtain\n",
            "f(λ) = (λ−α)/braceleftbigg\n",
            "(λ−σα)(λ−σ(1+bη))(λ−(1−α)γ−α)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=:f0(λ)\n",
            "−ησ\n",
            "1−γ[S(λ−(1−α)γ−α)+γcdMη+(1−α)(2+γ)Mcdη]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "=:f1(λ)/bracerightbigg\n",
            "−τη3γ\n",
            "(1−γ)2·2cdMσ,(129)\n",
            "where, for the notation simplicity, we let\n",
            "b:=M√\n",
            "N\n",
            "1−γ, (130a)\n",
            "c:=MN\n",
            "1−γ=√\n",
            "Nb, (130b)\n",
            "d:=2γ+ητ\n",
            "1−γ. (130c)\n",
            "Note that among all these new notation we introduce, S,dare dependent of η. To decouple the dependence,\n",
            "we give their upper bounds as follows\n",
            "d0:=1+γ\n",
            "1−γM≥d, (131)\n",
            "S0:=M√\n",
            "N/parenleftigg\n",
            "2+√\n",
            "2N+M√\n",
            "N\n",
            "τ/parenrightigg\n",
            "≥S, (132)\n",
            "where (131) follows from η≤(1−γ)/τ, and (132) uses the fact that α≤1 and 1−α≤1.\n",
            "Let\n",
            "λ⋆:= max/braceleftig3+σ\n",
            "4,1+(1−α)γ+α\n",
            "2/bracerightig\n",
            ". (133)\n",
            "SinceA(ρ) is a nonnegative matrix, by Perron-Frobenius Theorem (see Horn and Johnson (2012), The-\n",
            "orem 8.3.1), ρ(η) is an eigenvalue of A(ρ). So to verify ( 50), it suﬃces to show that f(λ)>0 for any\n",
            "λ∈[λ⋆,∞). To do so, in the following we ﬁrst show that f(λ⋆)>0, and then we prove that fis non-\n",
            "decreasing on [ λ⋆,∞).\n",
            "30•Showing f(λ⋆)>0.We ﬁrst lower bound f0(λ⋆). Sinceλ⋆≥3+σ\n",
            "4, we have\n",
            "λ⋆−σ(1+bη)≥1−σ\n",
            "4, (134)\n",
            "and from λ⋆≥1+(1−α)γ+α\n",
            "2we deduce\n",
            "λ⋆−(1−α)γ−α≥(1−γ)(1−α)\n",
            "2(135)\n",
            "and\n",
            "λ⋆>1+α\n",
            "2, (136)\n",
            "which gives\n",
            "λ⋆−σα≥1+α\n",
            "2−σα. (137)\n",
            "Combining ( 137), (134), (135), we have that\n",
            "f0(λ⋆)≥1−σ\n",
            "8/parenleftbigg1+α\n",
            "2−σα/parenrightbigg\n",
            "ητ . (138)\n",
            "To continue, we upper bound f1(λ⋆) as follows.\n",
            "f1(λ⋆)≤Sτη+γcdMη+2+γ\n",
            "1−γcMτη2\n",
            "=η/parenleftbigg\n",
            "τ/parenleftbigg\n",
            "S+2+γ\n",
            "1−γMcη/parenrightbigg\n",
            "+γcdM/parenrightbigg\n",
            ". (139)\n",
            "Plugging ( 138),(139) into (129) and using ( 136), we have\n",
            "f(λ⋆)>1−α\n",
            "2/parenleftbigg\n",
            "f0(λ⋆)−ησ\n",
            "1−γf1(λ⋆)/parenrightbigg\n",
            "−τη3γ\n",
            "(1−γ)2·2cdMσ\n",
            "≥τη2\n",
            "2(1−γ)/bracketleftbigg1−σ\n",
            "8τ/parenleftbigg\n",
            "1−σ+(1−α)(σ−1\n",
            "2)/parenrightbigg\n",
            "−ησ\n",
            "1−γ/parenleftbigg\n",
            "τ/parenleftbigg\n",
            "S+2+γ\n",
            "1−γMcη/parenrightbigg\n",
            "+5γcdM/parenrightbigg/bracketrightbigg\n",
            "=τη2\n",
            "2(1−γ)/bracketleftbigg(1−σ)2\n",
            "8τ−η\n",
            "1−γ/parenleftbigg\n",
            "Sτσ+2+γ\n",
            "1−γMcστη+τ2/parenleftbigg1\n",
            "2−σ/parenrightbigg\n",
            "·1−σ\n",
            "8+5γcdMσ/parenrightbigg/bracketrightbigg\n",
            "≥τη2\n",
            "2(1−γ)/bracketleftbigg(1−σ)2\n",
            "8τ−η\n",
            "1−γ/parenleftbigg\n",
            "S0τσ+(1−σ)2\n",
            "16τ2+(2+γ+5γd0)cMσ/parenrightbigg/bracketrightbigg\n",
            "≥0,\n",
            "where the penultimate inequality uses1\n",
            "2−σ≤1−σ\n",
            "2, and the last inequality follows from the deﬁnition\n",
            "ofζ(cf. (48)).\n",
            "•Provingfis non-decreasing on [λ⋆,∞).Note that\n",
            "η≤ζ≤(1−γ)(1−σ)2\n",
            "8S0σ,\n",
            "thus we have\n",
            "∀λ≥λ⋆:f′\n",
            "0(λ)−ησ\n",
            "1−γf′\n",
            "1(λ)≥(λ−σα)(λ−σ(1+bη))−η\n",
            "1−γSσ≥0,\n",
            "which indicates that f0−f1is non-decreasing on [ λ⋆,∞). Therefore, fis non-decreasing on [ λ⋆,∞).\n",
            "31B.3 Proof of Lemma 3\n",
            "Note that bounding u(t+1)(s,a) is identical to the proof in Appendix B.1and shall be omitted. The rest of\n",
            "the proof also follows closely that of Lemma 1, and we only highlight the diﬀerences due to approximation\n",
            "error for simplicity.\n",
            "Step 2: bound v(t+1)(s,a) =/vextenddouble/vextenddoubleT(t+1)(s,a)−/hatwideq(t+1)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2.Letq(t)\n",
            "τ:=/parenleftig\n",
            "qπ(t)\n",
            "1\n",
            "τ,1,···,qπ(t)\n",
            "N\n",
            "τ,N/parenrightig⊤\n",
            ".Similar to\n",
            "(104) we have\n",
            "/vextenddouble/vextenddoubleT(t+1)(s,a)−/hatwideq(t+1)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+σ/vextenddouble/vextenddoubleq(t+1)\n",
            "τ(s,a)−q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+σ/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble\n",
            "2+2∝bardble∝bardbl2. (140)\n",
            "Step 3: bound/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞.In the context of inexact updates, ( 105) writes\n",
            "/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞≤α/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞+(1−α)/vextenddouble/vextenddoubleQ⋆\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+(1−α)/vextenddouble/vextenddoubleQ(t)\n",
            "τ−/hatwideq(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞.\n",
            "For the last term, following a similar argument in ( 106) leads to\n",
            "/vextenddouble/vextenddoubleQ(t)\n",
            "τ−/hatwideq(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "nτ,n−1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "τ,n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1\n",
            "NN/summationdisplay\n",
            "n=1/parenleftig\n",
            "Qπ(t)\n",
            "nτ,n−qπ(t)\n",
            "nτ,n/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "≤M·1\n",
            "NN/summationdisplay\n",
            "n=1/vextenddouble/vextenddoublelogξ(t)\n",
            "n−logξ(t)/vextenddouble/vextenddouble\n",
            "∞+1\n",
            "NN/summationdisplay\n",
            "n=1en\n",
            "≤M/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+∝bardble∝bardbl∞.\n",
            "Combining the above two inequalities, we obtain\n",
            "/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble\n",
            "∞≤α/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞+(1−α)/vextenddouble/vextenddoubleQ⋆\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+(1−α)/parenleftig\n",
            "M/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddoublee/vextenddouble/vextenddouble\n",
            "∞/parenrightig\n",
            ".(141)\n",
            "Step 4: bound/vextenddouble/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble/vextenddouble\n",
            "2.We remark that the bound established in ( 111) still holds in\n",
            "the inexact setting, with the same deﬁnition for w(t):\n",
            "/vextenddouble/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ(s,a)−Q(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble/vextenddouble\n",
            "2≤M√\n",
            "N/vextenddouble/vextenddouble/vextenddoublew(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞. (142)\n",
            "To deal with the approximation error, we rewrite ( 113) as\n",
            "/vextenddouble/vextenddouble/vextenddoubleT(t)(s,a)−τlogξ(t)(s,a)−V⋆\n",
            "τ(s)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddoubleT(t)(s,a)−τlogξ(t)(s,a)−/parenleftbig\n",
            "Q⋆\n",
            "τ(s,a)−τlogπ⋆\n",
            "τ(a|s)/parenrightbig\n",
            "1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤/vextenddouble/vextenddoubleT(t)(s,a)−Q⋆\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+τ/vextenddouble/vextenddoublelogξ(t)(s,a)−logπ⋆\n",
            "τ(a|s)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "≤/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideqτ(s,a)1N/vextenddouble/vextenddouble\n",
            "2+/vextenddouble/vextenddouble/hatwideqτ(s,a)1N−Q⋆\n",
            "τ(s,a)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "+τ/vextenddouble/vextenddoublelogξ(t)(s,a)−logπ(t)(a|s)1N/vextenddouble/vextenddouble\n",
            "2+τ/vextenddouble/vextenddoublelogπ(t)(a|s)1N−logπ⋆\n",
            "τ(a|s)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "≤/vextenddouble/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)\n",
            "τ(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2+√\n",
            "N/vextendsingle/vextendsingle/hatwideq(t)\n",
            "τ(s,a)−Q⋆\n",
            "τ(s,a)/vextendsingle/vextendsingle\n",
            "+τ/vextenddouble/vextenddouble/vextenddoublelogξ(t)(s,a)−logπ(t)(a|s)1/vextenddouble/vextenddouble/vextenddouble\n",
            "2+τ√\n",
            "N/vextendsingle/vextendsinglelogπ(t)(a|s)−logπ⋆\n",
            "τ(a|s)/vextendsingle/vextendsingle, (143)\n",
            "where the second term can be upper-bounded by\n",
            "/vextendsingle/vextendsingle/hatwideq(t)\n",
            "τ(s,a)−Q⋆\n",
            "τ(s,a)/vextendsingle/vextendsingle≤/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddouble/vextenddouble/hatwideq(t)\n",
            "τ(s,a)−/hatwideQ(t)\n",
            "τ(s,a)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "32≤/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+∝bardble∝bardbl∞. (144)\n",
            "Combining ( 144), (143) and the established bounds in ( 112), (115), (117) leads to\n",
            "w(t)(s,a)≤/parenleftig\n",
            "2α+(1−α)·√\n",
            "2N/parenrightig/vextenddouble/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+1−α\n",
            "τ/vextenddouble/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "+1−α\n",
            "τ·√\n",
            "N/parenleftig/vextenddouble/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+∝bardble∝bardbl∞/parenrightig\n",
            "+1−α\n",
            "τ·2√\n",
            "N/vextenddouble/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞.\n",
            "Combining the above inequality with ( 142) and (140) gives\n",
            "/vextenddouble/vextenddoublev(t+1)/vextenddouble/vextenddouble\n",
            "∞≤σ/parenleftigg\n",
            "1+ηM√\n",
            "N\n",
            "1−γ/parenrightigg\n",
            "/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞+σM√\n",
            "N/braceleftigg/parenleftbigg\n",
            "2α+(1−α)·√\n",
            "2N+1−α\n",
            "τ·√\n",
            "NM/parenrightbigg/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞\n",
            "+1−α\n",
            "τ·√\n",
            "N/parenleftig/vextenddouble/vextenddoubleQ(t)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble\n",
            "∞+/vextenddouble/vextenddoublee/vextenddouble/vextenddouble\n",
            "∞/parenrightig\n",
            "+1−α\n",
            "τ·2√\n",
            "N/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t)/vextenddouble/vextenddouble\n",
            "∞/bracerightigg\n",
            "+2σ√\n",
            "N∝bardble∝bardbl∞.\n",
            "(145)\n",
            "Step 5: bound/vextenddouble/vextenddouble/vextenddoubleQ(t+1)\n",
            "τ−Q⋆\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞.It is straightforward to verify that ( 124) applies to the inexact updates\n",
            "as well:\n",
            "/vextenddouble/vextenddouble/vextenddoubleQ⋆\n",
            "τ−Q(t+1)\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤γ/vextenddouble/vextenddouble/vextenddoubleQ⋆\n",
            "τ−τlogξ(t+1)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+γ/parenleftbigg\n",
            "−min\n",
            "s,a/parenleftig\n",
            "Q(t+1)\n",
            "τ(s,a)−τlogξ(t+1)(s,a)/parenrightig/parenrightbigg\n",
            ".\n",
            "Plugging the above inequality into ( 141) and (145) establishes the bounds on Ω(t+1)\n",
            "3and Ω(t+1)\n",
            "2in (62),\n",
            "respectively.\n",
            "Step 6: bound −mins,a/parenleftbig\n",
            "Q(t+1)\n",
            "τ(s,a)−τlogξ(t+1)(s,a)/parenrightbig\n",
            ".We obtain the following lemma by interpreting\n",
            "the approximation error eas part of the consensus error/vextenddouble/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞in Lemma 9.\n",
            "Lemma 10 (inexact version of Lemma 9).Suppose 0< η≤(1−γ)/τ. For any state-action pair (s0,a0)∈\n",
            "S ×A, one has\n",
            "V(t+1)\n",
            "τ(s0)−V(t)\n",
            "τ(s0)≥1\n",
            "ηE\n",
            "s∼dπ(t+1)\n",
            "s0/bracketleftig\n",
            "αKL/parenleftbig\n",
            "π(t+1)(·|s0)∝bardblπ(t)(·|s0)/parenrightbig\n",
            "+KL/parenleftbig\n",
            "π(t)(·|s0)∝bardblπ(t+1)(·|s0)/parenrightbig/bracketrightig\n",
            "−2\n",
            "1−γ/parenleftig/vextenddouble/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+∝bardble∝bardbl∞/parenrightig\n",
            ", (146)\n",
            "Q(t+1)\n",
            "τ(s0,a0)−Q(t)\n",
            "τ(s0,a0)≥ −2γ\n",
            "1−γ/parenleftig/vextenddouble/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+∝bardble∝bardbl∞/parenrightig\n",
            ". (147)\n",
            "Using (147), we have\n",
            "Q(t+1)\n",
            "τ(s,a)−τ/parenleftigg\n",
            "αlogξ(t)(s,a)+(1−α)/hatwideQ(t)\n",
            "τ(s,a)\n",
            "τ/parenrightigg\n",
            "≥Q(t)\n",
            "τ(s,a)−τ/parenleftigg\n",
            "αlogξ(t)(s,a)+(1−α)/hatwideQ(t)\n",
            "τ(s,a)\n",
            "τ/parenrightigg\n",
            "−2γ\n",
            "1−γ/parenleftig/vextenddouble/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+∝bardble∝bardbl∞/parenrightig\n",
            "≥α/parenleftig\n",
            "Q(t)\n",
            "τ(s,a)−τlogξ(t)(s,a)/parenrightig\n",
            "−2γ+ητ\n",
            "1−γ/vextenddouble/vextenddouble/vextenddouble/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞−2γ\n",
            "1−γ∝bardble∝bardbl∞, (148)\n",
            "which gives\n",
            "−min\n",
            "s,a/parenleftig\n",
            "Q(t+1)\n",
            "τ(s,a)−τlogξ(t+1)(s,a)/parenrightig\n",
            "≤ −αmin\n",
            "s,a/parenleftig\n",
            "Q(t)\n",
            "τ(s,a)−τlogξ(t)(s,a)/parenrightig\n",
            "+2γ+ητ\n",
            "1−γM/vextenddouble/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+2γ\n",
            "1−γ∝bardble∝bardbl∞.(149)\n",
            "33B.4 Proof of Lemma 4\n",
            "Step 1: bound u(t+1)(s,a) =/vextenddouble/vextenddouble/vextenddoublelogξ(t+1)(s,a)−logξ(t+1)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2.Following the same strategy in es-\n",
            "tablishing ( 103), we have\n",
            "/vextenddouble/vextenddouble/vextenddoublelogξ(t+1)(s,a)−logξ(t+1)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig\n",
            "Wlogξ(t)(s,a)−logξ(t)(s,a)1N/parenrightig\n",
            "+η\n",
            "1−γ/parenleftig\n",
            "T(t)(s,a)−/hatwideQ(t)(s,a)1N/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2+η\n",
            "1−γ/vextenddouble/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2, (150)\n",
            "or equivalently/vextenddouble/vextenddoubleu(t+1)/vextenddouble/vextenddouble\n",
            "∞≤σ/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+η\n",
            "1−γ/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞. (151)\n",
            "Step 2: bound v(t+1)(s,a) =/vextenddouble/vextenddoubleT(t+1)(s,a)−/hatwideQ(t+1)(s,a)1N/vextenddouble/vextenddouble\n",
            "2.In the same vein of establishing ( 104), we\n",
            "have\n",
            "/vextenddouble/vextenddoubleT(t+1)(s,a)−/hatwideQ(t+1)(s,a)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+σ/vextenddouble/vextenddoubleQ(t+1)(s,a)−Q(t)(s,a)/vextenddouble/vextenddouble\n",
            "2, (152)\n",
            "The term/vextenddouble/vextenddoubleQ(t+1)(s,a)−Q(t)(s,a)/vextenddouble/vextenddouble\n",
            "2can be bounded in a similar way in ( 111):\n",
            "/vextenddouble/vextenddoubleQ(t+1)(s,a)−Q(t)(s,a)/vextenddouble/vextenddouble\n",
            "2≤(1+γ)γ\n",
            "(1−γ)2√\n",
            "N/vextenddouble/vextenddoublew(t)\n",
            "0/vextenddouble/vextenddouble\n",
            "∞, (153)\n",
            "where the coeﬃcient(1+γ)γ\n",
            "(1−γ)2comes from Min Lemma 8whenτ= 0, and w(t)\n",
            "0∈R|S||A|is deﬁned as\n",
            "∀(s,a)∈ S ×A :w(t)\n",
            "0(s,a):=/vextenddouble/vextenddouble/vextenddouble/vextenddoublelogξ(t+1)(s,a)−logξ(t)(s,a)−η\n",
            "1−γV⋆(s)1N/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "2.(154)\n",
            "It remains to bound/vextenddouble/vextenddoublew(t)\n",
            "0/vextenddouble/vextenddouble\n",
            "∞. Towards this end, we rewrite ( 112) as\n",
            "w(t)\n",
            "0(s,a)\n",
            "=/vextenddouble/vextenddoubleWlogξ(t)(s,a)+η\n",
            "1−γT(t)(s,a)−logξ(t)(s,a)−η\n",
            "1−γV⋆(s)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddouble/vextenddouble(W−I)/parenleftig\n",
            "logξ(t)(s,a)−logξ(t)(s,a)1N/parenrightig\n",
            "+η\n",
            "1−γ/parenleftig\n",
            "T(t)(s,a)−V⋆(s)1N/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤2/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+η\n",
            "1−γ/vextenddouble/vextenddoubleT(t)(s,a)−V⋆(s)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "≤2/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+η\n",
            "1−γ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideQ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+η\n",
            "1−γ·√\n",
            "N/vextendsingle/vextendsingle/hatwideQ(t)(s,a)−V⋆(s)/vextendsingle/vextendsingle.\n",
            "(155)\n",
            "Note that it holds for all ( s,a)∈ S ×A:\n",
            "/vextendsingle/vextendsingle/hatwideQ(t)(s,a)−V⋆(s)/vextendsingle/vextendsingle≤1\n",
            "1−γ\n",
            "since/hatwideQ(t)(s,a) andV⋆(s) are both in [0 ,1/(1−γ)]. This along with ( 155) gives\n",
            "w(t)\n",
            "0(s,a)≤2/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+η\n",
            "1−γ/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞+η√\n",
            "N\n",
            "(1−γ)2.\n",
            "Combining the above inequality with ( 153) and (152), we arrive at\n",
            "/vextenddouble/vextenddoublev(t+1)/vextenddouble/vextenddouble\n",
            "∞≤σ/parenleftigg\n",
            "1+(1+γ)γ√\n",
            "Nη\n",
            "(1−γ)3/parenrightigg\n",
            "/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞+(1+γ)γ\n",
            "(1−γ)2√\n",
            "Nσ/braceleftigg\n",
            "2/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+η\n",
            "(1−γ)2·√\n",
            "N/bracerightigg\n",
            ".(156)\n",
            "34Step 3: establish the descent equation. The following lemma characterizesthe improvement in φ(t)(η)\n",
            "for every iteration of Algorithm 1, with the proof postponed to Appendix C.4.\n",
            "Lemma 11 (Performance improvement of exact FedNPG) .For all starting state distribution ρ∈∆(S), we\n",
            "have the iterates of FedNPG satisfy\n",
            "φ(t+1)(η)≤φ(t)(η)+2η\n",
            "(1−γ)2/vextenddouble/vextenddouble/hatwideQ(t)−Q(t)/vextenddouble/vextenddouble\n",
            "∞−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            ", (157)\n",
            "where\n",
            "φ(t)(η):=Es∼dπ⋆\n",
            "ρ/bracketleftig\n",
            "KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(t)(·|s)/parenrightbig/bracketrightig\n",
            "−η\n",
            "1−γV(t)(dπ⋆\n",
            "ρ),∀t≥0. (158)\n",
            "It remains to control the term/vextenddouble/vextenddoubleQ(t)−/hatwideQ(t)/vextenddouble/vextenddouble\n",
            "∞. Similar to ( 106), for all t≥0, we have\n",
            "/vextenddouble/vextenddoubleQ(t)−/hatwideQ(t)/vextenddouble/vextenddouble\n",
            "∞=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "nn−1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "(a)\n",
            "≤(1+γ)γ\n",
            "(1−γ)2·1\n",
            "NN/summationdisplay\n",
            "n=1/vextenddouble/vextenddoublelogξ(t)\n",
            "n−logξ(t)/vextenddouble/vextenddouble\n",
            "∞\n",
            "(b)\n",
            "≤(1+γ)γ\n",
            "(1−γ)2/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞, (159)\n",
            "where (a) invokes Lemma 8withτ= 0 and (b) stems from the deﬁnition of u(t). This along with ( 157) gives\n",
            "φ(t+1)(η)≤φ(t)(η)+2(1+γ)γ\n",
            "(1−γ)4η/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            ".\n",
            "B.5 Proof of Lemma 5\n",
            "The bound on u(t+1)(s,a) is already established in Step 1 in Appendix B.1and shall be omitted. As usual\n",
            "we only highlight the key diﬀerences with the proof of Lemma 4due to approximation error.\n",
            "Step 1: bound v(t+1)(s,a) =/vextenddouble/vextenddoubleT(t+1)(s,a)−/hatwideq(t+1)(s,a)1N/vextenddouble/vextenddouble\n",
            "2.Letq(t):=/parenleftig\n",
            "qπ(t)\n",
            "1\n",
            "1,···,qπ(t)\n",
            "N\n",
            "N/parenrightig⊤\n",
            ". From (87),\n",
            "we have\n",
            "/vextenddouble/vextenddouble/vextenddoubleT(t+1)(s,a)−/hatwideq(t+1)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddoubleW/parenleftig\n",
            "T(t)(s,a)+q(t+1)(s,a)−q(t)(s,a)/parenrightig\n",
            "−/hatwideq(t+1)(s,a)1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "=/vextenddouble/vextenddouble/vextenddouble/parenleftig\n",
            "WT(t)(s,a)−/hatwideq(t)(s,a)1N/parenrightig\n",
            "+W/parenleftig\n",
            "q(t+1)(s,a)−q(t)(s,a)/parenrightig\n",
            "+/parenleftig\n",
            "/hatwideq(t)(s,a)−/hatwideq(t+1)(s,a)/parenrightig\n",
            "1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+σ/vextenddouble/vextenddouble/vextenddouble/parenleftig\n",
            "q(t+1)(s,a)−q(t)(s,a)/parenrightig\n",
            "+/parenleftig\n",
            "/hatwideq(t)(s,a)−/hatwideq(t+1)(s,a)/parenrightig\n",
            "1N/vextenddouble/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+σ/vextenddouble/vextenddoubleq(t+1)(s,a)−q(t)(s,a)/vextenddouble/vextenddouble\n",
            "2\n",
            "≤σ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+σ/vextenddouble/vextenddoubleQ(t+1)(s,a)−Q(t)(s,a)/vextenddouble/vextenddouble\n",
            "2+2σ√\n",
            "N∝bardble∝bardbl∞. (160)\n",
            "Note that ( 153) still holds for inexact FedNPG:\n",
            "/vextenddouble/vextenddouble/vextenddoubleQ(t+1)(s,a)−Q(t)(s,a)/vextenddouble/vextenddouble/vextenddouble\n",
            "2≤(1+γ)γ\n",
            "(1−γ)2√\n",
            "N/vextenddouble/vextenddouble/vextenddoublew(t)\n",
            "0/vextenddouble/vextenddouble/vextenddouble\n",
            "∞, (161)\n",
            "wherew(t)\n",
            "0is deﬁned in ( 154). We rewrite ( 155), the bound on w(t)\n",
            "0(s,a), as\n",
            "w(t)\n",
            "0(s,a)≤2/vextenddouble/vextenddoublelogξ(t)(s,a)−logξ(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2\n",
            "35+η\n",
            "1−γ/vextenddouble/vextenddoubleT(t)(s,a)−/hatwideq(t)(s,a)1N/vextenddouble/vextenddouble\n",
            "2+η\n",
            "1−γ·√\n",
            "N/vextendsingle/vextendsingle/hatwideq(t)(s,a)−V⋆(s)/vextendsingle/vextendsingle.(162)\n",
            "With the following bound\n",
            "∀(s,a)∈ S ×A :/vextendsingle/vextendsingle/hatwideq(t)(s,a)−V⋆(s)/vextendsingle/vextendsingle≤/vextenddouble/vextenddouble/hatwideq(t)−Q(t)/vextenddouble/vextenddouble\n",
            "∞+1\n",
            "1−γ\n",
            "in mind, we write ( 155) as\n",
            "w(t)\n",
            "0(s,a)≤2/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+η\n",
            "1−γ/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞+η\n",
            "1−γ·√\n",
            "N/parenleftbigg/vextenddouble/vextenddouble/hatwideq(t)−q(t)/vextenddouble/vextenddouble\n",
            "∞+1\n",
            "1−γ/parenrightbigg\n",
            ".\n",
            "Putting all pieces together, we obtain\n",
            "/vextenddouble/vextenddoublev(t+1)/vextenddouble/vextenddouble\n",
            "∞≤σ/parenleftigg\n",
            "1+(1+γ)γ√\n",
            "Nη\n",
            "(1−γ)3/parenrightigg\n",
            "/vextenddouble/vextenddoublev(t)/vextenddouble/vextenddouble\n",
            "∞\n",
            "+(1+γ)γ\n",
            "(1−γ)2√\n",
            "Nσ/braceleftigg/parenleftigg\n",
            "2+(1+γ)γ√\n",
            "Nη\n",
            "(1−γ)3/parenrightigg\n",
            "/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble\n",
            "∞+η√\n",
            "N\n",
            "(1−γ)2+η√\n",
            "N\n",
            "1−γ∝bardble∝bardbl∞/bracerightigg\n",
            "+2σ√\n",
            "N∝bardble∝bardbl∞.(163)\n",
            "Step 2: establish the descent equation. Note that Lemma 11directly applies by replacing /hatwideQ(t)with\n",
            "/hatwideq(t):\n",
            "φ(t+1)(η)≤φ(t)(η)+2η\n",
            "(1−γ)2/vextenddouble/vextenddouble/vextenddouble/hatwideq(t)−Q(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞−η/parenleftig\n",
            "V⋆(ρ)−V(t)(ρ)/parenrightig\n",
            ".\n",
            "To bound the middle term, for all t≥0, we have\n",
            "/vextenddouble/vextenddouble/vextenddoubleQ(t)−/hatwideq(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "nn−1\n",
            "NN/summationdisplay\n",
            "n=1Qπ(t)\n",
            "n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "≤(1+γ)γ\n",
            "(1−γ)2·1\n",
            "NN/summationdisplay\n",
            "n=1/vextenddouble/vextenddouble/vextenddoublelogξ(t)\n",
            "n−logξ(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+1\n",
            "N/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN/summationdisplay\n",
            "n=0/parenleftig\n",
            "qπ(t)\n",
            "nn−Qπ(t)\n",
            "nn/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+1\n",
            "NN/summationdisplay\n",
            "n=1en\n",
            "≤(1+γ)γ\n",
            "(1−γ)2/vextenddouble/vextenddouble/vextenddoubleu(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+∝bardble∝bardbl∞. (164)\n",
            "Hence, (93) is established by combining the above two inequalities.\n",
            "C Proof of auxiliary lemmas\n",
            "C.1 Proof of Lemma 6\n",
            "The ﬁrst claim is easily veriﬁed as log ξ(t)\n",
            "n(s,·) always deviate from log π(t)\n",
            "n(·|s) by a global constant shift, as\n",
            "long as it holds for t= 0:\n",
            "logξ(t+1)\n",
            "n(s,·) =αN/summationdisplay\n",
            "n′=1[W]n,n′logξ(t)\n",
            "n′(s,·)+(1−α)T(t)\n",
            "n(s,·)/τ\n",
            "=αN/summationdisplay\n",
            "n′=1[W]n,n′/parenleftig\n",
            "logπ(t)\n",
            "n′(s,·)+c(t)\n",
            "n′(s)1|A|/parenrightig\n",
            "+(1−α)T(t)\n",
            "n(s,·)/τ\n",
            "=αN/summationdisplay\n",
            "n′=1[W]n,n′logπ(t)\n",
            "n′(s,·)+(1−α)T(t)\n",
            "n(s,·)/τ−z(t)\n",
            "n(s)1|A|+c(t+1)\n",
            "n(s)1|A|\n",
            "= logπ(t+1)\n",
            "n(·|s)+c(t+1)\n",
            "n(s)1|A|,\n",
            "36wherez(t)\n",
            "nis the normalization term (cf. line 5, Algorithm 2) and{c(t)\n",
            "n(s)}are some constants. To prove the\n",
            "second claim, ∀t≥0,∀(s,a)∈ S ×A, let\n",
            "T(t)(s,a):=1\n",
            "N1⊤T(t)(s,a). (165)\n",
            "Taking inner product with1\n",
            "N1for both sides of ( 25) and using the double stochasticity property of W,\n",
            "we get\n",
            "T(t+1)(s,a) =T(t)(s,a)+/hatwideQ(t+1)\n",
            "τ(s,a)−/hatwideQ(t)\n",
            "τ(s,a). (166)\n",
            "By the choice of T(0)(line 2 of Algorithm 2), we have T(0)=/hatwideQ(0)\n",
            "τand hence by induction\n",
            "∀t≥0 :T(t)=/hatwideQ(t)\n",
            "τ. (167)\n",
            "This implies\n",
            "logξ(t+1)(s,a)−αlogξ(t)(s,a) = (1−α)/hatwideQ(t)\n",
            "τ(s,a)/τ\n",
            "= (1−α)T(t)(s,a)/τ\n",
            "=1\n",
            "N1⊤logξ(t+1)(s,a)−α1\n",
            "N1⊤logξ(t)(s,a).\n",
            "Therefore, to prove ( 98), it suﬃces to verify the claim for t= 0:\n",
            "1\n",
            "N1⊤logξ(0)(s,a) = log∝bardblexp(Q⋆\n",
            "τ(s,·)/τ)∝bardbl1+1\n",
            "N1⊤logπ(0)(a|s)−log/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleexp/parenleftigg\n",
            "1\n",
            "NN/summationdisplay\n",
            "n=1logπ(0)\n",
            "n(·|s)/parenrightigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "1\n",
            "= log∝bardblexp(Q⋆\n",
            "τ(s,·)/τ)∝bardbl1+logπ(0)(a|s) = logξ(0)(s,a).\n",
            "By taking logarithm over both sides of the deﬁnition of π(t+1)(cf. (24)), we get\n",
            "logπ(t+1)(a|s) =αlogπ(t)(a|s)+(1−α)/hatwideQ(t)(s,a)/τ−z(t)(s) (168)\n",
            "for some constant z(t)(s), which deviate from the update rule of log ξ(t+1)by a global constant shift and\n",
            "hence veriﬁes ( 99).\n",
            "C.2 Proof of Lemma 8\n",
            "For notational simplicity, we let Qθ′\n",
            "τandQθ\n",
            "τdenoteQπθ′\n",
            "τandQπθτ, respectively. From ( 7a) we immediately\n",
            "know that to bound/vextenddouble/vextenddouble/vextenddoubleQθ′\n",
            "τ−Qθ\n",
            "τ/vextenddouble/vextenddouble/vextenddouble\n",
            "∞, it suﬃces to control/vextendsingle/vextendsingleVθ\n",
            "τ(s)−Vθ′\n",
            "τ(s)/vextendsingle/vextendsinglefor each s∈ S. By (4) we have\n",
            "/vextendsingle/vextendsingleVθ\n",
            "τ(s)−Vθ′\n",
            "τ(s)/vextendsingle/vextendsingle≤/vextendsingle/vextendsingleVθ(s)−Vθ′(s)/vextendsingle/vextendsingle+τ/vextendsingle/vextendsingleH(s,πθ)−H(s,πθ′)/vextendsingle/vextendsingle, (169)\n",
            "so in the following we bound both terms in the RHS of ( 169).\n",
            "Step 1: bounding/vextendsingle/vextendsingleH(s,πθ)−H(s,πθ′)/vextendsingle/vextendsingle.We ﬁrst bound/vextendsingle/vextendsingleH(s,πθ)−H(s,πθ′)/vextendsingle/vextendsingleusing the idea in the proof\n",
            "of Lemma 14 in Mei et al. (2020). We let\n",
            "θt=θ+t(θ′−θ),∀t∈R, (170)\n",
            "and letht∈R|S|be\n",
            "∀s∈ S:ht(s):=−/summationdisplay\n",
            "a∈Aπθt(a|s)logπθt(a|s). (171)\n",
            "Note that ∝bardblht∝bardbl∞≤log|A|. We also denote Ht:S →R|A|×|A|by:\n",
            "∀s∈ S:Ht(s):=∂πθ(·|s)\n",
            "∂θ/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n",
            "θ=θt= diag{πθt(·|s)}−πθt(·|s)πθt(·|s)⊤, (172)\n",
            "37then we have\n",
            "∀s∈ S:/vextendsingle/vextendsingle/vextendsingle/vextendsingledht(s)\n",
            "dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/angbracketleftbigg∂ht(s)\n",
            "∂θt(·|s),θ′(s,·)−θ(s,·)/angbracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n",
            "=|∝an}bracketle{tHt(s)logπθt(·|s),θ′(s,·)−θ(s,·)∝an}bracketri}ht|\n",
            "≤ ∝bardblHt(s)logπθt(·|s)∝bardbl1∝bardblθ′(s,·)−θ(s,·)∝bardbl∞, (173)\n",
            "where∂ht(s)\n",
            "∂θt(·|s)stands for∂ht(s)\n",
            "∂θ(·|s)/vextendsingle/vextendsingle\n",
            "θ=θt. The ﬁrst term in ( 173) is further upper bounded as\n",
            "∝bardblHt(s)logπθt(·|s)∝bardbl1=/summationdisplay\n",
            "a∈Aπθt(a|s)/vextendsingle/vextendsinglelogπθt(a|s)−πθt(·|s)⊤logπθt(·|s)/vextendsingle/vextendsingle\n",
            "≤/summationdisplay\n",
            "a∈Aπθt(a|s)/parenleftbig\n",
            "|logπθt(a|s)|+/vextendsingle/vextendsingleπθt(·|s)⊤logπθt(·|s)/vextendsingle/vextendsingle/parenrightbig\n",
            "=−2/summationdisplay\n",
            "a∈Aπθt(a,s)logπθt(a|s)≤2log|A|.\n",
            "By Lagrange mean value theorem, there exists t∈(0,1) such that\n",
            "|h1(s)−h0(s)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingledht(s)\n",
            "dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤2log|A|∝bardblθ′(s,·)−θ(s,·)∝bardbl∞,\n",
            "where the inequality follows from ( 173) and the above inequality. Combining ( 5) with the above inequality,\n",
            "we arrive at/vextendsingle/vextendsingleH(s,πθ)−H(s,πθ′)/vextendsingle/vextendsingle≤2log|A|\n",
            "1−γ∝bardblθ′−θ∝bardbl∞. (174)\n",
            "Step 2: bounding/vextendsingle/vextendsingleVθ(s)−Vθ′(s)/vextendsingle/vextendsingle.Similar to the previous proof, we bound/vextendsingle/vextendsingleVθ(s)−Vθ′(s)/vextendsingle/vextendsingleby bounding/vextendsingle/vextendsingle/vextendsingledVθt\n",
            "dt(s)/vextendsingle/vextendsingle/vextendsingle. By Bellman’s consistency equation, the value function of πθtis given by\n",
            "Vθt(s) =/summationdisplay\n",
            "a∈Aπθt(a|s)r(s,a)+γ/summationdisplay\n",
            "aπθα(a|s)/summationdisplay\n",
            "s′∈SP(s′|s,a)Vθt(s′),\n",
            "which can be represented in a matrix-vector form as\n",
            "Vθt(s) =e⊤\n",
            "sMtrt, (175)\n",
            "wherees∈R|S|is a one-hot vector whose s-th entry is 1,\n",
            "Mt:= (I−γPt)−1, (176)\n",
            "withPt∈R|S|×|S|denoting the induced state transition matrix by πθt\n",
            "Pt(s,s′) =/summationdisplay\n",
            "a∈Aπθt(a|s)P(s′|s,a), (177)\n",
            "andrt∈R|S|is given by\n",
            "∀s∈ S:rt(s):=/summationdisplay\n",
            "a∈Aπθt(a|s)r(s,a). (178)\n",
            "Taking derivative w.r.t. tin (175), we obtain ( Petersen and Pedersen ,2008)\n",
            "dVθt(s)\n",
            "dt=γ·e⊤\n",
            "sMtdPt\n",
            "dtMtrt+e⊤\n",
            "sMtdrt\n",
            "dt. (179)\n",
            "We now calculate each term respectively.\n",
            "38•For the ﬁrst term, it follows that\n",
            "/vextendsingle/vextendsingle/vextendsingle/vextendsingleγ·e⊤\n",
            "sMtdPt\n",
            "dtMtrt/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤γ/vextenddouble/vextenddouble/vextenddouble/vextenddoubleMtdPt\n",
            "dtMtrt/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "≤γ\n",
            "1−γ/vextenddouble/vextenddouble/vextenddouble/vextenddoubledPt\n",
            "dtMtrt/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞\n",
            "≤2γ\n",
            "1−γ∝bardblMtrt∝bardbl∞∝bardblθ′−θ∝bardbl∞(180)\n",
            "≤2γ\n",
            "(1−γ)2∝bardblrt∝bardbl∞∝bardblθ′−θ∝bardbl∞\n",
            "≤2γ\n",
            "(1−γ)2∝bardblθ′−θ∝bardbl∞. (181)\n",
            "where the second and fourth lines use the fact ∝bardblMt∝bardbl1≤1/(1−γ) (Li et al.,2023b, Lemma 10), and\n",
            "the last line follow from\n",
            "∝bardblrt∝bardbl∞= max\n",
            "s∈S/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay\n",
            "a∈Aπθt(a|s)r(s,a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1.\n",
            "We defer the proof of ( 180) to the end of proof.\n",
            "•For the second term, it follows that\n",
            "/vextendsingle/vextendsingle/vextendsingle/vextendsinglee⊤\n",
            "sMtdrt\n",
            "dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1\n",
            "1−γ/vextenddouble/vextenddouble/vextenddouble/vextenddoubledrt\n",
            "dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤1\n",
            "1−γ∝bardblθ′−θ∝bardbl∞. (182)\n",
            "where the ﬁrst inequality follows again from ∝bardblMt∝bardbl1≤1/(1−γ), and the second inequality follows from\n",
            "/vextenddouble/vextenddouble/vextenddouble/vextenddoubledrt\n",
            "dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞= max\n",
            "s∈S/vextendsingle/vextendsingle/vextendsingle/vextendsingledrt(s)\n",
            "dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle= max\n",
            "s∈S/vextendsingle/vextendsingle/vextendsingle/vextendsingle/angbracketleftbigg∂πθt(·|s)⊤r(s,·)\n",
            "∂θt(s,·),θ′(s,·)−θ(s,·)/angbracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n",
            "≤max\n",
            "s∈S/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂πθt(·|s)⊤\n",
            "∂θt(s,·)r(s,·)/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "1∝bardblθ′(s,·)−θ(s,·)∝bardbl∞\n",
            "= max\n",
            "s∈S/parenleftigg/summationdisplay\n",
            "a∈Aπθt(a|s)/vextendsingle/vextendsingler(s,a)−πθt(·|s)⊤r(s,·)/vextendsingle/vextendsingle/parenrightigg\n",
            "∝bardblθ′(s,·)−θ(s,·)∝bardbl∞\n",
            "≤max\n",
            "s∈Smax\n",
            "a∈A/vextendsingle/vextendsingler(s,a)−πθt(·|s)⊤r(s,·)/vextendsingle/vextendsingle\n",
            "/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n",
            "≤1 since r(s,a)∈[0,1]∝bardblθ′(s,·)−θ(s,·)∝bardbl∞\n",
            "≤max\n",
            "s∈S∝bardblθ′(s,·)−θ(s,·)∝bardbl∞=∝bardblθ′−θ∝bardbl∞.\n",
            "Plugging the above two inequalities into ( 179) and using Lagrange mean value theorem, we have\n",
            "/vextendsingle/vextendsingleVθ(s)−Vθ′(s)/vextendsingle/vextendsingle≤1+γ\n",
            "(1−γ)2∝bardblθ′−θ∝bardbl∞. (183)\n",
            "Step 3: sum up. Combining ( 183), (174) and (169), we have\n",
            "∀s∈ S:/vextendsingle/vextendsingleVθ\n",
            "τ(s)−Vθ′\n",
            "τ(s)/vextendsingle/vextendsingle≤1+γ+2τ(1−γ)log|A|\n",
            "(1−γ)2∝bardbllogπ−logπ′∝bardbl∞. (184)\n",
            "Combining ( 184) and (7a), (107) immediately follows.\n",
            "39Proof of (180).For any vector x∈R|S|, we have\n",
            "/bracketleftbiggdPt\n",
            "dtx/bracketrightbigg\n",
            "s=/summationdisplay\n",
            "s′∈S/summationdisplay\n",
            "a∈Adπθt(a|s)\n",
            "dtP(s′|s,a)x(s′),\n",
            "from which we can bound the l∞norm as\n",
            "/vextenddouble/vextenddouble/vextenddouble/vextenddoubledPt\n",
            "dtx/vextenddouble/vextenddouble/vextenddouble/vextenddouble\n",
            "∞≤max\n",
            "s/summationdisplay\n",
            "a∈A/summationdisplay\n",
            "s′∈SP(s′|s,a)/vextendsingle/vextendsingle/vextendsingle/vextendsingledπθt(a|s)\n",
            "dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle∝bardblx∝bardbl∞\n",
            "= max\n",
            "s/summationdisplay\n",
            "a∈A/vextendsingle/vextendsingle/vextendsingle/vextendsingledπθt(a|s)\n",
            "dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle∝bardblx∝bardbl∞\n",
            "≤2∝bardblθ′−θ∝bardbl∞∝bardblx∝bardbl∞\n",
            "as desired, where the last line follows from the following fact:\n",
            "/summationdisplay\n",
            "a∈A/vextendsingle/vextendsingle/vextendsingle/vextendsingledπθt(a|s)\n",
            "dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/summationdisplay\n",
            "a∈A/vextendsingle/vextendsingle/vextendsingle/vextendsingle/angbracketleftbigg∂πθt(a|s)\n",
            "∂θt,θ′−θ/angbracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n",
            "=/summationdisplay\n",
            "a∈A/vextendsingle/vextendsingle/vextendsingle/vextendsingle/angbracketleftbigg∂πθt(a|s)\n",
            "∂θt(s,·),θ′(s,·)−θ(s,·)/angbracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n",
            "=/summationdisplay\n",
            "a∈Aπθt(a|s)/vextendsingle/vextendsingle(θ′(s,a)−θ(s,a))−πθt(·|s)⊤(θ′(s,·)−θ(s,·))/vextendsingle/vextendsingle\n",
            "≤max\n",
            "a|θ′(s,a)−θ(s,a)|+/vextendsingle/vextendsingleπθt(·|s)⊤(θ′(s,·)−θ(s,·))/vextendsingle/vextendsingle\n",
            "≤2∝bardblθ′−θ∝bardbl∞.\n",
            "C.3 Proof of Lemma 9\n",
            "To simplify the notation, we denote\n",
            "δ(t):=/hatwideQ(t)\n",
            "τ−Q(t)\n",
            "τ. (185)\n",
            "We ﬁrst rearrange the terms of ( 168) and obtain\n",
            "−τlogπ(t)(a|s)+/parenleftig\n",
            "Q(t)\n",
            "τ(s,a)+δ(t)(s,a)/parenrightig\n",
            "=1−γ\n",
            "η/parenleftig\n",
            "logπ(t+1)(a|s)−logπ(t)(a|s)/parenrightig\n",
            "+1−γ\n",
            "ηz(t)(s).(186)\n",
            "This in turn allows us to express V(t)\n",
            "τ(s0) for any s0∈ Sas follows\n",
            "V(t)\n",
            "τ(s0) =E\n",
            "a0∼π(t)(·|s0)/bracketleftig\n",
            "−τlogπ(t)(a0|s0)+Q(t)\n",
            "τ(s0,a0)/bracketrightig\n",
            "=E\n",
            "a0∼π(t)(·|s0)/bracketleftbigg1−γ\n",
            "ηz(t)(s0)/bracketrightbigg\n",
            "+E\n",
            "a0∼π(t)(·|s0)/bracketleftbigg1−γ\n",
            "η/parenleftig\n",
            "logπ(t+1)(a0|s0)−logπ(t)(a0|s0)/parenrightig\n",
            "−δ(t)(s0,a0)/bracketrightbigg\n",
            "=1−γ\n",
            "ηz(t)(s0)−1−γ\n",
            "ηKL/parenleftbig\n",
            "π(t)(·|s0)∝bardblπ(t+1)(·|s0)/parenrightbig\n",
            "−E\n",
            "a0∼π(t)(·|s0)/bracketleftig\n",
            "δ(t)(s0,a0)/bracketrightig\n",
            "=E\n",
            "a0∼π(t+1)(·|s0)/bracketleftbigg1−γ\n",
            "ηz(t)(s0)/bracketrightbigg\n",
            "−1−γ\n",
            "ηKL/parenleftbig\n",
            "π(t)(·|s0)∝bardblπ(t+1)(·|s0)/parenrightbig\n",
            "−E\n",
            "a0∼π(t)(·|s0)/bracketleftig\n",
            "δ(t)(s0,a0)/bracketrightig\n",
            ",\n",
            "(187)\n",
            "where the ﬁrst identity makes use of ( 7b), the second line follows from ( 186). Invoking ( 7b) again to rewrite\n",
            "thez(s0) appearing in the ﬁrst term of ( 187), we reach\n",
            "V(t)\n",
            "τ(s0)\n",
            "=E\n",
            "a0∼π(t+1)(·|s0)/bracketleftbigg\n",
            "−τlogπ(t+1)(a0|s0)+Q(t)\n",
            "τ(s0,a0)+/parenleftbigg\n",
            "τ−1−γ\n",
            "η/parenrightbigg/parenleftig\n",
            "logπ(t+1)(a0|s0)−logπ(t)(a|s)/parenrightig/bracketrightbigg\n",
            "40−1−γ\n",
            "ηKL/parenleftbig\n",
            "π(t)(·|s0)∝bardblπ(t+1)(·|s0)/parenrightbig\n",
            "−E\n",
            "a0∼π(t)(·|s0)/bracketleftig\n",
            "δ(t)(s0,a0)/bracketrightig\n",
            "+E\n",
            "a0∼π(t+1)(·|s0)/bracketleftig\n",
            "δ(t)(s0,a0)/bracketrightig\n",
            "=E\n",
            "a0∼π(t+1)(·|s0),\n",
            "s1∼P(·|s0,a0)/bracketleftig\n",
            "−τlogπ(t+1)(a0|s0)+r(s0,a0)+γV(t)\n",
            "τ(s0)/bracketrightig\n",
            "−/parenleftbigg1−γ\n",
            "η−τ/parenrightbigg\n",
            "KL/parenleftbig\n",
            "π(t+1)(·|s0)∝bardblπ(t)(·|s0)/parenrightbig\n",
            "−1−γ\n",
            "ηKL/parenleftbig\n",
            "π(t)(·|s0)∝bardblπ(t+1)(·|s0)/parenrightbig\n",
            "−E\n",
            "a0∼π(t)(·|s0)/bracketleftig\n",
            "δ(t)(s0,a0)/bracketrightig\n",
            "+E\n",
            "a0∼π(t+1)(·|s0)/bracketleftig\n",
            "δ(t)(s0,a0)/bracketrightig\n",
            ". (188)\n",
            "Note that for any ( s0,a0)∈ S ×A, we have\n",
            "−E\n",
            "a0∼π(t)(·|s0)/bracketleftig\n",
            "δ(t)(s0,a0)/bracketrightig\n",
            "+E\n",
            "a0∼π(t+1)(·|s0)/bracketleftig\n",
            "δ(t)(s0,a0)/bracketrightig\n",
            "=/summationdisplay\n",
            "a0∈A/parenleftig\n",
            "π(t+1)(a0|s0)−π(t)(a0|s0)/parenrightig\n",
            "δ(t)(s0,a0)\n",
            "≤/vextenddouble/vextenddoubleπ(t+1)(·|s0)−π(t)(·|s0)/vextenddouble/vextenddouble\n",
            "1/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞≤2/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞. (189)\n",
            "To ﬁnish up, applying ( 188) recursively to expand V(t)\n",
            "τ(si),i≥1 and making use of ( 189), we arrive at\n",
            "V(t)\n",
            "τ(s0)\n",
            "≤∞/summationdisplay\n",
            "i=1γi·2/vextenddouble/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+ E\n",
            "ai∼π(t+1)(·|si),\n",
            "si+1∼P(·|si,ai),∀i≥0/bracketleftigg∞/summationdisplay\n",
            "i=1γi/braceleftig\n",
            "r(si,ai)−τlogπ(t+1)(ai|si)/bracerightig\n",
            "−∞/summationdisplay\n",
            "i=1γi/braceleftbigg/parenleftbigg1−γ\n",
            "η−τ/parenrightbigg\n",
            "KL/parenleftbig\n",
            "π(t+1)(·|si)∝bardblπ(t)(·|si)/parenrightbig\n",
            "+1−γ\n",
            "ηKL/parenleftbig\n",
            "π(t)(·|si)∝bardblπ(t+1)(·|si)/parenrightbig/bracerightbigg/bracketrightigg\n",
            "=2\n",
            "1−γ/vextenddouble/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞+V(t+1)\n",
            "τ(s0)\n",
            "−E\n",
            "s∼dπ(t+1)\n",
            "s0/bracketleftbigg/parenleftbigg1\n",
            "η−τ\n",
            "1−γ/parenrightbigg\n",
            "KL/parenleftbig\n",
            "π(t+1)(·|si)∝bardblπ(t)(·|si)/parenrightbig\n",
            "+1\n",
            "ηKL/parenleftbig\n",
            "π(t)(·|si)∝bardblπ(t+1)(·|si)/parenrightbig/bracketrightbigg\n",
            ",(190)\n",
            "where the third line follows since V(t+1)\n",
            "τcan be viewed as the value function of π(t+1)with adjusted rewards\n",
            "r(t+1)(s,a):=r(s,a)−τlogπ(t+1)(s|a). And (125) follows immediately from the above inequality ( 190). By\n",
            "(7a) we can easily see that ( 126) is a consequence of ( 125).\n",
            "C.4 Proof of Lemma 11\n",
            "We ﬁrst introduce the famous performance diﬀerence lemma which w ill be used in our proof.\n",
            "Lemma 12 (Performance diﬀerence lemma) .For all policies π,π′and state s0, we have\n",
            "Vπ(s0)−Vπ′(s0) =1\n",
            "1−γEs∼dπs0Ea∼π(·|s)/bracketleftig\n",
            "Aπ′(s,a)/bracketrightig\n",
            ". (191)\n",
            "The proof of Lemma 12can be found in, for example, Appendix A of Agarwal et al. (2021).\n",
            "For allt≥0, we deﬁne the advantage function A(t)as:\n",
            "∀(s,a)∈ S ×A :A(t)(s,a):=Q(t)(s,a)−V(t)(s). (192)\n",
            "Then for Alg. 1, the update rule of π(Eq. (168)) can be written as\n",
            "logπ(t+1)(a|s) = logπ(t)(a|s)+η\n",
            "1−γ/parenleftig\n",
            "A(t)(s,a)+δ(t)(s,a)/parenrightig\n",
            "−log/hatwidez(t)(s), (193)\n",
            "41whereδ(t)is deﬁned in ( 185) and\n",
            "log/hatwidez(t)(s) = log/summationdisplay\n",
            "a′∈Aπ(t)(a′|s)exp/braceleftbiggη\n",
            "1−γ/parenleftig\n",
            "A(t)(s,a′)+δ(t)(s,a′)/parenrightig/bracerightbigg\n",
            "≥/summationdisplay\n",
            "a′∈Aπ(t)(a′|s)logexp/braceleftbiggη\n",
            "1−γ/parenleftig\n",
            "A(t)(s,a′)+δ(t)(s,a′)/parenrightig/bracerightbigg\n",
            "=η\n",
            "1−γ/summationdisplay\n",
            "a′∈Aπ(t)(a′|s)/parenleftig\n",
            "A(t)(s,a′)+δ(t)(s,a′)/parenrightig\n",
            "=η\n",
            "1−γ/summationdisplay\n",
            "a′∈Aπ(t)(a′|s)δ(t)(s,a′)≥ −η\n",
            "1−γ/vextenddouble/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble/vextenddouble\n",
            "∞, (194)\n",
            "where the ﬁrst inequality follows by Jensen’s inequality on the concav e function log xand the last equality\n",
            "uses/summationtext\n",
            "a′∈Aπ(t)(a′|s)A(t)(s,a′) = 0.\n",
            "For all starting state distribution µ, we use d(t+1)as shorthand for dπ(t+1)\n",
            "µ, the performance diﬀerence\n",
            "lemma (Lemma 12) implies:\n",
            "V(t+1)(µ)−V(t)(µ)\n",
            "=1\n",
            "1−γEs∼d(t+1)/summationdisplay\n",
            "a∈Aπ(t+1)(a|s)/parenleftig\n",
            "A(t)(s,a)+δ(t)(s,a)/parenrightig\n",
            "−1\n",
            "1−γEs∼d(t+1)Ea∼π(t+1)(·|s)/bracketleftig\n",
            "δ(t)(s,a)/bracketrightig\n",
            "=1\n",
            "ηEs∼d(t+1)/summationdisplay\n",
            "a∈Aπ(t+1)(a|s)logπ(t+1)(a|s)/hatwidez(t)(s)\n",
            "π(t)(a|s)−1\n",
            "1−γEs∼d(t+1)Ea∼π(t+1)(·|s)/bracketleftig\n",
            "δ(t)(s,a)/bracketrightig\n",
            "=1\n",
            "ηEs∼d(t+1)KL/parenleftbig\n",
            "π(t+1)(·|s)∝bardblπ(t)(·|s)/parenrightbig\n",
            "+1\n",
            "ηEs∼d(t+1)log/hatwidez(t)(s)−1\n",
            "1−γEs∼d(t+1)Ea∼π(t+1)(·|s)/bracketleftig\n",
            "δ(t)(s,a)/bracketrightig\n",
            "≥1\n",
            "ηEs∼d(t+1)/parenleftbigg\n",
            "log/hatwidez(t)(s)+η\n",
            "1−γ/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞/parenrightbigg\n",
            "−2\n",
            "1−γ/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞,\n",
            "from which we can see that\n",
            "V(t+1)(µ)−V(t)(µ)≥ −2\n",
            "1−γ/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞, (195)\n",
            "where we use ( 194), and that\n",
            "V(t+1)(µ)−V(t)(µ)≥1−γ\n",
            "ηEs∼µ/parenleftbigg\n",
            "log/hatwidez(t)(s)+η\n",
            "1−γ/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞/parenrightbigg\n",
            "−2\n",
            "1−γ/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞, (196)\n",
            "which follows from d(t+1)=dπ(t+1)\n",
            "µ≥(1−γ)µand the fact that log /hatwidez(t)(s)+η\n",
            "1−γ/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞≥0 (by (194)).\n",
            "For any ﬁxed ρ, we use d⋆as shorthand for dπ⋆\n",
            "ρ. By the performance diﬀerence lemma (Lemma 12),\n",
            "V⋆(ρ)−V(t)(ρ)\n",
            "=1\n",
            "1−γEs∼d⋆/summationdisplay\n",
            "a∈Aπ⋆(a|s)/parenleftig\n",
            "A(t)(s,a)+δ(t)(s,a)/parenrightig\n",
            "−1\n",
            "1−γEs∼d⋆Ea∼π⋆(·|s)/bracketleftig\n",
            "δ(t)(s,a)/bracketrightig\n",
            "=1\n",
            "ηEs∼d⋆/summationdisplay\n",
            "a∈Aπ⋆(a|s)logπ(t+1)(a|s)/hatwidez(t)(s)\n",
            "π(t)(a|s)−1\n",
            "1−γEs∼d⋆Ea∼π⋆(·|s)/bracketleftig\n",
            "δ(t)(s,a)/bracketrightig\n",
            "=1\n",
            "ηEs∼d⋆/parenleftig\n",
            "KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(t)(·|s)/parenrightbig\n",
            "−KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(t+1)(·|s)/parenrightbig\n",
            "+log/hatwidez(t)(s)/parenrightig\n",
            "−1\n",
            "1−γEs∼d⋆Ea∼π⋆(·|s)/bracketleftig\n",
            "δ(t)(s,a)/bracketrightig\n",
            "≤1\n",
            "ηEs∼d⋆/parenleftbigg\n",
            "KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(t)(·|s)/parenrightbig\n",
            "−KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(t+1)(·|s)/parenrightbig\n",
            "+/parenleftbigg\n",
            "log/hatwidez(t)(s)+η\n",
            "1−γ/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞/parenrightbigg/parenrightbigg\n",
            ",(197)\n",
            "where we use ( 193) in the second equality.\n",
            "42By applying ( 196) withµ=d⋆as the initial state distribution, we have\n",
            "1\n",
            "ηEs∼µ/parenleftig\n",
            "log/hatwidez(t)(s)+η\n",
            "1−γ/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞/parenrightig\n",
            "≤1\n",
            "1−γ/parenleftig\n",
            "V(t+1)(d⋆)−V(t)(d⋆)/parenrightig\n",
            "+2\n",
            "(1−γ)2/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞.\n",
            "Plugging the above equation into ( 197), we obtain\n",
            "V⋆(ρ)−V(t)(ρ)≤1\n",
            "ηEs∼d⋆/parenleftig\n",
            "KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(t)(·|s)/parenrightbig\n",
            "−KL/parenleftbig\n",
            "π⋆(·|s)∝bardblπ(t+1)(·|s)/parenrightbig/parenrightig\n",
            "+1\n",
            "1−γ/parenleftig\n",
            "V(t+1)(d⋆)−V(t)(d⋆)/parenrightig\n",
            "+2\n",
            "(1−γ)2/vextenddouble/vextenddoubleδ(t)/vextenddouble/vextenddouble\n",
            "∞,\n",
            "which gives Lemma 11.\n",
            "43Multi-task learning of convex combinations of forecasting models\n",
            "Giovanni Felici\n",
            "Institute for Systems Analysis and Computer Science “Antonio Ruberti”,\n",
            "National Research Council, Rome, Via dei Taurini 19, 00185, Italy\n",
            "Antonio M. Sudoso∗\n",
            "Department of Computer, Control and Management Engineering “Antonio Ruberti”,\n",
            "Sapienza University of Rome, Via Ariosto 25, 00185, Italy\n",
            "&\n",
            "Institute for Systems Analysis and Computer Science “Antonio Ruberti”,\n",
            "National Research Council, Rome, Via dei Taurini 19, 00185, Italy\n",
            "Abstract\n",
            "Forecast combination involves using multiple forecasts to create a single, more accurate prediction.\n",
            "Recently, feature-based forecasting has been employed to either select the most appropriate fore-\n",
            "casting models or to learn the weights of their convex combination. In this paper, we present a\n",
            "multi-task learning methodology that simultaneously addresses both problems. This approach is\n",
            "implemented through a deep neural network with two branches: the regression branch, which learns\n",
            "the weights of various forecasting methods by minimizing the error of combined forecasts, and the\n",
            "classification branch, which selects forecasting methods with an emphasis on their diversity. To\n",
            "generate training labels for the classification task, we introduce an optimization-driven approach\n",
            "that identifies the most appropriate methods for a given time series. The proposed approach elic-\n",
            "its the essential role of diversity in feature-based forecasting and highlights the interplay between\n",
            "model combination and model selection when learning forecasting ensembles. Experimental results\n",
            "on a large set of series from the M4 competition dataset show that our proposal enhances point\n",
            "forecast accuracy compared to state-of-the-art methods.\n",
            "Keywords: Forecasting, Forecast combination, Forecast diversity, Meta-learning, Time series\n",
            "features\n",
            "∗Corresponding author\n",
            "Email addresses: giovanni.felici@iasi.cnr.it (Giovanni Felici), antoniomaria.sudoso@uniroma1.it\n",
            "(Antonio M. Sudoso)\n",
            "Preprint submitted to Elsevier November 1, 2023arXiv:2310.20545v1  [cs.LG]  31 Oct 20231. Introduction\n",
            "Forecasting is an essential component of operational research (Fildes et al., 2008; Nikolopoulos,\n",
            "2021) having applications in different domains, such as finance (Tung & Wong, 2009), energy\n",
            "(Taylor, 2017), supply chains (Syntetos et al., 2016), and inventory management (Syntetos et al.,\n",
            "2009). Forecast combination - also known as ensemble forecasting - is a popular technique used\n",
            "in the field of forecasting, which involves the aggregation of multiple forecasts to produce a single\n",
            "and more accurate prediction (Clemen, 1989; Timmermann, 2006). Many studies have shown that\n",
            "forecast combinations lead to improvements in forecast accuracy, as they reduce the impact of biases\n",
            "in individual forecasts and considerably decrease their variance (Atiya, 2020). The M4 competition\n",
            "(Makridakis et al., 2020) has proven to be highly valuable for researchers, providing them with\n",
            "insights into the performance of various forecasting models and facilitating the establishment of\n",
            "best practices in the field (Hyndman, 2020). This competition has reaffirmed and built upon\n",
            "the knowledge gained from previous competitions, with a significant discovery being the success\n",
            "of forecast combinations. Out of the top 17 most accurate methods, 12 were combinations of\n",
            "forecasting models. This marks the first time that forecast combinations have demonstrated such\n",
            "strong dominance in a competition. However, it is important to note that the benefits of forecast\n",
            "combination are not guaranteed, and can depend on factors such as the quality of the individual\n",
            "forecasts, the nature of the data being forecasted, and the method used for combining the forecasts\n",
            "(Timmermann, 2006; Petropoulos et al., 2018; Atiya, 2020). The influential work by Bates &\n",
            "Granger (1969) indicates that combining forecasts can enhance accuracy, as long as the forecast sets\n",
            "incorporate independent information. Since then, researchers have demonstrated the effectiveness\n",
            "of forecast combinations through various weighting methods (Jose & Winkler, 2008; Andrawis et al.,\n",
            "2011; Cang & Yu, 2014; Lichtendahl & Winkler, 2020). The literature on forecast combinations\n",
            "covers a wide range of topics, including the methods used for combining forecasts, the benefits of\n",
            "using forecast combinations, and the potential pitfalls that can arise when using this technique\n",
            "(Bunn, 1988; De Menezes et al., 2000; Armstrong, 2001). One potential pitfall is overfitting, which\n",
            "occurs when the resulting method is excessively flexible and fits very well the individual forecasts\n",
            "in the training phase while failing in accurately forecasting out-of-sample data. Another pitfall is\n",
            "the risk of creating a forecast that is too complex or difficult to interpret, and hence harder to use\n",
            "in decision-making. Moreover, the success of ensemble forecasting heavily depends on the selection\n",
            "of component models as proven by many studies (Kourentzes et al., 2019; Atiya, 2020; Lichtendahl\n",
            "& Winkler, 2020). For a recent overview of forecast combinations, we direct the readers to the\n",
            "encyclopedic review article authored by Petropoulos et al. (2022).\n",
            "One of the most commonly used methods for combining forecasts is the simple average (Jose\n",
            "& Winkler, 2008), which involves taking the arithmetic mean of the individual forecasts. This\n",
            "approach is easy to implement and is often effective. For this reason, it is used in practice as a\n",
            "benchmark against which more sophisticated combination techniques are tested. Such techniques\n",
            "2include weighted averages, where weights can be determined through basic regression techniques\n",
            "(Winkler & Clemen, 1992), or with more sophisticated model-based approaches based on machine\n",
            "learning algorithms (Petropoulos et al., 2022). Our paper centers its focus on model-based ap-\n",
            "proaches designed to learn convex combinations of forecasting models. Formally, the goal is to\n",
            "combine time series predictions generated by a set of models, known as base forecasters . Let M\n",
            "andHbe the number of algorithms in the forecast pools and the forecast horizon, respectively.\n",
            "For a given time series {y1, . . . , y T}of length T, we denote the h-th step forecast produced by the\n",
            "i-th individual method as fh\n",
            "i, where i= 1, . . . , M andh= 1, . . . , H . We say that the combined\n",
            "forecast fh\n",
            "combat the h-th step is a convex combination of Mbase models when\n",
            "fh\n",
            "comb=MX\n",
            "i=1wifh\n",
            "is.t.MX\n",
            "i=1wi= 1, wi≥0∀i∈ {1, . . . , M }, (1)\n",
            "where wiis the weight of the forecast produced by the i-th method. Given this general framework,\n",
            "building an ensemble forecast involves two steps: selecting a methodology for training the base\n",
            "forecasters and choosing an appropriate method for combining their outputs based on the weights\n",
            "wi. The problem of choosing a suitable combination of forecasting methods is known in the liter-\n",
            "ature as the “forecast combination puzzle” and it is generally accepted that not all combinations\n",
            "perform well. Lichtendahl & Winkler (2020) conducted a study to investigate why some combi-\n",
            "nations performed better than others in the recent M4 competition. Their findings highlighted\n",
            "the significance of diversity and the accuracy of individual models as crucial factors for effective\n",
            "forecast combinations. The accuracy of individual forecasts is important because the accuracy of\n",
            "the combined forecast will be limited by the accuracy of the weakest individual forecast. At the\n",
            "same time, diversity is important because it can help to reduce the impact of biases and errors\n",
            "that may be present in individual forecasts (Atiya, 2020). Diversity can be achieved by selecting\n",
            "a range of forecasting methods that use different techniques, assumptions, and models. For exam-\n",
            "ple, a combination of statistical methods, machine learning algorithms, and expert judgment may\n",
            "provide a diverse set of forecasts (Makridakis et al., 2020).\n",
            "The ambiguity decomposition theory by Krogh & Vedelsby (1994) can be easily applied to the\n",
            "forecast combination task. It reveals the relationship between the error of the ensemble model and\n",
            "the error of base models for regression tasks and states that the overall mean squared error of a\n",
            "3weighted forecast combination model over the whole forecast horizon Hcan be written as\n",
            "MSE comb=1\n",
            "HHX\n",
            "h=1\u0010\n",
            "fh\n",
            "comb−yT+h\u00112\n",
            "(2)\n",
            "=1\n",
            "HHX\n",
            "h=1 MX\n",
            "i=1wi\u0010\n",
            "fh\n",
            "i−yT+h\u00112\n",
            "−MX\n",
            "i=1wi\u0010\n",
            "fh\n",
            "i−fh\n",
            "comb\u00112!\n",
            "(3)\n",
            "=1\n",
            "HHX\n",
            "h=1\n",
            "MX\n",
            "i=1wi\u0010\n",
            "fh\n",
            "i−yT+h\u00112\n",
            "−M−1X\n",
            "i=1MX\n",
            "j=1,j>iwiwj\u0010\n",
            "fh\n",
            "i−fh\n",
            "j\u00112\n",
            ". (4)\n",
            "The first term on the right-hand side is the weighted squared error of the individual forecast fihwith\n",
            "respect to the true observation yT+h. The second term quantifies the diversity of the ensemble and\n",
            "is the squared error spread of the individual forecast fh\n",
            "iaround fh\n",
            "comb, or equivalently, the degree\n",
            "of diversity between the i-th and j-th method in the pool of base forecasters. If two ensembles\n",
            "have the same weighted mean squared error, the one with more diversity will have a lower overall\n",
            "squared error. In other words, greater diversity among the forecasting methods in the pool results\n",
            "in improved overall forecast accuracy (Kang et al., 2022).\n",
            "Recent literature uses the term meta-learning to describe the process of automatically acquiring\n",
            "knowledge for model selection and combination (Prudˆ encio & Ludermir, 2004; Lemke & Gabrys,\n",
            "2010). The concept of meta-learning for time series forecasting is formalized by Talagala et al.\n",
            "(2018), where a machine learning approach is employed to acquire meta-knowledge by establishing\n",
            "connections between summarized features extracted from time series data, and the forecasting\n",
            "performance of base forecasters. This acquired knowledge, replacing human expertise, is then used\n",
            "to select appropriate forecasting methods. However, opting for a single apparently best forecasting\n",
            "model is risky due to the possibility of selecting an inadequately specified model. Conversely,\n",
            "employing meta-learning to combine different forecasts to mitigate model risk is a more promising\n",
            "approach employed by recent meta-learning approaches, reviewed in the next section.\n",
            "2. Forecast combination by meta-learning\n",
            "Meta-learning can be employed to either select the most appropriate model or to estimate the\n",
            "weights used to combine the different base forecasts. Methods of such type may share a pool of\n",
            "forecasting techniques and differ in how they identify and combine features from the time series.\n",
            "Indeed, regardless of the task (model selection or model combination), a common challenge in\n",
            "feature-based forecasting is the selection, extraction, or estimation of the right features. These fea-\n",
            "tures can be, for example, statistical representations of time series characteristics, such as mean,\n",
            "standard deviation, autocorrelation, and seasonality. Using feature-based time series representa-\n",
            "tions has gained interest in various time series mining tasks, including clustering, classification, and\n",
            "forecasting (Mancuso et al., 2021; Petropoulos et al., 2022). Successful applications of features in\n",
            "4time series forecasting are based on meta-learning. Talagala et al. (2018) developed a meta-learning\n",
            "approach called FFORMS (Feature-based FORecast Model Selection), which uses a Random For-\n",
            "est (RF) classifier to choose the best forecasting method from nine base forecasters based on a\n",
            "set of time series features. To build a reliable classifier, they proposed augmenting the set of ob-\n",
            "served time series by simulating new time series similar to those in the assumed population. Time\n",
            "series features are based on a set of manually selected 25 features for non-seasonal data and 30\n",
            "features for seasonal data. Montero-Manso et al. (2020) improved FFORMS and proposed a meta-\n",
            "learning approach to learn the weights of convex combinations of forecasting models, resulting in\n",
            "the framework called FFORMA (Feature-based FORecast Model Averaging). Prior to forecasting\n",
            "with FFORMA, 42 hand-crafted features are extracted from the original time series and the overall\n",
            "weighted average error of each forecasting method in the pool is computed. To determine the opti-\n",
            "mal weights of the combination, the problem is framed as a non-linear regression where time series\n",
            "features are linked with the forecasting errors using the XGBoost algorithm (Chen & Guestrin,\n",
            "2016). Di Gangi (2022) proposed a meta-learning system based on a Multilayer Perceptron (MLP)\n",
            "that takes as input the same pre-computed time series feature representations used in FFORMA\n",
            "and automatically provides sparse convex combinations of forecasting methods. One advantage of\n",
            "this approach is that it eliminates the need to compute forecasts for the excluded methods during\n",
            "the pre-processing stage. This leads to computational savings and improved reliability in real-\n",
            "time applications. However, the obtained results are worse in terms of forecast accuracy than the\n",
            "FFORMA approach by Montero-Manso et al. (2020). To demonstrate the importance of accuracy\n",
            "and diversity when selecting combination methods, Lichtendahl & Winkler (2020) analyzed the\n",
            "M4 competition’s top strategies submitted by Montero-Manso et al. (2020). They used a screen\n",
            "on accuracy to eliminate inaccurate methods and a screen on diversity to eliminate methods with\n",
            "highly dependent forecast errors; they then found that a simple trimmed mean of the subset of\n",
            "surviving methods was nearly as effective as the combination produced by Montero-Manso et al.\n",
            "(2020) through FFORMA. To incorporate diversity, Kang et al. (2022) tailored the state-of-the-art\n",
            "FFORMA framework to allow for the diversity of the forecasts as the inputs. This results in a\n",
            "supervised approach where time series features are extracted by looking at the diversity of forecasts\n",
            "among the methods in the pool. The newly obtained hand-crafted features are then employed to\n",
            "train the weighted combination of forecasting models. The inclusion of diverse methods within the\n",
            "combination scheme enhanced the point forecast accuracy of FFORMA. In contrast to the methods\n",
            "mentioned earlier, there exists a body of research focusing on the use of Deep Neural Networks\n",
            "(DNN). Ma & Fildes (2021) proposed a meta-learning algorithm designed for retail forecasting\n",
            "where time series features are automatically extracted through a Convolutional Neural Network\n",
            "(CNN). These features are then linked with a set of weights which are used to combine the forecast-\n",
            "ing methods. Li et al. (2020) adopted a similar approach where a CNN is employed to learn time\n",
            "series features from recurrence plots. First, time series data is converted into images via recurrence\n",
            "plots. Then, computer vision algorithms are applied to extract features from these recurrence\n",
            "5plots, and these features are further converted into weights for forecasting methods by minimizing\n",
            "the same weighted average loss function used in FFORMA. Table 1 provides a summary for a clear\n",
            "comparison of the recent research studies on time series forecasting with meta-learning.\n",
            "Paper Feature extraction Meta-learner Learning task Diversity\n",
            "Talagala et al. (2018)JudgmentalRF Classification NoUnsupervised\n",
            "Montero-Manso et al. (2020)JudgmentalXGBoost Regression NoUnsupervised\n",
            "Li et al. (2020)AutomaticDNN Regression NoSupervised\n",
            "Lichtendahl & Winkler (2020)JudgmentalXGBoost RegressionYes\n",
            "Unsupervised Post-processing\n",
            "Ma & Fildes (2021)AutomaticDNNRegression orNoSupervised classification\n",
            "Di Gangi (2022)JudgmentalMLPRegression orNoUnsupervised classification\n",
            "Kang et al. (2022)JudgmentalXGBoost RegressionYes\n",
            "Supervised Pre-processing\n",
            "This paperAutomaticDNNRegression and Yes\n",
            "Supervised classification Learned\n",
            "Table 1: Research studies on time series forecasting with meta-learning. In the “Feature extraction” column, the\n",
            "term “judgmental” denotes manually crafted features, while “automatic” signifies features identified by the meta-\n",
            "learner. The “Diversity” column assesses whether the meta-learner accounts for diversity and, if so, details how it is\n",
            "incorporated.\n",
            "Contributions and Outline. In this paper, we introduce a novel framework for combining forecasts.\n",
            "Building upon existing literature, we use meta-learning to optimize the weights of convex combi-\n",
            "nations of forecasting methods. We formulate this optimization problem as a multi-task learning\n",
            "(MTL) problem and propose a tailored deep neural network. MTL is a machine learning paradigm\n",
            "where a single model is trained to perform multiple related tasks simultaneously with the goal\n",
            "of improving the performance of each individual task (Caruana, 1997). Instead of training sepa-\n",
            "rate models for each task, MTL leverages the commonalities and relationships between tasks to\n",
            "enhance overall performance. In our meta-learning approach, we focus on two tasks: regression\n",
            "and classification. On the one hand, the regression task aims to learn the optimal weights of the\n",
            "base forecasting methods by minimizing the error of combined forecasts. On the other hand, the\n",
            "classification task aims to learn and select suitable forecasting methods based on accuracy and\n",
            "diversity among the base forecasters. To perform the two tasks, we design a neural network with\n",
            "two branches. The outputs of these branches are then combined, and the entire network is jointly\n",
            "trained on both tasks by minimizing a custom loss function via gradient descent optimization. We\n",
            "empirically demonstrate the effectiveness of our meta-learning algorithm by testing it on a large\n",
            "6number of series from the M4 competition dataset. As is shown in Table 1, the meta-learning\n",
            "framework proposed in this paper contributes to this stream of literature mainly in three aspects:\n",
            "1.Learning task : The main contribution of this paper is to define a classification problem from\n",
            "the native regression one and highlight the interplay between both classification and regression\n",
            "tasks in improving the accuracy of the forecast combinations through meta-learning. Our\n",
            "multi-task model jointly optimizes the weights used to combine the individual forecasts via\n",
            "regression and performs diversity-based model selection via classification. To tackle the\n",
            "classification task, we introduce a procedure for labeling each series. This involves solving an\n",
            "optimization problem where accuracy and diversity among the base learners are maximized.\n",
            "2.Diversity : In the literature, diversity among the base forecasters has been considered either as\n",
            "a pre-processing or a post-processing step. More in detail, in Lichtendahl & Winkler (2020),\n",
            "diversity is taken into account by using a trimmed mean of the forecasts produced by the set\n",
            "of base forecasting methods identified by FFORMA. In Kang et al. (2022), instead, diversity\n",
            "is encoded though time series features in a pre-processing step and given as input of FFORMA\n",
            "replacing the original hand-selected features identified by Montero-Manso et al. (2020). In\n",
            "our approach, we shift the focus and we propose to learn diversity from the outcome of an\n",
            "optimization model, during training. To some extent, we may say that diversity is learned\n",
            "by reproducing the solution of an optimization problem.\n",
            "3.Feature extraction : Drawing inspiration from deep learning techniques applied to time series\n",
            "data, we use CNNs to learn a supervised feature representation from raw time series automat-\n",
            "ically. Similar methods can be found in existing research, such as the work by Ma & Fildes\n",
            "(2021). Nevertheless, what sets our approach apart is that the learned features are not only\n",
            "associated with forecasting accuracy but also with the diversity of the individual methods.\n",
            "Moreover, to compensate for the reduced interpretability, we leverage gradient-based visual\n",
            "explanations. This technique makes the meta-learning model more transparent and allows\n",
            "the identification of the most discriminative regions in the input data that contributed to the\n",
            "network’s selection of a specific forecasting method in the pool.\n",
            "Using meta-learning, we leverage a set of time series to capture the diversity of their forecasts and\n",
            "determine the combination weights. Once the model is trained, we can calculate such weights for\n",
            "any new series that requires forecasting. Empirically, we show that incorporating diversity into the\n",
            "learning process leads to more accurate and robust point forecasts. The remainder of the paper is\n",
            "organized as follows. Section 3 presents the evaluation metrics and a description of the employed\n",
            "pool of forecasting methods. Section 4 outlines the proposed multi-task learning methodology.\n",
            "Section 5 describes the implementation details. Section 6 describes the experimental validation.\n",
            "Finally, Section 7 concludes the paper with possible future research directions.\n",
            "73. Forecasting methods and metrics\n",
            "This section describes the forecasting pool of methods and the evaluation metrics. We start with\n",
            "some definitions of measures that were used in the M4 competition for evaluating the forecasting\n",
            "accuracy. Throughout the paper, we denote vectors and matrices in bold. Consider a collection\n",
            "ofNunivariate time series\b\n",
            "y1, . . . ,yN\t\n",
            "where each individual time series, denoted as yi=\u0002\n",
            "yi\n",
            "1, . . . , yi\n",
            "Ti\u0003⊤, has length Ti. The value of the i-th time series at time t∈ {1, . . . , T i}is given\n",
            "byyi\n",
            "t. For each time series, the forecasting horizon is denoted as Hiand represents the number\n",
            "of future time steps to predict. The predicted values for the i-th series are denoted as ˆyi=\u0002\n",
            "ˆyi\n",
            "Ti+1, . . . , ˆyi\n",
            "Ti+Hi\u0003⊤. Here, ˆ yi\n",
            "Ti+hrepresents the predicted value of the i-th time series at time\n",
            "h∈ {1, . . . , H i}. The Overall Weighted Average (OWA) score is the measure used in the M4\n",
            "competition to determine the point forecasting winner (Makridakis et al., 2020). It is based on two\n",
            "commonly used forecasting error metrics that are the Symmetric Mean Absolute Percentage Error\n",
            "(SMAPE) and the Mean Absolute Scaled Error (MASE). For forecasts of the i-th time series at\n",
            "various forecast horizons, the SMAPE iis given by\n",
            "SMAPE i= sMAPE( yi,ˆyi) =1\n",
            "HiHiX\n",
            "h=1200·\f\fyi\n",
            "Ti+h−ˆyi\n",
            "Ti+h\f\f\n",
            "\f\f\fyi\n",
            "Ti+h\f\f\f+\f\f\fˆyi\n",
            "Ti+h\f\f\f. (5)\n",
            "The SMAPE is easy to interpret and has an upper bound of 200 when either actual or predicted\n",
            "values are zero or when actual and predicted are opposite signs. The MASE icompares the forecast\n",
            "accuracy between a specific forecast algorithm and the na¨ ıve method where the one-step-ahead\n",
            "forecast is equal to the most recent available observation (Hyndman & Koehler, 2006). It is\n",
            "defined as\n",
            "MASE i= MASE( yi,ˆyi) =1\n",
            "HiPHi\n",
            "t=1\f\fyi\n",
            "Ti+h−ˆyi\n",
            "Ti+h\f\f\n",
            "1\n",
            "Ti−siPTi\n",
            "t=si+1\f\fyi\n",
            "t−yi\n",
            "t−si\f\f, (6)\n",
            "where siis the frequency of the data considered by the organizers, e.g., 12 for monthly, 4 for\n",
            "quarterly, and 1 for yearly series (Makridakis et al., 2020). The numerator is the out-of-sample\n",
            "mean absolute error of the method evaluated across the forecast horizon Hi, and the denominator\n",
            "is the in-sample one-step-ahead na¨ ıve forecast with seasonal period si. The SMAPE iand MASE i\n",
            "can be used to compare forecasting methods on a single series and, because they are scale-free, to\n",
            "compare the forecasting accuracy across series. Then, the OWA measure is given by\n",
            "OWA =1\n",
            "2PN\n",
            "i=1sMAPE( yi,ˆyi)PN\n",
            "i=1sMAPE( yi,ˆzi)+1\n",
            "2PN\n",
            "i=1MASE( yi,ˆyi)PN\n",
            "i=1MASE( yi,ˆzi), (7)\n",
            "where ˆziis the vector obtained with the na¨ ıve method for the i-th time series from one to Histeps\n",
            "ahead (Makridakis et al., 1982). OWA provides a score for the entire collection of time series that\n",
            "may mislead the evaluation. For this reason, SMAPE and MASE can be normalized for each series\n",
            "8in the series-level OWA, introduced by Lichtendahl & Winkler (2020) as:\n",
            "sOWA i=1\n",
            "2sMAPE( yi,ˆyi)\n",
            "sMAPE( yi,ˆzi)+1\n",
            "2MASE( yi,ˆyi)\n",
            "MASE( yi,ˆzi). (8)\n",
            "Note that, having a score for each specific series permits an evaluation of the accuracy risk, which\n",
            "relates to the variability of the accuracy of a forecasting method or combination across the set of\n",
            "series. This offers insights into the potential risk of producing a highly inaccurate forecast for an\n",
            "individual series. We use the nine most popular time series forecasting methods as candidates for\n",
            "forecast combinations, which are also used in recent studies (Montero-Manso et al., 2020; Li et al.,\n",
            "2020; Di Gangi, 2022; Kang et al., 2022). The nine forecasting methods in the pool are described in\n",
            "Table 2 and implemented in the forecast package in R (Hyndman & Khandakar, 2008; Hyndman\n",
            "et al., 2023).\n",
            "Method Description R function\n",
            "ARIMA Automated ARIMA algorithm (Hyndman & Khandakar, 2008) auto.arima()\n",
            "ETS Automated exponential smoothing algorithm (Hyndman et al., 2002) ets()\n",
            "NNETAR Feed-forward neural networks with a single hidden layer and AR inputs nnetar()\n",
            "TBATS TBATS model (De Livera et al., 2011) tbats()\n",
            "STLMSeasonal and trend decomposition using Loess with AR seasonally ad-\n",
            "justed series (Cleveland et al., 1990)stlm(modelfunction=‘ar’)\n",
            "RW Random walk with drift rwf(drift=TRUE)\n",
            "THETATheta method (Assimakopoulos & Nikolopoulos, 2000; Spiliotis et al.,\n",
            "2020)thetaf()\n",
            "NAIVE Na¨ ıve method (forecasting based on the most recent observation) naive()\n",
            "SNAIVESeasonal na¨ ıve method (forecasting based on the most recent seasonal\n",
            "period)snaive()\n",
            "Table 2: The nine individual forecasting methods of the pool, or base learners. The table provides the acronym of\n",
            "the method used throughout the paper (first column), the main reference where the method was introduced (second\n",
            "column), and the R function used for the experiments (third column).\n",
            "4. Meta-learner: regression and classification tasks\n",
            "As anticipated, our meta-learning approach aims to determine a set of weights that combine\n",
            "forecasts generated from a pool of methods, with the goal of exploiting accuracy and diversity\n",
            "among these methods. The meta-learning approach in (Talagala et al., 2018) involves selecting the\n",
            "best method for each series from the pool of methods based on the smallest forecasting error. This\n",
            "transforms the problem into a traditional classification problem, where the individual forecasting\n",
            "methods are encoded as classes and the best method becomes the target class for each time series.\n",
            "However, there might be other methods that yield similar forecast errors to the best method,\n",
            "making the specific class chosen less relevant compared to the forecast error produced by each\n",
            "method. Indeed, the problem of finding a function that assigns weights to each forecasting method\n",
            "is usually framed as a regression task where the objective is to minimize the error of the combined\n",
            "9forecasts. The regression approach can be seen as a classification task with varying per-class\n",
            "weights for each instance, combined with per-instance weights that put greater importance on\n",
            "certain series (Montero-Manso et al., 2020). This implies that the nature of the regression task\n",
            "is closely related to the classification one. Thus, in the following, we describe how to exploit the\n",
            "interplay between these two tasks within a multi-task learning methodology and provide a new\n",
            "meta-learning approach where both tasks are solved simultaneously.\n",
            "As shown in Figure 1, the proposed meta-learning framework consists of two distinct phases:\n",
            "meta-data generation & model training (offline phase), and forecasting (online phase). Each time\n",
            "series is divided into training and testing periods, where the length of the testing period matches\n",
            "its forecasting horizon. For each time series, we fit the forecasting methods in the pool on the\n",
            "training period and extract the forecasts produced by different forecasting methods on the testing\n",
            "period. The forecasts from different methods are gathered into a matrix and then compared with\n",
            "the actual observations of the test period, leading to the matrix of forecasting errors. From this\n",
            "matrix, accuracy and diversity information are summarized as a binary vector of labels by solving\n",
            "an optimization problem. Subsequently, a meta-learner implemented by a deep neural network is\n",
            "trained using gradient descent optimization, minimizing a custom loss function. This step aims\n",
            "to estimate combination weights for each series, allowing the production of weights and hence the\n",
            "combined forecasts for any target series in the online phase.\n",
            "4.1. Neural network design\n",
            "In our multi-task methodology, the meta-learning model is a deep neural network composed of\n",
            "two subnetworks, or branches: the first one solves a regression task while the second one solves a\n",
            "classification task. The goal of the regression task is to learn the weights of the base forecasting\n",
            "methods by minimizing the error of combined forecasts, while the selection of forecasting methods\n",
            "according to the diversity criterion is treated as an auxiliary classification task. The output of both\n",
            "tasks is then combined to obtain the final weights of the convex combination.\n",
            "Extending the notation already used in the previous sections, we consider a collection of Ntime\n",
            "series{s1, . . . ,sN}of length T, a set of Mforecasting methods and a forecasting horizon of length\n",
            "H. Every time series siis split into training xi= [yi\n",
            "1, . . . , yi\n",
            "T]⊤and test yi= [yi\n",
            "T+1, . . . , yi\n",
            "T+H]⊤\n",
            "periods. We denote by ˆFi∈RH×Mthe matrix of forecasts produced by the Mmethods for the\n",
            "entire forecasting horizon H, where ˆFi\n",
            "hmis the h-step ahead forecast produced by the method\n",
            "m∈ {1, . . . , M }for the series i∈ {1, . . . , N }. Let 1Mbe the vector of all ones of length M, then\n",
            "Fi=yi1⊤\n",
            "Mdenotes the matrix where each column is equal to ground-truth observations yi, and\n",
            "Ei=Fi−ˆFiis the matrix of forecasting errors produced by the Mmethods over the forecasting\n",
            "horizon Hfor the series i∈ {1, . . . , N }.\n",
            "The regression subnetwork takes as input raw time series xiof length T, extracts time series\n",
            "features by means of convolutional layers, and returns a set of Mun-normalized weights. We\n",
            "10Figure 1: The metal-learning framework proposed in this paper. On the left the off-line meta-data generation and\n",
            "model training are depicted; on the right the simpler steps that compose the online use of the method to provide\n",
            "forecasts for new series.\n",
            "denote the un-normalized weights estimation model freg:RT→RMas\n",
            "ˆoi\n",
            "reg=freg(xi;θreg).\n",
            "Thus, the function fregis a function parameterized by the subnetwork weights θregwhich first\n",
            "maps a time series xi∈RTto a hidden representation hi\n",
            "reg∈RD, where Dis the dimension of\n",
            "the learned feature vector, and then outputs a set of un-normalized weights ˆoi\n",
            "reg∈RMof the base\n",
            "forecasters for that time series.\n",
            "11We emphasize that our network design choices are guided by the principles of accuracy and\n",
            "diversity. In this context, we present an approach aimed at learning the base models that are\n",
            "most appropriate for forecasting a specific time series. We frame the resulting learning problem\n",
            "as a multi-label classification problem, where the individual forecasting methods are encoded as\n",
            "classes and the most accurate and diverse methods become the target classes for each time series.\n",
            "To generate training labels for the classification task we adapt the Quadratic Programming (QP)\n",
            "feature selection method proposed by Rodriguez-Lujan et al. (2010). Given a set of Mforecasting\n",
            "methods, our goal is to select a subset of them in order to maximize the combination’s performance\n",
            "while satisfying two key requirements: accuracy and diversity among the methods. The resulting\n",
            "optimization problem for the i-th time series can be expressed as:\n",
            "min\n",
            "x1\n",
            "2(1−α)x⊤Qix−αx⊤ci\n",
            "s.t.1⊤\n",
            "Mx= 1,x≥0M.(QP-LAB)\n",
            "where xis an Mdimensional vector representing the relative importance of each method, 1Mis\n",
            "the vector of all ones of size M,Qiis an M×Msymmetric positive semidefinite matrix, which\n",
            "represents the redundancy among the forecasting methods, and ciis an Mdimensional vector of\n",
            "non-negative values, which represents the accuracy of each forecasting methods for the time series\n",
            "i∈ {1, . . . , N }. The diversity extraction procedure is based on the correlation between forecasting\n",
            "methods. More precisely, the pairwise forecast diversity among the Mmethods is evaluated using\n",
            "the Pearson correlation coefficient of the individual methods among their forecasting errors Eiand\n",
            "stored in the matrix Qi. The components of the relevance term ciare computed as the opposite\n",
            "of sOWA iand scaled between 0 and 1. The scalar quantity α∈[0,1] represents the relative\n",
            "importance of non-redundancy amongst the methods and their relevance. Our choice of αaims to\n",
            "achieve equilibrium between the quadratic and linear terms in the objective function. This balance\n",
            "is reached when αsatisfies the equation (1 −α)¯q=α¯c, where ¯ qis the mean value of the elements of\n",
            "the matrix Qiand ¯cis the mean value of the elements of vector ci. Problem (QP-LAB) is convex\n",
            "since the correlation matrix Qiis positive semidefinite. By solving an optimization problem for\n",
            "each time series, we can identify a subset of accurate and diverse forecasting methods that can be\n",
            "combined to produce high-quality forecasts for that series. The label vector oi∈ {0,1}Mfor the\n",
            "i-th time series is then constructed from the optimal solution x⋆as follows:\n",
            "oi\n",
            "j=\n",
            "\n",
            "1,ifx⋆\n",
            "j≥τ\n",
            "0,otherwise∀j∈ {1, . . . , M },\n",
            "where τis a user-defined threshold. Note that, if τis too large, the number of target forecasting\n",
            "methods may decrease up to the point where only one forecasting method is selected. This turns\n",
            "the multi-label classification problem into a conventional single-label one, thereby overlooking the\n",
            "12benefits of forecast combinations. In our experiments, we set τ=1\n",
            "Mas it was observed to represent\n",
            "a good trade-off between the two cases and produce a balanced distribution of labels, with all\n",
            "methods being equally represented. The empirical analysis of this choice will be evident in the\n",
            "computational results. Thus, the auxiliary neural network fcls:RT→[0,1]Msolves a multi-label\n",
            "classification problem by learning the mapping\n",
            "ˆoi\n",
            "cls=fcls(xi;θcls).\n",
            "The function fclsis parameterized by the subnetwork weights θcls. Similarly to the regression\n",
            "subnetwork, it first maps a time series xi∈RTto a time series future vector hi\n",
            "cls∈RDand then\n",
            "outputs a set of predicted labels ˆoi\n",
            "cls∈RM. More in detail, each element in ˆoi\n",
            "clsis a continuous\n",
            "value between 0 and 1 and represents the probability that a method is appropriate for forecasting\n",
            "thei-th time series based on accuracy and diversity principles.\n",
            "Finally, we apply the softmax function to the combined output of the subnetworks which is\n",
            "obtained by multiplying the output of both branches as\n",
            "ˆwi\n",
            "j= softmax j(ˆoi\n",
            "reg⊙ˆoi\n",
            "cls) =exp\u0000\n",
            "(ˆoi\n",
            "reg⊙ˆoi\n",
            "cls)j\u0001\n",
            "PM\n",
            "k=1exp\u0000\n",
            "(ˆoireg⊙ˆoi\n",
            "cls)k\u0001,∀j∈ {1, . . . , M },\n",
            "where ⊙represents the element-wise multiplication. The softmax function maps vectors from the\n",
            "Euclidean space to probability distributions, thus allowing to output the estimated weights ˆwi∈\n",
            "RMof the convex combination for the i-th time series. Note that, the element-wise multiplication\n",
            "enables to share the knowledge between the main regression and the auxiliary classification task.\n",
            "Generally speaking, if the j-th forecasting method is suitable for the i-th time series, i.e., ( ˆoi\n",
            "cls)j= 1,\n",
            "the output of the overall network ˆ wi\n",
            "jwill be large. Similarly, the estimated weight ˆ wi\n",
            "jof the j-th\n",
            "forecasting method should be close to zero if it is not suitable for that time series, i.e., ( ˆoi\n",
            "cls)j= 0.\n",
            "4.2. Loss function design\n",
            "Generally, for optimizing multi-task learning architectures, it is necessary to weigh the im-\n",
            "portance of each task. Specific to our problem, the regression task represents the main forecast\n",
            "combination task and hence should receive more attention, whereas the classification task is treated\n",
            "as an auxiliary task to enhance the performance of the main task. Given this requirement, we now\n",
            "present the components of the loss function used for training the overall model.\n",
            "As pointed out by Ma & Fildes (2021), one limitation of FFORMA is that it focuses on mini-\n",
            "mizing the combined errors of the base forecasters, not the combined forecasts directly, which can\n",
            "lead to suboptimal combinations. Let Θ={θreg, θcls}be the the parameters of the overall neural\n",
            "network. The meta-learner is trained by minimizing the scaled mean squared error with respect to\n",
            "13Θ, which is defined as\n",
            "Lcomb(xi,yi,ˆFi;Θ) =1\n",
            "NNX\n",
            "i=1∥ˆyi−yi∥2\n",
            "2\n",
            "∥¯yi−yi∥2\n",
            "2=1\n",
            "NNX\n",
            "i=1∥ˆFiˆwi−yi∥2\n",
            "2\r\r1\n",
            "MˆFi1M−yi\r\r2\n",
            "2, (9)\n",
            "where ∥ · ∥2\n",
            "2is the squared ℓ2norm, ˆyi=ˆFiˆwi= [ˆyi\n",
            "T+1, . . . , ˆyi\n",
            "T+H]⊤∈RHcontains the weighted\n",
            "combination of the forecasts, ¯yi=1\n",
            "MˆFi1Mis the forecast combination obtained by averaging\n",
            "the forecast produced by the Mmethods. This scaling is useful in training phase to perceive the\n",
            "differences between the performance of forecast combinations provided by the meta-learner and the\n",
            "simple average combinations which represent a strong baseline. Note that, Lcombis jointly decided\n",
            "byˆoi\n",
            "regandˆoi\n",
            "cls. We here perform some additional analysis to better understand the impact of\n",
            "Lcomb on the gradient. To compute the gradient of the combined loss with respect to the k-th\n",
            "parameter Θ k∈Θof the network, we can follow the chain rule of differentiation. We first rewrite\n",
            "the softmax activation function as\n",
            "ˆwi\n",
            "j=exp(ˆzi\n",
            "j)\n",
            "PM\n",
            "t=1exp(ˆzi\n",
            "t),∀j∈ {1, . . . , M },\n",
            "where ˆ zi\n",
            "j= (ˆoi\n",
            "reg⊙ˆoi\n",
            "cls)j. Then, we have\n",
            "∂Lcomb\n",
            "∂Θk=NX\n",
            "i=1∂Li\n",
            "comb\n",
            "∂Θk=NX\n",
            "i=1MX\n",
            "j=1∂Li\n",
            "comb\n",
            "∂ˆwi\n",
            "jMX\n",
            "t=1∂ˆwi\n",
            "j\n",
            "∂ˆzi\n",
            "t·∂ˆzi\n",
            "t\n",
            "∂Θk. (10)\n",
            "One can verify that the derivative terms are given by:\n",
            "∂Li\n",
            "comb\n",
            "∂ˆwi\n",
            "j=2\n",
            "NHX\n",
            "h=1\u0010PM\n",
            "k=1ˆFi\n",
            "hkˆwi\n",
            "k−yi\n",
            "h\u0011\n",
            "ˆFi\n",
            "hj\n",
            "\r\r1\n",
            "MˆFi1M−yi\r\r2\n",
            "2, (11)\n",
            "∂ˆwi\n",
            "j\n",
            "∂ˆzi\n",
            "t= ˆwi\n",
            "j(δjt−ˆwi\n",
            "t), (12)\n",
            "∂ˆzi\n",
            "t\n",
            "∂Θk=∂(ˆoi\n",
            "reg)t\n",
            "∂Θk·(ˆoi\n",
            "cls)t+ (ˆoi\n",
            "reg)t·∂(ˆoi\n",
            "cls)t\n",
            "∂Θk, (13)\n",
            "where δjtis the Kronecker delta function. Therefore, after plugging the above equations in Equation\n",
            "(10), it can be seen that the individual outputs of regression and classification tasks contribute to\n",
            "the parameter updates of the overall network through backpropagation.\n",
            "The classification subnetwork is trained to predict the output labels we are ultimately interested\n",
            "14in and naturally induces the following loss function:\n",
            "Lcls(xi,oi\n",
            "cls;Θ) =−1\n",
            "NNX\n",
            "i=1MX\n",
            "j=1\u0010\n",
            "(oi\n",
            "cls)jlog(ˆoi\n",
            "cls)j+ (1−(oi\n",
            "cls)j) log(1 −(ˆoi\n",
            "cls)j)\u0011\n",
            ", (14)\n",
            "that is the binary cross-entropy loss for a multi-label classification task and should be minimized\n",
            "with respect to the overall neural network weights.\n",
            "For the computation of time series features we consider two distinct feature extractors placed\n",
            "independently through the overall network. One reason to learn distinct feature representations\n",
            "is that a single and shared feature extractor may not have enough expressive power for both\n",
            "tasks (Zhang & Yang, 2021). Another motivation is that our meta-learning model consists of\n",
            "two learners: the regression and the classification subnetworks. After these subnetworks make\n",
            "their respective predictions, their outputs are combined. Consequently, similar to the ideas behind\n",
            "ensemble methods, where combining diverse methods leads to better results, we let these distinct\n",
            "learners to acquire different knowledge. Such considerations motivate the following orthogonality\n",
            "term, which penalizes redundant latent representations and encourages the two feature extractors\n",
            "to encode different aspects of the input time series - an addition whose effectiveness will be made\n",
            "empirically evident later:\n",
            "Lort(xi;Θ) =∥HregH⊤\n",
            "cls∥2\n",
            "F, (15)\n",
            "where ∥ · ∥2\n",
            "Fis the squared Frobenius norm, Hreg∈RN×DandHcls∈RN×Dare two matrices,\n",
            "whose rows are the output of the task-specific feature extractors of an input time series. This loss\n",
            "is added to the overall training objective, in order to encourage the task-specific features to be\n",
            "orthogonal.\n",
            "The overall cost function is given by the sum of the cost functions of the main task and the\n",
            "auxiliary task. The goal of training is to minimize the following loss with respect to the network’s\n",
            "parameters Θ:\n",
            "L(xi,yi,ˆFi,oi\n",
            "cls;Θ) =Lcomb(xi,yi,ˆFi;Θ) +Lcls(xi,oi\n",
            "cls;Θ) +λLort(xi;Θ) (16)\n",
            "=1\n",
            "NNX\n",
            "i=1∥ˆFiˆwi−yi∥2\n",
            "2\r\r1\n",
            "MˆFi1M−yi\r\r2\n",
            "2(17)\n",
            "−1\n",
            "NNX\n",
            "i=1MX\n",
            "j=1\u0010\n",
            "(oi\n",
            "cls)jlog(ˆoi\n",
            "cls)j+ (1−(oi\n",
            "cls)j) log(1 −(ˆoi\n",
            "cls)j)\u0011\n",
            "(18)\n",
            "+λ∥HregH⊤\n",
            "cls∥2\n",
            "F, (19)\n",
            "where λis a hyperparameter chosen by cross-validation. The overall meta-learning approach is\n",
            "shown in Algorithm 1.\n",
            "15Algorithm 1: Forecast combination based on multi-task learning (DNN-MTL)\n",
            "/* OFFLINE PHASE: BUILD THE METADATA AND TRAIN THE LEARNING MODEL */\n",
            "Input Dataset of Ntime series {s1, . . . ,sN}, set of Mmethods, forecasting horizon H.\n",
            "Output A function fmetafrom a time series to a set of Mweights, one for each method.\n",
            "fori= 1 : Ndo\n",
            "1. Split siinto training xi= [yi\n",
            "1, . . . , yi\n",
            "T]⊤and test yi= [yi\n",
            "T+1, . . . , yi\n",
            "T+H]⊤periods.\n",
            "2. Fit each base forecasting method over the training period xiand generate the matrix of\n",
            "forecasts ˆFi∈RH×Mover the test period yi.\n",
            "3. Extract the diversity matrix Qi∈RM×Mand the relevance vector ci∈RMover the test\n",
            "period yifrom the matrix of forecasting errors Ei.\n",
            "4. Given Qiandci, solve Problem (QP-LAB) and construct the label vector oi\n",
            "clsindicating\n",
            "the most accurate and diverse base forecasters.\n",
            "end\n",
            "Train a learning model fmeta:RT→RMon the meta-data by solving\n",
            "min\n",
            "ΘLcomb(xi,yi,ˆFi;Θ) +Lcls(xi,oi\n",
            "cls;Θ) +λLort(xi;Θ)\n",
            "/* ONLINE PHASE: FORECAST NEW TIME SERIES */\n",
            "Input Trained meta-learner fmeta, dataset of Ktime series {˜s1, . . . , ˜sK}.\n",
            "Output Combined forecast for each time series.\n",
            "fori= 1 : Kdo\n",
            "1. Use the meta-learner to produce the vector of weights ˜wi=fmeta(˜si).\n",
            "2. Compute the individual forecasts of the Mmethods in the pool and store them in the\n",
            "forecasting matrix ˜Fi∈RH×M.\n",
            "3. Compute the convex combination as ˜yi=˜Fi˜wiand output the forecasts ˜yi.\n",
            "end\n",
            "4.3. Neural network architecture\n",
            "In this section, we describe the architectural components of the deep neural network. Our\n",
            "network is built on CNNs, which employ convolution operations to automatically extract features.\n",
            "They are comprised of a series of convolution layers and nonlinear activation functions, which are\n",
            "trained to identify valuable and complex features within the input data (Kraus et al., 2020). As\n",
            "a result, CNNs are commonly used in various computer vision tasks, including object detection,\n",
            "image segmentation, and image classification (Li et al., 2021). However, CNNs’ advantages are\n",
            "16not limited to visual data alone, as they are also used in applications that handle one-dimensional\n",
            "sequential data, such as time-series forecasting (Liu et al., 2019; Semenoglou et al., 2023).\n",
            "Since the CNN architecture itself is not the main focus of our proposal, we employ for both\n",
            "subnetworks the same CNN architecture used by Ma & Fildes (2021). To extract features, each\n",
            "subnetwork consists of three stacked temporal convolutional blocks. Each convolutional block\n",
            "comprises a convolutional layer and a ReLU activation function. Furthermore, convolutional blocks\n",
            "incorporate a squeeze and excite layer (Hu et al., 2018), which uses global average pooling to\n",
            "generate summary statistics over the learned feature map, capturing contextual information outside\n",
            "each filter’s focused feature of the input series. The excite operation in the squeeze and excite blocks\n",
            "introduces dynamic dependencies among the learned features, allowing the network to assign more\n",
            "importance to relevant features when needed. The filters of the three convolutional layers in\n",
            "each subnetwork are set to 64, 128, and 64 respectively, whereas the kernel sizes are set to 2,\n",
            "4, and 8. As we can see in Figure 2, our architecture consists of two identical and independent\n",
            "feature extractors with task-specific output branches. In both subnetworks, the last temporal\n",
            "convolutional block is followed by a global average pooling layer to reduce network parameters. In\n",
            "the classification subnetwork, a dense layer with a sigmoid activation function is used to convert the\n",
            "learned features into a set of labels, where the dimension matches the number of base forecasters.\n",
            "In contrast, the regression subnetwork outputs a set of unnormalized weights, also through a dense\n",
            "layer with linear activation function, with a dimension equal to the number of base forecasters. To\n",
            "effectively leverage information between tasks, the unnormalized weights of the convex combination\n",
            "are then element-wise multiplied with the labels learned by the auxiliary task. The classification\n",
            "task allows the network to emphasize forecasting methods that are more accurate and diverse\n",
            "for the corresponding task, and downplay the effect of inaccurate and highly correlated ones.\n",
            "Both branches are then followed by a softmax layer that is trained to predict the final weighted\n",
            "combination of the forecasting methods.\n",
            "5. Experimental setup\n",
            "In line with the existing research, we use the M4 competition dataset to evaluate the forecasting\n",
            "accuracy of the proposed methodology. This dataset includes 100,000 time series of varying fre-\n",
            "quencies and is publicly available in the “M4comp2018” R package (Montero-Manso et al., 2019).\n",
            "We focus on the yearly, quarterly, and monthly series which represent 95% of the competition’s\n",
            "series. At the conclusion of the competition, each time series in the M4 dataset is already split into\n",
            "two parts: a training period (historical data) and a testing period (future data). The observations\n",
            "from the testing period are exclusively used to evaluate the forecasts generated by the trained\n",
            "meta-learner. Therefore, the only information known by the meta-leaner during training time is\n",
            "the training part of the M4 dataset. The yearly subset includes 23,000 series with lengths ranging\n",
            "from 13 to 835 observations and with forecast horizons of 6 periods. The quarterly subset con-\n",
            "17Figure 2: The network architecture designed to output the weights of the convex combination of base forecasting\n",
            "models for an input time series. In each convolutional layer, “F” and “K” denote the number of filters and the kernel\n",
            "size, respectively.\n",
            "sists of 24,000 series with 8 forecast horizons and the series length ranges from 16 to 866 periods.\n",
            "Finally, the monthly subset contains 48,000 time series with a forecasting horizon of 18 periods\n",
            "ranging from 42 to 2,794 sample observations. For each frequency, we use the corresponding M4\n",
            "series to create the input data and we optimize the combination weights by training a distinct\n",
            "meta-learner for each group of series. Before feeding data to the neural network, all the input\n",
            "time series are standardized so that the mean of observed values is 0 and the standard deviation\n",
            "is 1. To ensure that all the input time series of a given frequency have the same length, we pad\n",
            "and truncate the data when needed. To determine this target length, a statistical analysis of\n",
            "lengths is carried out for each type of time series. More in detail, for series with a length shorter\n",
            "than the target length we apply a pre-padding operation, which involves adding the value 0 at\n",
            "the beginning of the series until the defined length is reached. Conversely, for time series with a\n",
            "longer length, a truncation operation is performed, removing observations from the beginning to\n",
            "achieve the fixed length. As a result, for yearly, quarterly, and monthly time series we consider\n",
            "lengths of 32, 64, and 128 observations, respectively. Adam optimizer (Kingma & Ba, 2014) with\n",
            "18an initial learning rate set to 0.001 and a batch size of 32 series is used to minimize the custom\n",
            "loss function. Furthermore, a grid search cross-validation strategy has been set up to validate\n",
            "the hyperparameter λ∈ {1×10−3,5×10−3, . . . , 1×10−1,5×10−1}in the loss function. As\n",
            "a result, the best λvalues are 1 ×10−1, 5×10−3, 1×10−2for yearly, quarterly, and monthly\n",
            "time series, respectively. The neural network is written in Python 3.9 and implemented with Ten-\n",
            "sorFlow 2.12. QP programs are solved using “quadprog” package in R. All the experiments are\n",
            "performed on a laptop with an Intel(R) Core(TM) i7-8565U processor clocked at 1.80GHz with 4\n",
            "cores, 16 GB of RAM and Ubuntu 20.04 LTS. Finally, to enhance the reproducibility of our pro-\n",
            "posed meta-learner, we have made the corresponding source code available through the following\n",
            "link: https://github.com/antoniosudoso/mtl-comb .\n",
            "6. Experimental results\n",
            "In this section, we first provide an analysis of the distribution of the base learners within the\n",
            "classification datasets. Then, we compare our approach against state-of-the-art algorithms based\n",
            "on meta-learning. Finally, we provide visual explanations showing the discriminative regions in the\n",
            "input data that contribute to the network’s selection of a specific forecasting method in the pool.\n",
            "6.1. Distribution of methods within the training dataset\n",
            "To evaluate the level of class imbalance of the training data for the classification task we\n",
            "examine the distribution of base forecasting methods across the labels. These labels are generated\n",
            "for the training series in the offline phase by solving (QP-LAB) and then rounding the solution with\n",
            "a threshold τ. We empirically find that setting τ=1\n",
            "Myields satisfactory results in terms of the\n",
            "balance among the methods of the pool. To illustrate this, we compute the relative presence of each\n",
            "base learner and show these statistics in Figure 3. Each chart represents a set of time series grouped\n",
            "by frequency. The horizontal axis lists the individual methods, while the vertical bars depict the\n",
            "ratio between the number of series having the corresponding forecasting method in the label and the\n",
            "total number of series in that particular group. For each group of series, a consistent and nearly\n",
            "uniform distribution of labels is observed, with each method being almost equally represented.\n",
            "Such evidence indicates that there is no discernible class imbalance issue. Consequently, our\n",
            "classification models will not exhibit a bias toward any particular method during training, thereby\n",
            "enhancing their ability to generalize effectively across the spectrum of base forecasters. In the\n",
            "general case, if some base forecasters are not sufficiently represented, one can adjust the threshold\n",
            "τto reach a sufficiently balanced distribution of labels, or employ various balancing techniques,\n",
            "such as resampling methods or cost-sensitive loss functions (Charte et al., 2015; Tarekegn et al.,\n",
            "2021).\n",
            "19Figure 3: Relative presence of each forecasting method across the generated classification datasets. Base forecasters\n",
            "are listed horizontally.\n",
            "6.2. Comparison with benchmark methods\n",
            "We compare the point forecast performance of the proposed multi-task forecast combination\n",
            "approach (DNN-MTL) against the following benchmark methods:\n",
            "•The simple average approach, where the forecasts from all nine methods in the forecasting\n",
            "pool are combined with equal weights (AVERAGE).\n",
            "•The meta-learner introduced by (Montero-Manso et al., 2020) that uses XGBoost to link\n",
            "hand-crafted statistical time series features with forecasting errors (FFORMA).\n",
            "•The recent method proposed by Kang et al. (2022) that employs XGBoost to connect\n",
            "diversity-based time series features with forecasting errors (FFORMA-DIV).\n",
            "Additionally, we perform ablation studies on DNN-MTL to evaluate the individual contributions of\n",
            "each introduced component. In the first ablation experiment, to assess the impact of the classifica-\n",
            "tion task on promoting diversity among the base learners, we remove the classification branch from\n",
            "the neural network. This transforms our meta-learner into a single-task regression model with a\n",
            "convolutional feature extractor, aligning with the approach presented by Ma & Fildes (2021). We\n",
            "denote this modified meta-learner as DNN-REG. In the second ablation experiment, we focus on\n",
            "exploring the impact of the hyperparameter λon the loss function. This hyperparameter modu-\n",
            "lates the contribution to the objective function of the orthogonality among the features extracted\n",
            "by the regression subnetwork and those extracted by the classification subnetwork. For this study,\n",
            "we set λto 0 so that its contribution to the loss function is excluded. This meta-learner is denoted\n",
            "as DNN-MTL (λ=0)and is trained and evaluated under the same experimental conditions as the\n",
            "previous experiments.\n",
            "We assess the forecasting performance on the M4 test dataset by using two error metrics:\n",
            "the overall weighted average (OWA) and the mean series-level overall weighted average (MsOWA).\n",
            "20Lower values for both OWA and MsOWA indicate better forecasting accuracy. Moreover, we include\n",
            "the standard deviation (SD) to provide insight into the variability of accuracy across the series, of-\n",
            "fering a perspective on accuracy risk (Lichtendahl & Winkler, 2020). We present numerical results\n",
            "for each group of series in Table 3. Our meta-learner DNN-MTL consistently outperforms other\n",
            "approaches in terms of OWA and MsOWA, surpassing the state-of-the-art meta-learner FFORMA-\n",
            "DIV, which also incorporates diversity information. In contrast, the simple average combination\n",
            "consistently falls short when compared to all the meta-learners. Furthermore, our first ablation\n",
            "experiment reveals valuable observations. While the meta-learner with the regression branch alone\n",
            "(DNN-REG) can produce reasonably accurate forecasts, being competitive with FFORMA, the\n",
            "removal of the classification branch strongly worsens OWA and MsOWA metrics. This decrease\n",
            "in forecasting accuracy can be attributed to the absence of diversity information among the base\n",
            "learners. The second ablation experiment underscores the importance of the orthogonality term in\n",
            "the loss function. Notably, setting λto zero negatively impacts the model’s performance. This ob-\n",
            "servation indicates that incorporating orthogonality empirically leads to improved results, enabling\n",
            "the subnetworks to better exploit task-specific time series features. Another noteworthy finding is\n",
            "that our meta-learner consistently provides forecasts with lower accuracy risk, as measured by SD,\n",
            "when compared to other methods.\n",
            "To formally test whether the performances among the considered methods are statistically\n",
            "different, we employ the non-parametric Friedman test and post-hoc Multiple Comparisons with\n",
            "the Best (MCB) Nemenyi test (Koning et al., 2005), as implemented in the R package “tsutils”\n",
            "(Kourentzes, 2022). As outlined by Kourentzes & Athanasopoulos (2019), the Friedman test ini-\n",
            "tially assesses whether at least one of the methods significantly differs from the others. If such\n",
            "a difference exists, the Nemenyi test is applied to identify groups of forecasting methods that do\n",
            "not exhibit statistically significant differences. This testing approach offers the advantage of not\n",
            "imposing any assumptions about the distribution of data and avoids the need for multiple pairwise\n",
            "comparisons between forecasts, which could bias the test results. We apply these tests on each\n",
            "data frequency based on the sOWA errors as shown in Figure 4. One can interpret the results in\n",
            "the following manner: lower average ranks indicate better performance, but there are no significant\n",
            "performance differences between any two methods if their confidence intervals overlap. According\n",
            "to Figure 4 we observe that: (i) although FFORMA-DIV outperforms FFORMA on average, their\n",
            "differences are not statistically significant. These findings align with those documented in the re-\n",
            "cent study by Kang et al. (2022); (ii) our method, DNN-MTL, takes the top position and generates\n",
            "forecasts that exhibit statistically significant differences when compared to those produced by other\n",
            "meta-learners, except in the case of monthly series, where the prediction intervals of DNN-MTL\n",
            "and DNN-MTL (λ=0)slightly overlap.\n",
            "21Yearly Quarterly Monthly\n",
            "Method OWA MsOWA (SD)OWA MsOWA (SD)OWA MsOWA (SD)\n",
            "AVERAGE 0.949 1.204 (0.729) 0.916 0.955 (0.476) 0.911 0.952 (0.374)\n",
            "FFORMA 0.799 0.996 (0.866) 0.847 0.910 (0.542) 0.858 0.905 (0.384)\n",
            "FFORMA-DIV 0.798 0.979 (0.738) 0.842 0.899 (0.465) 0.851 0.904 (0.377)\n",
            "DNN-REG 0.801 0.896 (0.565) 0.845 0.902 (0.497) 0.862 0.909 (0.476)\n",
            "DNN-MTL (λ=0) 0.798 0.879 (0.401) 0.839 0.891 (0.482) 0.849 0.898 (0.338)\n",
            "DNN-MTL 0.792 0.866 (0.261) 0.830 0.873 (0.343) 0.841 0.886 (0.229)\n",
            "Table 3: Test set evaluation metrics for yearly, quarterly, and monthly time series. OWA, mean (M) of sOWA, and\n",
            "standard deviation (SD) of sOWA for each combination method. Lower OWA and MsOWA correspond to better\n",
            "forecasting accuracy. The best-performing method is highlighted in bold.\n",
            "Figure 4: MCB Nemenyi test results, average ranks, and 95% confidence intervals for yearly, quarterly, and monthly\n",
            "time series. Forecast combination methods are sorted vertically according to the sOWA mean rank. The mean rank\n",
            "of each approach is shown to the right of its name. Statistical differences in performance are observed if the intervals\n",
            "of two forecast combination procedures do not overlap.\n",
            "226.3. Visual explanations\n",
            "After training, the classification subnetwork can be disconnected and used to predict, for an\n",
            "input time series, the probability that a forecasting method aligns with the principles of accuracy\n",
            "and diversity. This network extracts relevant features from the series by means of convolutional\n",
            "layers and subsequently maps these features to a vector of labels, where each label corresponds to\n",
            "a forecasting method. However, the interpretability of the features derived from CNNs frequently\n",
            "presents difficulties. To address this limitation and provide insights into the decision-making pro-\n",
            "cess of the meta-learner, we employ Gradient-weighted Class Activation Mapping (Grad-CAM)\n",
            "(Selvaraju et al., 2017). Grad-CAM is a method that enhances the interpretability of deep neu-\n",
            "ral networks by producing visual heatmaps highlighting the regions of the input that significantly\n",
            "influence the network’s decision for a particular class. This technique leverages the gradients of\n",
            "the predicted class score by the model with respect to the feature maps of the last convolutional\n",
            "layer. Specific to our problem, by computing the weighted sum of these gradients, we can identify\n",
            "the most discriminative regions in the input data that contributed to the network’s selection of a\n",
            "specific forecasting method in the pool. Moreover, by overlaying the Grad-CAM heatmaps onto\n",
            "the original time series, we can visually understand the network’s reasoning, aiding domain experts\n",
            "in comprehending the model’s decision-making process. We consider a threshold of 0.5 to convert\n",
            "each predicted probability into a 0-1 binary label from which the gradient of the predicted class\n",
            "score is computed. In Figures 5, 6, and 7, for each base forecasting method selected by the network,\n",
            "we analyze the heatmap produced by applying Grad-CAM on a sample of yearly, quarterly, and\n",
            "monthly time series, respectively. At each timestep, an importance score ranging from 0 to 1 is as-\n",
            "signed. A value close to 1 means high significance of the corresponding timestep, while values near\n",
            "0 indicate timesteps with low importance for the classification outcome. Looking at the figures,\n",
            "the heatmaps exhibit diversity in terms of the temporal regions they focus on within the input\n",
            "time series. This finding implies that the neural network is not merely relying on a single common\n",
            "pattern present in the time series but is considering multiple relevant segments. Furthermore, these\n",
            "distinctive areas of interest indicate that the network is leveraging different features for different\n",
            "methods, likely reflecting the unique characteristics and strengths of each base forecaster. Finally,\n",
            "it is important to note that while the network is trained to learn both accurate and diverse meth-\n",
            "ods, certain regions with high heatmap values appear to be shared among multiple methods. These\n",
            "common regions indicate the particular temporal segments that hold significance in the context of\n",
            "more than one forecasting technique. This phenomenon indicates the presence of features in the\n",
            "input time series that possess inherent characteristics that are beneficial for multiple forecasting\n",
            "methods.\n",
            "23Figure 5: Grad-CAM visual explanations of the predicted base learners for a sample of 4 yearly test series. Time series\n",
            "have been normalized to zero mean and unit variance. For an input series, there is a heatmap for each forecasting\n",
            "method selected by the classification subnetwork.\n",
            "24Figure 6: Grad-CAM visual explanations of the predicted base learners for a sample of 4 quarterly test series.\n",
            "Time series have been normalized to zero mean and unit variance. For an input series, there is a heatmap for each\n",
            "forecasting method selected by the classification subnetwork.\n",
            "25Figure 7: Grad-CAM visual explanations of the predicted base learners for a sample of 4 monthly test series. Time\n",
            "series have been normalized to zero mean and unit variance. For an input series, there is a heatmap for each\n",
            "forecasting method selected by the classification subnetwork.\n",
            "267. Conclusion\n",
            "We present a multi-task learning methodology to improve the forecasting performance of con-\n",
            "vex combinations of forecasting models. Building on the literature, we use meta-learning to link\n",
            "the features of time series with the forecasts provided by a pool of base learners. Our meta-learner\n",
            "simultaneously addresses two related tasks: combining forecasts based on their accuracy and select-\n",
            "ing models based on their diversity. To accomplish this, we employ a deep neural network to jointly\n",
            "solve the associated regression and classification problems. To provide labels for the classification\n",
            "task, we introduce an optimization-driven approach to identify the most appropriate method for a\n",
            "given time series, considering accuracy and diversity among the methods in the pool. Experimen-\n",
            "tal results on the M4 competition dataset demonstrate that this approach enhances the accuracy\n",
            "of point forecasts compared to state-of-the-art meta-learning methods. Moreover, gradient-based\n",
            "visual explanations provide interesting insights into discriminative regions in the input series that\n",
            "contribute to the network’s selection of a specific forecasting method. Our approach presents an\n",
            "automated and adaptable tool for optimizing forecasting procedures, featuring the following key\n",
            "advantages. First, it relieves forecasters from the burden of filtering the methods according to\n",
            "diversity and accuracy criteria when conducting feature-based forecasting, as it effectively learns\n",
            "diversity information during training. Second, it can accommodate forecasts from a variety of\n",
            "methods, including statistical techniques, nonlinear approaches, and judgment-based forecasting.\n",
            "As a drawback, the proposed method is limited to producing only point forecasts; this motivates\n",
            "future research in the direction of adapting it to output interval forecasts. Another potential di-\n",
            "rection for future research is the extension to probability density forecasting, as described in the\n",
            "work by Hall & Mitchell (2007). In this context, meta-learning could play a role in generating\n",
            "a weighted combination of forecast distributions from various models or in creating a weighted\n",
            "average of these distributions.\n",
            "Acknowledgments\n",
            "The work presented in this paper has been supported by PNRR MUR project PE0000013-FAIR\n",
            "and CNR DIT.AD106.097 project UISH - Urban Intelligence Science Hub.\n",
            "Declaration of interest\n",
            "The authors declare that they have no known competing financial interests or personal rela-\n",
            "tionships that could have appeared to influence the work reported in this paper.\n",
            "References\n",
            "Andrawis, R. R., Atiya, A. F., & El-Shishiny, H. (2011). Combination of long term and short term forecasts, with\n",
            "application to tourism demand forecasting. International Journal of Forecasting ,27, 870–886.\n",
            "27Armstrong, J. S. (2001). Combining forecasts. In Principles of Forecasting: A Handbook for Researchers and\n",
            "Practitioners (pp. 417–439). Boston, MA: Springer US.\n",
            "Assimakopoulos, V., & Nikolopoulos, K. (2000). The theta model: a decomposition approach to forecasting. Inter-\n",
            "national Journal of Forecasting ,16, 521–530.\n",
            "Atiya, A. F. (2020). Why does forecast combination work so well? International Journal of Forecasting ,36, 197–200.\n",
            "Bates, J. M., & Granger, C. W. (1969). The combination of forecasts. Journal of the Operational Research Society ,\n",
            "20, 451–468.\n",
            "Bunn, D. W. (1988). Combining forecasts. European Journal of Operational Research ,33, 223–229.\n",
            "Cang, S., & Yu, H. (2014). A combination selection algorithm on forecasting. European Journal of Operational\n",
            "Research ,234, 127–139.\n",
            "Caruana, R. (1997). Multitask learning. Machine Learning ,28, 41–75.\n",
            "Charte, F., Rivera, A. J., del Jesus, M. J., & Herrera, F. (2015). Addressing imbalance in multilabel classification:\n",
            "Measures and random resampling algorithms. Neurocomputing ,163, 3–16.\n",
            "Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM\n",
            "SIGKDD International Conference on Knowledge Discovery and Data Mining KDD ’16 (p. 785–794). New York,\n",
            "NY, USA: Association for Computing Machinery.\n",
            "Clemen, R. T. (1989). Combining forecasts: A review and annotated bibliography. International Journal of Fore-\n",
            "casting ,5, 559–583.\n",
            "Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). STL: A seasonal-trend decomposition.\n",
            "Journal of Official Statistics ,6, 3–73.\n",
            "De Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series with complex seasonal patterns\n",
            "using exponential smoothing. Journal of the American Statistical Association ,106, 1513–1527.\n",
            "De Menezes, L. M., Bunn, D. W., & Taylor, J. W. (2000). Review of guidelines for the use of combined forecasts.\n",
            "European Journal of Operational Research ,120, 190–204.\n",
            "Di Gangi, L. (2022). Sparse convex combinations of forecasting models by meta learning. Expert Systems with\n",
            "Applications ,200, 116938.\n",
            "Fildes, R., Nikolopoulos, K., Crone, S. F., & Syntetos, A. A. (2008). Forecasting and operational research: a review.\n",
            "Journal of the Operational Research Society ,59, 1150–1172.\n",
            "Hall, S. G., & Mitchell, J. (2007). Combining density forecasts. International Journal of Forecasting ,23, 1–13.\n",
            "Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on\n",
            "Computer Vision and Pattern Recognition (pp. 7132–7141).\n",
            "Hyndman, R., Athanasopoulos, G., Bergmeir, C., Caceres, G., Chhay, L., O’Hara-Wild, M., Petropoulos, F., Razbash,\n",
            "S., Wang, E., & Yasmeen, F. (2023). forecast: Forecasting functions for time series and linear models . URL:\n",
            "https://pkg.robjhyndman.com/forecast/ R package version 8.21.\n",
            "Hyndman, R. J. (2020). A brief history of forecasting competitions. International Journal of Forecasting ,36, 7–14.\n",
            "Hyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: the forecast package for R. Journal of\n",
            "Statistical Software ,27, 1–22.\n",
            "Hyndman, R. J., & Koehler, A. B. (2006). Another look at measures of forecast accuracy. International Journal of\n",
            "Forecasting ,22, 679–688.\n",
            "Hyndman, R. J., Koehler, A. B., Snyder, R. D., & Grose, S. (2002). A state space framework for automatic forecasting\n",
            "using exponential smoothing methods. International Journal of Forecasting ,18, 439–454.\n",
            "Jose, V. R. R., & Winkler, R. L. (2008). Simple robust averages of forecasts: Some empirical results. International\n",
            "Journal of Forecasting ,24, 163–169.\n",
            "Kang, Y., Cao, W., Petropoulos, F., & Li, F. (2022). Forecast with forecasts: Diversity matters. European Journal\n",
            "of Operational Research ,301, 180–190.\n",
            "Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , .\n",
            "28Koning, A. J., Franses, P. H., Hibon, M., & Stekler, H. O. (2005). The M3 competition: Statistical tests of the\n",
            "results. International Journal of Forecasting ,21, 397–409.\n",
            "Kourentzes, N. (2022). tsutils: Time Series Exploration, Modelling and Forecasting . URL: https://CRAN.R-project.\n",
            "org/package=tsutils r package version 0.9.3.\n",
            "Kourentzes, N., & Athanasopoulos, G. (2019). Cross-temporal coherent forecasts for australian tourism. Annals of\n",
            "Tourism Research ,75, 393–409.\n",
            "Kourentzes, N., Barrow, D., & Petropoulos, F. (2019). Another look at forecast selection and combination: Evidence\n",
            "from forecast pooling. International Journal of Production Economics ,209, 226–235.\n",
            "Kraus, M., Feuerriegel, S., & Oztekin, A. (2020). Deep learning in business analytics and operations research: Models,\n",
            "applications and managerial implications. European Journal of Operational Research ,281, 628–641.\n",
            "Krogh, A., & Vedelsby, J. (1994). Neural network ensembles, cross validation, and active learning. Advances in\n",
            "Neural Information Processing Systems ,7.\n",
            "Lemke, C., & Gabrys, B. (2010). Meta-learning for time series forecasting and forecast combination. Neurocomputing ,\n",
            "73, 2006–2016.\n",
            "Li, X., Kang, Y., & Li, F. (2020). Forecasting with time series imaging. Expert Systems with Applications ,160,\n",
            "113680.\n",
            "Li, Z., Liu, F., Yang, W., Peng, S., & Zhou, J. (2021). A survey of convolutional neural networks: analysis,\n",
            "applications, and prospects. IEEE Transactions on Neural Networks and Learning Systems , .\n",
            "Lichtendahl, K. C., & Winkler, R. L. (2020). Why do some combinations perform better than others? International\n",
            "Journal of Forecasting ,36, 142–149.\n",
            "Liu, S., Ji, H., & Wang, M. C. (2019). Nonpooling convolutional neural network forecasting for seasonal time series\n",
            "with trends. IEEE Transactions on Neural Networks and Learning Systems ,31, 2879–2888.\n",
            "Ma, S., & Fildes, R. (2021). Retail sales forecasting with meta-learning. European Journal of Operational Research ,\n",
            "288, 111–128.\n",
            "Makridakis, S., Andersen, A., Carbone, R., Fildes, R., Hibon, M., Lewandowski, R., Newton, J., Parzen, E., &\n",
            "Winkler, R. (1982). The accuracy of extrapolation (time series) methods: Results of a forecasting competition.\n",
            "Journal of Forecasting ,1, 111–153.\n",
            "Makridakis, S., Spiliotis, E., & Assimakopoulos, V. (2020). The m4 competition: 100,000 time series and 61 fore-\n",
            "casting methods. International Journal of Forecasting ,36, 54–74.\n",
            "Mancuso, P., Piccialli, V., & Sudoso, A. M. (2021). A machine learning approach for forecasting hierarchical time\n",
            "series. Expert Systems with Applications ,182, 115102.\n",
            "Montero-Manso, P., Athanasopoulos, G., Hyndman, R. J., & Talagala, T. S. (2020). Fforma: Feature-based forecast\n",
            "model averaging. International Journal of Forecasting ,36, 86–92.\n",
            "Montero-Manso, P., Netto, C., & Talagala, T. (2019). M4comp2018: Data from the M4-Competition . R package\n",
            "version 0.2.0.\n",
            "Nikolopoulos, K. (2021). We need to talk about intermittent demand forecasting. European Journal of Operational\n",
            "Research ,291, 549–559.\n",
            "Petropoulos, F., Apiletti, D., Assimakopoulos, V., Babai, M. Z., Barrow, D. K., Taieb, S. B., Bergmeir, C., Bessa,\n",
            "R. J., Bijak, J., Boylan, J. E. et al. (2022). Forecasting: theory and practice. International Journal of Forecasting ,\n",
            "38, 705–871.\n",
            "Petropoulos, F., Hyndman, R. J., & Bergmeir, C. (2018). Exploring the sources of uncertainty: Why does bagging\n",
            "for time series forecasting work? European Journal of Operational Research ,268, 545–554.\n",
            "Prudˆ encio, R. B., & Ludermir, T. B. (2004). Meta-learning approaches to selecting time series models. Neurocom-\n",
            "puting ,61, 121–137.\n",
            "Rodriguez-Lujan, I., Elkan, C., Santa Cruz Fern´ andez, C., Huerta, R. et al. (2010). Quadratic programming feature\n",
            "selection. Journal of Machine Learning Research , .\n",
            "29Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explana-\n",
            "tions from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on\n",
            "computer vision (pp. 618–626).\n",
            "Semenoglou, A.-A., Spiliotis, E., & Assimakopoulos, V. (2023). Image-based time series forecasting: A deep convo-\n",
            "lutional neural network approach. Neural Networks ,157, 39–53.\n",
            "Spiliotis, E., Assimakopoulos, V., & Makridakis, S. (2020). Generalizing the theta method for automatic forecasting.\n",
            "European Journal of Operational Research ,284, 550–558.\n",
            "Syntetos, A. A., Babai, Z., Boylan, J. E., Kolassa, S., & Nikolopoulos, K. (2016). Supply chain forecasting: Theory,\n",
            "practice, their gap and the future. European Journal of Operational Research ,252, 1–26.\n",
            "Syntetos, A. A., Boylan, J. E., & Disney, S. M. (2009). Forecasting for inventory planning: a 50-year review. Journal\n",
            "of the Operational Research Society ,60, S149–S160.\n",
            "Talagala, T. S., Hyndman, R. J., Athanasopoulos, G. et al. (2018). Meta-learning how to forecast time series. Monash\n",
            "Econometrics and Business Statistics Working Papers ,6, 16.\n",
            "Tarekegn, A. N., Giacobini, M., & Michalak, K. (2021). A review of methods for imbalanced multi-label classification.\n",
            "Pattern Recognition ,118, 107965.\n",
            "Taylor, J. W. (2017). Probabilistic forecasting of wind power ramp events using autoregressive logit models. European\n",
            "Journal of Operational Research ,259, 703–712.\n",
            "Timmermann, A. (2006). Forecast combinations. Handbook of Economic Forecasting ,1, 135–196.\n",
            "Tung, H. K., & Wong, M. C. (2009). Financial risk forecasting with nonlinear dynamics and support vector regression.\n",
            "Journal of the operational research society ,60, 685–695.\n",
            "Winkler, R. L., & Clemen, R. T. (1992). Sensitivity of weights in combining forecasts. Operations Research ,40,\n",
            "609–614.\n",
            "Zhang, Y., & Yang, Q. (2021). A survey on multi-task learning. IEEE Transactions on Knowledge and Data\n",
            "Engineering ,34, 5586–5609.\n",
            "30GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning\n",
            "Adrian Cosma\n",
            "University Politehnica of Bucharest\n",
            "Bucharest, Romania\n",
            "cosma.i.adrian@gmail.comEmilian Radoi\n",
            "University Politehnica of Bucharest\n",
            "Bucharest, Romania\n",
            "emilian.radoi@upb.ro\n",
            "Abstract\n",
            "Gait analysis is proven to be a reliable way to per-\n",
            "form person identification without relying on subject co-\n",
            "operation. Walking is a biometric that does not signif-\n",
            "icantly change in short periods of time and can be re-\n",
            "garded as unique to each person. So far, the study of gait\n",
            "analysis focused mostly on identification and demograph-\n",
            "ics estimation, without considering many of the pedes-\n",
            "trian attributes that appearance-based methods rely on.\n",
            "In this work, alongside gait-based person identification,\n",
            "we explore pedestrian attribute identification solely from\n",
            "movement patterns. We propose DenseGait, the largest\n",
            "dataset for pretraining gait analysis systems containing\n",
            "217K anonymized tracklets, annotated automatically with\n",
            "42 appearance attributes. DenseGait is constructed by au-\n",
            "tomatically processing video streams and offers the full ar-\n",
            "ray of gait covariates present in the real world. We make\n",
            "the dataset available to the research community. Addition-\n",
            "ally, we propose GaitFormer, a transformer-based model\n",
            "that after pretraining in a multi-task fashion on DenseGait,\n",
            "achieves 92.5% accuracy on CASIA-B and 85.33% on FVG,\n",
            "without utilizing any manually annotated data. This corre-\n",
            "sponds to a +14.2% and +9.67% accuracy increase com-\n",
            "pared to similar methods. Moreover, GaitFormer is able\n",
            "to accurately identify gender information and a multitude\n",
            "of appearance attributes utilizing only movement patterns.\n",
            "The code to reproduce the experiments is made publicly.\n",
            "1. Introduction\n",
            "Technologies relying on facial and pedestrian analysis\n",
            "play a crucial role in intelligent video surveillance and se-\n",
            "curity systems. Facial and pedestrian analysis systems have\n",
            "become the norm in video intelligence, such systems being\n",
            "deployed ubiquitously. However, appearance-based pedes-\n",
            "trian re-identification [72] and facial recognition models\n",
            "[65] invariably suffer from extrinsic factors related to cam-\n",
            "era viewpoint and resolution, and to the change in a per-\n",
            "son’s appearance such as different clothing, hairstyles andaccessories. Moreover, due to the proliferation of privacy\n",
            "laws such as GDPR, it is increasingly difficult to deploy\n",
            "appearance-based solutions for video-intelligence. Human\n",
            "movement is highly correlated with many internal and ex-\n",
            "ternal aspects of a particular individual including age, gen-\n",
            "der, body mass index, clothing, carrying conditions, emo-\n",
            "tions and personality [47]. The manner of walking is unique\n",
            "to each person, it does not significantly change in short peri-\n",
            "ods of time [43] and cannot be easily faked to impersonate\n",
            "another person [29]. Gait analysis has gained significant\n",
            "attention in recent years [42, 53], due to solving many of\n",
            "the problems of appearance-based technologies without re-\n",
            "lying on the direct cooperation of subjects. However, com-\n",
            "pared to appearance-based methods, gait analysis is intrin-\n",
            "sically harder to perform with reliable accuracy, due to the\n",
            "influence of many confounding factors that affect the man-\n",
            "ner of walking. This problem is tackled in literature in two\n",
            "major ways, either by building specialized neural architec-\n",
            "tures that are invariant to walking variations [50, 60, 78],\n",
            "or by creating large-scale and diverse datasets for train-\n",
            "ing [12, 41, 68, 73, 79].\n",
            "One of the first attempts of building a large-scale gait\n",
            "recognition dataset is OU-ISIR [68], which is comprised of\n",
            "10,307 identities that walk in a straight line for a short dura-\n",
            "tion of time. Such a dataset is severely limited by its lack of\n",
            "walking variability, having only viewpoint change as a con-\n",
            "founding factor. Building sufficiently large datasets that ac-\n",
            "count for all the walking variations imply an immense anno-\n",
            "tation effort. For example, the GREW benchmark [79] for\n",
            "gait-based identification, reportedly took 3 months of con-\n",
            "tinuous manual annotation by 20 workers. In contrast, auto-\n",
            "matic, weakly annotated datasets are much easier to gather\n",
            "by leveraging existing state-of-the-art models—UWG [12],\n",
            "a comparatively large dataset of individual walking tracklets\n",
            "proved to be a promising new direction in the field. Increas-\n",
            "ing the dataset size is indeed correlated with performance on\n",
            "downstream gait recognition benchmarks [12], even though\n",
            "no manual annotations are provided. One limitation of\n",
            "these datasets is that they are annotated with attributes per\n",
            "individual only sparsely, and not addressing the problem\n",
            "1arXiv:2310.19418v1  [cs.CV]  30 Oct 2023of pedestrian attribute identification (PAI), currently per-\n",
            "formed only through appearance-based methods [26,39,58].\n",
            "Walking pedestrians are often annotated only with their gen-\n",
            "der, age, and camera viewpoint [68,73,78,79]. Even though\n",
            "gait-based demographic identification is a viable method for\n",
            "pedestrian analysis [7], it is also severely limited by the lack\n",
            "of data. Also, many attributes from PAI networks such as\n",
            "gender, age and body type have a definite impact on walking\n",
            "patterns [10,28,61], and we posit that they can be identified\n",
            "with a reasonable degree of accuracy using only movement\n",
            "patterns and not utilizing appearance information.\n",
            "We propose DenseGait, the largest gait dataset for pre-\n",
            "training to date, containing 217k anonymized tracklets in\n",
            "the form of skeleton sequences, automatically gathered by\n",
            "processing real-world surveillance streams through state-\n",
            "of-the-art models for pose estimation and pose tracking.\n",
            "An ensemble of PAI networks was used to densely anno-\n",
            "tate each skeleton sequence with 42 appearance attributes\n",
            "such as their gender, age group, body fat, camera viewpoint,\n",
            "clothing information and apparent action. The purpose of\n",
            "DenseGait is to be used for pretraining networks for gait\n",
            "recognition and attribute identification, it is not suitable for\n",
            "evaluation since it is annotated automatically and does not\n",
            "contain manual, ground-truth labels. DenseGait contains\n",
            "walking individuals in real scenarios, it is markerless, non-\n",
            "treadmill, and avoids unnatural and constrictive laboratory\n",
            "conditions, which have been shown to affect gait [57]. It\n",
            "practically contains the full array of factors that are present\n",
            "in real world gait patterns.\n",
            "The dataset is fully anonymized, and any information\n",
            "pertaining to individual identities is removed, such as the\n",
            "time, location and source of the video stream, and the ap-\n",
            "pearance and height information of the person. DenseGait\n",
            "is a gait analysis dataset primarily intended for pretraining\n",
            "neural models—using it to explicitly identify the individ-\n",
            "uals within it is highly unfeasible, requiring extensive ex-\n",
            "ternal information about the individuals, such as personal\n",
            "identifying information (i.e., their name or ID) and a base-\n",
            "line gait pattern. According to GDPR1legislation, data used\n",
            "for research purposes can be used if anonymized. Moreover,\n",
            "anonymized data does not conform to the rigors of personal\n",
            "data and can be processed without explicit consent. Never-\n",
            "theless, any attempt to use of DenseGait to explicitly iden-\n",
            "tify individuals present in it is highly discouraged.\n",
            "We chose to utilize only skeleton sequences for gait anal-\n",
            "ysis, as current appearance-based methods that rely on sil-\n",
            "houettes are not privacy preserving, potentially allowing\n",
            "for identification based only on the person’s appearance,\n",
            "rather than their movement [40]. Skeleton sequences en-\n",
            "code only the movement of the person, abstracting away\n",
            "any visual queues regarding identity and attributes. More-\n",
            "1https://eur-lex.europa.eu/eli/reg/2016/679/oj ,\n",
            "accessed on 1 July 2022)over, skeleton-based solutions have the potential to gener-\n",
            "alize across tasks such as action recognition, allowing for a\n",
            "flexible and extensible computation.\n",
            "DenseGait, compared to other similar datasets [12], con-\n",
            "tains 10 ×more sequences and is automatically annotated\n",
            "with 42 appearance attributes through a pretrained PAI en-\n",
            "semble (Table 1). In total, 60 h of video streams were pro-\n",
            "cessed, having a cumulative walking duration of pedestrians\n",
            "of 410 h. We release the dataset under open credentialized\n",
            "access, for research purposes only, under CC-BY-NC-ND-\n",
            "4.02License.\n",
            "We also propose GaitFormer, a multi-task transformer-\n",
            "based architecture [62] that is pretrained on DenseGait in a\n",
            "self-supervised manner, being able to perform exceptionally\n",
            "well in zero-shot gait recognition scenarios on benchmark\n",
            "datasets, achieving 92.5% identification accuracy from di-\n",
            "rect transfer on the popular CASIA-B dataset, without us-\n",
            "ing any manually annotated data. Moreover, it obtains good\n",
            "results on demographic and pedestrian attribute identifica-\n",
            "tion from walking patterns, with no manual annotations.\n",
            "GaitFormer represents the first use of a plain transformer\n",
            "encoder architecture in gait skeleton sequence processing,\n",
            "without relying on hand-crafted architectural modifications\n",
            "as in the case of graph neural networks [48, 71].\n",
            "This paper makes the following contributions:\n",
            "1. We release DenseGait, the largest dataset of skeleton\n",
            "walking sequences, densely annotated with appearance\n",
            "information, for use in pretraining neural architectures\n",
            "that can be further fine-tuned on specific gait analysis\n",
            "tasks. The dataset can be found at https://bit.\n",
            "ly/3SLO8RW , under open credentialized access, for\n",
            "research purposes only.\n",
            "2. We propose GaitFormer, a multi-task transformer that\n",
            "is pretrained on the DenseGait dataset and achieves ex-\n",
            "ceptional results in zero-shot gait recognition scenarios\n",
            "on benchmark datasets, achieving 92.52% accuracy on\n",
            "CASIA-B and 85.33% on FVG, without training on\n",
            "any manually annotated data (+14.2% and +9.67% in-\n",
            "crease compared to similar methods [12]). The code\n",
            "is made publicly available at: https://github.\n",
            "com/cosmaadrian/gaitformer\n",
            "3. We explore the performance of GaitFormer on other\n",
            "gait analysis tasks, such as gait-based gender estima-\n",
            "tion and attribute identification.\n",
            "2. Related Work\n",
            "2.1. Gait Analysis\n",
            "Video gait analysis encompasses research efforts dedi-\n",
            "cated to automatically estimate and predict various aspects\n",
            "2https://creativecommons.org/licenses/by-nc-nd/\n",
            "4.0/legalcode , accessed on 1 July 2022\n",
            "2Table 1. List of attributes extracted by each network in the PAI ensemble. Each network is trained on a different dataset, with a separate\n",
            "set of attributes. After coalescing similar attributes and eliminating appearance-only attributes, we obtain 42 appearance attributes.\n",
            "PA100k PETA RAP\n",
            "Female, AgeOver60, Age18-60,\n",
            "AgeLess18, Front, Side, Back,\n",
            "Hat, Glasses, HandBag, Shoul-\n",
            "derBag, Backpack, HoldObjectsIn-\n",
            "Front, ShortSleeve, LongSleeve,\n",
            "UpperStride, UpperLogo, Upper-\n",
            "Plaid, UpperSplice, LowerStripe,\n",
            "LowerPattern, LongCoat, Trousers,\n",
            "Shorts, Skirt & Dress, BootsAge16–30, Age31–45, Age46–60, Age-\n",
            "Above61, Backpack, CarryingOther,\n",
            "Casual lower, Casual upper, Formal\n",
            "lower, Formal upper, Hat, Jacket, Jeans,\n",
            "LeatherShoes, Logo, LongHair, Male,\n",
            "Messenger Bag, Muffler, No accessory,\n",
            "No carrying, Plaid, PlasticBags, San-\n",
            "dals, Shoes, Shorts, Short Sleeve, Skirt,\n",
            "Sneaker, Stripes, Sunglasses, Trousers,\n",
            "TShirt, UpperOther, V-NeckFemale, AgeLess16, Age17–30, Age31–45,\n",
            "BodyFat, BodyNormal, BodyThin, Customer,\n",
            "Clerk, BaldHead, LongHair, BlackHair, Hat,\n",
            "Glasses, Muffler, Shirt, Sweater, Vest, TShirt,\n",
            "Cotton, Jacket, Suit-Up, Tight, ShortSleeve,\n",
            "LongTrousers, Skirt, ShortSkirt, Dress, Jeans,\n",
            "TightTrousers, LeatherShoes, SportShoes,\n",
            "Boots, ClothShoes, CasualShoes, Backpack,\n",
            "SSBag, HandBag, Box, PlasticBags, PaperBag,\n",
            "HandTrunk, OtherAttchment, Calling, Talk-\n",
            "ing, Gathering, Holding, Pusing, Pulling,\n",
            "CarryingbyArm, CarryingbyHand\n",
            "of a walking person. Research has been mostly dedicated\n",
            "into gait-based person recognition, with many benchmark\n",
            "datasets [9,12,25,49,52,68,78,79] available for training and\n",
            "testing models. Moreover, there have been improvements in\n",
            "areas such as estimating demographics information [7, 15],\n",
            "emotion detection [69] and ethnicity estimation [75] from\n",
            "only movement patterns. [51] proposed a taxonomy to or-\n",
            "ganize the existing works in the field of gait recognition.\n",
            "In this work, we focus mainly on body representation, as\n",
            "we made a deliberate choice of providing DenseGait with\n",
            "only movement information for anonymization. Broadly,\n",
            "works in gait analysis can be divided into two major ap-\n",
            "proaches in terms of body representation: silhouette-based\n",
            "and skeleton-based.\n",
            "2.1.1 Silhouette-based Solutions\n",
            "Silhouette-based approaches make use of silhouettes of\n",
            "walking individuals estimated either through background\n",
            "subtraction methods or through instance segmentation and\n",
            "tracking. Silhouettes are used in various forms, either in a\n",
            "condensed representation [2, 22, 63], or as a sequences, as it\n",
            "is the norm in more modern methods [8, 19, 37, 78]. Most\n",
            "notably, GaitSet [8] processes the silhouettes as a set, as op-\n",
            "posed to preserving the temporal information present in a\n",
            "sequence. As such, the authors can include silhouettes from\n",
            "multiple videos of the same walking subjects, achieving\n",
            "good invariance to walking variations. GaitPart [19] pro-\n",
            "cesses the temporal variation of each individual body part\n",
            "separately in a Micro-motion Capture Module (MCM), tak-\n",
            "ing inspiration from model-based approaches. Each body\n",
            "part exhibits different visual queues and temporal variation\n",
            "and the authors propose to combine the each feature part\n",
            "to construct the final gait representation. Recently, Lin et\n",
            "al. [37], advance the construction of neural architectures\n",
            "for processing silhouette sequences by proposing a Global-Local Feature Extractor (GLFE), which obtains good results\n",
            "on benchmark datasets. Zhang et al. [78] propose GaitNet,\n",
            "a model which directly makes use of the appearance of the\n",
            "individual and is able to output invariant feature represen-\n",
            "tations for gait recognition. Moreover, they also propose\n",
            "FVG, a dataset with 226 individuals, only from the front-\n",
            "view angle, one of the more challenging angles in gait anal-\n",
            "ysis, due to the lack of perceived variation in limb move-\n",
            "ments.\n",
            "2.1.2 Skeleton-based Solutions\n",
            "Skeleton-based approaches, on the other hand, avoid mak-\n",
            "ing use of appearance information in the form of silhou-\n",
            "ettes, and instead focus on the moving anatomical skeleton\n",
            "of the person, effectively processing only movement pat-\n",
            "terns. Approaches typically imply processing walking se-\n",
            "quences with a pose estimation [32] model, and process-\n",
            "ing the resulting skeletons with a neural network, either\n",
            "by adapting conventional CNN modules [11], or with an\n",
            "LSTM [1,36]. More modern approaches make use of graph\n",
            "neural networks to model the relationships between human\n",
            "joints [33,34]. Liao et al. [36] make use of a combined CNN\n",
            "and LSTM architecture to model 2D skeleton sequences. A\n",
            "later improvement makes use of 3D skeletons [1] to fur-\n",
            "ther improve results. Li et al. [34] propose a graph-based\n",
            "convolutional architecture to process skeleton sequences,\n",
            "and a Joints Relationship Pyramid Mapping to map spatio-\n",
            "temporal gait features into a discriminative feature space. Li\n",
            "and Zhao [33] propose CycleGait, a graph-based approach\n",
            "that incorporates multiple walking paces in the augmenta-\n",
            "tion procedure and obtains robust results in gait recognition\n",
            "on CASIA-B. In contrast to these approaches, we opted to\n",
            "take a data-driven approach, instead of an algorithmic ap-\n",
            "proach, and use a standard transformer architecture and pre-\n",
            "train it on a large amount of weakly-labelled data. Recently,\n",
            "3[12] proposed an approach called WildGait to skeleton-\n",
            "based gait recognition, in which they automatically mine\n",
            "surveillance streams and pretrain a ST-GCN [71] model in\n",
            "a self-supervised manner. Through fine-tuning, good re-\n",
            "sults are obtained in recognition on CASIA-B and FVG.\n",
            "Similarly to WildGait, we also process publicly available\n",
            "surveillance streams, but increase the DenseGait dataset\n",
            "size by an order of magnitude. Moreover, we densely anno-\n",
            "tate each skeleton sequence with 42 appearance attributes\n",
            "for use in zero-shot attribute identification scenarios.\n",
            "However, model-based approaches still lag behind meth-\n",
            "ods utilizing appearance (i.e., silhouettes). This is most\n",
            "likely due to the imperfect extraction of skeletons by mod-\n",
            "ern pose estimators, which struggle to accurately detect\n",
            "fine-grained movements at a distance. Moreover, using\n",
            "appearance-based methods is fundamentally easier, since a\n",
            "single silhouette can contain identifying information about\n",
            "a subject. For instance Xu et al. [67] obtained reasonable\n",
            "results for gait recognition using a single silhouette, which\n",
            "cannot be considered gait, as no temporal movement is be-\n",
            "ing processed at all. This implies that recognition is per-\n",
            "formed through “shortcuts” in the form of appearance fea-\n",
            "tures (i.e., body composition, height, haircut, side-profile\n",
            "etc). For this reason, a more privacy-aware approach is\n",
            "to process only movement patterns, which constitutes the\n",
            "motivation for releasing DenseGait with only anonymized\n",
            "skeleton sequences, and disregarding silhouettes.\n",
            "2.2. Transformers and Self-Supervised Learning\n",
            "In recent years, there has been a insurgence of research\n",
            "in the area of self-supervised learning, mostly due to the ex-\n",
            "tremely high performance obtained in natural language pro-\n",
            "cessing with models such as BERT [14] and GPT [5]. Self-\n",
            "supervised learning presumes training models using aspects\n",
            "of the data itself as a supervisory signal. While initial ef-\n",
            "forts in computer vision relied on creating artificial pretext\n",
            "tasks [16, 21, 64], the field is moving towards contrastive-\n",
            "based approaches [6, 9, 74]. Methods such as SimCLR [9],\n",
            "Barlow Twins [74] and Dino [6] obtaining almost similar\n",
            "performance to direct supervision. Moreover, the trans-\n",
            "former has proven to be a flexible architecture, capable of\n",
            "handling a multitude of modalities such as text [14], im-\n",
            "ages [18], video [76], speech [17], and highly benefit from\n",
            "large-scale pretraining [3]. Taking inspiration from related\n",
            "efforts to process non-textual data with transformers [18],\n",
            "we construct GaitFormer by processing flattened skeletons\n",
            "as input “tokens”. In this manner, any human bias related to\n",
            "hand-crafted graph relationships between the body joints is\n",
            "eliminated. Moreover, as opposed to graph networks such\n",
            "as ST-GCN [48], training a similarly large transformer en-\n",
            "coder make more efficient use of computational resources,\n",
            "significantly reducing training time.3. Method\n",
            "3.1. Dataset Construction\n",
            "For building the DenseGait dataset, we made use of\n",
            "public video streams (e.g., street cams), and processed\n",
            "them with AlphaPose [32], a modern, state-of-the-art multi-\n",
            "person pose estimation model. AlphaPose’s raw output is\n",
            "comprised of skeletons with (x, y, c )coordinates for each\n",
            "of the 18 joints of the COCO skeleton format Lin et al. [38],\n",
            "corresponding to 2D coordinates in the image plane and a\n",
            "prediction confidence score. We performed intra-camera\n",
            "tracking for each skeleton with on SortOH [46]. SortOH\n",
            "is based on the SORT [4] algorithm, which relies only on\n",
            "coordinate information and not on appearance information.\n",
            "As opposed to DeepSORT [66] which makes use of person\n",
            "re-identification models, SortOH is only using coordinates\n",
            "and bounding box size for faster computation time while\n",
            "having comparably similar performance. SortOH ensures\n",
            "that tracking is not significantly affected by occlusions.\n",
            "To ensure that the skeleton sequences can be properly\n",
            "processed by a deep learning model, we performed exten-\n",
            "sive data cleaning. We have filtered low confidence skele-\n",
            "tons by computing the average confidence of each of the\n",
            "18 joints, and in each sequence, skeletons with an average\n",
            "confidence of less than 0.5 were removed. Furthermore,\n",
            "skeletons with feet confidence less than 0.4 were removed.\n",
            "This step guarantees that the feet are visible and confidently\n",
            "detected—leg movement is one of the most important sig-\n",
            "nals for gait analysis. In our processing, we chose a period\n",
            "length Tof 48 frames, which corresponds to approximately\n",
            "2 full gait cycles on average [45]. Surveillance streams do\n",
            "not have the same frame rate between them, which makes\n",
            "the sequences have different paces and durations. As such,\n",
            "we filtered short tracklets which have a duration of less than\n",
            "T∗fps\n",
            "24. We consider 24 FPS to be real-time video speed, and\n",
            "each video was processed according to its own frame rate.\n",
            "Moreover, skeletons are linearly interpolated such that the\n",
            "pace and duration is unified across video streams.\n",
            "Similar to [12], we further normalized each skeleton\n",
            "by centering at the pelvis coordinates (xpelvis, ypelvis )and\n",
            "scaling vertically by the distance between the head and the\n",
            "hips ( yneck−ypelvis ) and horizontally by the distance be-\n",
            "tween the shoulders ( (xR.shoulder −xL.shoulder )). This pro-\n",
            "cedure is detailed in Equations (1) and (2). The normaliza-\n",
            "tion procedure aligns the skeleton sequences in a similar\n",
            "manner to the alignment step in face recognition pipelines\n",
            "[70]. This step eliminated the height and body type infor-\n",
            "mation about the subject, ensuring that the person cannot be\n",
            "directly identified.\n",
            "xjoint =xjoint−xpelvis\n",
            "|xR.shoulder −xL.shoulder |(1)\n",
            "yjoint =yjoint−ypelvis\n",
            "|yneck−ypelvis|(2)\n",
            "4However, body type information should be preserved\n",
            "through the analysis of the walking patterns. Moreover,\n",
            "normalization obscures the human position in the frame, to\n",
            "prevent identification of the source video stream.\n",
            "Finally, we filtered standing/non-walking skeletons in\n",
            "each sequence by computing the average movement speed\n",
            "of the legs, which is indicative of the action the person is\n",
            "performing. As such, if the average leg speed is less than\n",
            "0.0015 and higher than 0.09, the sequence was removed.\n",
            "The thresholds were determined through manual inspection\n",
            "of the sequences. This eliminated both standing skeleton\n",
            "sequences as well as sequences with erratic leg movement,\n",
            "which is most probably due to poor pose estimation output\n",
            "in that case.\n",
            "DenseGait is fully anonymized. Any information re-\n",
            "garding the identity of particular individuals in the dataset\n",
            "is eliminated, including appearance information (by keep-\n",
            "ing only movement information in the form of skeleton se-\n",
            "quences), height and body proportions (through normaliza-\n",
            "tion), and the time, location, and source of the video stream.\n",
            "Identifying individuals in DenseGait is highly unfeasible, as\n",
            "it requires external information (i.e., name, email, ID, etc.)\n",
            "and specific collection of gait patterns.\n",
            "The final dataset contains 217k anonymized tracklets,\n",
            "with a combined length of 410 h. DenseGait is currently\n",
            "the largest dataset of skeleton sequences for use in pretrain-\n",
            "ing gait analysis models. Table 2 showcases a compari-\n",
            "son between DenseGait and other popular gait recognition\n",
            "datasets. Since the skeleton sequences are collected auto-\n",
            "matically through pose tracking, it is impossible to quan-\n",
            "tify exactly the number of different identities in the dataset,\n",
            "as, in some cases, tracking might be lost due to occlusions.\n",
            "However, DenseGait contains a significantly larger number\n",
            "of tracklets compared to other available datasets while also\n",
            "being automatically densely annotated with 42 appearance\n",
            "attributes. In the case of UWG [12] and DenseGait, the\n",
            "datasets do not contain explicit covariates for each iden-\n",
            "tity, but rather covariates in terms of viewing angle, carrying\n",
            "conditions, clothing change, and apparent action are present\n",
            "across the tracklet duration, similar to GREW [79].\n",
            "Similarly to UWG [12], DenseGait does not contain mul-\n",
            "tiple walks per person, rather each tracklet is considered\n",
            "a unique identity. Compared to other large-scale datasets,\n",
            "DenseGait tracks individuals for a longer duration, which\n",
            "makes it suitable for use in self-supervised pretraining, as\n",
            "longer tracked walking usually contains more variability for\n",
            "a single person. Figure 1 shows boxplots with a five-number\n",
            "summary descriptive statistics for the distribution of track\n",
            "durations in each dataset. DenseGait has a mean tracklet\n",
            "duration of 162 frames, which is significantly larger (z-test\n",
            "p<0.0001) compared to other datasets: CASIA-B [73]—\n",
            "83 frames, FVG [78]—97 frames, GREW [79]—98 frames,\n",
            "UWG [12]—136 frames). Due to potential loss of trackinginformation, the dataset is noisy, and can be used only for\n",
            "self-supervised pretraining.\n",
            "Figure 1. Comparison between existing large-scale skeleton gait\n",
            "databases and DenseGait in terms of distributions of tracklet du-\n",
            "ration. DenseGait is an order of magnitude larger than the next\n",
            "largest skeleton database, while having a longer average duration\n",
            "(136 frames UWG vs 162 frames DenseGait).\n",
            "3.2. Annotations with Appearance Attributes\n",
            "Appearance attributes are essential for pretraining for\n",
            "tasks such as gender estimation [7], age estimation [35] and\n",
            "pedestrian attribute identification [26,39,58]. To ensure that\n",
            "the dataset is densely annotated with appearance attributes,\n",
            "we made use of an ensemble of pretrained PAI networks,\n",
            "each trained on different popular PAI datasets. Specifically,\n",
            "we employed three InceptionV3 [56] networks trained on\n",
            "RAP [31], PETA [13] and PA100k [39], respectively. Fig-\n",
            "ure 2 showcases the annotation procedure.\n",
            "Since each dataset has a different set of pedestrian at-\n",
            "tributes, we averaged similar classes (e.g., AgeLess16 and\n",
            "AgeLess18 into AgeChild ), coalesced similar classes (e.g.,\n",
            "Formal and Suit-Up into FormalWear ) and removed at-\n",
            "tributes that cannot evidently be estimated from movement\n",
            "patterns (e.g., BaldHead ,Hat,V-Neck ,Glasses ,Plaid etc.).\n",
            "For a particular sequence, we take the cropped image\n",
            "of the pedestrian at every Tframes (where Tis the pe-\n",
            "riod length), and randomly augment it k= 4 times (e.g.,\n",
            "random horizontal flips, color jitter and small random rota-\n",
            "tion). For each crop, each augmented version is then pro-\n",
            "cessed by a PAI network and the results are averaged such\n",
            "that the output is robust to noise [55]. Finally, to have a\n",
            "unified prediction for the walking sequence, results are av-\n",
            "eraged according to the size of the bounding box relative\n",
            "to the image, similar to Catruna et al. [7]. Predictions on\n",
            "larger crops have a higher weight, with the assumption that\n",
            "the pedestrian appearance is more clearly distinguishable\n",
            "when closer to the camera.\n",
            "Figure 3 showcases the final list of attributes, and their\n",
            "distribution across the dataset. We have a total of 42 at-\n",
            "tributes, split into 8 groups: Gender ,Age Group ,Body Type ,\n",
            "Viewpoint ,Carry Conditions ,Clothing ,Footwear andAp-\n",
            "parent Action . For the final annotations, we chose to keep\n",
            "the soft-labels and not round them, as utilizing soft-labels\n",
            "5Table 2. Comparison of popular datasets for gait recognition. DenseGait is an order of magnitude larger, has more identities in terms of\n",
            "skeleton sequences (highlighted in bold ), and each sequence is annotated with 42 appearance attributes. * Approximate number given by\n",
            "pose tracker. †Implicit covariates across tracking duration.\n",
            "Dataset # IDs Sequences Covariates Views Env.\n",
            "USF HumanID [49] 122 1870 Y 2 Outdoor\n",
            "TUM-GAID [25] 305 3370 Y 1 Outdoor\n",
            "FVG [78] 226 2857 Y 1 Outdoor\n",
            "CASIA-B [73] 124 13,640 Y 11 Indoor\n",
            "OU-ISIR [68] 10,307 144,298 N 14 Indoor\n",
            "GREW [79] 26,000 128,000 Y - Outdoor\n",
            "UWG [12] 38,502 * 38,502 Y†- Outdoor\n",
            "DenseGait ( ours ) 217,954 * 217,954 Y†- Outdoor\n",
            "Figure 2. Overview of the automatic annotation procedure for the 42 appearance attributes. To robustly annotate attributes, an ensemble\n",
            "of pretrained networks is used in conjunction with multiple augmentations of the same crop. Predictions across the sequence are averaged\n",
            "according to their bounding-box area.\n",
            "for model training was shown to be a more robust approach\n",
            "when dealing with noisy data [55].\n",
            "Figure 4 showcases selected examples of attribute pre-\n",
            "dictions from the PAI ensemble. Since surveillance cam-\n",
            "eras usually have low resolution and the subject might be\n",
            "far away from the camera, some pedestrian crops are blurry\n",
            "and might affect prediction by the PAI ensemble. For\n",
            "gender, age group, body composition and viewpoint, the\n",
            "models are confidently identifying these attributes. How-\n",
            "ever, for specific pieces of clothing (i.e., footwear: San-\n",
            "dals/LeatherShoes), predictions are not always reliable, due\n",
            "to the low resolution of some of the crops, but the errors are\n",
            "negligible when taking into account the scale of the dataset.\n",
            "3.3. Description of Model Architecture\n",
            "For pretraining on the DenseGait dataset for the tasks of\n",
            "gait-based recognition and attribute identification, we chose\n",
            "to adapt the popular transformer encoder architecture [62]to handle skeleton sequences. Initially, transformers were\n",
            "immensely successful in handling sequential data in the\n",
            "form of text, effectively replacing LSTM [24] networks,\n",
            "the de facto approach for these problems. However, lately,\n",
            "transformers have been used in a variety of problems, be-\n",
            "ing able to handle images [18], video [76] and multi-modal\n",
            "data [20]. Moreover, transformer architectures in particu-\n",
            "lar highly benefit from large-scale, self-supervised pretrain-\n",
            "ing [5, 6, 14], allowing models to be effectively fine-tuned\n",
            "on more specific datasets with small amounts of annotated\n",
            "data.\n",
            "To handle skeleton sequences, we abstain from making\n",
            "any hand-crafted architectural modifications, as in the case\n",
            "of Plizzari et al. [48], which uses a hybrid approach by com-\n",
            "bining graph computation on the skeleton and using multi-\n",
            "head attention on the extracted features. Instead, we take\n",
            "inspiration from ViT [18], which processes images as a se-\n",
            "quence of flattened patches that are fed into a standard trans-\n",
            "6Figure 3. Distribution of the 42 appearance attributes in DenseGait. The dataset is annotated in a fine-grained manner with attributes\n",
            "ranging from internal aspects of the person (Gender, Age Group, Body Type) to appearance only labels (Clothing, Footwear).\n",
            "Figure 4. Qualitative examples for selected attributes from the PAI\n",
            "ensemble. The networks correctly identify gender, age group and\n",
            "viewpoint. However, in some cases, clothing and, more specif-\n",
            "ically, footwear are more difficult to estimate in low resolution\n",
            "scenarios.\n",
            "former encoder network. Figure 5 showcases the training\n",
            "procedure for GaitFormer in the multi-task training regime.\n",
            "Each skeleton is flattened into a 54 dimensional vector and\n",
            "is linearly projected with a standard learnable feed-forward\n",
            "layer into a 256 dimensional space. Each skeleton projec-\n",
            "tion is then fed into a transformer encoder network. We\n",
            "opted for learnable positional embedding that is added to\n",
            "each projection instead of concatenated, to avoid increasing\n",
            "the dimensionality. After the transformer encoder, repre-\n",
            "sentations for each skeleton are averaged, and a final lin-\n",
            "ear feed-forward layer of 256 elements is used as the fi-\n",
            "nal embedding. Further, as described in SimCLR [9], we\n",
            "used an additional 128-dimension linear layer for training\n",
            "with a supervised contrastive objective [27]. Additionally,a linear layer is used as appearance head to estimate the\n",
            "pedestrian attributes that is trained using a standard binary-\n",
            "crossentropy loss.\n",
            "We used three different model sizes for the transformer\n",
            "encoder in our experiments, with 4 encoder layers ( SM),\n",
            "8 encoder layers ( MD) and 12 encoder layers ( XL). In all\n",
            "types of architectures, 8 attention heads were used, and the\n",
            "internal feed-forward dimensionality was 256 [62].\n",
            "3.4. Training Details\n",
            "For training on DenseGait, we chose to use contrastive\n",
            "learning [9] as a supervisory signal. By design, contrastive\n",
            "methods work by attracting representations belonging to the\n",
            "same class, while simultaneously repelling samples from\n",
            "different classes. This paradigm is identical to the objec-\n",
            "tive for recognition problems, which constitues one of the\n",
            "main tasks in gait analysis. Specifically, we used SupCon-\n",
            "Loss [27], with a temperature of τ= 0.001, alongside a\n",
            "two-view sampler for each skeleton in the batch. SupCon-\n",
            "Loss assumes a multi-viewed batch, with multiple augmen-\n",
            "tations for the same sample. Each view of a skeleton se-\n",
            "quence is randomly augmented by the standard suite of aug-\n",
            "mentations for this data modality: random sequence crops\n",
            "of fixed length of T= 48 , random flips with 50% probabil-\n",
            "ity, random paces [64], and random gaussian noise added to\n",
            "joints coordinates. Let i∈I≡ {1. . .2N}be the index of\n",
            "an arbitrary augmented sample. SupConLoss is defined as:\n",
            "Lsup=X\n",
            "i∈I−1\n",
            "|P(i)|X\n",
            "p∈P(i)logexp(zi·zp/τ)P\n",
            "a∈A(i)exp(zi·za/τ)\n",
            "(3)\n",
            "In Equation (3), zl=Enc(exl)denotes the embedding\n",
            "of a skeleton sequence xl, “·” denotes the dot product oper-\n",
            "ation and A(i)≡I\\ {i}. Moreover, P(i)≡ {p∈A(i) :\n",
            "7Figure 5. Overview of GaitFormer (Multi-Task) training procedure. Flattened skeletons are linearly projected using a standard feed-forward\n",
            "layer and fed into a transformer encoder. The vectorized representations are average pooled and the resulting 256-dimensional vector is\n",
            "used for estimating the identity and to estimate the 42 appearance attributes through the “Appearance Head”. The contrastive objective\n",
            "(SupConLoss) is applied to a lower 128-dimensional linear projection, similar to the approach in SimCLR [9] .\n",
            "eyp=eyi}is the set of indices of all positives in the multi-\n",
            "viewed batch distinct from i. In our case, the positive pairs\n",
            "are constructed by two different augmentations of the same\n",
            "skeleton sequence. The variability of the two augmentations\n",
            "is higher if the skeleton is tracked for a longer duration of\n",
            "time, as the walking individual might change direction.\n",
            "As suggested in Chen et al. [9], the supervisory signal\n",
            "given by SupConLoss is applied to a lower dimensional em-\n",
            "bedding (128 dimensions) to avoid the curse of dimension-\n",
            "ality.\n",
            "For predicting appearance attributes, which is a multi-\n",
            "label problem, we used a standard binary-crossentropy loss\n",
            "between each appearance label ( pi) and its corresponding\n",
            "prediction ( yi) (Equation (4)). As previously mentioned,\n",
            "we keep the soft labels as a supervisory signal, to prevent\n",
            "the network from overfitting and be more robust to noisy or\n",
            "incorrect labels [44]. Moreover, since learning appearance\n",
            "labels can regarded as a knowledge distillation problem be-\n",
            "tween the PAI ensemble and the transformer network, soft\n",
            "labels help improve the distillation process [23].\n",
            "Lappearance =−(yilog(pi) + (1 −yi) log(1 −pi)))(4)\n",
            "In multi-task (MT) training scenarios, we used a combi-\n",
            "nation of the two losses, with a weight penalty of λ= 0.5\n",
            "on the appearance loss Lappearance . We chose λ= 0.5em-\n",
            "pirically, such that the two losses have similar magnitudes.\n",
            "The final loss function is defined as:\n",
            "Lfinal =LSupCon +λLappearance (5)In plain contrastive training scenarios, we employ\n",
            "only the SupConLoss, without predicting attributes (i.e.,\n",
            "Lfinal =LSupCon ).\n",
            "The motivation for pre-training the network in a multi-\n",
            "task setting is that the network not only learns to cluster\n",
            "walking sequences by their identity, but also to take ap-\n",
            "pearance attributes into account. For instance, predicting\n",
            "the gender and age, even if they are not completely reli-\n",
            "able, could prove useful for gait recognition, as demograph-\n",
            "ics can be considered soft-biometrics, allowing the network\n",
            "to automatically filter identities by these attributes. On the\n",
            "other hand, in contrastive-only scenario, the network is un-\n",
            "der a classical self-supervised regime.\n",
            "We used a batch size of 1024 across our experiments,\n",
            "with a cyclical learning rate [54] ranging from 0.0001 and\n",
            "0.001 across 20 epochs. We trained all models for 400\n",
            "epochs.\n",
            "4. Experiments and Results\n",
            "This section explores the performance of GaitFormer\n",
            "on gait-based recognition, gender identification and pedes-\n",
            "trian attribute identification. We are primarily interested in\n",
            "evaluating the model in scenarios with low amounts of an-\n",
            "notated data and we opted to use the two popular bench-\n",
            "mark datasets originally constructed for gait recognition:\n",
            "CASIA-B [73] and FVG [78]. For gender estimation, we\n",
            "manually annotated the gender information for each iden-\n",
            "tity in the two datasets and constructed CASIA-gender and\n",
            "FVG-gender. We briefly describe each dataset below.\n",
            "8We chose CASIA-B to compare with other skeleton-\n",
            "based gait recognition models, since it is one of the most\n",
            "popular gait recognition datasets in literature. It con-\n",
            "tains 124 subjects walking indoors in a straight line, cap-\n",
            "tured with 11 synchronized cameras with three walking\n",
            "variations—normal walking (NM), clothing change (CL)\n",
            "and carry conditions (BG). According to Yu et al. [73], the\n",
            "first 62 subjects are used for training and the rest for evalu-\n",
            "ation. CASIA-gender consists of manually annotated the\n",
            "subjects in CASIA-B with gender information, having a\n",
            "split of 92 males and 32 females. We maintain the train-\n",
            "ing and validation splits from the recognition task, using\n",
            "the first 62 subjects for training (44 males and 18 females)\n",
            "and the rest for validation (48 males and 14 females). We\n",
            "use FVG to evaluate the robustness of GaitFormer, as it\n",
            "contains different covariates than CASIA-B such as vary-\n",
            "ing degrees of walking speed, the passage of time and clut-\n",
            "tered background. Moreover, FVG only contains walks\n",
            "from the front-view angle, which is more difficult for gait\n",
            "processing due to lower perceived limb variation. Accord-\n",
            "ing to Zhang et al. [78], from the 226 identities present in\n",
            "FVG, the first 136 are used for training and the rest for test-\n",
            "ing. Similarly, FVG-gender contains manual annotations\n",
            "with gender information, obtaining 149 males and 77 fe-\n",
            "males. We maintain the training and validation splits from\n",
            "the recognition task, utilizing the first 136 individuals for\n",
            "training (83 males and 53 females) and the rest for valida-\n",
            "tion (66 males and 24 females).\n",
            "4.1. Recognition\n",
            "We initially trained GaitFormer under two regimes: (i)\n",
            "contrastive only and (ii) multi-task (MT), which implies\n",
            "training with SupConLoss [27] on the tracklet ID while si-\n",
            "multaneously estimating the appearance attributes (Figure\n",
            "5). We experiment with three models sizes: SM—4 en-\n",
            "coder layers (2.24M parameters), MD—8 encoder layers\n",
            "(4.35M parameters) and XL—12 encoder layers (6.46M pa-\n",
            "rameters).\n",
            "We pretrain GaitFormer on the DenseGait dataset under\n",
            "the mentioned conditions and directly evaluate recognition\n",
            "performance in terms of accuracy on CASIA-B and FVG,\n",
            "without fine-tuning. In all experiments we perform a deter-\n",
            "ministic crop in the middle of the skeleton sequences of T\n",
            "= 48 frames, and use no test-time augmentations. For each\n",
            "cropped skeleton sequence, features are extracted using the\n",
            "256-dimensional representation and are normalized with the\n",
            "l2norm. In Table 3 we present results on the walking varia-\n",
            "tions for each model size and training regime. For CASIA-\n",
            "B, we show mean accuracy where the gallery set contains all\n",
            "viewpoints except the probe angle, in the three evaluation\n",
            "scenarios: normal walking (NM), change in clothing (CL)\n",
            "and carry bag (CB). For FVG, we show accuracy results\n",
            "based on the evaluation protocols mentioned by Zhang etal. [78], corresponding to different walking scenarios (walk\n",
            "speed (WS), change in clothing (CL), carrying bag (CB),\n",
            "cluttered background (CBG) and ALL). Results show that\n",
            "unsupervised pretraining on DenseGait is a viable way to\n",
            "perform gait recognition, achieving an accuracy of 92.52%\n",
            "on CASIA-B and 85.33% on FVG, without any manually\n",
            "annotated data available. Notably, multi-task learning on\n",
            "appearance attributes provides a consistent positive gap in\n",
            "the downstream performance.\n",
            "Model size in terms of number of layers does not seem\n",
            "to considerably affect performance on benchmark datasets.\n",
            "GaitFormerMD (8 layers) fairs consistently better than\n",
            "GaitFormerXL (12 layers), while being similarly close to\n",
            "GaitFormerSM (4 layers).\n",
            "Figure 6 compares GaitFormerMD pretrained on\n",
            "DenseGait in the two training regimes (contrastive only—\n",
            "Cont. and Multi-Task—MT) and GaitFormerMD randomly\n",
            "initialized. The networks were fine-tuned on progressively\n",
            "larger samples of the corresponding datasets: for CASIA-B,\n",
            "we sampled multiple runs for the same identity (from 1 to\n",
            "10 runs per ID), and for FVG, we randomly sampled a per-\n",
            "centage of runs per each identity. Models were fine-tuned\n",
            "using Layer-wise Learning Rate Decay (LLRD) [77], which\n",
            "implies a higher learning rate for top layers and a progres-\n",
            "sively lower learning rate for bottom layers. The learning\n",
            "rate was decreased linearly from 0.0001 to 0, across 200\n",
            "epochs. The results show that unsupervised pretraining has\n",
            "a substantial effect on downstream performance especially\n",
            "in low data scenarios (direct transfer and 10% of available\n",
            "data). Moreover, pretraining the model in Multi-Task learn-\n",
            "ing regime, in which the network was tasked to estimate ap-\n",
            "pearance attributes from movement alongside with the iden-\n",
            "tity, provides a consistent increase in performance.\n",
            "Table 4 presents state-of-the-art results compared with\n",
            "other skeleton-based gait recognition models. We show-\n",
            "case the results of GaitFormerSM trained in the Multi-Task\n",
            "(MT) regime, without fine-tuning (direct) and tuned with\n",
            "all the available training data in CASIA-B. For compari-\n",
            "son, we include WildGait [12] with and without fine-tuning,\n",
            "as this model is also pretrained on a large dataset of skele-\n",
            "ton sequences. We also compare with our implementation\n",
            "of GaitGraph Teepe et al. [59]—a multi-branch ST-GCN\n",
            "which processes joint coordinates, velocities and bone an-\n",
            "gles, achieving great results on CASIA-B—and with a ST-\n",
            "GCN pretrained on DenseGait.\n",
            "It is clear that the fine-tuned GaitFormerSM has very\n",
            "good results even without fine-tuning, achieving compara-\n",
            "ble results with the state of the art. Fine-tuning marginally\n",
            "increases the performance, achieving 96.2% accuracy on\n",
            "normal walking (NM) and 72.5% performance in carry bag\n",
            "(CB).\n",
            "9Table 3. GaitFormer direct transfer performance on gait recognition on CASIA-B and FVG datasets. We highlight in bold the best overall\n",
            "result for each dataset.\n",
            "CASIA-B FVG\n",
            "Size Training NM CL CB WS CB CL CBG ALL\n",
            "SM Contrastive 89.00 22.36 61.88 77.33 81.82 54.27 86.75 77.33\n",
            "MD Contrastive 90.18 23.46 60.78 78.33 72.73 49.15 83.33 78.33\n",
            "XL Contrastive 91.79 21.11 63.12 76.33 69.70 48.29 87.61 76.33\n",
            "SM MT 92.52 22.73 67.16 84.67 81.82 59.40 91.45 84.67\n",
            "MD MT 92.52 23.31 65.10 85.33 87.88 53.42 88.89 85.33\n",
            "XL MT 90.69 20.75 60.34 85.00 81.82 51.71 91.03 85.00\n",
            "Figure 6. Fine-tuning results on gait recognition on CASIA-B and FVG, on progressively larger number of runs per identity. Compared\n",
            "to the same network randomly initialized, pretraining on DenseGait offers substantial improvements, even in the direct transfer regime. A\n",
            "consistent performance increase is obtained when also estimating attributes.\n",
            "4.2. Comparison with ST-GCN and Other Pretrain-\n",
            "ing Datasets\n",
            "In Table 5, we compare GaitFormer with ST-GCN [71]\n",
            "under different pretraining datasets. Reported results are\n",
            "mean accuracy across all angles for CASIA-B, under nor-\n",
            "mal walking (NM) scenario, and accuracy under ALL sce-\n",
            "nario for FVG. The networks were not fine-tuned on these\n",
            "datasets; we present direct transfer performance after pre-\n",
            "training. We chose to pretrain on OU-ISIR [41], as this\n",
            "dataset is one of the most popular, large-scale datasets for\n",
            "gait recognition. However, OU-ISIR lacks data diversity, as\n",
            "all individuals are walking on a treadmill for a short dura-tion, which is not the case for DenseGait. We also chose to\n",
            "pretrain on GREW [79], as it is also a diverse dataset col-\n",
            "lected in the wild, but contains fewer identities that walk for\n",
            "a comparably shorter duration of time.\n",
            "Results show that, as a pretraining dataset, DenseGait\n",
            "is consistently outperforming GREW and OU-ISIR across\n",
            "the two architectures. These results are consistent with the\n",
            "insights in Figure 1, in which we posit that longer tracking\n",
            "duration for the individuals imply larger data diversity when\n",
            "pretraining in a contrastive self-supervised fashion, which\n",
            "directly improves performance.\n",
            "10Table 4. GaitFormer comparison to other skeleton-based gait recognition methods on CASIA-B dataset. In all methods the gallery set\n",
            "contains all viewpoints except the proble angle. In bold and underline we highlight the best and second best results for a particular\n",
            "viewpoint and walking condition.\n",
            "Method 0◦18◦36◦54◦72◦90◦108◦126◦144◦162◦180◦Mean\n",
            "NMGaitGraph 79.8 89.5 91.1 92.7 87.9 89.5 94.35 95.1 92.7 93.5 80.6 89.7\n",
            "ST-GCN (DenseGait) 89.5 89.5 95.1 87.9 81.4 68.5 64.5 89.5 88.7 84.6 82.2 83.8\n",
            "WildGait—direct 72.6 84.6 90.3 83.8 63.7 62.9 66.1 83.0 86.3 84.6 83.0 78.3\n",
            "PoseFrame 66.9 90.3 91.1 55.6 89.5 97.6 98.4 97.6 89.5 69.4 68.5 83.1\n",
            "GaitFormerSM—MT—direct 94.3 97.5 99.2 98.4 79.8 80.6 89.5 100.0 94.3 95.1 88.7 92.5\n",
            "WildGait—tuned 86.3 96.0 97.6 94.3 92.7 94.3 94.3 98.4 97.6 91.1 83.8 93.4\n",
            "GaitFormerSM—MT—tuned 96.7 99.2 100.0 99.2 91.9 91.9 95.1 98.4 96.7 97.6 91.1 96.2\n",
            "CLGaitGraph 27.4 33.0 40.3 37.1 33.8 33.0 35.4 33.8 34.6 21.7 17.7 31.6\n",
            "ST-GCN (DenseGait) 18.5 22.5 25.0 21.7 13.7 18.5 21.7 31.4 21.7 21.7 16.9 21.2\n",
            "WildGait—direct 12.1 33.0 25.8 18.5 12.9 11.3 21.7 24.2 20.1 26.6 16.1 20.2\n",
            "PoseFrame 13.7 29.0 20.2 19.4 28.2 53.2 57.3 52.4 25.8 26.6 21.0 31.5\n",
            "GaitFormerSM/MT—direct 12.9 21.7 29.0 25.8 16.1 18.5 22.5 29.0 27.4 26.6 20.1 22.7\n",
            "WildGait—tuned 29.0 32.2 35.5 40.3 26.6 25.0 38.7 38.7 31.4 34.6 31.4 33.0\n",
            "GaitFormerSM/MT—tuned 35.5 35.5 33.8 33.8 20.9 30.6 31.4 31.4 28.2 42.7 29.8 32.2\n",
            "BGGaitGraph 64.5 69.3 70.1 62.9 61.2 58.8 59.6 58.0 57.2 55.6 45.9 60.3\n",
            "ST-GCN (DenseGait) 78.2 68.5 71.7 60.4 59.6 45.9 46.7 58.0 58.0 58.0 51.6 59.7\n",
            "WildGait—direct 67.7 60.5 63.7 51.6 47.6 39.5 41.1 50.0 52.4 51.6 42.7 51.7\n",
            "PoseFrame 45.2 66.1 60.5 42.7 58.1 84.7 79.8 82.3 65.3 54.0 50.0 62.6\n",
            "GaitFormerSM/MT—direct 78.2 71.7 84.7 74.2 56.4 50.0 57.2 66.1 69.3 70.9 59.6 67.1\n",
            "WildGait—fine-tuned 66.1 70.1 72.6 65.3 56.4 64.5 65.3 67.7 57.2 66.1 52.4 64.0\n",
            "GaitFormerSM/MT—tuned 82.2 80.6 83.8 72.6 62.9 69.3 68.5 70.1 69.3 77.4 60.4 72.5\n",
            "Table 5. Comparison between GaitFormer and ST-GCN pretrained\n",
            "with Supervised Contrastive on GREW [79], OU-ISIR [41] and\n",
            "our proposed DenseGait. Performance is directly correlated with\n",
            "mean tracklet duration on each dataset as shown in Figure 1.. We\n",
            "highlight in bold the best results for each architecture and dataset.\n",
            "Backbone Pretraining Data CASIA-B (NM) FVG (ALL)\n",
            "ST-GCNOU-ISIR 55.65 63.33\n",
            "GREW 61.14 56.67\n",
            "DenseGait (ours) 83.80 75.28\n",
            "GaitFormer (ours)OU-ISIR 25.73 51.34\n",
            "GREW 65.40 64.04\n",
            "DenseGait (ours) 89.0 77.33\n",
            "4.3. Gait-Based Gender Detection\n",
            "Table 6 presents results for direct transfer (zero-shot)\n",
            "performance for gender estimation on CASIA-gender and\n",
            "FVG-gender. In this case, we compared different sizes\n",
            "of GaitFormer trained on DenseGait in two manners: i)\n",
            "only estimating attributes, without a constrastive objective\n",
            "(Attributes Only), and ii) estimating attributes and iden-\n",
            "tity using a constrastive objective (MT). Similarly to the\n",
            "case of gait recognition, the Multi-Task networks consis-\n",
            "tently outperforms the other training regime. Moreover, the\n",
            "networks achieved reasonable performance in terms of F 1\n",
            "score (76.18% for CASIA-gender and 86.81% for FVG-\n",
            "gender), considering that the networks were not exposed toany manually annotated data.\n",
            "Table 6. GaitFormer direct transfer performance on gait-based\n",
            "gender estimation on CASIA-gender and FVG-gender. We high-\n",
            "light in bold the best overall results for each dataset.\n",
            "CASIA-Gender FVG-Gender\n",
            "Size Training Prec. Recall F 1 Prec. Recall F 1\n",
            "SM Attributes 94.54 62.46 72.10 85.87 84.9 85.10\n",
            "MD Attributes 94.48 63.66 73.07 82.03 79.87 80.27\n",
            "XL Attributes 94.59 62.43 72.08 84.36 84.43 84.26\n",
            "SM MT 94.84 63.61 73.00 86.47 86.34 86.33\n",
            "MD MT 94.71 61.50 71.31 86.96 86.87 86.81\n",
            "XL MT 94.72 67.67 76.18 87.06 86.21 86.38\n",
            "Figure 7 presents the performance under fine-tuning of\n",
            "GaitFormerXL on CASIA-gender and FVG-gender, in sim-\n",
            "ilar conditions to the recognition task. All networks are\n",
            "trained with a binary-crossentropy objective on the gender\n",
            "estimation task, without taking the person identity into ac-\n",
            "count at training time. GaitFormerXL under Multi-Task\n",
            "training regime is consistently superior to a network initial-\n",
            "ized from random weights, achieving an F 1score of 93.09%\n",
            "on CASIA-gender and of 91.51% on FVG-gender. The pre-\n",
            "trained models significantly benefit from fine-tuning when\n",
            "small amounts of training data is available. Performance\n",
            "slightly increases with the availability of more training data.\n",
            "11Figure 7. Fine-tuning results for GaitFormer on CASIA-gender and FVG-gender, trained on progressively larger samples of the datasets.\n",
            "Compared to a randomly initialized network, GaitFormer benefits significantly from fine-tuning in extremely low data regimes (e.g., 10% of\n",
            "available annotated data). Compared to only pretraining on predicting attributes (Attributes Only), the Multi-Task network has consistently\n",
            "better performance across all fractions of the datasets.\n",
            "4.4. Gait-Based Pedestrian Attribute Identification\n",
            "For pedestrian attribute identification, we process a 10-\n",
            "h surveillance stream, corresponding to 10,733 tracklets,\n",
            "and use it for testing. For evaluation, we use the attribute\n",
            "pseudo-labels annotated automatically by the PAI ensem-\n",
            "ble. Figure 8 showcases R2score results for GaitFormerMD\n",
            "trained with a multi-task objective. This score is computed\n",
            "relative to the soft pseudo-labels estimated by the PAI en-\n",
            "semble. We emphasize that the model only uses movement\n",
            "information to estimate these labels, and has no informa-\n",
            "tion regarding appearance. Using a skeleton-based model\n",
            "for pedestrian attribute identification is useful in situations\n",
            "where the appearance of the person is unavailable (i.e., in\n",
            "privacy-critical scenarios). The model is effectively distill-\n",
            "ing external appearance into movement representations.\n",
            "The model obtains good results in categories such as\n",
            "Gender ,AgeGroup ,BodyType andViewpoint . The model is\n",
            "able to obtain better than average performance on categories\n",
            "such as Footwear , and some types of clothing. However,\n",
            "some clothing categories have proven to be very difficult to\n",
            "model, especially LongCoat andTrousers . We hypothesize\n",
            "that such pieces of clothing negatively affect the accuracy\n",
            "of the pose estimation model, resulting in low quality ex-\n",
            "tracted skeletons.\n",
            "These are promising results which show that external ap-\n",
            "pearance and movement are intrinsically linked together.\n",
            "This is evident in the more explicit relationship between,\n",
            "for example, footwear and gait, in which, intuitively, gait is\n",
            "severely affected by the walker’s choice of shoes. Clothing,accessories, and actions while walking can be regarded as\n",
            "“distractor” attributes, which affect gait only temporarily.\n",
            "However, there are more subtle information cues which are\n",
            "present in gait, related to the developmental aspects of the\n",
            "person (e.g., gender, age, body composition, mental state\n",
            "etc). These attributes are more stable in time, and can pro-\n",
            "vide insights into the internal workings of the walker. We\n",
            "posit that, in the future, works in gait analysis will tackle\n",
            "more rigorously the problem of estimating the internal state\n",
            "of the walker (i.e., personality/mental issues) through spe-\n",
            "cialized datasets and methods.\n",
            "4.5. Inference Time\n",
            "Using transformer architectures for processing gait has\n",
            "other advantages besides a noticeable increase in down-\n",
            "stream performance. Transformers have been shown to be\n",
            "more efficient in terms of inference time when compared to\n",
            "convolutional networks [18]. This effect is not directly cor-\n",
            "related with the number of parameters, but is rather more\n",
            "influenced by the network structure [30].\n",
            "In Figure 9, we show a comparison between multi-\n",
            "ple sizes of GaitFormer, a plain transformer module min-\n",
            "imally adapted for processing skeleton sequences, with the\n",
            "ST-GCN network, a popular architecture for skeleton ac-\n",
            "tion recognition [71] and gait analysis [34]. We computed\n",
            "the inference time across multiple period lengths (from 12\n",
            "frames to 96 frames) to evaluate the scalability when pro-\n",
            "cessing shorter/longer sequences. For each period length,\n",
            "we run 100 experiments with a batch size of 512 and show\n",
            "the mean inference time in seconds, along with the stan-\n",
            "12Figure 8. GaitFormerMD performance in terms of R2score. GaitFormerMD was trained with the multi-task objective. The model uses\n",
            "only movement information to predict attributes, and no information regarding the appearance of the individual.\n",
            "dard deviation. All experiments were run on a NVIDIA\n",
            "RTX 3060 GPU. Even with comparable and exceeding\n",
            "number of parameters (ST-GCN from [12] has 3.11M pa-\n",
            "rameters), the transformer architecture clearly outperforms\n",
            "graph-convolutional models for processing gait sequences\n",
            "across multiple sequence lengths.\n",
            "Figure 9. Inference times across processed walking duration\n",
            "length (period length) for ST-GCN and the various sizes of Gait-\n",
            "Former. We report the mean and stardard deviation across 100\n",
            "runs, for each period length.\n",
            "5. Conclusions\n",
            "In this work, we presented DenseGait, currently the\n",
            "largest dataset for pretraining gait analysis models, consist-\n",
            "ing of 217K anonymized skeleton sequences. Each skeleton\n",
            "sequence is automatically annotated with 42 appearance at-\n",
            "tributes by making use of an ensemble of pretrained PAI\n",
            "networks. We make DenseGait available to the research\n",
            "community, under open credentialized access, to promote\n",
            "further advancement in the skeleton-based gait analysis\n",
            "field. We proposed GaitFormer, a transformer that is pre-trained on DenseGait in a self-supervised and multi-task\n",
            "fashion. The model obtains 92.5% accuracy on CASIA-B\n",
            "and 85.3% accuracy on FVG, without processing any man-\n",
            "ually annotated data, achieving higher performance even\n",
            "compared to fully supervised methods. GaitFormer repre-\n",
            "sents the first application of plain transformer encoders for\n",
            "skeleton-based gait analysis, without any hand-crafted ar-\n",
            "chitectural modifications. We explored pedestrian attribute\n",
            "identification based solely on movement, without utilizing\n",
            "appearance information. GaitFormer achieves good results\n",
            "in gender, age body type, and clothing attributes.\n",
            "6. Acknowledgements\n",
            "This work was partly supported by CRC Research Grant\n",
            "2021, with funds from UEFISCDI in project CORNET\n",
            "(PN-III 1/2018) and by the Google IoT/Wearables Student\n",
            "Grants.\n",
            "References\n",
            "[1] Weizhi An, Rijun Liao, Shiqi Yu, Yongzhen Huang,\n",
            "and Pong Chi Yuen. Improving gait recognition with\n",
            "3d pose estimation. In CCBR , 2018.\n",
            "[2] Khalid Bashir, Tao Xiang, and Shaogang Gong. Gait\n",
            "recognition using gait entropy image. In 3rd Inter-\n",
            "national Conference on Imaging for Crime Detection\n",
            "and Prevention (ICDP 2009) , pages 1–6, 2009.\n",
            "[3] Josh Beal, Hao-Yu Wu, Dong Huk Park, Andrew Zhai,\n",
            "and Dmitry Kislyuk. Billion-scale pretraining with\n",
            "vision transformers for multi-task visual representa-\n",
            "tions. In Proceedings of the IEEE/CVF Winter Confer-\n",
            "ence on Applications of Computer Vision , pages 564–\n",
            "573, 2022.\n",
            "[4] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos,\n",
            "and Ben Upcroft. Simple online and realtime tracking.\n",
            "13In2016 IEEE international conference on image pro-\n",
            "cessing (ICIP) , pages 3464–3468. IEEE, 2016.\n",
            "[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie\n",
            "Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
            "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
            "Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
            "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
            "Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\n",
            "Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\n",
            "teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\n",
            "Christopher Berner, Sam McCandlish, Alec Radford,\n",
            "Ilya Sutskever, and Dario Amodei. Language mod-\n",
            "els are few-shot learners. In H. Larochelle, M. Ran-\n",
            "zato, R. Hadsell, M. F. Balcan, and H. Lin, editors,\n",
            "Advances in Neural Information Processing Systems ,\n",
            "volume 33, pages 1877–1901. Curran Associates, Inc.,\n",
            "2020.\n",
            "[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e\n",
            "J´egou, Julien Mairal, Piotr Bojanowski, and Armand\n",
            "Joulin. Emerging properties in self-supervised vi-\n",
            "sion transformers. In Proceedings of the IEEE/CVF\n",
            "International Conference on Computer Vision , pages\n",
            "9650–9660, 2021.\n",
            "[7] Andy Catruna, Adrian Cosma, and Ion Emilian Radoi.\n",
            "From face to gait: Weakly-supervised learning of gen-\n",
            "der information from walking patterns. In 2021 16th\n",
            "IEEE International Conference on Automatic Face\n",
            "and Gesture Recognition (FG 2021) , pages 1–5. IEEE,\n",
            "2021.\n",
            "[8] Hanqing Chao, Yiwei He, Junping Zhang, and Jian-\n",
            "feng Feng. Gaitset: Regarding gait as a set for cross-\n",
            "view gait recognition. In Proceedings of the AAAI\n",
            "conference on artificial intelligence , volume 33, pages\n",
            "8126–8133, 2019.\n",
            "[9] Ting Chen, Simon Kornblith, Mohammad Norouzi,\n",
            "and Geoffrey Hinton. A simple framework for con-\n",
            "trastive learning of visual representations. In Interna-\n",
            "tional conference on machine learning , pages 1597–\n",
            "1607. PMLR, 2020.\n",
            "[10] Hyejung Choi, Jongil Lim, and Sukho Lee. Body fat-\n",
            "related differences in gait parameters and physical fit-\n",
            "ness level in weight-matched male adults. Clinical\n",
            "Biomechanics , 81:105243, 2021.\n",
            "[11] Adrian Cosma and Ion Emilian Radoi. Multi - task\n",
            "learning of confounding factors in pose-based gait\n",
            "recognition. In 2020 19th RoEduNet Conference:\n",
            "Networking in Education and Research (RoEduNet) ,\n",
            "pages 1–6, 2020.\n",
            "[12] Adrian Cosma and Ion Emilian Radoi. Wildgait:\n",
            "Learning gait representations from raw surveillance\n",
            "streams. Sensors , 21(24):8387, 2021.[13] Yubin Deng, Ping Luo, Chen Change Loy, and Xiaoou\n",
            "Tang. Pedestrian attribute recognition at far distance.\n",
            "InProceedings of the 22nd ACM international confer-\n",
            "ence on Multimedia , pages 789–792, 2014.\n",
            "[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
            "Kristina Toutanova. BERT: Pre-training of deep bidi-\n",
            "rectional transformers for language understanding. In\n",
            "Proceedings of the 2019 Conference of the North\n",
            "American Chapter of the Association for Computa-\n",
            "tional Linguistics: Human Language Technologies,\n",
            "Volume 1 (Long and Short Papers) , pages 4171–4186,\n",
            "Minneapolis, Minnesota, June 2019. Association for\n",
            "Computational Linguistics.\n",
            "[15] Trung Dung Do, Van Huan Nguyen, and Hakil Kim.\n",
            "Real-time and robust multiple-view gender classifica-\n",
            "tion using gait features in video surveillance. Pattern\n",
            "Analysis and Applications , 23(1):399–413, 2020.\n",
            "[16] Carl Doersch, Abhinav Gupta, and Alexei A Efros.\n",
            "Unsupervised visual representation learning by con-\n",
            "text prediction. In Proceedings of the IEEE interna-\n",
            "tional conference on computer vision , pages 1422–\n",
            "1430, 2015.\n",
            "[17] Linhao Dong, Shuang Xu, and Bo Xu. Speech-\n",
            "transformer: A no-recurrence sequence-to-sequence\n",
            "model for speech recognition. In 2018 IEEE Inter-\n",
            "national Conference on Acoustics, Speech and Signal\n",
            "Processing (ICASSP) , pages 5884–5888, 2018.\n",
            "[18] Alexey Dosovitskiy, Lucas Beyer, Alexander\n",
            "Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\n",
            "Thomas Unterthiner, Mostafa Dehghani, Matthias\n",
            "Minderer, Georg Heigold, Sylvain Gelly, et al. An\n",
            "image is worth 16x16 words: Transformers for image\n",
            "recognition at scale. In International Conference on\n",
            "Learning Representations , 2020.\n",
            "[19] Chao Fan, Yunjie Peng, Chunshui Cao, Xu Liu, Sai-\n",
            "hui Hou, Jiannan Chi, Yongzhen Huang, Qing Li, and\n",
            "Zhiqiang He. Gaitpart: Temporal part-based model\n",
            "for gait recognition. In Proceedings of the IEEE/CVF\n",
            "conference on computer vision and pattern recogni-\n",
            "tion, pages 14225–14233, 2020.\n",
            "[20] Valentin Gabeur, Chen Sun, Karteek Alahari, and\n",
            "Cordelia Schmid. Multi-modal transformer for video\n",
            "retrieval. In European Conference on Computer Vi-\n",
            "sion, pages 214–229. Springer, 2020.\n",
            "[21] Spyros Gidaris, Praveer Singh, and Nikos Komodakis.\n",
            "Unsupervised representation learning by predicting\n",
            "image rotations. In International Conference on\n",
            "Learning Representations , 2018.\n",
            "[22] Ju Han and Bir Bhanu. ”individual recognition us-\n",
            "ing gait energy image”. IEEE transactions on pat-\n",
            "tern analysis and machine intelligence , 28:316–22, 03\n",
            "2006.\n",
            "14[23] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.\n",
            "Distilling the knowledge in a neural network. In NIPS\n",
            "Deep Learning and Representation Learning Work-\n",
            "shop , 2015.\n",
            "[24] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\n",
            "term memory. Neural computation , 9(8):1735–1780,\n",
            "1997.\n",
            "[25] Martin Hofmann, J ¨urgen Geiger, Sebastian Bach-\n",
            "mann, Bj ¨orn Schuller, and Gerhard Rigoll. The tum\n",
            "gait from audio, image and depth (gaid) database:\n",
            "Multimodal recognition of subjects and traits. Journal\n",
            "of Visual Communication and Image Representation ,\n",
            "25(1):195–206, 2014.\n",
            "[26] Jia Jian, Huang Houjing, Yang Wenjie, Chen Xi-\n",
            "aotang, and Huang Kaiqi. Rethinking of pedestrian\n",
            "attribute recognition: Realistic datasets with efficient\n",
            "method. arXiv preprint arXiv:2005.11909 , 2020.\n",
            "[27] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron\n",
            "Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,\n",
            "Ce Liu, and Dilip Krishnan. Supervised contrastive\n",
            "learning. In H. Larochelle, M. Ranzato, R. Hadsell,\n",
            "M. F. Balcan, and H. Lin, editors, Advances in Neu-\n",
            "ral Information Processing Systems , volume 33, pages\n",
            "18661–18673. Curran Associates, Inc., 2020.\n",
            "[28] Seung-uk Ko, Jeffrey M Hausdorff, and Luigi Fer-\n",
            "rucci. Age-associated differences in the gait pattern\n",
            "changes of older adults during fast-speed and fatigue\n",
            "conditions: results from the baltimore longitudinal\n",
            "study of ageing. Age and ageing , 39(6):688–694,\n",
            "2010.\n",
            "[29] Rajesh Kumar, Can Isik, and Vir V Phoha. Tread-\n",
            "mill assisted gait spoofing (tags) an emerging threat\n",
            "to wearable sensor-based gait authentication. Digital\n",
            "Threats: Research and Practice , 2(3):1–17, 2021.\n",
            "[30] David Langerman, Alex Johnson, Kyle Buettner, and\n",
            "Alan D. George. Beyond floating-point ops: Cnn\n",
            "performance prediction with critical datapath length.\n",
            "In2020 IEEE High Performance Extreme Computing\n",
            "Conference (HPEC) , pages 1–9, 2020.\n",
            "[31] Dangwei Li, Zhang Zhang, Xiaotang Chen, and Kaiqi\n",
            "Huang. A richly annotated pedestrian dataset for\n",
            "person retrieval in real surveillance scenarios. IEEE\n",
            "transactions on image processing , 28(4):1575–1590,\n",
            "2019.\n",
            "[32] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-\n",
            "Shu Fang, and Cewu Lu. Crowdpose: Efficient\n",
            "crowded scenes pose estimation and a new bench-\n",
            "mark. In Proceedings of the IEEE/CVF conference\n",
            "on computer vision and pattern recognition , pages\n",
            "10863–10872, 2019.[33] Na Li and Xinbo Zhao. A strong and robust skeleton-\n",
            "based gait recognition method with gait periodicity\n",
            "priors. IEEE Transactions on Multimedia , pages 1–\n",
            "1, 2022.\n",
            "[34] Na Li, Xinbo Zhao, and Chong Ma. Jointsgait: A\n",
            "model-based gait recognition method based on gait\n",
            "graph convolutional networks and joints relationship\n",
            "pyramid mapping. arXiv e-prints , pages arXiv–2005,\n",
            "2020.\n",
            "[35] Xiang Li, Yasushi Makihara, Chi Xu, Yasushi Yagi,\n",
            "and Mingwu Ren. Gait-based human age estima-\n",
            "tion using age group-dependent manifold learning\n",
            "and regression. Multimedia Tools and Applications ,\n",
            "77(21):28333–28354, Nov 2018.\n",
            "[36] Rijun Liao, Chunshui Cao, Edel B. Garcia, Shiqi Yu,\n",
            "and Yongzhen Huang. Pose-based temporal-spatial\n",
            "network (ptsn) for gait recognition with carrying and\n",
            "clothing variations. In Jie Zhou, Yunhong Wang,\n",
            "Zhenan Sun, Yong Xu, Linlin Shen, Jianjiang Feng,\n",
            "Shiguang Shan, Yu Qiao, Zhenhua Guo, and Shiqi\n",
            "Yu, editors, Biometric Recognition , pages 474–483,\n",
            "Cham, 2017. Springer International Publishing.\n",
            "[37] Beibei Lin, Shunli Zhang, and Xin Yu. Gait recog-\n",
            "nition via effective global-local feature representation\n",
            "and local temporal aggregation. In Proceedings of the\n",
            "IEEE/CVF International Conference on Computer Vi-\n",
            "sion, pages 14648–14656, 2021.\n",
            "[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\n",
            "Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\n",
            "C Lawrence Zitnick. Microsoft coco: Common ob-\n",
            "jects in context. In European conference on computer\n",
            "vision , pages 740–755. Springer, 2014.\n",
            "[39] Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng,\n",
            "Jing Shao, Shuai Yi, Junjie Yan, and Xiaogang Wang.\n",
            "Hydraplus-net: Attentive deep features for pedestrian\n",
            "analysis. In Proceedings of the IEEE international\n",
            "conference on computer vision , pages 350–359, 2017.\n",
            "[40] Zongyi Liu, Laura Malave, Adebola Osuntogun, Prek-\n",
            "sha Sudhakar, and Sudeep Sarkar. Toward understand-\n",
            "ing the limits of gait recognition. In Biometric Tech-\n",
            "nology for Human Identification , volume 5404, pages\n",
            "195–205. SPIE, 2004.\n",
            "[41] Y . Makihara, H. Mannami, A. Tsuji, M.A. Hossain,\n",
            "K. Sugiura, A. Mori, and Y . Yagi. The ou-isir gait\n",
            "database comprising the treadmill dataset. IPSJ Trans.\n",
            "on Computer Vision and Applications , 4:53–62, Apr.\n",
            "2012.\n",
            "[42] Yasushi Makihara, Mark S Nixon, and Yasushi Yagi.\n",
            "Gait recognition: Databases, representations, and ap-\n",
            "plications. Computer Vision: A Reference Guide ,\n",
            "pages 1–13, 2020.\n",
            "15[43] Chris A. McGibbon. Toward a better understanding of\n",
            "gait changes with age and disablement: Neuromuscu-\n",
            "lar adaptation. Exercise and Sport Sciences Reviews ,\n",
            "31(2), 2003.\n",
            "[44] Rafael M ¨uller, Simon Kornblith, and Geoffrey E Hin-\n",
            "ton. When does label smoothing help? Advances in\n",
            "neural information processing systems , 32, 2019.\n",
            "[45] M. Pat Murray, A. Bernard Drought, and Ross C.\n",
            "Kory. Walking patterns of normal men. JBJS , 46(2),\n",
            "1964.\n",
            "[46] Mohammad Hossein Nasseri, Hadi Moradi, Reshad\n",
            "Hosseini, and Mohammadreza Babaee. Simple online\n",
            "and real-time tracking with occlusion handling. arXiv\n",
            "preprint arXiv:2103.04147 , 2021.\n",
            "[47] Mark S Nixon, Tieniu N Tan, and Rama Chellappa.\n",
            "Human identification based on gait (the kluwer inter-\n",
            "national series on biometrics), 2005.\n",
            "[48] Chiara Plizzari, Marco Cannici, and Matteo Mat-\n",
            "teucci. Spatial temporal transformer network for\n",
            "skeleton-based action recognition. In International\n",
            "Conference on Pattern Recognition , pages 694–701.\n",
            "Springer, 2021.\n",
            "[49] Sudeep Sarkar, P Jonathon Phillips, Zongyi Liu,\n",
            "Isidro Robledo Vega, Patrick Grother, and Kevin W\n",
            "Bowyer. The humanid gait challenge problem: Data\n",
            "sets, performance, and analysis. IEEE transactions on\n",
            "pattern analysis and machine intelligence , 27(2):162–\n",
            "177, 2005.\n",
            "[50] Alireza Sepas-Moghaddam and Ali Etemad. View-\n",
            "invariant gait recognition with attentive recurrent\n",
            "learning of partial representations. IEEE Transac-\n",
            "tions on Biometrics, Behavior, and Identity Science ,\n",
            "3(1):124–137, 2020.\n",
            "[51] Alireza Sepas-Moghaddam and Ali Etemad. Deep gait\n",
            "recognition: A survey. IEEE Transactions on Pattern\n",
            "Analysis and Machine Intelligence , 2022.\n",
            "[52] Jamie D Shutler, Michael G Grant, Mark S Nixon, and\n",
            "John N Carter. On a large sequence-based human gait\n",
            "database. In Applications and Science in Soft Comput-\n",
            "ing, pages 339–346. Springer, 2004.\n",
            "[53] Jasvinder Pal Singh, Sanjeev Jain, Sakshi Arora, and\n",
            "Uday Pratap Singh. Vision-based gait recognition: A\n",
            "survey. IEEE Access , 6:70497–70527, 2018.\n",
            "[54] Leslie N Smith. Cyclical learning rates for training\n",
            "neural networks. In 2017 IEEE winter conference on\n",
            "applications of computer vision (WACV) , pages 464–\n",
            "472. IEEE, 2017.\n",
            "[55] Kihyuk Sohn, David Berthelot, Nicholas Carlini,\n",
            "Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Do-\n",
            "gus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fix-\n",
            "match: Simplifying semi-supervised learning withconsistency and confidence. Advances in Neural In-\n",
            "formation Processing Systems , 33:596–608, 2020.\n",
            "[56] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\n",
            "Jon Shlens, and Zbigniew Wojna. Rethinking the in-\n",
            "ception architecture for computer vision. In Proceed-\n",
            "ings of the IEEE conference on computer vision and\n",
            "pattern recognition , pages 2818–2826, 2016.\n",
            "[57] Naoto Takayanagi, Motoki Sudo, Yukari Yamashiro,\n",
            "Sangyoon Lee, Yoshiyuki Kobayashi, Yoshifumi\n",
            "Niki, and Hiroyuki Shimada. Relationship between\n",
            "daily and in-laboratory gait speed among healthy\n",
            "community-dwelling older adults. Scientific reports ,\n",
            "9(1):1–6, 2019.\n",
            "[58] Chufeng Tang, Lu Sheng, Zhaoxiang Zhang, and Xi-\n",
            "aolin Hu. Improving pedestrian attribute recognition\n",
            "with weakly-supervised multi-scale attribute-specific\n",
            "localization. In Proceedings of the IEEE/CVF Interna-\n",
            "tional Conference on Computer Vision , pages 4997–\n",
            "5006, 2019.\n",
            "[59] Torben Teepe, Ali Khan, Johannes Gilg, Fabian Her-\n",
            "zog, Stefan H ¨ormann, and Gerhard Rigoll. GaitGraph:\n",
            "Graph convolutional network for skeleton-based gait\n",
            "recognition. In 2021 IEEE International Conference\n",
            "on Image Processing (ICIP) , pages 2314–2318, 2021.\n",
            "[60] Daksh Thapar, Aditya Nigam, Divyansh Aggarwal,\n",
            "and Punjal Agarwal. Vgr-net: A view invariant gait\n",
            "recognition network. In 2018 IEEE 4th international\n",
            "conference on identity, security, and behavior analysis\n",
            "(ISBA) , pages 1–8. IEEE, 2018.\n",
            "[61] Seung uk Ko, Magdalena I. Tolea, Jeffrey M. Haus-\n",
            "dorff, and Luigi Ferrucci. Sex-specific differences\n",
            "in gait patterns of healthy older adults: Results from\n",
            "the baltimore longitudinal study of aging. Journal of\n",
            "Biomechanics , 44(10):1974–1979, 2011.\n",
            "[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
            "Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\n",
            "Kaiser, and Illia Polosukhin. Attention is all you need.\n",
            "In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\n",
            "R. Fergus, S. Vishwanathan, and R. Garnett, editors,\n",
            "Advances in Neural Information Processing Systems ,\n",
            "volume 30. Curran Associates, Inc., 2017.\n",
            "[63] Chen Wang, Junping Zhang, Jian Pu, Xiaoru Yuan,\n",
            "and Liang Wang. Chrono-gait image: A novel tempo-\n",
            "ral template for gait recognition. In Kostas Daniilidis,\n",
            "Petros Maragos, and Nikos Paragios, editors, Com-\n",
            "puter Vision – ECCV 2010 , pages 257–270, Berlin,\n",
            "Heidelberg, 2010. Springer Berlin Heidelberg.\n",
            "[64] Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu. Self-\n",
            "supervised video representation learning by pace pre-\n",
            "diction. In European conference on computer vision ,\n",
            "pages 504–521. Springer, 2020.\n",
            "16[65] Mei Wang and Weihong Deng. Deep face recognition:\n",
            "A survey. Neurocomputing , 429:215–244, 2021.\n",
            "[66] Nicolai Wojke, Alex Bewley, and Dietrich Paulus.\n",
            "Simple online and realtime tracking with a deep as-\n",
            "sociation metric. In 2017 IEEE International Confer-\n",
            "ence on Image Processing (ICIP) , pages 3645–3649.\n",
            "IEEE, 2017.\n",
            "[67] Chi Xu, Yasushi Makihara, Xiang Li, Yasushi Yagi,\n",
            "and Jianfeng Lu. Gait recognition from a single im-\n",
            "age using a phase-aware gait cycle reconstruction net-\n",
            "work. In European Conference on Computer Vision ,\n",
            "pages 386–403. Springer, 2020.\n",
            "[68] Chi Xu, Yasushi Makihara, Gakuto Ogi, Xiang Li, Ya-\n",
            "sushi Yagi, and Jianfeng Lu. The ou-isir gait database\n",
            "comprising the large population dataset with age and\n",
            "performance evaluation of age estimation. IPSJ Trans.\n",
            "on Computer Vision and Applications , 9(24):1–14,\n",
            "2017.\n",
            "[69] Shihao Xu, Jing Fang, Xiping Hu, Edith Ngai, Yi Guo,\n",
            "Victor Leung, Jun Cheng, and Bin Hu. Emotion recog-\n",
            "nition from gait analyses: Current research and future\n",
            "directions. arXiv preprint arXiv:2003.11461 , 2020.\n",
            "[70] Xiaqing Xu, Qiang Meng, Yunxiao Qin, Jianzhu Guo,\n",
            "Chenxu Zhao, Feng Zhou, and Zhen Lei. Searching\n",
            "for alignment in face recognition. In Proceedings of\n",
            "the AAAI Conference on Artificial Intelligence , vol-\n",
            "ume 35, pages 3065–3073, 2021.\n",
            "[71] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial\n",
            "temporal graph convolutional networks for skeleton-\n",
            "based action recognition. In Thirty-second AAAI con-\n",
            "ference on artificial intelligence , 2018.\n",
            "[72] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling\n",
            "Shao, and Steven CH Hoi. Deep learning for person\n",
            "re-identification: A survey and outlook. IEEE Trans-\n",
            "actions on Pattern Analysis & Machine Intelligence ,\n",
            "(01):1–1, 2021.\n",
            "[73] Shiqi Yu, Daoliang Tan, and Tieniu Tan. A frame-\n",
            "work for evaluating the effect of view angle, cloth-\n",
            "ing and carrying condition on gait recognition. In\n",
            "18th International Conference on Pattern Recognition\n",
            "(ICPR’06) , volume 4, pages 441–444. IEEE, 2006.\n",
            "[74] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and\n",
            "St´ephane Deny. Barlow twins: Self-supervised learn-\n",
            "ing via redundancy reduction. In International Con-\n",
            "ference on Machine Learning , pages 12310–12320.\n",
            "PMLR, 2021.\n",
            "[75] De Zhang, Yunhong Wang, and Bir Bhanu. Ethnic-\n",
            "ity classification based on gait using multi-view fu-\n",
            "sion. In 2010 IEEE Computer Society Conference on\n",
            "Computer Vision and Pattern Recognition-Workshops ,\n",
            "pages 108–115. IEEE, 2010.[76] Hao Zhang, Yanbin Hao, and Chong-Wah Ngo. To-\n",
            "ken shift transformer for video classification. In Pro-\n",
            "ceedings of the 29th ACM International Conference\n",
            "on Multimedia , pages 917–925, 2021.\n",
            "[77] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q\n",
            "Weinberger, and Yoav Artzi. Revisiting few-sample\n",
            "bert fine-tuning. In International Conference on\n",
            "Learning Representations , 2020.\n",
            "[78] Ziyuan Zhang, Luan Tran, Feng Liu, and Xiaoming\n",
            "Liu. On learning disentangled representations for gait\n",
            "recognition. In IEEE Transactions on Pattern Analysis\n",
            "and Machine Intelligence, Sep. 2019 , June 2019.\n",
            "[79] Zheng Zhu, Xianda Guo, Tian Yang, Junjie Huang,\n",
            "Jiankang Deng, Guan Huang, Dalong Du, Jiwen Lu,\n",
            "and Jie Zhou. Gait recognition in the wild: A bench-\n",
            "mark. In IEEE International Conference on Computer\n",
            "Vision (ICCV) , 2021.\n",
            "17 1 Multi-task deep learning for large-scale building detail extraction from high-resolution satellite imagery Zhen Qian a,b,c, Min Chen a,b,c,*, Zhuo Sun a,b,c, Fan Zhang d, Qingsong Xu e, Jinzhao Guo a,b,c, Zhiwei Xie a,b,c,f, Zhixin Zhang g a Key Laboratory of Virtual Geographic Environment (Ministry of Education of PRC), Nanjing Normal University, Nanjing, 210023, China b State Key Laboratory Cultivation Base of Geographical Environment Evolution, Nanjing, 210023, China c Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing, 210023, China d Institute of Remote Sensing and Geographical Information System, School of Earth and Space Sciences, Peking University e Department of Aerospace and Geodesy, Data Science in Earth Observation, Technical University of Munich, Arcisstraße 21, Munich, 80333, Bavaria, Germany f School of Transportation and Geomatics Engineering, Shenyang Jianzhu University, Shenyang, China g College of Geography & Marine, Nanjing University, Nanjing, PO Box 2100913, P .R. China * Corresponding author: chenmin0902@163.com Highlights l A multi-task network extracting building details from satellite images is proposed. l Sampling with dual objectives for representative sample selection is designed. l Representative samples boost prediction accuracy without restructuring network. l Comprehensive experiments validate the method's accuracy and efficiency. l Large-scale applications prove generalization, and generated datasets are released.  2 Abstract Understanding urban dynamics and promoting sustainable development requires comprehensive insights about buildings, including their spatial locations, rooftop configurations, physical forms, and urban functions. While geospatial artificial intelligence has advanced the extraction of such details from Earth observational data, existing methods often suffer from computational inefficiencies and inconsistencies when compiling unified building-related datasets for practical applications. To bridge this gap, we introduce the Multi-task Building Refiner (MT-BR), an adaptable neural network tailored for simultaneous extraction of spatial and attributional building details from high-resolution satellite imagery, exemplified by building rooftops, urban functional types, and roof architectural types. Notably, MT-BR can be fine-tuned to incorporate additional building details, extending its applicability. For large-scale applications, we devise a novel spatial sampling scheme that strategically selects limited but representative image samples. This process optimizes both the spatial distribution of samples and the urban environmental characteristics they contain, thus enhancing extraction effectiveness while curtailing data preparation expenditures. We further enhance MT-BR’s predictive performance and generalization capabilities through the integration of advanced augmentation techniques. Our quantitative results highlight the efficacy of the proposed methods. Specifically, networks trained with datasets curated via our sampling method demonstrate improved predictive accuracy relative to those using alternative sampling approaches, with no alterations to network architecture. Moreover, MT-BR consistently outperforms other state-of-the-art methods in extracting building details across various metrics, achieving gains between 2% and 20%, with a further 5% improvement when augmentation strategies were adopted. The real-world practicality of our approaches is also demonstrated in an application across Shanghai, generating a unified dataset that encompasses both the spatial and attributional details of buildings. This endeavor also underscores our methodology's potential contribution to urban studies and sustainable development initiatives. Keywords: Building details; Deep learning; Multi-task learning; Spatial sampling; High-resolution satellite imagery 1. Introduction Buildings are not only vital components of urban infrastructure but also represent markers of a city's historical, cultural, and developmental trajectory (Mısırlısoy and Günçe, 2016). Furthermore, the layout and design of buildings are closely tied to the well-being and quality of life of urban inhabitants (Altomonte et al., 2020).  3 Comprehensive details about buildings, including their spatial locations, rooftop configurations, physical forms, and urban functions, are essential for accurate urban dynamics modeling (Mohajeri et al., 2018; Zhu et al., 2023b). These details are also crucial for guiding decision-making in urban sustainable development, such as leveraging solar energy in the transition towards urban sustainability (Ali et al., 2020; Z. Zhang et al., 2022). The advent of technological advancements in geoinformatics and remote sensing has ushered in a new era of methodologies for gathering information about building infrastructure (Shi et al., 2020; Zhu et al., 2023b). Traditional data collection methods, which primarily depended on manual interpretation and on-site surveys, have been increasingly replaced by automated, data-driven techniques (Liu et al., 2019). For instance, computer vision methods based on data-driven paradigms have been instrumental in extracting building-related information from Earth observation data sources, such as high-resolution satellite imagery (Wang et al., 2014), digital surface models (Weidner and Förstner, 1995), and light detection and ranging data (Du et al., 2017). In the last decade, the surge in the volume of observational data, along with remarkable strides in computational capabilities, has catalyzed the rise of deep learning techniques (Chen et al., 2023; LeCun et al., 2015). Owing to the ability of deep neural networks to decipher intricate semantic features from large and diverse datasets, the efficacy of computer vision techniques has witnessed significant augmentation (Voulodimos et al., 2018; Xu et al., 2023; F. Zhang et al., 2023). Numerous deep learning-based applications have been developed to acquire a variety of building details essential for advanced urban sustainable development (Cao and Huang, 2021; Z. Zhang et al., 2022).  However, existing building extraction applications primarily concentrate on detecting specific building information from observational datasets, processes generally categorized as single-task operations. While some studies have explored multi-task methodologies, they often emphasize incorporating auxiliary tasks, such as edge detection, to enhance the accuracy of primary tasks like building segmentation (Guo et al., 2021a). This singular focus introduces two technical challenges. Firstly, the straightforward application of existing single-task methods becomes less efficient on larger geographical scales, owing to escalating computational demands and labor costs of training and deploying individual methods to extract varied building details. Secondly, the independently produced datasets often adhere to specific standards, which include data formats and geographic coordinate systems. This strict adherence poses challenges when aiming to merge them into a unified dataset (Qian et al., 2022a). Attempts to address these inconsistencies through post-processing could add more complexity and uncertainty, especially for those in data processing roles (Volk et al., 2014). Given the aforementioned challenges, multi-task deep learning—integrating deep neural networks with multi-task learning—emerges as a viable solution. Multi-task networks excel at simultaneously processing and  4 refining multiple learning tasks, leveraging shared information across these tasks (Zhang and Yang, 2018). In relation to extracting varied building details from Earth observation data, these networks not only enhance processing efficiency but also improve prediction accuracy (Zhou et al., 2023). Additionally, a significant benefit of multi-task networks is their capacity to produce datasets with consistent data formats and geographic references, ensuring alignment with the standards of the source observational data. Such consistency addresses the data inconsistency challenges inherent to single-task approaches, enhancing the relevance and utility of the extracted building details in wider applications. Moreover, empirical studies suggest that the diverse physical characteristics of buildings manifest as distinct spatial and spectral patterns in remote sensing images (Qian et al., 2022b). Such diversity introduces complexity to building identification via deep learning, especially when confronting issues like class imbalance (Yang et al., 2023; Zhu et al., 2023a). Much of the current research is geared towards devising advanced methods to surmount these challenges, often leaning on established benchmark datasets (Ji et al., 2018; Maggiori et al., 2017). However, this reliance can inadvertently neglect the pivotal role of data preparation and the necessity of selecting representative samples for effective model training in real-world scenarios. Furthermore, without meticulous sample selection, samples may contain redundant information, as nearby buildings often exhibit similar patterns. This redundancy not only increases annotation costs but also has the potential to cause overfitting of deep learning models. Spatial sampling is advocated as an effective strategy to select representative samples, thereby enhancing the training of neural networks. Previous research corroborates its efficacy in extracting building footprints from high-resolution satellite imagery (Sun et al., 2022; Z. Zhang et al., 2022; Zhong et al., 2021). These studies emphasize the \"representativeness\" of samples, a concept rooted in geostatistics (Zhang and Zhu, 2019), to assess sample quality. Leveraging representative samples offers two main benefits. Firstly, they provide a diverse range of samples, addressing class imbalance and improving model accuracy and generalizability (Wen et al., 2022). Secondly, a few representative samples can be as effective as a large amount of common samples for model training, thus eliminating annotation and computing costs (Sun et al., 2022). Nonetheless, current spatial sampling methodologies, which adopt stratification strategies or constraints on sample spatial distribution, may not sufficiently capture the nuanced urban environmental characteristics inherent in samples. This oversight might forego potentially valuable insights essential for optimal representative sample selection. In this study, we present the Multi-task Building Refiner (MT-BR), a multi-task oriented neural network designed to concurrently extract spatial and attributional building details from high-resolution satellite imagery. Specifically, we exemplify building rooftops as spatial details, while taking building’s urban functional type and  5 roof architectural type as examples for the attributional aspects. These selected elements play a pivotal role in aiding the urban transition towards sustainable renewable energy and in assessing the impacts of climate change (Z. Zhang et al., 2023). Enhanced with a suite of augmentation techniques, MT-BR excels in discerning a variety of building entities within complex urban environmental scenes. To facilitate extraction of building information across vast geographical expanses, we devised a novel sampling scheme. This method incorporates dual optimization objectives into stratified sampling, considering both the spatial distribution of image samples and the nuanced urban environmental characteristics. This methodology emphasizes the selection of limited but representative samples for training MT-BR, with the goal of enhancing predictive performance while minimizing data preparation costs. The primary contributions of this study can be summarized as follows: (1) We propose a spatial sampling scheme designed to select limited yet representative samples during the data preparation phase. This approach prioritizes dual objectives related to the spatial distribution of image samples and the nuanced urban environmental characteristics they represent. An enhanced simulated annealing algorithm optimizes these objectives. As a result, the proposed sampling method promotes building information extraction across broad geographical areas while reducing expenses. (2) We present MT-BR, a multi-task network, crafted for the simultaneous extraction of various building details from high-resolution satellite imagery in an end-to-end manner. The adaptable architecture of MT-BR allows for easy extensions to cater to additional building detail extraction needs. Augmentation techniques are integrated to boost MT-BR's predictive capabilities and enhance its generalization for widespread applications. (3) Our methods prove effective in large-scale applications, yielding a comprehensive dataset that includes various building details and is available for public access. The structure of this paper is as follows: Section 2 reviews related work. Section 3 elucidates the methodology we advocate. Section 4 describes the essential materials and outlines our experimental framework. Section 5 presents and discusses our experimental findings. Finally, Section 6 offers our conclusions. 2. Related work 2.1 Multi-task networks for building detail extraction Multi-task networks harness the power of shared representations across individual tasks, utilizing both hard and soft parameter sharing mechanisms (Ruder, 2017). This approach yields three pivotal benefits: firstly, multi- 6 task networks typically have a smaller size compared to the combined sizes of several distinct single-task networks, leading to more efficient storage utilization (Rago et al., 2020). Secondly, thanks to shared feature maps, these networks deliver faster inference, sidestepping the redundancy of calculations for each separate task (Luvizon et al., 2020). Thirdly, the specialized branches tailored for different learning tasks have the potential to enrich the information they process. Notably, integrating a parameter regularization mechanism within these networks can substantially boost the accuracy and generalizability of their predictions (Liu et al., 2015). Recent studies have explored the several advantages of multi-task networks, with a particular focus on the appeal of parameter regularization, in order to enhance the effectiveness of extracting building information from observational datasets. Many of these studies intefgrate auxiliary tasks to bolster the performance of the main task. For instance, by folding in auxiliary tasks such as edge detection (Guo et al., 2021b; Yin et al., 2022), and frequency and spatial feature representation (Hui et al., 2018; Yu et al., 2022), the prediction accuracy for the core task—identifying building footprints—receives a noticeable uplift. Conversely, tasks like building footprint extraction can also be employed as auxiliary mechanisms, enhancing primary tasks like detecting changes in buildings (Sun et al., 2020). While some studies have explored the extraction of various building details using multi-task networks, the spotlight has largely been on spatial details like building footprints and edges; the attributional information is frequently overlooked. 2.2 Spatial sampling scheme Acknowledging the spatial dependence and heterogeneity inherent in spatial data, numerous geostatistical studies have evolved beyond traditional random sampling. These studies adopt auxiliary data and existing knowledge to enhance the representativeness of their samples. For instance, Wang et al. (2016) proposed an advanced approach for spatial clustering stratification, which encompasses additional data types like land use and land cover (LULC), administrative boundaries, and expert insights on local areas. Drawing on these geostatistical principles, both Zhong et al. (2021) and Zhang et al. (2022) employed a stratification technique. They divided urban areas into strata based on different levels of built-up density, with the anticipation that strata with high built-up density would yield samples rich in building features. In contrast, strata with lower built-up densities are more likely to generate a higher volume of negative samples, or images devoid of buildings. This strategy is instrumental in mitigating the challenges of foreground-background imbalance encountered in building segmentation from high-resolution satellite imagery.  7 However, existing sampling methods within various strata largely depend on random mechanisms, often neglecting the spatial and environmental characteristics of the samples. Sun et al. (2022) advanced this approach by integrating spatial optimization within each stratum, with the goal of maximizing the average distance between samples. Their sampling scheme proved to generate more representative image samples compared to those from naive random sampling and stratified sampling, with building segmentation models trained using their dataset achieving superior performance. This distance optimization strategy received additional validation from Liu et al. (2023) in a geomorphological context, further emphasizing the efficacy of such refined spatial sampling. Despite considering the spatial qualities of the samples, this method falls short in adequately incorporating their environmental characteristics during the sampling stage. This limitation highlights an opportunity for enhancement, potentially achievable through the inclusion of more detailed semantic information in the sampling process. 3. Methodology 3.1 Overview This study endeavors to simultaneously extract spatial and attributional building details from high-resolution satellite imagery across extensive geographic expanses efficiently and cost-effectively. To accomplish this, we commence with the procurement of image samples characterized by high representativeness, crucial for the efficacy of subsequent model training. The workflows for this data preparation are depicted in Fig. 1. Our initial step involves overlaying a grid on the study area, with each grid cell considered a potential sampling unit. We then utilize land use and land cover (LULC) data to ascertain the proportion of built-up areas within each grid cell, while points of interest (POIs) help determine the intricate environmental characteristics — specifically, the mixed-use levels at the grid scale. Following the methodology proposed by Zhang et al. (2022) and Sun et al. (2022), we execute a stratification of the grid cells into building-dense and building-sparse categories, based on the built-up proportion. Within this stratified framework, we introduce two optimization objectives that consider both the spatial distribution of samples and their urban environmental characteristics within each stratum. These objectives specifically aim to 1) maximize the average distance between each sample; and 2) maximize the mixed-use levels represented by each sample, which are addressed through an enhanced simulated annealing algorithm. The optimal samples identified are then utilized to extract specific patches from the high-resolution satellite imagery, corresponding to their grid-coordinated locations. These image samples are subsequently annotated  8 manually to delineate rooftops and architectural roof types, while urban functional types are derived using areas of interest (AOIs). The curated datasets are finally apportioned into distinct sets for training, validation, and testing. Subsequent to these preliminary stages, we present the MT-BR, architected with multiple branches diverging from shared feature maps. Each branch is purposefully designed to extract specific aspects of building information and retains the flexibility for future expansion to accommodate additional building details. The MT-BR incorporates deformable convolutional networks to detect building entities of varying scales and geometries. Moreover, the MT-BR operates under a strategically formulated loss function, which not only guarantees concentrated convergence during training with the selected optimal samples but also establishes a balancing mechanism for the simultaneous learning of heterogeneous building details. To further enhance the MT-BR's prediction performance, we integrate a range of augmentation strategies, including the adoption of advanced backbone structures, ensemble learning techniques, and post-processing procedures. The model's performance is rigorously evaluated against a comprehensive suite of criteria, designed to provide an unbiased assessment of its predictive accuracy and efficiency across diverse building details. Finally, we apply our refined approaches throughout Shanghai, employing geographic information techniques to enhance the quality of the datasets produced. \n",
            " Fig. 1. Flowchart of the proposed methods. The process is divided into three phases: 1) preparation of representative datasets, 2) development of a multi-task deep learning network, and 3) large-scale application of \n",
            " 9 the proposed methods for building identification. The segments highlighted in pink indicate the three primary contributions of our study. LULC is land use and land cover data. POIs are points of interest data. HRSI is high-resolution satellite imagery. AOIs are areas of interest data. DCN is deformable convolutional network. FPN is feature pyramid network. RPN is region proposal network. RoI is region of interest. NMS is non-maximum suppression. FC is fully connected operation. Conv is convolutional operation. FCN is fully convolutional network. 3.2 Optimized spatial stratified sampling 3.2.1 Mixed-use levels of urban environments Mixed-use levels provide a detailed representation of urban environmental features, particularly highlighting the abundance and diversity of building entities. Various methodologies, encompassing the application of Shannon entropy, have been devised to measure the mixed-use level. Nonetheless, such indices exhibit inherent limitations. Specifically, entropy principally measures uncertainties associated with building distribution, rather than offering a holistic capture of their diversity. Addressing this gap, Yue et al. (2017) turned to Hill numbers, a conceptual framework grounded in ecological research, and proficiently translated it for urban studies application. Hill numbers provide a consolidated representation of diversity metrics, amalgamating richness, entropy, and the Simpson's index (Jost, 2006). The formulation of Hill numbers, as defined by Yue et al. (2017), is given by: 𝐷\t\"\t≡\t$%𝑝#\"$#\t%\t&'&\t/\t(&\t)\t\") (1) where 𝐷 signifies the diversity quantification associated with POIs, 𝑠 denotes the number of distinct POI categories, while 𝑝# represents the proportion of POIs ascribed to the 𝑖th category. Additionally, 𝑞 functions as a diversity order parameter. When 𝑞 is 0, the metric reverts to a richness index, signifying the number of unique POI categories within a delineated region. In contrast, when 𝑞 takes the value of 1, it evolves into an exponential of the Shannon entropy, providing insights into the level of orderliness in both POI categories and quantities. Upon reaching 𝑞=2, the metric converges to the inverse of the Simpson index, accounting for both the variance in POI types and the relative proportions of disparate POI categories (Yue et al., 2017). Hill numbers offer a holistic and quantifiable mechanism to delineate both the richness and diversity inherent to buildings. Conforming to the methodology proposed by Yue et al. (2017), our investigation likewise deploys POIs to compute Hill numbers, thereby assessing the mixed-use levels of urban environments. To synthesize a descriptive metric amalgamating the three measures derived from Hill numbers, we execute a standardization and  10 subsequent averaging. This synthesized metric furnishes a quantitative representation of mixed-use levels to inform subsequent sampling procedures: 𝑀𝑈𝐿+=∑std(𝐷\t\"),\"%-\t𝑄+1 (2) where 𝑀𝑈𝐿+ signifies a synthesized metric representing the mixed-use level for the 𝑛th sample, as calculated via Hill numbers, std(∙) indicates the min-max normalization operation, and 𝑄 stands for the order of the diversity and abundance, set to 2 in our study. 3.2.2 Simulated annealing-based dual-objective optimization Moving beyond the approach that prioritizes only the spatial distribution of samples as the optimization objective, we expanded the sampling scheme to integrate the mixed-use levels of urban environment. As a result, the dual optimization objectives now focus on minimizing cost functions related to the inverse of the average nearest neighbor (𝐶𝑜𝑠𝑡.//) (Clark and Evans, 1954) and the inverse of the average mixed-use level (𝐶𝑜𝑠𝑡.012). These objectives are articulated as follows: 𝐶𝑜𝑠𝑡.//=>∑ /+%&𝐷+𝑁A12B𝑁/𝐴EF)& (3) 𝐶𝑜𝑠𝑡.012=G∑ /+%&𝑀𝑈𝐿+𝑁H)& (4) min\t\t\t{Cost344,𝐶𝑜𝑠𝑡.012}𝑠.t.\t\t\t𝑁>0,𝐴>0,𝐷+≥0,𝐴𝑁𝑁≥0,0≤𝑀𝑈𝐿+≤1 (5) where, 𝐷+ denotes the distance between the 𝑛th sample and its nearest neighboring sample, 𝑁 is the total number of samples, and 𝐴 indicates the area covering all samples. The simulated annealing algorithm is improved to handle both objectives together. This approach is inspired by the annealing process in metallurgy, a process where a material is heated and then cooled to reduce defects (Rutenbar, 1989). In optimization, simulated annealing uses a similar approach to find an optimal solution in a complex space. This technique embodies a stochastic approach that empowers the algorithm to navigate solution spaces and circumvent local optima. The algorithm operates by iteratively investigating neighboring solutions,  11 and intriguingly, it accepts new solutions even when they might be worse than the current one. This acceptance criterion, see Eqs. (6) and (7), characterized by a decreasing probability over time, introduces an element of randomness (Bertsimas and Tsitsiklis, 1993). It permits the algorithm to occasionally accept suboptimal solutions, as the following formulations, thereby facilitating its ability to transcend local optima and persistently explore the expansive solution space. 𝑃(𝑆567#89:)=X1,ΔCost<0exp^);<=>?@!_,ΔCost≥0  (6) 𝑇AB&=𝛼∙𝑇A (7) where 𝑃(𝑆567#89:) denotes the probability of accepting the optimal samples chosen during the 𝑒th iteration, ΔCost represents the change in the cost function between the (𝑒−1)th iteration and the 𝑒th iteration, 𝑇A is the temperature parameter for the 𝑒th iteration, commencing with the initial temperature when 𝑒=0, and 𝛼 is the decay parameter, often set as a constant value smaller than 1. Within the enhanced simulated annealing framework, the algorithm adeptly manages two distinct solutions pertaining to different objectives. The stepwise procedures for handling these solutions are outlined as follows: Algorithm 1 Enhanced Simulated Annealing for Dual-Objective Optimization Input: 𝐶𝑜𝑠𝑡.// and 𝐶𝑜𝑠𝑡.012, cost functions. 𝑆7579:, full set of samples; each sample defined by ID (𝑖𝑑+), longitude (𝑋+), latitude (𝑌+), and mixed-use levels (𝑀𝑈𝐿+) attributes. 𝑁, desired sampling count. 𝐴, sampling area. 𝑇-, initial temperature. 𝛼, temperature decay rate. 𝑇75:, minimum tolerable temperature. 𝐸, maximum iteration count. Initialize ΔCost344=0, ΔCost3CDE=0, and e=0; Randomly select 𝑁 samples from 𝑆7579: to form the initial sampling solution 𝑆567#89:; Calculate 𝐶𝑜𝑠𝑡.//! and 𝐶𝑜𝑠𝑡.012! using Eqs. (3) and (4);  12 while 𝑇A>𝑇75: and e<E do Increment 𝑒 by 1; Perturb 𝑆567#89: as follows: 1. Randomly select a sample from the difference set of 𝑆7579: and 𝑆567#89: to replace the sample in 𝑆567#89: with the shortest nearest neighboring distance; 2. Randomly select another sample from the difference set of 𝑆7579: and 𝑆567#89: to replace the sample in 𝑆567#89: with the lowest mixed-use level; Compute 𝐶𝑜𝑠𝑡.//!, 𝐶𝑜𝑠𝑡.012!, ΔCost344, and ΔCost3CDE using Eqs. (3) and (4); Determine 𝑇A and 𝑃(𝑆567#89:) using Eqs. (6) and (7); Note: 𝑃(𝑆567#89:) consists of 𝑃.//(𝑆567#89:), and 𝑃.012(𝑆567#89:); if 𝑃i𝑆567#89:j>random\tnumber\tbetween\t0\tand\t1 then Accept the perturbed samples; else Retain the original sampling solution; end if end while Update 𝑆85$7_567#89: to 𝑆567#89:; Output: 𝑆85$7_567#89:, the best set of samples obtained. 3.3 Multi-task building refiner To holistically extract various building details, we propose a multi-task network, named MT-BR. This architecture is essentially an intuitive extension of the Mask R-CNN framework (He et al., 2017). The overall structure of MT-BR is depicted in Fig. 2. MT-BR is characterized by its multiple specialized branches, each meticulously crafted for versatility and scalability. Key to its performance is the integration of deformable convolutional networks, which are adept at identifying buildings that span a range of scales and possess distinct geometrical features. Accompanying this is a tailored loss function, specifically crafted for the demands of multi- 13 task learning. Furthermore, to enhance the model's performance and generalizability, we have infused MT-BR with three strategic augmentation techniques. \n",
            " Fig. 2. Overview of the MT-BR architecture. The network utilizes a Deformable Convolutional Network (DCN) to accommodate buildings of various scales and shapes. Its scalable branches, characterized by an integrated use of convolutional (Conv) and fully connected (FC) layers, are crafted within a multi-task learning paradigm, allowing for the concurrent extraction of various building details. The custom-designed loss function facilitates multi-task learning and can be expanded to accommodate additional building information extraction tasks. RPN is region proposal network. RoI is region-of-interest. NMS is non-maximum suppression. FC is full connected operation. Avg is average pooling. Deconv is deconvolutional operation. 3.3.1 Multi-branch network architecture The MT-BR architecture incorporates several distinct branches, intuitively extended from the Mask R-CNN, to simultaneously extract heterogenous building details. These branches have designated roles: segmenting rooftops, localizing building bounding boxes, classifying roof architectural types, and identifying urban functional types of buildings. The bounding box localization primarily plays a supportive auxiliary role. All these branches operate collaboratively, leveraging shared region-of-interest (RoI) feature maps. Considering the diversity in learning tasks between these branches, especially in object classification and spatial localization, our methodology draws inspiration from Wu et al. (2020). Specifically, to enhance the precision in extracting building details, \n",
            " 14 attributional classification branches utilize fully connected layers, while localization branches are built on convolutional layers. A standout feature of the MT-BR is its flexible multi-branch design. This flexibility allows for the seamless integration of additional convolutional or fully connected branches based on the characteristics of specific learning tasks. 3.3.2 Deformable convolutional network Traditional convolutional networks, characterized by their fixed spatial sampling locations stemming from regular geometric structures, often struggle to accurately represent targets with diverse shapes, sizes, and orientations. Such diversity is commonly observed in geographic objects present in remote sensing images. To overcome these representation challenges, deformable convolution and deformable RoI pooling were introduced (Dai et al., 2017; Zhu et al., 2019) , granting neural networks the ability to adapt their spatial sampling locations. This adaptability is achieved through the incorporation of learnable offsets, added to the conventional convolution operations as depicted in Eq. (8). Notably, no additional supervision is needed for these offsets. An interesting perspective on deformable convolution is its resemblance to a local attention mechanism, which enables the model to prioritize salient patterns in a localized manner. This attention mechanism enhances the model's capacity to discern contextual information across various scales. Within the context of satellite imagery, the deformable strategies exhibit the capability to discern distinct building objects, minimizing misinterpretations arising from intricate environmental backgrounds (Zhu et al., 2018). 𝑦(𝑝-)=%𝑤(𝑝+)∙𝑥(𝑝-+𝑝++∆𝑝+)6\"∈H (8) where 𝑥 represents the input feature map, 𝑦 denotes the output feature map, 𝑝+ is the position set for a convolution operation—with 𝑝+ belonging to a set that includes positions like (−1,−1), (−1,0), up to (0,1) and (1,1) for a 3 × 3 convolution—and ∆𝑝+ corresponds to the learnable offset. 3.3.3 Loss function The primary objective of the loss function in our approach is to facilitate gradient backpropagation for multi-task learning, specifically tailored for extracting various building details. Our methodology extends the original loss function from Mask R-CNN (He et al., 2017), with a modification being the use of the mean to compute the loss for attributional information learning, emphasizing its efficiency and simplicity. This design choice also  15 ensures that the model remains scalable and can efficiently integrate any potential building attributional classification tasks in the future. The multi-task loss function, as detailed in our study, is articulated in the subsequent equations: ℒIJ>=−𝑦IJ>Klog𝑦wIJ> (9) ℒJ=I=x0.5|𝑦J=I−𝑦wJ=I|L,|𝑦:5M−𝑦wJ=I|<1|𝑦J=I−𝑦wJ=I|−0.5,|𝑦:5M−𝑦wJ=I|≥1 (10) ℒNO>P=−1𝑛L%𝑦NO>P#Qlog𝑦wNO>P#Q+(1−𝑦NO>P#Q)log(1−𝑦wNO>P#Q)&R#,QR+\t(11) ℒ?=?OJ=12iℒIJ>#$%&+ℒIJ>'()*j+ℒJ=I+ℒNO>P (12) where ℒ!\"# is the cross-entropy loss for classification task (Ren et al., 2015), ℒ\"$! is the smooth-L1 loss for localization task (Ren et al., 2015), ℒ%&#' is the binary cross-entropy loss for segmentation task (He et al., 2017), 𝑦IJ> and 𝑦wIJ> are the ground-truth and predicted labels of RoIs, 𝑦:5M and 𝑦wJ=I are the ground-truth and predicted coordinates of RoIs, 𝑛 is the size of output images, 𝑦NO>P#Q and 𝑦wNO>P#Q are the ground-truth and predicted labels for the image cell at position (𝑖,𝑗). 3.3.4 Prediction augmentation To bolster the prediction accuracy of MT-BR and address potential inconsistencies in its outputs, especially for real-world applications, we have integrated three prediction augmentation strategies. 1) HRNet Backbone The High-Resolution Network (HRNet) offers a solution to the prevalent challenge of diminishing resolution in feature maps as convolution layers proliferate (Sun et al., 2019). This proficiency stems from its innovative architecture, characterized by key features such as multi-scale fusion, resilient skip connections, and consistent high-resolution representation. These features collectively ensure the preservation of vital contextual information from input data, guaranteeing the retention of intricate details, irrespective of the network's depth. Given that extraction tasks necessitate nuanced and sharp feature representations to distinguish diverse building attributes, HRNet's architecture is optimally suited to meet these demands. 2) Ensemble Strategies  16 Integrating deep neural networks with ensemble learning techniques has consistently demonstrated its efficacy in amplifying prediction accuracy and bolstering model generalization (K. Zhang et al., 2022). Two prominent techniques, multi-model decision fusion and test-time augmentation, have been particularly effective in refining post-training results, establishing their utility across a spectrum of applications. In our methodology, we capitalize on ensemble learning by training an array of MT-BRs, each equipped with distinct backbones, drawing from the Bagging concept. This approach that the model ensembles exhibit a wide variance in their predictions, potentially bolstering holistic performance. During the inference process, we adopt a test-time data augmentation strategy, subjecting the data to manipulations such as multi-scale resizing and rotation to enrich input diversity. The predictions derived from these varied inputs are then aggregated to produce consolidated outcomes. By integrating these ensemble techniques, our aim is to harness the collective strengths of multiple models and varied data inputs. This approach seeks to enhance both the prediction accuracy and generalization capability when extracting diverse building details from high-resolution satellite imagery. 3) Post-processing During the inference stage of the MT-BR, multiple detection boxes—each indicating different predicted labels—can overlay the same building entity. This overlap can result in discrepancies within the predicted pixels of a singular building entity. To address this issue, we assess the prevalence of each predicted pixel class for individual building entities. Following this, the most commonly predicted label is designated as the conclusive attributional label for that entity. This process can be mathematically described as: 𝑎𝑡𝑡𝑟#=argmaxM(count(𝑐,𝐶#)|𝐶#|) (13) where 𝑎𝑡𝑡𝑟# denotes the definitive attributional label of the 𝑖th building entity, 𝐶𝑖 represents the set of predicted pixel classes for the 𝑖th entity, count(𝑐,𝐶#) is the frequency of class 𝑐 within 𝐶𝑖, and |𝐶#| corresponds to the aggregate count of 𝐶𝑖. 4. Experimental preparation 4.1 Materials 4.1.1 Study area Shanghai, situated in China's eastern region and depicted in Fig. 3, serves as the study area of this research. Covering an expansive administrative area of approximately 6340 km², Shanghai boasts a population exceeding  17 24 million, solidifying its position as one of China's most significant and densely populated metropolises. Nationally recognized for its leading role in construction and economic progression, Shanghai's intense urbanization and intricate architectural tapestry render it a fitting subject for our study. The LULC status of Shanghai, displayed on the right side of Fig. 3, was sourced from ESRI in 2022 and possesses a spatial resolution of 10 meters (Karra et al., 2021). This data reveals that built-up regions account for over 60% of Shanghai's total land area. The city's urban layout is distinguished by its complex patterns and varied functional zones. Extracting building details from high-resolution satellite imagery within such a multifaceted urban setting presents formidable challenges. Nevertheless, accomplishing this task in Shanghai holds significant value, with potential implications for enhancing urban sustainability initiatives, refining management strategies, and enriching the repository of urban geographic information. \n",
            " Fig. 3. Geographic representation of Shanghai's location and its land use and land cover distribution. 4.1.2 High-resolution satellite imagery High-resolution satellite imagery was obtained from satellites like the WorldView-2/3 series, GeoEye-1, SkySat, and Pleiades in 2022. These images combine both panchromatic and multispectral data captured at the same time by the satellite and are normalized to a 0.5 m resolution (Guo et al., 2023). An illustrative example is presented in Fig. 4 (a). \n",
            " 18  Fig. 4. Datasets used in this study. (a) High-resolution satellite image. (b) POIs. (c) AOIs. 4.1.3 Social sensing data The social sensing data utilized in this study encompasses POIs and AOIs. These datasets not only identify geographic object locations but also provide insights into their functional roles. Moreover, they offer a glimpse into the socio-economic context and the dominant urban environmental characteristics of the area. Through open application programming interfaces from AMap, we have aggregated an extensive dataset for Shanghai as of 2022, which includes 1.5 million POIs and 2.8 thousand AOIs. The information captured within these POIs and AOIs spans various attributes, including title, type, province, city, address, longitude, and latitude. Due to the distinct spatial scales at which POIs and AOIs are depicted, disparities in the urban functions they depict may arise. Aligned with our research objectives, we undertook a reclassification of the functional types of the collated POIs and AOIs. Furthermore, we streamlined them into three primary categories: residential, industrial-commercial, ad \n",
            " 19 public services, as shown in Fig.4 (b) and Fig.4 (c). This classification adhered to AMap's categorization guidelines and insights from Zhang et al. (Z. Zhang et al., 2023). 4.2 Description and assessment of various building details 4.2.1 Building spatial information Our study defines rooftops as a representation of building spatial information, visualized as a two-dimensional projection of building roofs (see Fig. 5). A technical challenge emerges due to the disparity in size between original satellite image and the typical image patch size fed into deep neural networks. Such mismatches can lead to scenarios where a single building is split across multiple image patches, causing alignment issues upon stitching. Moreover, the precision of pixel-level building roof recognition can sometimes yield rough edges. These challenges can be mitigated using techniques such as expansion prediction and vector simplification (Z. Zhang et al., 2022). \n",
            " Fig. 5. Illustration of building rooftops. (a) High-resolution satellite image. (b) Annotated rooftops. (c) High-resolution satellite image overlaid with rooftop annotations. To gauge the efficacy of delineating rooftops, we turn to established pixel-level evaluation metrics: the F1-score and the Intersection over Union (IoU) (Li et al., 2019). Prior to computing these metrics, foundational metrics like precision and recall need to be derived from the confusion matrix. Their respective equations are provided below: Precision=TPTP+FP (14) Recall=TPTP+FN (15) \n",
            " 20 F1=2∗Precision∗RecallPrecision+Recall (16) IoU=TPTP+FN+FP (17) where TP represents true positive predictions, FP stands for false positive predictions, and FN denotes false negative predictions. 4.2.2 Building attributional information The attributional information of buildings covers the urban functional types and roof architectural types in this study. Urban functional types encompass the fundamental responsibilities that buildings undertake in urban environments, such as residential, industrial, commercial, and public utilities. Past research has delineated urban functional types at a more macroscopic level, focusing on territorial units like contiguous grids or adjacent blocks (Lu et al., 2022; Qian et al., 2020). Our endeavor, however, targets the identification of specific functional categories at the granularity of individual building entities. This shift in scale considerably amplifies the intricacy of the modeling process relative to earlier methodologies. Moreover, based on empirical observations, it has been discovered that buildings with similar designs in specific places, such as industrial zones, tend to be classified as either commercial or industrial. Hence, our examination centers on three discrete classifications: residential, industrial-commercial, and public service categories, as shown in Fig.4 (c). Roof architectural types shed light on a city's architectural progression and cultural heritage, crucial for cultural preservation and urban planning research (Sun et al., 2017; Wang et al., 2022). In this study, Shanghai's roof architectural types are broadly divided into four main categories: flat, gable, hip, and complex, following the classifications by Mohajeri et al. (2018). These categories are visually presented in Fig. 6. Complex roofs encompass a variety of designs, combining features from multiple basic architectural types or presenting distinct designs indicative of a building's style.   21  Fig. 6. Depiction of various roof architectural types. To assess the efficacy of our approach in extracting building attributional information, we adopt the Kappa coefficient, a metric that has garnered widespread adoption in the domain of remote sensing image classification (Huang et al., 2018). The Kappa coefficient oscillates between -1 and 1, with values exceeding 0.6 typically denoting a robust alignment between the derived classification outcomes and the ground truth, thereby vouching for the robustness of the results. The Kappa coefficient, , as defined by Chmura Kraemer et al. (2002), is given as:  Kappa=6+\t)\t6!&\t)\t6!  （18） where 𝑝5 represents the observed accuracy, signifying the fraction of accurately classified instances, 𝑝A symbolizes the expected accuracy under random classification scenarios, deduced as the mean of the probabilities associated with each category emanating from the classifier's output. 4.3 Experimental setup Our experiments are structured around two main components: data sampling and building information extraction. Data sampling is primarily conducted on a personal computer, equipped with an Intel i7-10700K CPU, 64GB RAM, and an NVIDIA GeForce RTX 3090 GPU. The software stack for this phase includes the Windows operating system, the ArcGIS platform, and a Python development environment. In contrast, the building information extraction phase is executed on a robust supercomputing platform that runs on the Ubuntu operating system. The computational framework is fitted with eight NVIDIA GeForce RTX 3090 GPUs and leverages \n",
            " 22 software tools such as the Python environment, PyTorch (Paszke et al., 2019), and MMdetection (Chen et al., 2019b). To guarantee the integrity and fairness of our experiments, we've adhered to uniform standards across the board. Specifically, the input data size for each model remains fixed at 512 × 512 pixels. Additionally, we've standardized model hyperparameters, such as the backbone network, optimizer, and learning rate, for consistent multi-model comparisons. A detailed description of these hyperparameters is available in Table 1. Acknowledging the stochastic nature in training deep neural networks, we repeated every training and evaluation sessions five times. For transparency, we present our quantitative metric outcomes as mean values accompanied by standard deviations. Table 1 Experimental configuration details. Hyperparameter Value Image size 512 × 512 Backbone ResNet101 Optimizer Stochastic gradient descent Learning rate 0.001 Batch size 4 Epoch 108 5. Experimental results and discussions 5.1 Data sampling experiments 5.1.1 Sampling preparation For the collection of image samples that are highly representative for training our model, we overlay a grid on the study area, where each cell measures 1 × 1 km. Each grid cell serves as a distinct sampling unit, and image samples are subsequently extracted from the high-resolution satellite imagery corresponding to the spatial coordinates of optimal units. To determine the urbanization degree within each grid, ESRI LULC data is used to calculate the proportion of built-up areas in every grid, presented in Fig. 7 (a). Subsequently, POIs are harnessed to calculate the mixed-use levels at the grid level, as shown in Fig. 7(b), in line with our proposed method. Spatial patterns of both built-up proportions and mixed-use levels are closely correlated, particularly evident in urbanized  23 districts such as Huangpu, Changning, Jing'an, Xuhui, Yangpu, Hongkou, and Putuo. To differentiate between building-dense and building-sparse strata, we use the lower quartile value of the built-up proportion, established at 0.16, as recommended by Zhang et al. (2022) and Sun et al. (2022). This stratification is illustrated in Fig. 7(c) and Fig. 7(d). \n",
            " Fig. 7. Depictions of built-up proportion and mixed-use levels. (a) Built-up proportion mapping. (b) Mixed-use level mapping. (c) Mixed-use levels within the building-dense stratum. (d) Mixed-use levels within the building-sparse stratum. 5.1.2 Sampling results and analysis Utilizing the proposed sampling schme with dual-objective optimization, we select a total of 100 sampling units, each spanning an area of 1 × 1 km. The majority, 80 units, are focused on the building-dense stratum, with the remaining 20 targeting the building-sparse stratum. The optimization process, which uses the simulated annealing algorithm, undergo 5000 iterations, with its progression charted in Fig. 8. \n",
            " 24  Fig. 8. Dual-objective optimization progression. (a) Mixed-use levels optimization within the building-dense stratum. (b) Mixed-use levels optimization within the building-sparse stratum. (c) Spatial distribution optimization within the building-dense stratum. (d) Spatial distribution optimization within the building-sparse stratum. Convergence in the building-sparse stratum is achieved more rapidly than in the building-dense stratum, due to its smaller area and reduced number of sampling units. Approximately 3000 iterations are needed for the building-sparse stratum, while the building-dense stratum necessitates around 4000 iterations. During the optimization, spatial sample distribution exhibits more fluctuations, while the optimization for mixed-use levels remains relatively stable. This disparity arises from the intertwined interactions during the dual-objective optimization. The spatial locations of these optimal samples are visualized in Fig. 9, which highlights samples spread across different locales, primarily in regions with high mixed-use levels. \n",
            " 25  Fig. 9. Spatial distribution of optimal samples. (a) Optimal sample locations (marked in orange) within the building-dense stratum. (b) Optimal sample locations (marked in blue) within the building-sparse stratum. To validate the efficacy of our proposed sampling method, we conduct a hands-on inspection of the chosen image samples. Some of these are presented in Fig. 10. A clear spatial distinction is evident between the building-dense and building-sparse strata. The former prominently showcases a higher concentration of buildings compared to the latter. The high diversity of building entities across these samples stems from the optimization of mixed-use levels in our sampling technique. Even within the building-sparse stratum, most samples indicate a sporadic presence of buildings, rather than a complete absence. Given the stochastic nature of the simulated annealing optimization process, a few samples might not align with the described characteristics. However, this deviation doesn't undermine the overall sample quality and representativeness. Such a diverse sampling outcome equips the MT-BR to grasp varied building characteristics, especially in geographically complex contexts. \n",
            " 26  Fig. 10. A selection from the optimal image samples. (a) Image samples within the building-dense stratum. (b) Image samples within the building-sparse stratum. In recent study, Sun et al. (2022) proposed a geospatial stratified and optimized sampling (GSOS) method, which emphasizes single-objective optimization centered on the spatial distribution of samples. The efficacy of GSOS was demonstrated to surpass traditional sampling methods, such as random and stratified sampling. Based on their work, this study introduces a dual-objective optimization-based sampling technique. To validate our method’s effectiveness, we compare it against three other sampling strategies: random sampling, stratified sampling, and GSOS. We utilize four distinct sampling schemes to compile both training and testing datasets. Each training dataset comprises 500 sampling units, whereas each test dataset comprises 100 units. Moreover, to manage labeling costs effectively, this comparative research is primarily centered on rooftop extraction. Following Sun et al. (2022), we utilize the DeepLab V3+, training it on our assembled training datasets using a publicly available vectorized rooftop dataset for annotations (Z. Zhang et al., 2022). To ensure a balanced evaluation, networks trained with different sampling approaches are assessed against four distinct test datasets. As illustrated in Fig. 11, networks trained on datasets derived from our sampling method exhibit a distinct superiority. When assessed against identical test datasets, these networks achieve superior F1 scores and IoU metrics in comparison to those trained with alternative datasets, presenting a margin of roughly 0.5% to 1%. This empirical assessment underscores the efficacy of our sampling technique in curating representative samples. Additionally, stratified sampling and methods that consider the optimization of spatial distribution appear to offer advantages over naive random sampling in the selection of representative samples. The outcomes also provide \n",
            " 27 compelling evidence of the pivotal role such representative samples hold in effectively training deep neural networks. Consequently, these networks can achieve enhanced predictive accuracy without necessitating modifications to sophisticated network architectures. \n",
            " Fig. 11. Comparative performance of various sampling methods, represented by F1-score and IoU metrics across different test datasets. Each row highlights results from a distinct test dataset, and individual boxes denote performance statistics of DeepLab V3+ trained with different training datasets. (a-b) Results tested on datasets from random sampling (RS). (c-d) Results tested on datasets from stratified sampling (SS). (e-f) Results tested on datasets employing geospatial stratified and optimized sampling (GSOS). Notably, GSOS, as proposed by Sun et al. (2022), utilizes single-objective optimization focusing on the spatial distribution of samples. (g-h) Results tested on datasets derived from our sampling scheme. \n",
            " 28 5.1.3 Data pre-processing Upon curating representative samples, the subsequent phase involves labeling and annotation activities, which include image slicing and dataset segmentation. For this task, we employ ArcGIS Pro for manual annotations of rooftops within the image data and the identification of roof architectural types. Using the reclassified AOIs data, rooftops were matched to determine the urban functional type of each building. For rooftops without a direct overlay from the AOIs, the urban functional type was manually determined. Given our model's requirement for an input dimension of 512×512 pixels, both the primary images and their associated labels were processed through mask cropping, utilizing a consistent 512×512 pixel sliding window. This process generated 900 segmented image patches. Of these, 80% were designated for training, 10% for validation, and the remaining 10% for testing. Fig. 12 shows that the distribution of building details is consistent across the training, validation, and testing datasets. \n",
            " \n",
            " 29 Fig. 12. Distribution of annotated building details across the datasets. The red graphs denote the training dataset, green graphs correspond to the validation dataset, and blue graphs represent the test dataset. (a-c) Rooftop density distribution across the respective datasets. (d-f) Urban functional type distribution across the respective datasets. (g-i) Roof architectural type distribution across the respective datasets. 5.2 Comparative and ablation experiments In order to evaluate the effectiveness of the proposed methods, we conduct a series of comparison and ablation experiments using our optimally selected dataset. This encompasses both qualitative and quantitative analyses, with an emphasis on the consistency of predicted outcomes, as well as the precision and efficiency of inferences. The methods involved in the comparative evaluation encompass five networks dedicated to semantic segmentation tasks: UNet (Ronneberger et al., 2015), DeepLab V3+ (Chen et al., 2018), UPerNet (Xiao et al., 2018), SegFormer (Xie et al., 2021), and SETR (Zheng et al., 2021), as well as five networks designed for instance segmentation tasks: Mask RCNN (He et al., 2017), MS R-CNN (Huang et al., 2019), HTC (Chen et al., 2019a), SOLO V2 (Wang et al., 2020), and Mask2Former (Cheng et al., 2022). This diverse set of networks encompasses a broad range of cutting-edge techniques in visual recognition, ranging from convolutional neural network-based to Transformer-based designs, and spanning both single and dual-stage methodologies. A concise overview of these comparative methods is presented below. • UNet (Ronneberger et al., 2015) is a renowned architecture for image segmentation, and features a U-shaped network structure designed to effectively capture contextual information. • DeepLab V3+ (Chen et al., 2018) is distinguished by its deep convolutional neural network equipped with atrous convolutions, allowing it to handle large receptive fields and intricate details. • UPerNet (Xiao et al., 2018) focuses on multi-scale feature integration and refinement, enhancing the representation of objects in various sizes within an image. • SegFormer (Xie et al., 2021) introduces Transformer architecture to the segmentation task and is recognized for its strong performance in capturing global contextual information. For our experiments, we employ the robust SegFormerB5 model. • SETR (Zheng et al., 2021) captures long-range dependencies and global context in images by leveraging the self-attention mechanism, addressing intricate spatial correlations and fine-grained details effectively. • Mask RCNN (He et al., 2017) is a two-stage instance segmentation method that combines object detection with pixel-wise mask prediction, making it suitable for precise object localization.  30 • MS R-CNN (Huang et al., 2019) extends the Mask RCNN by addressing multi-scale challenges, improving the handling of objects at different sizes and resolutions. • HTC (Chen et al., 2019a), or Hybrid Task Cascade, enhances instance segmentation by sequentially refining object masks, resulting in better accuracy. • SOLO V2 (Wang et al., 2020) is a single-stage instance segmentation model, eliminating the need for multi-stage processing while maintaining high accuracy and speed. • Mask2Former (Cheng et al., 2022) is a Transformer-based model for instance segmentation, directly predicting object masks and classes, efficiently handling complex scenes without region proposals. Furthermore, a straightforward approach to enhance the capability of single-task networks to handle multiple learning tasks is by merging tasks to simultaneously manage the extraction of various attributional details. For instance, identifying urban functional types can be structured as a tri-class classification task, while recognizing roof architectural types can be approached as a quad-class classification task. Merging these tasks culminates in a composite task featuring a twelve-class classification. Subsequently, the predictions can be deconstructed to yield results for discrete tasks. We incorporate this mixed-class strategy within the instance segmentation methods, thereby establishing pertinent experimental benchmarks. 5.2.1 Analysis of result consistency Initially, we examine the results produced by different approaches to understand their unique characteristics, using Mask R-CNN, DeepLab V3+, Mask R-CNN with the mixed-class strategy, and MT-BR as representative examples. When employing Mask R-CNN and DeepLab V3+ to extract diverse building details, separate sequential runs are required, resulting in inconsistencies in their respective prediction outcomes. These discrepancies are evident in the red boxes of Fig. 13, showcasing predictions from Mask R-CNN and DeepLab V3+. In contrast, the mixed-class methodology and our proposed MT-BR adeptly sidestep this inconsistency issue. Furthermore, rooftops derived using the semantic segmentation method, such as DeepLab V3+, tend to merge into a single entity. This contrasts with the outputs of the instance segmentation method, Mask R-CNN, which delineates distinct and countable instances. This unique capability of instance segmentation method influenced our choice to adopt it as a baseline method.  31  Fig. 13. Depiction of prediction result consistency. 5.2.2 Assessment of prediction accuracy We conduct a series of ablation experiments to validate the architectural design and augmentation strategies of MT-BR, with the quantitative findings presented in Tables 2 and 3. The outcomes underscore the efficacy of adopting both fully-connected and convolutional approaches in multi-branches, as well as the utilization of deformable convolution in bolstering prediction accuracy for both spatial and attributional information extraction. Additionally, the tailored augmentation strategies further enhance MT-BR's predictive accuracy. When compared with multi-model decision fusion, test-time augmentation exhibits a decline in prediction accuracy. Such results shed light on the potential significance of image scales and positioning in the extraction of various building details from high-resolution satellite imagery. Post-processing not only offers improvements in the accuracy of building attributional information extraction, but its implementation is also crucial to ensuring consistent pixel types within individual building instances. Consequently, subsequent experiments and practical applications in Shanghai employ the proposed augmentation strategies, excluding test-time augmentation. Table 2 Ablation analysis for prediction accuracy of MT-BR’s architecture. The baseline embodies the extended Mask R-CNN with multi-branches. FC-Conv denotes the incorporation of fully-connected and convolutional layers in multi-branches. DCN stands for deformable convolutional network. A higher score (↑) signifies better performance. Baseline FC-Conv branches DCN Rooftop  Urban functional type Roof architectural type F1 (↑) IoU (↑) Kappa (↑) Kappa (↑) P   78.89 (± 1.01) 65.14 (± 1.38) 69.60 (± 0.70) 61.90 (± 0.84) \n",
            " 32 P P  79.07 (± 0.56) 65.39 (± 0.76) 69.56 (± 0.76) 62.33 (± 0.14) P  P 79.67 (± 0.54) 66.22 (± 0.74) 70.20 (± 0.72) 63.08 (± 0.83) P P P 80.18 (± 0.48) 66.91 (± 0.66) 70.72 (± 0.71) 64.47 (± 0.82) Table 3 Ablation analysis for prediction accuracy of augmentation strategies. TTA represents test time augmentation, while MMDF indicates multi-model decision fusion. A higher score (↑) signifies better performance. MT-BR HRNet Ensemble Strategies Post- processing Rooftop  footprint Urban  functional  type Roof  architecture  type TTA MMDF F1 (↑) IoU (↑) Kappa (↑) Kappa (↑) P     80.18 (± 0.48) 66.91 (± 0.66) 70.72 (± 0.71) 64.47 (± 0.82) P P    82.03 (± 0.49) 69.54 (± 0.71) 71.87 (± 0.62) 67.20 (± 0.46) P P P   81.50 (± 0.70) 68.78 (± 0.99) 71.76 (± 0.36) 65.12 (± 1.04) P P  P  84.30 (± 0.14) 72.86 (± 0.21) 74.57 (± 0.38) 69.81 (± 0.32) P P P P  83.95 (± 0.13) 72.34 (± 0.19) 74.21 (± 0.09) 68.71 (± 0.39) P P  P P 84.30 (± 0.14) 72.86 (± 0.21) 74.67 (± 0.59) 70.04 (± 0.37) We subsequently compare our proposed methods with selected state-of-the-art techniques, with the results delineated in Table 4. Given the inconsistencies in rooftops extracted by single-task methods, we average the evaluation metrics for their separate predictions. The results reveal that MT-BR, especially when enhanced with augmentation strategies, showcases marked superiority over other methods in both spatial and attributional information extraction. While the mixed-class approach can handle multiple tasks simultaneously, its accuracy in attributional information extraction is notably lower. This underperformance might be attributed to a pronounced class imbalance. Specifically, upon examining the attributes of labeled building instances, we observe an imbalanced distribution, as illustrated in Fig. 14. The distributions of urban functional types and roof architectural types are skewed, and their combined representation further amplifies this imbalance, posing challenges for deep neural networks. Moreover, the comparison underscores that methods designed for semantic segmentation tasks underperform in spatial information extraction compared to their counterparts designed for instance segmentation. This insight offers a valuable reference for real-world applications. It's also noteworthy that sophisticated methods based on Transformer, such as SegFormer, SETR, and Mask2Former, have not achieved as remarkable results as  33 they might in other general datasets. This could be attributed to the fact that training these intricate models typically demands a large number of datasets, which are limited in our study. Additionally, when deploying these methods for multi-task operations, tailored designs and adjustments could be essential to harness their full potential. Table 4 Assessment of prediction accuracy. A higher score (↑) signifies better performance. Methods Rooftop Urban functional type Roof architectural type F1 (↑) IoU (↑) Kappa (↑) Kappa (↑) UNet 71.11 (± 1.11) 55.22 (± 1.34) 63.47 (± 0.60) 51.49 (± 0.82) DeepLab V3+ 77.11 (± 0.58) 62.76 (± 0.77) 68.83 (± 0.65) 61.10 (± 1.42) UPerNet 77.55 (± 0.76) 63.35 (± 1.01) 69.65 (± 0.90) 62.88 (± 0.70) SegFormer 72.77 (± 0.80) 57.23 (± 0.98) 66.03 (± 0.99) 54.73 (± 1.67) SETR 63.10 (± 0.82) 46.13 (± 0.86) 56.45 (± 0.72) 45.09 (± 1.13) Mask R-CNN 78.85 (± 0.56) 65.11 (± 0.76) 69.25 (± 1.08) 62.59 (± 1.29) MS R-CNN 78.56 (± 0.57) 64.70 (± 0.76) 68.96 (± 1.04) 62.29 (± 0.54) HTC 75.96 (± 0.58) 61.24 (± 0.75) 65.00 (± 0.90) 57.82 (± 0.81) SOLOv2 74.20 (± 0.65) 59.01 (± 0.81) 65.32 (± 1.36) 56.96 (± 1.56) Mask2Former 77.16 (± 0.85) 62.83 (± 1.13) 69.38 (± 1.39) 61.98 (± 1.27) Mask R-CNN (Mixed class) 77.43 (± 0.93) 63.18 (± 1.23) 60.88 (± 0.73) 48.47 (± 0.86) MS R-CNN (Mixed class) 77.45 (± 1.23) 63.21 (± 1.65) 60.19 (± 1.60) 47.06 (± 1.36) HTC (Mixed class) 69.42 (± 3.65) 53.26 (± 4.29) 53.38 (± 4.01) 41.32 (± 2.72) SOLO V2 (Mixed class) 69.10 (± 0.97) 52.79 (± 1.13) 54.01 (± 1.20) 43.73 (± 1.02) Mask2Former (Mixed class) 72.27 (± 1.49) 56.60 (± 1.82) 56.94 (± 1.56) 45.10 (± 1.14) MT-BR 80.18 (± 0.48) 66.91 (± 0.66) 70.72 (± 0.71) 64.47 (± 0.82) MT-BR (Augmentation) 84.30 (± 0.14) 72.86 (± 0.21) 74.67 (± 0.59) 70.04 (± 0.37)  34  Fig. 14. Distribution of building instance attributes. (a) Distribution of urban functional types. (b) Distribution of roof architectural types. (c) Combined distribution of both types. Subsequently, we undertake a qualitative assessment of the outputs from various methods, as depicted in Figs. 15-17. For this analysis, we chose a mix of urban and rural images, showcasing varying building densities, ranging from highly dense areas to sparsely populated ones. The visualized predictions are consistent with our earlier quantitative observations. Our proposed method demonstrates commendable precision and recall in identifying building rooftops. It successfully identifies buildings often missed by other methods, all the while upholding a high accuracy. While the proposed approach occasionally misclassifies certain attributes, its outputs are largely congruent with the ground truth, underscoring its robust feature representation and inference capabilities. However, the semantic segmentation methods encounter difficulties in accurately defining specific buildings, often resulting in an amalgamation of their outcomes, as evident in Fig. 15. In contrast, instance segmentation-based methods excel in demarcating individual structures and correlating them with the appropriate attributes, as illustrated in Fig.16. Similar to the quantitative evaluation, instance segmentation methods employing the mixed-class strategy reveal certain limitations, such as missing building entities and misclassifying \n",
            " 35 attributes, especially in densely populated images, as seen in Fig.17. Through empirical evaluations across diverse geographical settings, the MT-BR, enhanced with augmentation techniques, consistently showcases superior prediction capabilities and robust generalization. \n",
            " Fig. 15. Predictive visualization from semantic segmentation methods. (a) Urban functional types associated with rooftops. (b) Roof architectural types associated with rooftops. “Aug.” is the prediction augmentation. \n",
            " 36  Fig. 16. Predictive visualization from instance segmentation methods. (a) Urban functional types associated with rooftops. (b) Roof architectural types associated with rooftops. “Aug.” is the prediction augmentation. \n",
            " 37  Fig. 17. Predictive visualization from instance segmentation methods utilizing the mixed-class strategy. (a) Urban functional types associated with rooftops. (b) Roof architectural types associated with rooftops. “Mix.” is the mixed-class strategy, and “Aug.” is the prediction augmentation. 5.2.3 Assessment of inference efficiency The extraction of various building information requires the single-task methods to run through multiple workflows, rather than in an end-to-end manner. Consequently, we assess the inference efficiency of the single-task methods, ideally calculating time costs by summing the durations needed for different tasks. The efficiency comparisons, tabulated in Table 5, highlight that employing a mixed-class strategy significantly bolsters multi-tasking efficiency, resulting in the fastest inference times. It's worth noting that methods based on semantic segmentation prove to be more time-efficient than their instance segmentation counterparts. This discrepancy \n",
            " 38 arises from the inherently intricate structures of instance segmentation methods, designed to tackle both detection and segmentation tasks concurrently. While MT-BR boasts an end-to-end design and inference capability, its speed does not markedly surpass that of mixed-class strategy methods. To delve deeper, we conducted an ablation study on MT-BR's inference efficiency, with findings summarized in Table 6. Several factors can impede the achievement of optimal inference speeds, the most significant one of which is the simultaneous utilization of fully-connected and convolutional layers across multiple branches. Furthermore, the incorporation of deformable convolution can slightly reduce the inference duration. By avoiding from utilizing the methodology that combines fully-connected and convolutional operations in multi-branches, the inference speed of MT-BR outperforms the majority of the chosen comparative methods, while yet maintaining a commendable level of prediction accuracy (see Table 2 and Table 4). These experimental findings provide prospective users with valuable insights, enabling them to make informed decisions on deploying MT-BR based on their specific requirements—whether they prioritize swift computation or heightened prediction accuracy. Table 5 Assessment of inference efficiency. Lower inference times (↓) and higher FPS scores (↑) signifies faster processing speed. Methods Inference times per image / ms (↓) FPS (↑) UNet 53.10 (± 0.40) 18.83 (± 0.14) DeepLab V3+ 69.78 (± 0.26) 14.33 (± 0.05) UPerNet 76.44 (± 0.66) 13.08 (± 0.11) SegFormer 117.99 (± 0.61) 8.48 (± 0.04) SETR 126.82 (± 0.76) 7.89 (± 0.05) Mask R-CNN 80.99 (± 0.20) 12.35 (± 0.03) MS R-CNN 84.58 (± 0.38) 11.82 (± 0.05) HTC 146.02 (± 1.91) 6.85 (± 0.09) SOLOv2 82.91 (± 0.26) 12.06 (± 0.04) Mask2Former 221.34 (± 0.81) 4.52 (± 0.02) Mask R-CNN (Mixed class) 40.75 (± 0.26) 24.54 (± 0.16) MS R-CNN (Mixed class) 43.11 (± 0.30) 23.20 (± 0.16) HTC (Mixed class) 84.74 (± 1.21) 11.80 (± 0.17)  39 SOLO V2 (Mixed class) 41.71 (± 0.21) 23.98 (± 0.12) Mask2Former (Mixed class) 108.96 (± 0.47) 9.18 (± 0.04) MT-BR 119.04 (± 1.56) 8.40 (± 0.11) Table 6 Ablation analysis for inference efficiency of MT-BR. Lower inference times (↓) and higher FPS scores (↑) signifies faster processing speed. Baseline FC-Conv branches DCN Inference times per image / ms (↓) FPS (↑) P   57.61 (± 0.23) 17.36 (± 0.07) P P  111.61 (± 0.60) 8.96 (± 0.05) P  P 67.39 (± 0.21) 14.84 (± 0.05) P P P 119.04 (± 1.56) 8.40 (± 0.11) 5.3 Generalization assessment and large-scale application In the final phase, we employ the MT-BR equipped with augmentation strategies to extract comprehensive building details across the entirety of Shanghai. We implement the expansion prediction technique to address the challenges posed by uneven transitions during the stitching of image patches. The originally rasterized dataset is converted to the ESRI Shapefile format and streamlined using the Douglas-Peucker algorithm for the sake of data management and transmission. Fig. 18 showcases the extracted building details across Shanghai. The generated dataset reveals approximately 1.77 million building entities, closely aligning with the number of buildings detailed in the 2020 publicly available dataset for Shanghai, which registers around 1.72 million buildings (Z. Zhang et al., 2022). The delineated building rooftops mirror the administrative boundary of Shanghai, with the city's urban core displaying prominent building clusters, indicative of Shanghai's robust economic development. When compared to the background data provided by the high-resolution satellite imagery, the extracted building entities demonstrate a notable level of generality and precision. The success of this work can be attributed to our sampling method that ensured the inclusion of representative data samples as input, and the development of MT-BR with augmentation strategies, which effectively managed multiple recognition tasks. The approach finely distinguishes between individual buildings and intricate geographical contexts, ensuring the accurate delineation of rooftops across both urban and rural settings. Moreover, each building is discernibly mapped with roof architectural types, as seen in Fig. 18 (a), and urban functional types as in Fig. 18 (b). This mapping aligns coherently with Shanghai's current development and planning. A significant portion of the buildings, 48.40%, have gable roofs, while flat roofs constitute 32.31%.  40 Urban central areas predominantly showcase flat-roofed buildings, whereas rural or suburban regions lean towards gable roofs. Functionally, residential buildings dominate the landscape, making up 76.78%, while industrial-commercial and public facility buildings account for 18.93% and 4.29%, respectively. Among residential building, gable roofs are prevalent, accounting for 50.83%, with flat and complex roof types each covering around 20%. On the other hand, commercial, industrial, and public facility buildings primarily exhibit flat and gable types. It is worth noting that the dataset generated in this work is openly available to the public and holds significant promise for advancing the understanding of urban dynamics and fostering sustainable development. \n",
            " Fig. 18. Building details delineated in Shanghai. (a) Urban functional types associated with rooftops. (b) Roof architectural types associated with rooftops. \n",
            " 41 6. Conclusion This study presented the MT-BR, an approach designed to address the limitations of single-task methodologies. Focusing on building details like rooftops, urban functional types, and architectural roof types, we validated the utility of MT-BR. Its flexible design allows for potential extensions to capture diverse building information from high-resolution satellite imagery. For large-scale applicability, we developed a dual-objective optimized spatial sampling scheme. This scheme prioritizes both the spatial distribution of samples and the urban characteristics they represent, aiming to select limited but representative training samples and enhance MT-BR's predictive performance. The effectiveness of this approach is confirmed by an empirical experiment. Subsequently, we introduce augmentation strategies to refine MT-BR's capabilities. In comparative tests, MT-BR shows consistent improvements over other deep learning methods, with performance gains ranging from 2% to 20%, further amplified by 5% with our augmentation techniques. Our ablation studies also provide guidance for users, allowing them to customize MT-BR's architecture based on their specific requirements. At last, the application of our methodologies in Shanghai demonstrates their practical utility and the quality of the generated building-related datasets. Despite the advancements presented in this study, there remain avenues for further research. While high-resolution satellite imagery offers detailed physical insights, it sometimes falls short in providing a comprehensive view of building information, especially when discerning subtle attribute types. A more holistic understanding of urban environments might be achieved by incorporating diverse data sources, such as social media feeds or multi-sensor satellite imagery. With the emerging prominence of Transformer-based methods tailored for multi-modal data, future efforts could explore these methodologies. However, they necessitate vast datasets and judicious architectural design. Furthermore, the estimation of building height, an essential parameter, poses its own set of challenges and will be a central focus in our forthcoming research endeavors. Author contribution Zhen Qian: Conceptualization, Methodology, Software, Writing - Original Draft, Writing - Review & Editing. Min Chen: Conceptualization, Methodology, Supervision, Writing - Review & Editing, Funding acquisition. Zhuo Sun: Methodology, Writing – review & editing. Fan Zhang: Methodology, Writing – review & editing. Qingsong Xu: Methodology, Formal analysis, Writing – review & editing. Jinzhao Guo: Investigation. Zhiwei Xie: Writing – review & editing. Zhixin Zhang: Formal analysis, Validation, Writing – review & editing.  42 Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Data availability The primary code and dataset obtained from this work, containing building details from Shanghai, are available for public access at the following repository: https://github.com/ChanceQZ/BuildingDetails-Multitask. Acknowledgments This work was supported in part by the National Natural Science Foundation of China (No. 42325107), in part by the National Natural Science Foundation of China (No. 42101353), and in part by the Humanities and Social Sciences Foundation of the Ministry of Education of China (General Program) (No. 21YJC790129). References Ali, U., Shamsi, M.H., Bohacek, M., Purcell, K., Hoare, C., Mangina, E., O’Donnell, J., 2020. A data-driven approach for multi-scale GIS-based building energy modeling for analysis, planning and support decision making. Applied Energy 279, 115834. Altomonte, S., Allen, J., Bluyssen, P.M., Brager, G., Heschong, L., Loder, A., Schiavon, S., Veitch, J.A., Wang, L., Wargocki, P., 2020. Ten questions concerning well-being in the built environment. Building and Environment 180, 106949. Bertsimas, D., Tsitsiklis, J., 1993. Simulated annealing. Statistical science 8, 10–15. Cao, Y ., Huang, X., 2021. A deep learning method for building height estimation using high-resolution multi-view imagery over urban areas: A case study of 42 Chinese cities. Remote Sensing of Environment 264, 112590. Chen, K., Pang, J., Wang, J., Xiong, Y ., Li, X., Sun, S., Feng, W., Liu, Z., Shi, J., Ouyang, W., others, 2019a. Hybrid task cascade for instance segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4974–4983. Chen, K., Wang, J., Pang, J., Cao, Y ., Xiong, Y ., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., others, 2019b. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155.  43 Chen, L.-C., Zhu, Y ., Papandreou, G., Schroff, F., Adam, H., 2018. Encoder-decoder with atrous separable convolution for semantic image segmentation, in: Proceedings of the European Conference on Computer Vision (ECCV). pp. 801–818. Chen, M., Qian, Z., Boers, N., Jakeman, A.J., Kettner, A.J., Brandt, M., Kwan, M.-P., Batty, M., Li, W., Zhu, R., Luo, W., Ames, D.P., Barton, C.M., Cuddy, S.M., Koirala, S., Zhang, F., Ratti, C., Liu, Jian, Zhong, T., Liu, Junzhi, Wen, Y ., Yue, S., Zhu, Z., Zhang, Z., Sun, Z., Lin, J., Ma, Z., He, Y ., Xu, K., Zhang, C., Lin, H., Lü, G., 2023. Iterative integration of deep learning in hybrid Earth surface system modelling. Nature Reviews Earth & Environment. https://doi.org/10.1038/s43017-023-00452-7 Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R., 2022. Masked-attention mask transformer for universal image segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1290–1299. Chmura Kraemer, H., Periyakoil, V .S., Noda, A., 2002. Kappa coefficients in medical research. Statistics in medicine 21, 2109–2129. Clark, P.J., Evans, F.C., 1954. Distance to nearest neighbor as a measure of spatial relationships in populations. Ecology 35, 445–453. Dai, J., Qi, H., Xiong, Y ., Li, Y ., Zhang, G., Hu, H., Wei, Y ., 2017. Deformable convolutional networks, in: Proceedings of the IEEE International Conference on Computer Vision. pp. 764–773. Du, S., Zhang, Y ., Zou, Z., Xu, S., He, X., Chen, S., 2017. Automatic building extraction from LiDAR data fusion of point and grid-based features. ISPRS journal of photogrammetry and remote sensing 130, 294–307. Guo, H., Shi, Q., Marinoni, A., Du, B., Zhang, L., 2021a. Deep building footprint update network: A semi-supervised method for updating existing building footprint from bi-temporal remote sensing images. Remote Sensing of Environment 264, 112589. Guo, H., Shi, Q., Marinoni, A., Du, B., Zhang, L., 2021b. Deep building footprint update network: A semi-supervised method for updating existing building footprint from bi-temporal remote sensing images. Remote Sensing of Environment 264, 112589. Guo, J., Xu, Q., Zeng, Y ., Liu, Z., Zhu, X.X., 2023. Nationwide urban tree canopy mapping and coverage assessment in Brazil from high-resolution remote sensing images using deep learning. ISPRS Journal of Photogrammetry and Remote Sensing 198, 1–15. He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask r-cnn, in: Proceedings of the IEEE International  44 Conference on Computer Vision. pp. 2961–2969. Huang, B., Zhao, B., Song, Y ., 2018. Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery. Remote Sensing of Environment 214, 73–86. Huang, Z., Huang, L., Gong, Y ., Huang, C., Wang, X., 2019. Mask scoring r-cnn, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6409–6418. Hui, J., Du, M., Ye, X., Qin, Q., Sui, J., 2018. Effective building extraction from high-resolution remote sensing images with multitask driven deep neural network. IEEE Geoscience and Remote Sensing Letters 16, 786–790. Ji, S., Wei, S., Lu, M., 2018. Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set. IEEE Transactions on geoscience and remote sensing 57, 574–586. Jost, L., 2006. Entropy and diversity. Oikos 113, 363–375. Karra, K., Kontgis, C., Statman-Weil, Z., Mazzariello, J.C., Mathis, M., Brumby, S.P., 2021. Global land use/land cover with Sentinel 2 and deep learning, in: 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS. IEEE, pp. 4704–4707. LeCun, Y ., Bengio, Y ., Hinton, G., 2015. Deep learning. nature 521, 436–444. Li, W., He, C., Fang, J., Zheng, J., Fu, H., Yu, L., 2019. Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source GIS data. Remote Sensing 11, 403. Liu, D., Cheng, W., Qian, Z., Deng, J., Liu, J., Wang, X., 2023. Boundary Delineator for Martian Crater Instances with Geographic Information and Deep Learning. Remote Sensing 15, 4036. https://doi.org/10.3390/rs15164036 Liu, P., Liu, X., Liu, M., Shi, Q., Yang, J., Xu, X., Zhang, Y ., 2019. Building footprint extraction from high-resolution images via spatial residual inception convolutional neural network. Remote Sensing 11, 830. Liu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y .-Y. ,  2 0 1 5 .  R e p r e s e n t a t i o n  l e a r n i n g  u s i n g  m u l t i-task deep neural networks for semantic classification and information retrieval. Lu, W., Tao, C., Li, H., Qi, J., Li, Y ., 2022. A unified deep learning framework for urban functional zone extraction based on multi-source heterogeneous data. Remote Sensing of Environment 270, 112830.  45 Luvizon, D.C., Picard, D., Tabia, H., 2020. Multi-task deep learning for real-time 3D human pose estimation and action recognition. IEEE transactions on pattern analysis and machine intelligence 43, 2752–2764. Maggiori, E., Tarabalka, Y ., Charpiat, G., Alliez, P., 2017. Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark, in: 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE, pp. 3226–3229. Mısırlısoy, D., Günçe, K., 2016. Adaptive reuse strategies for heritage buildings: A holistic approach. Sustainable cities and society 26, 91–98. Mohajeri, N., Assouline, D., Guiboud, B., Bill, A., Gudmundsson, A., Scartezzini, J.-L., 2018. A city-scale roof shape classification using machine learning for solar energy applications. Renewable Energy 121, 81–93. https://doi.org/10.1016/j.renene.2017.12.096 Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., others, 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32. Qian, Z., Chen, M., Yang, Y ., Zhong, T., Zhang, F., Zhu, R., Zhang, K., Zhang, Z., Sun, Z., Ma, P., Lü, G., Ye, Y., Yan, J., 2022a. Vectorized dataset of roadside noise barriers in China using street view imagery. Earth Syst. Sci. Data 14, 4057–4076. https://doi.org/10.5194/essd-14-4057-2022 Qian, Z., Chen, M., Zhong, T., Zhang, F., Zhu, R., Zhang, Z., Zhang, K., Sun, Z., Lü, G., 2022b. Deep Roof Refiner: A detail-oriented deep learning network for refined delineation of roof structure lines using satellite imagery. International Journal of Applied Earth Observation and Geoinformation 107, 102680. https://doi.org/10.1016/j.jag.2022.102680 Qian, Z., Liu, X., Tao, F., Zhou, T., 2020. Identification of Urban Functional Areas by Coupling Satellite Images and Taxi GPS Trajectories. Remote Sensing 12, 2449. https://doi.org/10.3390/rs12152449 Rago, A., Piro, G., Boggia, G., Dini, P., 2020. Multi-task learning at the mobile edge: An effective way to combine traffic classification and prediction. IEEE Transactions on Vehicular Technology 69, 10362–10374. Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems 28. Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomedical image segmentation, in: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015:  46 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, pp. 234–241. Ruder, S., 2017. An Overview of Multi-Task Learning in Deep Neural Networks. Rutenbar, R.A., 1989. Simulated annealing algorithms: An overview. IEEE Circuits and Devices magazine 5, 19–26. Shi, Y ., Li, Q., Zhu, X.X., 2020. Building segmentation through a gated graph convolutional neural network with deep structured feature embedding. ISPRS Journal of Photogrammetry and Remote Sensing 159, 184–197. Sun, K., Xiao, B., Liu, D., Wang, J., 2019. Deep high-resolution representation learning for human pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5693–5703. Sun, L., Tang, Y ., Zhang, L., 2017. Rural building detection in high-resolution imagery based on a two-stage CNN model. IEEE Geoscience and Remote Sensing Letters 14, 1998–2002. Sun, Y ., Zhang, X., Huang, J., Wang, H., Xin, Q., 2020. Fine-grained building change detection from very high-spatial-resolution remote sensing images based on deep multitask learning. IEEE Geoscience and Remote Sensing Letters 19, 1–5. Sun, Z., Zhang, Z., Chen, M., Qian, Z., Cao, M., Wen, Y ., 2022. Improving the Performance of Automated Rooftop Extraction through Geospatial Stratified and Optimized Sampling. Remote Sensing 14, 4961. https://doi.org/10.3390/rs14194961 Vo u l o d i m o s ,  A . ,  D o u l a m i s ,  N . ,  D o u l a m i s ,  A . ,  P r o t o p a p a d a k i s ,  E . ,  o t h e r s ,  2 0 1 8 .  D e e p  l e a r n i n g  f o r  c o m p u t e r  vision: A brief review. Computational intelligence and neuroscience 2018. Wang, J., Yang, X., Qin, X., Ye, X., Qin, Q., 2014. An efficient approach for automatic rectangular building extraction from very high resolution optical satellite imagery. IEEE Geoscience and Remote Sensing Letters 12, 487–491. Wang, J.-F., Zhang, T.-L., Fu, B.-J., 2016. A measure of spatial stratified heterogeneity. Ecological indicators 67, 250–256. Wang, X., Zhang, R., Kong, T., Li, L., Shen, C., 2020. Solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems 33, 17721–17732. Wang, Y., Li, S., Teng, F., Lin, Y., Wang, M., Cai, H., 2022. Improved mask R-CNN for rural building roof type recognition from uav high-resolution images: a case study in hunan province, China. Remote  47 Sensing 14, 265. Weidner, U., Förstner, W., 1995. Towards automatic building extraction from high-resolution digital elevation models. ISPRS journal of Photogrammetry and Remote Sensing 50, 38–49. Wen, Y., Li, X., Mu, H., Zhong, L., Chen, H., Zeng, Y., Miao, S., Su, W., Gong, P., Li, B., others, 2022. Mapping corn dynamics using limited but representative samples with adaptive strategies. ISPRS Journal of Photogrammetry and Remote Sensing 190, 252–266. Wu, Y., Chen, Y., Yuan, L., Liu, Z., Wang, L., Li, H., Fu, Y., 2020. Rethinking classification and localization for object detection, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10186–10195. Xiao, T., Liu, Y ., Zhou, B., Jiang, Y ., Sun, J., 2018. Unified perceptual parsing for scene understanding, in: Proceedings of the European Conference on Computer Vision (ECCV). pp. 418–434. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P., 2021. SegFormer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems 34, 12077–12090. Xu, Q., Shi, Y ., Guo, J., Ouyang, C., Zhu, X.X., 2023. UCDFormer: Unsupervised Change Detection Using a Transformer-driven Image Translation. IEEE Transactions on Geoscience and Remote Sensing. Yang, J., Matsushita, B., Zhang, H., 2023. Improving building rooftop segmentation accuracy through the optimization of UNet basic elements and image foreground-background balance. ISPRS Journal of Photogrammetry and Remote Sensing 201, 123–137. Yin, J., Wu, F., Qiu, Y., Li, A., Liu, C., Gong, X., 2022. A Multiscale and Multitask Deep Learning Framework for Automatic Building Extraction. Remote Sensing 14, 4744. https://doi.org/10.3390/rs14194744 Yu, B., Chen, F., Wang, N., Yang, L., Yang, H., Wang, L., 2022. MSFTrans: a multi-task frequency-spatial learning transformer for building extraction from high spatial resolution remote sensing images. GIScience & Remote Sensing 59, 1978–1996. Yue, Y., Zhuang, Y., Yeh, A.G.O., Xie, J.-Y. ,  M a ,  C .-L., Li, Q.-Q., 2017. Measurements of POI-based mixed use and their relationships with neighbourhood vibrancy. International Journal of Geographical Information Science 31, 658–675. https://doi.org/10.1080/13658816.2016.1220561 Zhang, F., Miranda, A.S., Duarte, F., Vale, L., Hack, G., Chen, M., Liu, Y ., Batty, M., Ratti, C., 2023. Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery.  48 Zhang, G., Zhu, A.-X., 2019. A representativeness-directed approach to mitigate spatial bias in VGI for the predictive mapping of geographic phenomena. International Journal of Geographical Information Science 33, 1873–1893. https://doi.org/10.1080/13658816.2019.1615071 Zhang, K., Qian, Z., Yang, Y ., Chen, M., Zhong, T., Zhu, R., Lv, G., Yan, J., 2022. Using street view images to identify road noise barriers with ensemble classification model and geospatial analysis. Sustainable Cities and Society 78, 103598. https://doi.org/10.1016/j.scs.2021.103598 Zhang, Y ., Yang, Q., 2018. An overview of multi-task learning. National Science Review 5, 30–43. https://doi.org/10.1093/nsr/nwx105 Zhang, Z., Chen, M., Zhong, T., Zhu, R., Qian, Z., Zhang, F., Yang, Y ., Zhang, K., Santi, P., Wang, K., others, 2023. Carbon mitigation potential afforded by rooftop photovoltaic in China. Nature Communications 14, 2347. Zhang, Z., Qian, Z., Zhong, T., Chen, M., Zhang, K., Yang, Y ., Zhu, R., Zhang, F., Zhang, H., Zhou, F., Yu, J., Zhang, B., Lü, G., Yan, J., 2022. Vectorized rooftop area data for 90 cities in China. Sci Data 9, 66. https://doi.org/10.1038/s41597-022-01168-x Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., Fu, Y ., Feng, J., Xiang, T., Torr, P.H., others, 2021. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6881–6890. Zhong, T., Zhang, Z., Chen, M., Zhang, K., Zhou, Z., Zhu, R., Wang, Y ., Lü, G., Yan, J., 2021. A city-scale estimation of rooftop solar photovoltaic potential based on deep learning. Applied Energy 298, 117132. Zhou, Z., Fu, C., Weibel, R., 2023. Move and remove: Multi-task learning for building simplification in vector maps with a graph convolutional neural network. ISPRS Journal of Photogrammetry and Remote Sensing 202, 205–218. Zhu, J., Fang, L., Ghamisi, P., 2018. Deformable convolutional neural networks for hyperspectral image classification. IEEE Geoscience and Remote Sensing Letters 15, 1254–1258. Zhu, R., Guo, D., Wong, M.S., Qian, Z., Chen, M., Yang, B., Chen, B., Zhang, H., You, L., Heo, J., others, 2023a. Deep solar PV refiner: A detail-oriented deep learning network for refined segmentation of photovoltaic areas from satellite imagery. International Journal of Applied Earth Observation and Geoinformation 116, 103134.  49 Zhu, R., Kwan, M.-P., Perera, A., Fan, H., Yang, B., Chen, B., Chen, M., Qian, Z., Zhang, H., Zhang, X., others, 2023b. GIScience can facilitate the development of solar cities for energy transition. Advances in Applied Energy 100129. Zhu, X., Hu, H., Lin, S., Dai, J., 2019. Deformable convnets v2: More deformable, better results, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9308–9316.  Episodic Multi-Task Learning with\n",
            "Heterogeneous Neural Processes\n",
            "Jiayi Shen1, Xiantong Zhen1,2∗, Qi (Cheems) Wang3, Marcel Worring1\n",
            "1University of Amsterdam, Netherlands, {j.shen, m.worring}@uva.nl\n",
            "2Inception Institute of Artificial Intelligence, Abu Dhabi, UAE, zhenxt@gmail.com\n",
            "3Kaiyuan Mathematical Sciences Institute, Changsha, China, hhq123go@gmail.com\n",
            "Abstract\n",
            "This paper focuses on the data-insufficiency problem in multi-task learning within\n",
            "an episodic training setup. Specifically, we explore the potential of heterogeneous\n",
            "information across tasks and meta-knowledge among episodes to effectively tackle\n",
            "each task with limited data. Existing meta-learning methods often fail to take ad-\n",
            "vantage of crucial heterogeneous information in a single episode, while multi-task\n",
            "learning models neglect reusing experience from earlier episodes. To address the\n",
            "problem of insufficient data, we develop Heterogeneous Neural Processes (HNPs)\n",
            "for the episodic multi-task setup. Within the framework of hierarchical Bayes,\n",
            "HNPs effectively capitalize on prior experiences as meta-knowledge and capture\n",
            "task-relatedness among heterogeneous tasks, mitigating data-insufficiency. Mean-\n",
            "while, transformer-structured inference modules are designed to enable efficient\n",
            "inferences toward meta-knowledge and task-relatedness. In this way, HNPs can\n",
            "learn more powerful functional priors for adapting to novel heterogeneous tasks in\n",
            "each meta-test episode. Experimental results show the superior performance of the\n",
            "proposed HNPs over typical baselines, and ablation studies verify the effectiveness\n",
            "of the designed inference modules.\n",
            "1 Introduction\n",
            "Deep learning models have made remarkable progress with the help of the exponential increase in\n",
            "the amount of available training data [ 1]. However, many practical scenarios only have access to\n",
            "limited labeled data [ 2]. Such data-insufficiency sharply degrades the model’s performance [ 2,3].\n",
            "Both meta-learning and multi-task learning have the potential to alleviate the data-insufficiency issue.\n",
            "Meta-learning can extract meta-knowledge from past episodes and thus enables rapid adaptation\n",
            "to new episodes with a few examples only [ 4–7]. Meanwhile, multi-task learning exploits the\n",
            "correlation among several tasks and results in more accurate learners for all tasks simultaneously [ 8–\n",
            "11]. However, the integration of meta-learning and multi-task learning in overcoming the data-\n",
            "insufficiency problem is rarely investigated.\n",
            "In episodic training [ 4], existing meta-learning methods [ 4–7,12,13] in every meta-training or\n",
            "meta-test episode learn a single-task. In this paper, we refer to this conventional setting as episodic\n",
            "single-task learning . This setting restricts the potential for these models to explore task-relatedness\n",
            "within each episode, leaving the learning of multiple heterogeneous tasks in a single episode under-\n",
            "explored. We consider multiple tasks in each episode as episodic multi-task learning . The crux\n",
            "of episodic multi-task learning is to generalize the ability of exploring task-relatedness from meta-\n",
            "training to meta-test episodes. The differences between episodic single-task learning and episodic\n",
            "multi-task learning are illustrated in Figure 1. To be specific, we restrict the scope of the problem\n",
            "∗Currently with United Imaging Healthcare, Co., Ltd., China.\n",
            "37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2310.18713v1  [cs.LG]  28 Oct 2023Figure 1: Illustration of episodic multi-task learning. Each row corresponds to a meta-training\n",
            "or meta-test episode. Different colors represent different label spaces among episodes; the same\n",
            "color with different shades represents different categories in the same task. Compared with episodic\n",
            "single-task learning, episodic multi-task learning simultaneously handles several related tasks in a\n",
            "single episode.\n",
            "setup to the case where tasks in each meta-training or meta-test episode are heterogeneous but also\n",
            "relate to each other by sharing the same target space.\n",
            "The neural process (NP) family [ 12,13], as typical meta-learning probabilistic models [ 14], efficiently\n",
            "quantifies predictive uncertainty with limited data, making it in principle well-suited for tackling\n",
            "the problem of data-insufficiency. However, in practice, it is challenging for vanilla NPs [ 12] with a\n",
            "global latent variable to encode beneficial heterogeneous information in each episode. This issue is\n",
            "also known as the expressiveness bottleneck [ 15,16], which weakens the model’s capacity to handle\n",
            "insufficient data, especially when faced with diverse heterogeneous tasks.\n",
            "To better resolve the data-insufficiency problem, we develop Heterogeneous Neural Processes (HNPs)\n",
            "for episodic multi-task learning. As a new member of the NP family, HNPs improve the expressiveness\n",
            "of vanilla NPs by introducing a hierarchical functional space with global and local latent variables.\n",
            "The remainder of this work is structured as follows: We introduce our method in Section (2). Related\n",
            "work is overviewed in Section (3). We report experimental results with analysis in Section (4), after\n",
            "which we conclude with a technical discussion, existing limitations, and future extensions. In detail,\n",
            "our technical contributions are two-fold:\n",
            "•Built on the hierarchical Bayes framework, our developed HNPs can simultaneously gen-\n",
            "eralize meta-knowledge from past episodes to new episodes and exploit task-relatedness\n",
            "across heterogeneous tasks in every single episode. This mechanism makes HNPs more\n",
            "powerful when encoding complex conditions into functional priors.\n",
            "•We design transformer-structured inference modules to infer the hierarchical latent variables,\n",
            "capture task-relatedness, and learn a set of tokens as meta-knowledge. The designed modules\n",
            "can fuse the meta-knowledge and heterogeneous information from context samples in a\n",
            "unified manner, boosting the generalization capability of HNPs across tasks and episodes.\n",
            "Experimental results show that the proposed HNPs together with transformer-structured inference\n",
            "modules, can exhibit superior performance on regression and classification tasks under the episodic\n",
            "multi-task setup.\n",
            "2 Methodology\n",
            "Notations2.We will now formally define episodic multi-task learning. For a single episode τ,\n",
            "we consider Mheterogeneous but related tasks I1:M\n",
            "τ={Im\n",
            "τ}M\n",
            "m=1. Notably, the subscript denotes\n",
            "an episode, while superscripts are used to distinguish tasks in this episode. In the episodic multi-\n",
            "task setup, tasks in a single episode are heterogeneous since they are sampled from different task\n",
            "distributions {p(Im)}M\n",
            "m=1, but are related at the same time as they share the target space Yτ.\n",
            "2For ease of presentation, we abbreviate a set {(·)m}M\n",
            "m=1as(·)1:M, where Mis a positive integer. Likewise,\n",
            "{(·)o}O\n",
            "o=1is abbreviated as (·)1:O. For convenience, the notation table is provided in Appendix B.\n",
            "2To clearly relate to the modeling of vanilla neural processes [ 12], this paper follows its nomenclature\n",
            "to define each task. Note that in vanilla neural processes context andtarget are often respectively\n",
            "called support andquery in conventional meta-learning [ 4,5]. Each task Im\n",
            "τcontains a context\n",
            "set with limited training data Cm\n",
            "τ={¯xm\n",
            "τ,i,¯ym\n",
            "τ,i}NC\n",
            "i=1and a target set Tm\n",
            "τ={xm\n",
            "τ,j, ym\n",
            "τ,j}NT\n",
            "j=1, where\n",
            "NCandNTare the numbers of context samples and target samples, respectively. ¯xm\n",
            "τ,iandxm\n",
            "τ,j\n",
            "represent features of context and target samples; while ¯ym\n",
            "τ,i, ym\n",
            "τ,j∈ Yτare their corresponding targets,\n",
            "where i= 1,2, ..., N C;j= 1,2, .., NT;m= 1,2, ..., M . For simplicity, we denote the set of target\n",
            "samples and their corresponding ground-truths by xm\n",
            "τ={xm\n",
            "τ,j}NT\n",
            "j=1,ym\n",
            "τ={ym\n",
            "τ,j}NT\n",
            "j=1. For an episode\n",
            "τ, episodic multi-task learning aims to perform simultaneously well on each corresponding target set\n",
            "Tm\n",
            "τ, m= 1,2.., M , given the collection of context sets C1:M\n",
            "τ.\n",
            "For classification, this paper follows the protocol of meta models [ 4,5,17], such as O-way K-shot\n",
            "setup, clearly suffering from the data-insufficiency problem. Thus, episodic multi-task classification\n",
            "can be cast as a M-task O-way K-shot supervised learning problem. An episode has Mrelated\n",
            "classification tasks, and each of them has a context set with Kdifferent instances from each of the O\n",
            "classes [ 5]. It is worth mentioning that the target spaces of meta-training episodes do not overlap\n",
            "with any categories in those of meta-test episodes.\n",
            "2.1 Modeling and Inference of Heterogeneous Neural Processes\n",
            "We now present the proposed heterogeneous neural process. The proposed model inherits the\n",
            "advantages of multi-task learning and meta-learning, which can exploit task-relatedness among\n",
            "heterogeneous tasks and extract meta-knowledge from previous episodes. Next, we characterize the\n",
            "generative process, clarify the modeling within the hierarchical Bayes framework, and derive the\n",
            "approximate evidence lower bound (ELBO) in optimization.\n",
            "Generative Processes. To get to our proposed method HNPs, we extend the distribution over a\n",
            "single function p(fτ)as used in vanilla NPs to a joint distribution of multiple functions p(f1:M\n",
            "τ)for\n",
            "all heterogeneous tasks in a single episode τ. In detail, the underlying multi-task function distribution\n",
            "p(f1:M\n",
            "τ)is inferred from a collection of context sets C1:M\n",
            "τand learnable meta-knowledge ω, ν1:M.\n",
            "Note that ωrepresents the shared meta-knowledge for all tasks, and νmdenotes the task-specific\n",
            "meta-knowledge corresponding to the task distribution p(Im). Hence, we can formulate the predictive\n",
            "distribution for every single episode as follows:\n",
            "p(T1:M\n",
            "τ|C1:M\n",
            "τ;ω, ν1:M) =Z\n",
            "p(y1:M\n",
            "τ|x1:M\n",
            "τ, f1:M\n",
            "τ)p(f1:M\n",
            "τ|C1:M\n",
            "τ;ω, ν1:M)d f1:M\n",
            "τ, (1)\n",
            "where p(f1:M\n",
            "τ|C1:M\n",
            "τ;ω, νm)denotes the data-dependent functional prior for multiple tasks of the\n",
            "episode τ. The functional prior encodes context sets from all heterogeneous tasks and quantifies\n",
            "uncertainty in the functional space. Nevertheless, it is less optimal to characterize multi-task function\n",
            "generative processes with vanilla NPs, since the single latent variable limits the capacity of the latent\n",
            "space to specify the complicated functional priors. This expressiveness bottleneck in vanilla NPs is\n",
            "particularly severe for our episodic multi-task learning since each episode has diverse heterogeneous\n",
            "tasks with insufficient data.\n",
            "Figure 2: Graphical model of the proposed\n",
            "HNPs in a single episode. Filled shapes indi-\n",
            "cate observations. Probabilistic and deterministic\n",
            "variables are indicated by unfilled circles and dia-\n",
            "monds, respectively.Modeling within the Hierarchical Bayes\n",
            "Framework. To mitigate the expressiveness\n",
            "bottleneck of vanilla NPs, we model HNPs\n",
            "by parameterizing each task-specific function\n",
            "within a hierarchical Bayes framework. As il-\n",
            "lustrated in Figure 2, HNPs integrate a global\n",
            "latent representation zm\n",
            "τand a set of local latent\n",
            "parameters wm\n",
            "τ,1:Oto model each task-specific\n",
            "function fm\n",
            "τ. Specifically, the latent variables\n",
            "are introduced at different levels: zm\n",
            "τencodes\n",
            "task-specific context information from Cm\n",
            "τand\n",
            "νmin the representation level. wm\n",
            "τ,1:Oencode\n",
            "prediction-aware information for a task-specific\n",
            "decoder from C1:M\n",
            "τandωin the parameter level,\n",
            "where Ois the dimension of the decoder. For\n",
            "example, the dimension is the size of the target space when performing classification tasks.\n",
            "3Notably, each local latent parameter is conditioned on the global latent representation, which controls\n",
            "access to all context sets in the episode for the corresponding task. Our method differs from previous\n",
            "hierarchical architectures [ 16,18–20] in the NP family since the local latent parameters of our HNPs\n",
            "are prediction-aware and explicitly constitute a decoder for the subsequent inference processes.\n",
            "In practice, we assume that distributions of each task-specific function are conditionally independent.\n",
            "Thus, with the introduced hierarchical latent variables for each task in the episode, we can factorize\n",
            "the prior distribution over multiple functions in Eq. (1) into:\n",
            "p(f1:M\n",
            "τ|C1:M\n",
            "τ;ω, ν1:M) =MY\n",
            "m=1p(zm\n",
            "τ|Cm\n",
            "τ;νm)p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω), (2)\n",
            "where p(zm\n",
            "τ|Cm\n",
            "τ;νm)andp(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω)are prior distributions of the global latent represen-\n",
            "tation and the local latent parameters to induce the task-specific function distribution.\n",
            "By integrating Eq. (2) into Eq. (1), we rewrite the modeling of HNPs in the following form:\n",
            "p(T1:M\n",
            "τ|C1:M\n",
            "τ;ω, ν1:M) =MY\n",
            "m=1ZnZ\n",
            "p(ym\n",
            "τ|xm\n",
            "τ,wm\n",
            "τ,1:O)\n",
            "p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω)dwm\n",
            "τ,1:Oo\n",
            "p(zm\n",
            "τ|Cm\n",
            "τ;νm)dzm\n",
            "τ,(3)\n",
            "where p(ym\n",
            "τ|xm\n",
            "τ,wm\n",
            "τ,1:O)is the function distribution for the task Im\n",
            "τin HNPs. This distribution is\n",
            "obtained by the matrix multiplication of xm\n",
            "τand all local latent parameters wm\n",
            "τ,1:O.\n",
            "Compared with most NP models [ 12,16,18,19] employing only latent representations, HNPs infer\n",
            "both latent representations and parameters in the hierarchical architecture from multiple heterogeneous\n",
            "context sets and learnable meta-knowledge. Our model specifies a richer and more intricate functional\n",
            "space by leveraging the hierarchical uncertainty inherent in the context sets and meta-knowledge.\n",
            "This theoretically yields more powerful functional priors to induce multi-task function distributions.\n",
            "Moreover, we claim that the developed model constitutes an exchangeable stochastic process and\n",
            "demonstrate this via Kolmogorov Extension Theorem [ 21]. Please refer to Appendix B for the proof.\n",
            "Approximate ELBO. Since both exact functional posteriors and priors are intractable, we apply\n",
            "variational inference to the proposed HNPs in Eq. (3). This results in the approximate ELBO:\n",
            "LHNPs(ω, ν1:M, θ, ϕ) =MX\n",
            "m=1\u001a\n",
            "Eqθ(zmτ|Tmτ;νm)n\n",
            "Eqϕ(wm\n",
            "τ,1:O|zmτ,T1:Mτ;ω)[logp(ym\n",
            "τ|xm\n",
            "τ,wm\n",
            "τ,1:O)]\n",
            "−DKL[qϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,T1:M\n",
            "τ;ω)||pϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω)]o\n",
            "−DKL[qθ(zm\n",
            "τ|Tm\n",
            "τ;νm)||pθ(zm\n",
            "τ|Cm\n",
            "τ;νm)]\u001b\n",
            ",\n",
            "(4)\n",
            "where qθ(zm\n",
            "τ|Tm\n",
            "τ;νm)andqϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,T1:M\n",
            "τ;ω)are variational posteriors of their corresponding\n",
            "latent variables. θandϕare parameters of inference modules for zm\n",
            "τandwm\n",
            "τ,1:O, respectively.\n",
            "Following the protocol of vanilla NPs [ 12], the priors use the same inference modules as variational\n",
            "posteriors for tractable optimization. In this way, the KL-divergence terms in Eq. (4) encourage\n",
            "all latent variables inferred from the context sets to stay close to those inferred from the target\n",
            "sets, enabling effective function generation with few examples. Details on the derivation of the\n",
            "approximate ELBO and its tractable optimization are attached in Appendix C.\n",
            "2.2 Transformer-Structured Inference Module\n",
            "In order to infer the prior and variational posterior distributions in Eq. (4), it is essential to develop well-\n",
            "designed approximate inference modules. This is non-trivial and closely related to the performance\n",
            "of HNPs. Here we adopt a transformer structure as the inference module to better exploit task-\n",
            "relatedness from the meta-knowledge and the context sets in the episode. More specifically, the\n",
            "previously mentioned meta-knowledge ω=ω1:Oandν1:Mare instantiated as learnable tokens to\n",
            "induce the distributions of hierarchical latent variables in the proposed model.\n",
            "Without loss of generality, in the next, we provide an example of transformer-structured inference\n",
            "modules for prior distributions in classification scenarios. Details of the inference modules in\n",
            "regression scenarios can be found in Appendix D. In Figure 3, a diagram of the transformer-structured\n",
            "4Figure 3: A diagram of transformer-structured inference modules of HNPs for the first meta-\n",
            "training episode in Figure 1 under the 3-task 5-way 1-shot setting. For clarity, we display the\n",
            "inference process of the local latent parameters specific to the third task in the episode.\n",
            "inference modules is displayed under the 3-task 5-way 1-shot setting. In this case, the number\n",
            "of context samples is the same as the size of the target space, and thus we have Cm\n",
            "τ={¯xm\n",
            "τ,o,¯ym\n",
            "τ,o}O\n",
            "o=1,\n",
            "where Ois set as 5. In episodic training, labels in context sets are always available during inference.\n",
            "Transformer-Structured Inference Module {θ, νm}forzm\n",
            "τ.In the proposed HNPs, each global\n",
            "latent representation encodes task-specific information relevant to the considered task in the episode\n",
            "aspθ(zm\n",
            "τ|Cm\n",
            "τ;νm). The learnable token νmpreserves the meta-knowledge from previous episodes\n",
            "for specific tasks, which are sampled from the corresponding task distribution p(Im). The role of νm\n",
            "is to help the model adapt efficiently to such specific tasks in meta-test episodes.\n",
            "In detail, we set the dimension of the learnable token νmto the same as that of the features ¯xm\n",
            "τ,1:O.\n",
            "Then the transformer-structured inference module θfuses them in a unified manner by taking\n",
            "[¯xm\n",
            "τ,1:O;νm]as the input. The module θoutputs the mean and variance of the corresponding prior\n",
            "distribution. The inference steps for the global latent representation zm\n",
            "τare:\n",
            "[exm\n",
            "τ,1:O;eνm] =MSA(LN([¯xm\n",
            "τ,1:O;νm])) + [¯ xm\n",
            "τ,1:O;νm], (5)\n",
            "[bxm\n",
            "τ,1:O;bνm] =MLP(LN([exm\n",
            "τ,1:O;eνm])) + [exm\n",
            "τ,1:O;eνm], (6)\n",
            "pθ(zm\n",
            "τ|Cm\n",
            "τ;νm) =N(zm\n",
            "τ;µzmτ, σzmτ), (7)\n",
            "where µzmτ=MLP(bνm), σzmτ=Softplus (MLP(bνm)). The transformer-structured inference module\n",
            "includes a multi-headed self-attention ( MSA) and three multi-layer perceptrons ( MLP). The layer\n",
            "normalization ( LN) is \"pre-norm\" as done in [ 22].Softplus is the activation function to output the\n",
            "appropriate value as the variance of the prior distribution [23].\n",
            "Transformer-Structured Inference Module {ϕ, ω 1:O}forwm\n",
            "τ,1:O.Likewise, each learnable token\n",
            "ωocorresponds to a local latent parameter wm\n",
            "τ,o. With the learnable tokens ω1:O, we reformulate the\n",
            "prior distribution of local latent parameters as pϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω1:O). In this way, we learn the\n",
            "shared knowledge, inductive biases across all tasks, and their distribution at a parameter level, which\n",
            "in practical settings can capture epistemic uncertainty.\n",
            "To be specific, the prior distribution can be factorized asQO\n",
            "o=1pϕ(wm\n",
            "τ,o|zm\n",
            "τ,C1:M\n",
            "τ;ωo), where all local\n",
            "latent parameters are assumed to be conditionally independent. For each local latent parameter wm\n",
            "τ,o,\n",
            "the transformer-structured inference module ϕtakes [¯x1:M\n",
            "τ,o, ωo]as input and outputs the corresponding\n",
            "prior distribution, where ¯x1:M\n",
            "τ,oare deep features from the same class oin the episode and ωois the\n",
            "corresponding learnable token. Here the inference steps for wm\n",
            "τ,oare as follows:\n",
            "[ex1:M\n",
            "τ,o;eωo] =MSA(LN([¯x1:M\n",
            "τ,o;ωo])) + [¯ x1:M\n",
            "τ,o;ωo], (8)\n",
            "5[bx1:M\n",
            "τ,o;bωo] =MLP(LN([ex1:M\n",
            "τ,o;eωo])) + [ex1:M\n",
            "τ,o;eωo], (9)\n",
            "pϕ(wm\n",
            "τ,o|zm\n",
            "τ,C1:M\n",
            "τ;ωo) =N(wm\n",
            "τ,o;µwmτ,o, σwmτ,o), (10)\n",
            "where µwmτ,o=MLP(bωo,zm\n",
            "τ(i)), σwmτ,o=Softplus (MLP(bωo,zm\n",
            "τ(i))).zm\n",
            "τ(i)is a Monte Carlo sample\n",
            "from the variational posterior of the corresponding global latent representation during meta-training.\n",
            "Both transformer-structured inference modules use the refined tokens bνmandbωoto obtain a global\n",
            "latent representation and a local latent parameter, respectively. The introduced tokens preserve the\n",
            "specific meta-knowledge for each latent variable during inference. Compared with the θ-parameterised\n",
            "inference module exploring the intra-task relationships, the ϕ-parameterised inference module enables\n",
            "the exploitation of the inter-task relationships to reason over each local latent parameter. Thus, the\n",
            "introduced tokens can be refined with relevant information from the heterogeneous context sets. By\n",
            "integrating meta-knowledge and heterogeneous context sets, HNPs can reduce the negative transfer\n",
            "of task-specific knowledge among heterogeneous tasks in each episode. Please refer to Appendix E\n",
            "for algorithms.\n",
            "3 Related Work\n",
            "Multi-Task Learning. Multi-task learning can operate in various settings [ 9]. Here we roughly\n",
            "separate the settings of MTL into two branches: (1) Single-input multi-output (SIMO) [ 24–30], where\n",
            "tasks are defined by different supervision information for the same input. (2) Multi-input multi-output\n",
            "(MIMO) [ 11,10,31–34], where heterogeneous tasks follow different data distributions. This work\n",
            "considers the MIMO setup of multi-task learning with episodic training.\n",
            "In terms of modeling methods, from a processing perspective, existing MTL methods can be roughly\n",
            "categorized into two groups: (1) Probabilistic MTL methods [ 11,19,35–41], which employ the\n",
            "Bayes framework to characterize probabilistic dependencies among tasks. (2) Deep MTL models\n",
            "[10,32,24–26,42–48], which directly utilize deep neural networks to discover information-sharing\n",
            "mechanisms across tasks. However, deep MTL models rely on large amounts of training data and tend\n",
            "to overfit when encountering the data-insufficiency problem. Meanwhile, previous probabilistic MTL\n",
            "methods consider a small number of tasks that occur at the same time, limiting their applicability in\n",
            "real-world systems.\n",
            "Meta-Learning. Meta-learning aims to find strategies to quickly adapt to unseen tasks with a\n",
            "few examples [ 49,4,5,50]. There exist a couple of branches in meta-learning methods, such as\n",
            "metrics-based methods [ 6,51–57] and optimization-based methods [ 5,58–68]. Our paper focuses\n",
            "on a probabilistic meta-learning method, namely neural processes, that can quantify predictive\n",
            "uncertainty. Models in this family [ 7,12,13,15,16,18,69–73] can approximate stochastic processes\n",
            "in neural networks. Vanilla NPs [ 12] usually encounter the expressiveness bottleneck because their\n",
            "functional priors are not rich enough to generate complicated functions [ 15,16]. [7] introduces\n",
            "deterministic variables to model predictive distributions for meta-learning scenarios directly. Most\n",
            "NP-based methods only focus on a single task during inference [ 7,12,15,16,14], which leaves\n",
            "task-relatedness between heterogeneous tasks in a single episode an open problem.\n",
            "This paper combines multi-task learning and meta-learning paradigms to tackle the data-insufficiency\n",
            "problem. Our work shares the high-level goal of exploiting task-relatedness in an episode with [ 19,\n",
            "74,75]. Concerning the multi-task scenarios, the main differences are: [ 19,74,75] handles multiple\n",
            "attributes and multi-sensor data under the SIMO setting, while our work performs for the MIMO\n",
            "setting where tasks are heterogeneous and distribution shifts exist. Moreover, [ 76] theoretically\n",
            "addresses the conclusion that MTL methods are powerful and efficient alternatives to gradient-based\n",
            "meta-learning algorithms. However, our method inherits the advantages of multi-task learning\n",
            "and meta-learning: simultaneously generalizing meta-knowledge from past to new episodes and\n",
            "exploiting task-relatedness across heterogeneous tasks in every single episode. Thus, our method\n",
            "is more suitable for solving the data-insufficiency problem. Intuitive comparisons with related\n",
            "paradigms such as cross-domain few-shot learning [77–82],multimodal meta-learning [83–87,56]\n",
            "andcross-modality few-shot learning [88–90] are provided in Appendix A.\n",
            "64 Experiments\n",
            "We evaluate the proposed HNPs and baselines on three benchmark datasets under the episodic\n",
            "multi-task setup. Sec. 4.1 and Sec. 4.2 provide experimental results for regression and classification,\n",
            "respectively. Ablation studies are in Sec. 4.3. More comparisons with recent works on extra datasets\n",
            "are provided in Appendix F. Additional results under the convectional MIMO setup without episodic\n",
            "training can be found in Appendix G & H.\n",
            "4.1 Episodic Multi-Task Regression\n",
            "Dataset and Settings. To evaluate the benefit of HNPs over typical NP baselines in uncertainty quan-\n",
            "tification, we conduct experiments in several 1D regression tasks. The baselines include conditional\n",
            "neural processes (CNPs [ 13]), vanilla neural processes (NPs [ 12]), and attentive neural processes\n",
            "(ANPs [ 15]). As a toy example, we construct multiple tasks with different task distributions: each\n",
            "task’s input set is defined on separate intervals without overlap.\n",
            "Figure 4: Performance comparisons on the\n",
            "episodic multi-task 1-D function regression us-\n",
            "ing5context points (black dots) for each task.\n",
            "Black curves are ground truth, and blue ones are\n",
            "predicted results. The shadow regions are ±3stan-\n",
            "dard derivations from the mean [18].Given four different tasks in an episode, their\n",
            "input sets are x1:4\n",
            "τ. Each input set contains\n",
            "a few instances, drawn uniformly at random\n",
            "from separate intervals, such as x1\n",
            "τ∈[−4,−2),\n",
            "x2\n",
            "τ∈[−2,0),x3\n",
            "τ∈[0,2), and x4\n",
            "τ∈[2,4).\n",
            "All tasks in an episode are related by shar-\n",
            "ing the same ground truth function. Follow-\n",
            "ing [12,18], function-fitting tasks are generated\n",
            "with Gaussian processes (GPs). Here a zero\n",
            "mean Gaussian process y(0)∼ GP (0, k(·,·))\n",
            "is used to produce y1:4\n",
            "τfor the inputs from all\n",
            "tasks x1:4\n",
            "τ. A radial basis kernel k(x, x′) =\n",
            "σ2exp(−(x−x′)2)/2l2), with l= 0.4and\n",
            "σ= 1.0is used.\n",
            "Results and Discussions. As shown in Fig-\n",
            "ure 4, CNPs, ANPs, and our HNPs exhibit more\n",
            "reasonable uncertainty than NPs in Figure 4:\n",
            "lower variances are predicted around observed\n",
            "(context) points with higher variances around\n",
            "unobserved points. Furthermore, NPs and ANPs\n",
            "detrimentally impact the smoothness of the pre-\n",
            "dicted curves, whereas HNPs yield smoother\n",
            "predictive curves with reliable uncertainty esti-\n",
            "mation. These observations suggest that inte-\n",
            "grating correlation information across related\n",
            "tasks and meta-knowledge in HNPs can improve uncertainty quantification in multi-task regression.\n",
            "Table 1: Average negative log-likelihoods over\n",
            "target points from all tasks.\n",
            "Methods CNPs NPs ANPs HNPs\n",
            "Avg. NLL 0.0935 0.8649 -0.1165 -0.5207To quantify uncertainty we use the average neg-\n",
            "ative log-likelihood (the lower, the better). As\n",
            "shown in Table 1, our HNPs achieve a lower\n",
            "average negative log-likelihood than baselines,\n",
            "demonstrating our method’s effectiveness in un-\n",
            "certainty estimation.\n",
            "4.2 Episodic Multi-task Classification\n",
            "Datasets and Settings. We use Office-Home [91] and DomainNet [92] as episodic multi-task\n",
            "classification datasets. Office-Home contains images from four domains: Artistic (A), Clipart (C),\n",
            "Product (P) and Real-world (R). Each domain contains images from 65categories collected from\n",
            "office and home environments. Note that all domains share the whole target space. The numbers\n",
            "of meta-training classes and meta-test classes are 40and25, respectively. There are about 15,500\n",
            "images in total. DomainNet has six distinct domains: Clipart, Infograph, Painting, Quickdraw, Real\n",
            "and Sketch. It includes approximately 0.6 million images distributed over 345categories. The\n",
            "7Table 2: Comparative results (95% confidence interval) for episodic multi-task classification on\n",
            "Office-Home andDomainNet .Best results are indicated in bold.\n",
            "Office-Home DomainNet\n",
            "4-task 5-way 4-task 20-way 6-task 5-way 6-task 20-way\n",
            "Method 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot\n",
            "ERM [93] 66.04 ±0.61 73.62 ±0.55 39.25 ±0.24 47.14 ±0.18 59.95 ±0.52 68.52 ±0.44 38.62 ±0.22 47.85 ±0.20\n",
            "VMTL [11] 49.71 ±0.48 65.75 ±0.47 27.50 ±0.14 42.82 ±0.13 42.24 ±0.39 57.37 ±0.43 18.05 ±0.11 31.38 ±0.15\n",
            "MAML [5] 60.58 ±0.60 75.29 ±0.53 34.29 ±0.19 48.39 ±0.20 53.21 ±0.46 65.24 ±0.47 17.10 ±0.12 20.35 ±0.14\n",
            "Proto. Net. [6] 57.19 ±0.53 74.97 ±0.46 32.72 ±0.18 49.75 ±0.16 53.71 ±0.48 68.80 ±0.42 31.90 ±0.19 47.59 ±0.18\n",
            "DGPs [94] 65.89 ±0.53 79.96 ±0.38 31.48 ±0.18 49.46 ±0.18 50.93 ±0.42 63.32 ±0.38 25.46 ±0.15 38.63 ±0.17\n",
            "CNPs [13] 43.33 ±0.56 55.07 ±0.63 10.57 ±0.10 12.02 ±0.11 37.90 ±0.45 40.53 ±0.44 5.12±0.10 5.14±0.10\n",
            "NPs [12] 33.66 ±0.48 53.99 ±0.60 5.25±0.16 11.40 ±0.11 20.58 ±0.51 20.53 ±0.53 5.12±0.09 5.11±0.09\n",
            "TNP-D [95] 65.49 ±0.53 78.94 ±0.43 41.61 ±0.22 59.19 ±0.21 49.10 ±0.42 67.39 ±0.40 28.83 ±0.17 47.69 ±0.18\n",
            "HNPs 76.29 ±0.51 80.80 ±0.42 51.82 ±0.23 59.97 ±0.18 62.36 ±0.53 69.38 ±0.42 39.32 ±0.23 48.56 ±0.19\n",
            "numbers of meta-training classes and meta-test classes are 276and69, respectively. Here one domain\n",
            "corresponds to a specific task distribution in the episodic multi-task setting.\n",
            "When it comes to the episodic multi-task classification, we compare HNPs with the following three\n",
            "branches: (1) Multi-task learning methods : ERM [ 93] directly expands the training set of the current\n",
            "task with samples of related tasks. VMTL [ 11] is one of the state-of-the-art under the MIMO setting\n",
            "of multi-task learning. (2) Meta-learning methods : MAML [ 5], Proto.Net [ 6] and DGPs [ 94] address\n",
            "each task separately with no mechanism to leverage task-relatedness in a single episode. (3) Methods\n",
            "from the NP family : CNPs [ 13] and NPs [ 12] are established methods in the NP family. TNP-D [ 95]\n",
            "is recent NP work in sequential decision-making for a single task in each episode.\n",
            "Results and Discussions. The experimental results for episodic multi-task classification on\n",
            "Office-Home andDomainNet are reported in Table 2. We use the average accuracy across all\n",
            "task distributions as the evaluation metric. It can be seen that HNPs consistently outperform all\n",
            "baseline methods, demonstrating the effectiveness of HNPs in handling each task with limited data\n",
            "under the episodic multi-task classification setup.\n",
            "NPs and CNPs do not work well under all episodic multi-task classification cases. This can be\n",
            "attributed to their limited expressiveness of the global representation and the weak capability to\n",
            "extract discriminative information from multiple contexts. In contrast, HNPs explicitly abstract\n",
            "discriminative information for each task in the episode with the help of local latent parameters,\n",
            "enhancing the expressiveness of the functional prior.\n",
            "We also find that HNPs significantly surpass other baselines on 1-shot Office-Home and\n",
            "DomainNet , both under the 4/6-task 5-way and4/6-task 20-way settings. This further implies\n",
            "that HNPs can circumvent the effect of the problem of data-insufficiency by simultaneously exploiting\n",
            "task-relatedness across heterogeneous tasks and meta-knowledge among episodes.\n",
            "4.3 Ablation Studies\n",
            "Influence of Hierarchical Latent Variables. We first investigate the roles of the global latent\n",
            "representation zm\n",
            "τand the local latent parameters wm\n",
            "τ,1:Oby leaving out individual inference modules.\n",
            "These experiments are performed on Office-home under the 4-task 5-way 1-shot setting. We\n",
            "report the detailed performance for tasks sampled from a single task distribution (A/C/P/R) and the\n",
            "average accuracy across all task distributions (Avg.) in Table 3. The variants without specific latent\n",
            "variables are included in the comparison by removing the corresponding inference modules.\n",
            "Table 3: Effectiveness of global latent representations\n",
            "zm\n",
            "τand local latent parameters wm\n",
            "τ,1:Oin the model.\n",
            "✓and✗denote whether the variants of HNPs have the corre-\n",
            "sponding latent variable or not.\n",
            "zm\n",
            "τwm\n",
            "τ,1:O A C P R Avg.\n",
            "✗ ✗ 62.64 ±0.72 56.87 ±0.71 75.18 ±0.79 73.68 ±0.77 67.09 ±0.63\n",
            "✗ ✓ 69.39 ±0.60 63.10 ±0.61 80.66 ±0.67 79.99 ±0.62 73.29 ±0.51\n",
            "✓ ✗ 67.02 ±0.67 60.70 ±0.69 78.26 ±0.76 78.47 ±0.72 71.11 ±0.59\n",
            "✓ ✓ 73.31 ±0.63 64.92 ±0.68 83.38 ±0.66 83.54 ±0.64 76.29 ±0.51As shown in Table 3, both zm\n",
            "τand\n",
            "wm\n",
            "τ,1:Obenefit overall performance.\n",
            "Our method with hierarchical latent\n",
            "variables performs 9.20% better than\n",
            "the variant without both latent vari-\n",
            "ables, 3.00% better than the variant\n",
            "without zm\n",
            "τ, and5.18% better than the\n",
            "variant without wm\n",
            "τ,1:O. This indicates\n",
            "that latent variables of HNPs comple-\n",
            "ment each other in representing con-\n",
            "8text sets from multiple tasks and meta-knowledge. The variant without wm\n",
            "τ,1:Ounderperforms the\n",
            "variant without zm\n",
            "τby2.18%, in terms of the average accuracy. This demonstrates that zm\n",
            "τsuffers\n",
            "more from the expressiveness bottleneck than wm\n",
            "τ,1:O, weakening the models’ discriminative ability.\n",
            "For classification, local latent parameters are more crucial than a global latent representation in\n",
            "revealing the discriminating knowledge from multiple heterogeneous context sets.\n",
            "Influence of Transformer-Structured Inference Modules. To further understand our transformer-\n",
            "structured inference modules (Trans. w learnable tokens), we examine the performance against\n",
            "two other options: inference modules that solely utilize a multi-layer perceptron (MLP) and the\n",
            "variants that do not incorporate any learnable tokens (Trans. w/o learnable tokens). We also compare\n",
            "the probabilistic and deterministic versions of such inference modules. The deterministic variants\n",
            "consider the deterministic embedding for the hierarchical latent variables.\n",
            "Table 4: Performance comparisons between our trans-\n",
            "former inference modules (Trans. w learnable tokens)\n",
            "and other alternatives.\n",
            "Inference networks 1-shot 5-shot\n",
            "DeterministicMLP 64.93 ±0.66 72.39 ±0.56\n",
            "Trans. w/o learnable tokens 70.22 ±0.62 76.15 ±0.54\n",
            "Trans. w learnable tokens 70.61 ±0.56 76.70 ±0.50\n",
            "ProbabilisticMLP 73.30 ±0.59 77.94 ±0.48\n",
            "Trans. w/o learnable tokens 75.25 ±0.55 80.42 ±0.47\n",
            "Trans. w learnable tokens 76.29 ±0.51 80.80 ±0.42As shown in Table 4, our infer-\n",
            "ence modules consistently outperform\n",
            "the variants, regardless of whether\n",
            "the inference network is probabilis-\n",
            "tic or deterministic. When using the\n",
            "probabilistic one, our inference mod-\n",
            "ules respectively achieve 1.04% and\n",
            "2.99% performance gains over Trans.\n",
            "w/o learnable tokens and MLP under\n",
            "the4-task 5-way 1-shot setting.\n",
            "This implies the importance of learn-\n",
            "able tokens and task-relatedness in formulating transformer-structured inference modules, which\n",
            "reduces negative transfer among heterogeneous tasks in each meta-test episode. Moreover, the\n",
            "variants with probabilistic inference modules consistently beat deterministic ones in performance,\n",
            "demonstrating the advantages of considering uncertainty during modeling and inference.\n",
            "Table 5: Performance comparisons of different implemen-\n",
            "tations of generating each local latent parameter wm\n",
            "τ,o\n",
            "from the condition zm\n",
            "τandC1:M\n",
            "τ.\n",
            "Methods A C P R Avg.\n",
            "Concat 65.69 ±0.59 58.64 ±0.61 77.54 ±0.68 77.10 ±0.64 69.74 ±0.51\n",
            "Add 69.92 ±0.69 63.73 ±0.71 78.81 ±0.77 79.03 ±0.78 72.87 ±0.61\n",
            "Ours 73.31 ±0.63 64.92 ±0.68 83.38 ±0.66 83.54 ±0.64 76.29 ±0.51Effects of Different Ways to Gener-\n",
            "ate Local Latent Parameters. We\n",
            "investigate the effects of different\n",
            "ways to generate each wm\n",
            "τ,ofrom the\n",
            "shared condition zm\n",
            "τandC1:M\n",
            "τ. Given\n",
            "a Monte Carlo sample of global la-\n",
            "tent variables as zm\n",
            "τ(i), in Table 5,\n",
            "we compare with two alternatives:\n",
            "1) Concat directly concatenates each context feature and zm\n",
            "τ(i), and takes the concatenation as\n",
            "inputs of the transformer-structured inference network ϕ.2) Add sums up each context feature and\n",
            "zm\n",
            "τ(i)and takes the result as the input. 3) Ours incorporates zm\n",
            "τ(i)into the transformer-structured\n",
            "inference module by merging it with the refined learnable tokens in Eq. (10). As shown in Table 5,\n",
            "Ours consistently performs the best. This implies that incorporating the conditional variables into the\n",
            "inference module is more effective than the direct combinations of zm\n",
            "τ(i)and instance features.\n",
            "Effects of More \"Shots\" or \"Classes\". To investigate the effects of more \"shots\" or \"classes\" in\n",
            "the episodic multi-task classification setup, we conduct experiments by increasing KorOin the\n",
            "defined M-task O-way K-shot setup.\n",
            "Table 6: Performance comparisons on Office-Home un-\n",
            "der the 4-task 5-way K-shot setup.\n",
            "Methods 1-shot 5-shot 10-shot 20-shot\n",
            "TNP-D 65.49 ±0.53 78.94 ±0.43 80.81 ±0.32 81.12 ±0.68\n",
            "HNPs 76.29 ±0.51 80.80 ±0.42 81.28 ±0.38 81.56 ±0.36As shown in Table 6, the proposed\n",
            "HNPs have more advantages over the\n",
            "baseline method with the context data\n",
            "points below ten shots. With shots\n",
            "larger than ten, both methods will\n",
            "reach a performance bottleneck.\n",
            "Moreover, Table 7 shows that our method consistently outperforms the baseline method as the number\n",
            "of classes increases from 20 to 40 in step 5. However, the performance gap between them narrows\n",
            "slightly with more classes. The main reason could be that the setting with more classes suffers from\n",
            "less data insufficiency.\n",
            "9Table 7: Performance comparisons on DomainNet under the 6-task O-way 1-shot setup.\n",
            "Methods 5-way 20-way 25-way 30-way 35-way 40-way\n",
            "TNP-D 49.10 ±0.42 28.83 ±0.17 25.93 ±0.14 24.08 ±0.12 22.62 ±0.11 21.64 ±0.53\n",
            "HNPs 62.36 ±0.53 39.32 ±0.23 35.72 ±0.19 32.27 ±0.17 31.27 ±0.14 29.31 ±0.13\n",
            "Figure 5: Average accuracy and runtime of HNPs with\n",
            "different numbers of Monte Carlo samples. NzandNw\n",
            "are sampling numbers of zm\n",
            "τandwm\n",
            "τ,1:O, respectively.Sensitivity to the Number of Monte\n",
            "Carlo Samples. For the hierarchi-\n",
            "cal latent variables in the HNPs, we\n",
            "investigate the model’s sensitivity to\n",
            "the number of Monte Carlo samples.\n",
            "Specifically, the sampling number of\n",
            "the global latent representation zm\n",
            "τ\n",
            "and local latent parameters wm\n",
            "τ,1:O\n",
            "varies from 1to30. We examine\n",
            "onOffice-Home under the 4-task\n",
            "5-way 1-shot setting. In Figure 5,\n",
            "the runtime per iteration grows rapidly\n",
            "as the number of samples increases.\n",
            "However, there is no clear correlation\n",
            "between the performance and the num-\n",
            "ber of Monte Carlo samples. There are two sweet spots in terms of average accuracy, one of which\n",
            "has favorable computation time. Hence, we set NzandNwto5and10, respectively.\n",
            "Table 8: Inference time of different NP-based methods.\n",
            "Methods CNPs NPs TNP-D HNPs\n",
            "Inference time(s) 0.04 0.05 0.08 0.15We also investigate the inference\n",
            "time of NP-based models per iter-\n",
            "ation on Office-Home under the\n",
            "4task5way1shot setup. As shown in\n",
            "Table 8, our model needs more infer-\n",
            "ence time than other NP-based meth-\n",
            "ods for performance gains. The cost mainly comes from inferring the designed hierarchical latent\n",
            "variables; however, we consider this a worthwhile trade-off for the extra performance.\n",
            "5 Conclusion\n",
            "Technical Discussion. This work develops heterogeneous neural processes by introducing hierar-\n",
            "chical latent variables and transformer-structured inference modules for episodic multi-task learning.\n",
            "With the help of heterogeneous context information and meta-knowledge, the proposed model can\n",
            "exploit task-relatedness, reason about predictive function distributions, and efficiently distill past\n",
            "knowledge to unseen heterogeneous tasks with limited data.\n",
            "Limitation & Extension. Although the hierarchical probabilistic framework could mitigate the\n",
            "expressiveness bottleneck, the model needs more inference time than other NP-based methods for\n",
            "performance gains. Besides, the proposed method requires the target space to be the same across all\n",
            "tasks in a single episode. This requirement could limit the method’s applicability in realistic scenarios\n",
            "where target spaces may differ across tasks. Our work could be extended to the new case without\n",
            "the shared target spaces, where the model should construct higher-order task-relatedness to improve\n",
            "knowledge sharing among tasks. Our code3is provided to facilitate such extensions.\n",
            "Acknowledgment\n",
            "This work is financially supported by the Inception Institute of Artificial Intelligence, the University\n",
            "of Amsterdam and the allowance Top consortia for Knowledge and Innovation (TKIs) from the\n",
            "Netherlands Ministry of Economic Affairs and Climate Policy.\n",
            "3https://github.com/autumn9999/HNPs.git\n",
            "10References\n",
            "[1]Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning . MIT Press, 2016. http:\n",
            "//www.deeplearningbook.org .\n",
            "[2]Shichao Xu, Lixu Wang, Yixuan Wang, and Qi Zhu. Weak adaptation learning: Addressing cross-domain\n",
            "data insufficiency with weak annotator. In IEEE Conference on Computer Vision and Pattern Recognition ,\n",
            "2021.\n",
            "[3]Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal of\n",
            "Big Data , 6(1):1–54, 2019.\n",
            "[4]Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot\n",
            "learning. In Advances in Neural Information Processing Systems , 2016.\n",
            "[5]Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\n",
            "deep networks. In International Conference on Machine Learning , 2017.\n",
            "[6]Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances\n",
            "in Neural Information Processing Systems , 2017.\n",
            "[7]James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner. Fast\n",
            "and flexible multi-task classification using conditional neural adaptive processes. In Advances in Neural\n",
            "Information Processing Systems , 2019.\n",
            "[8] Rich Caruana. Multitask learning. Machine learning , 28(1):41–75, 1997.\n",
            "[9]Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and Data\n",
            "Engineering , 2021.\n",
            "[10] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and S Yu Philip. Learning multiple tasks with multilinear\n",
            "relationship networks. In Advances in neural information processing systems , 2017.\n",
            "[11] Jiayi Shen, Xiantong Zhen, Marcel Worring, and Ling Shao. Variational multi-task learning with\n",
            "gumbel-softmax priors. In Advances in Neural Information Processing Systems , 2021.\n",
            "[12] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and\n",
            "Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622 , 2018.\n",
            "[13] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan,\n",
            "Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International\n",
            "Conference on Machine Learning , 2018.\n",
            "[14] Wessel Bruinsma, Stratis Markou, James Requeima, Andrew Y . K. Foong, Tom Andersson, Anna Vaughan,\n",
            "Anthony Buonomo, Scott Hosking, and Richard E Turner. Autoregressive conditional neural processes.\n",
            "InInternational Conference on Learning Representations , 2023.\n",
            "[15] Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol\n",
            "Vinyals, and Yee Whye Teh. Attentive neural processes. In International Conference on Learning\n",
            "Representations , 2019.\n",
            "[16] Qi Wang and Herke van Hoof. Learning expressive meta-representations with mixture of expert neural\n",
            "processes. In Advances in Neural Information Processing Systems , 2022.\n",
            "[17] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\n",
            "Conference on Learning Representations , 2017.\n",
            "[18] Qi Wang and Herke Van Hoof. Doubly stochastic variational inference for neural processes with\n",
            "hierarchical latent variables. In International Conference on Machine Learning , 2020.\n",
            "[19] Donggyun Kim, Seongwoong Cho, Wonkwang Lee, and Seunghoon Hong. Multi-task processes. arXiv\n",
            "preprint arXiv:2110.14953 , 2021.\n",
            "[20] Zongyu Guo, Cuiling Lan, Zhizheng Zhang, Yan Lu, and Zhibo Chen. Versatile neural processes for\n",
            "learning implicit neural representations. In International Conference on Learning Representations , 2023.\n",
            "[21] Achim Klenke. Probability theory: a comprehensive course . Springer Science & Business Media, 2013.\n",
            "[22] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution\n",
            "or region supervision. In International Conference on Machine Learning , 2021.\n",
            "11[23] Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder\n",
            "variational autoencoders. In Advances in Neural Information Processing Systems , 2016.\n",
            "[24] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for\n",
            "scene geometry and semantics. In IEEE Conference on Computer Vision and Pattern Recognition , 2018.\n",
            "[25] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In\n",
            "IEEE Conference on Computer Vision and Pattern Recognition , 2019.\n",
            "[26] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for\n",
            "multi-task learning. In IEEE Conference on Computer Vision and Pattern Recognition , 2016.\n",
            "[27] Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin Liao,\n",
            "and Wayne Zhang. Towards impartial multi-task learning. In International Conference on Learning\n",
            "Representations , 2020.\n",
            "[28] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In Advances in\n",
            "Neural Information Processing Systems , 2018.\n",
            "[29] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese.\n",
            "Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer\n",
            "vision and pattern recognition , 2018.\n",
            "[30] Deblina Bhattacharjee, Tong Zhang, Sabine Süsstrunk, and Mathieu Salzmann. Mult: an end-to-end\n",
            "multitask learning transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
            "Pattern Recognition , 2022.\n",
            "[31] Yi Zhang, Yu Zhang, and Wei Wang. Multi-task learning via generalized tensor trace norm. In Proceedings\n",
            "of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining , 2021.\n",
            "[32] Jiayi Shen, Zehao Xiao, Xiantong Zhen, Cees GM Snoek, and Marcel Worring. Association graph\n",
            "learning for multi-task classification with category shifts. arXiv preprint arXiv:2210.04637 , 2022.\n",
            "[33] Yunlong Liang, Fandong Meng, Jinan Xu, Yufeng Chen, and Jie Zhou. Scheduled multi-task learning for\n",
            "neural chat translation. arXiv preprint arXiv:2205.03766 , 2022.\n",
            "[34] Yi Zhang, Yu Zhang, and Wei Wang. Learning linear and nonlinear low-rank structure in multi-task\n",
            "learning. IEEE Transactions on Knowledge and Data Engineering , 2022.\n",
            "[35] Bart Bakker and Tom Heskes. Task clustering and gating for bayesian multitask learning. Journal of\n",
            "Machine Learning Research , 2003.\n",
            "[36] Kai Yu, V olker Tresp, and Anton Schwaighofer. Learning gaussian processes from multiple tasks. In\n",
            "International Conference on Machine Learning , 2005.\n",
            "[37] Michalis K Titsias and Miguel Lázaro-Gredilla. Spike and slab variational inference for multi-task and\n",
            "multiple kernel learning. In Advances in neural information processing systems , 2011.\n",
            "[38] Neil D Lawrence and John C Platt. Learning to learn with the informative vector machine. In International\n",
            "Conference on Machine Learning , 2004.\n",
            "[39] Fariba Yousefi, Michael Thomas Smith, and Mauricio A Álvarez. Multi-task learning for aggregated data\n",
            "using gaussian processes. arXiv preprint arXiv:1906.09412 , 2019.\n",
            "[40] Diane Oyen and Terran Lane. Leveraging domain knowledge in multitask bayesian network structure\n",
            "learning. In Proceedings of the AAAI Conference on Artificial Intelligence , 2012.\n",
            "[41] Weizhu Qian, Bowei Chen, Yichao Zhang, Guanghui Wen, and Franck Gechter. Multi-task variational\n",
            "information bottleneck. arXiv preprint arXiv:2007.00339 , 2020.\n",
            "[42] Gjorgji Strezoski, Nanne van Noord, and Marcel Worring. Learning task relatedness in multi-task learning\n",
            "for images in context. In International Conference on Multimedia Retrieval , 2019.\n",
            "[43] Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. Adashare: Learning what to share for\n",
            "efficient deep multi-task learning. arXiv preprint arXiv:1911.12423 , 2019.\n",
            "[44] Gjorgji Strezoski, Nanne van Noord, and Marcel Worring. Many task learning with task routing. In IEEE\n",
            "International Conference on Computer Vision , 2019.\n",
            "12[45] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient\n",
            "surgery for multi-task learning. arXiv preprint arXiv:2001.06782 , 2020.\n",
            "[46] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for\n",
            "multi-task learning. In Advances in Neural Information Processing Systems , 2021.\n",
            "[47] Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying\n",
            "task groupings for multi-task learning. In Advances in Neural Information Processing Systems , 2021.\n",
            "[48] Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In\n",
            "International Conference on Machine Learning , 2020.\n",
            "[49] Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn ,\n",
            "pages 3–17. Springer, 1998.\n",
            "[50] Timothy M Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J Storkey. Meta-learning in neural\n",
            "networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2021.\n",
            "[51] Kelsey R Allen, Evan Shelhamer, Hanul Shin, and Joshua B Tenenbaum. Infinite mixture prototypes for\n",
            "few-shot learning. In International Conference on Machine Learning , 2019.\n",
            "[52] Boris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. Tadam: Task dependent adaptive metric\n",
            "for improved few-shot learning. In Advances in neural information processing systems , 2018.\n",
            "[53] Sung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task-adaptive\n",
            "projection for few-shot learning. In International Conference on Machine Learning , 2019.\n",
            "[54] Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In International Conference\n",
            "on Learning Representations , 2018.\n",
            "[55] Tianshi Cao, Marc Law, and Sanja Fidler. A theoretical analysis of the number of shots in few-shot\n",
            "learning. arXiv preprint arXiv:1909.11722 , 2019.\n",
            "[56] Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,\n",
            "Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset\n",
            "of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096 , 2019.\n",
            "[57] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning\n",
            "to compare: Relation network for few-shot learning. In IEEE Conference on Computer Vision and Pattern\n",
            "Recognition , 2018.\n",
            "[58] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint\n",
            "arXiv:1803.02999 , 2018.\n",
            "[59] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,\n",
            "and Raia Hadsell. Meta-learning with latent embedding optimization. In International Conference on\n",
            "Learning Representations , 2019.\n",
            "[60] Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn: Meta-critic\n",
            "networks for sample efficient learning. arXiv preprint arXiv:1706.09529 , 2017.\n",
            "[61] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot\n",
            "learning. arXiv preprint arXiv:1707.09835 , 2017.\n",
            "[62] Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner. Meta-\n",
            "learning probabilistic inference for prediction. In International Conference on Learning Representations ,\n",
            "2019.\n",
            "[63] Harrison Edwards and Amos Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185 ,\n",
            "2016.\n",
            "[64] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Advances in\n",
            "Neural Information Processing Systems , 2018.\n",
            "[65] Steindór Sæmundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta reinforcement learning with\n",
            "latent variable gaussian processes. arXiv preprint arXiv:1803.07551 , 2018.\n",
            "[66] Sungyong Baik, Junghoon Oh, Seokil Hong, and Kyoung Mu Lee. Learning to forget for meta-learning\n",
            "via task-and-layer-wise attenuation. IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n",
            "2021.\n",
            "13[67] Myungsub Choi, Janghoon Choi, Sungyong Baik, Tae Hyun Kim, and Kyoung Mu Lee. Test-time\n",
            "adaptation for video frame interpolation via meta-learning. IEEE Transactions on Pattern Analysis and\n",
            "Machine Intelligence , 2021.\n",
            "[68] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?\n",
            "towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157 , 2019.\n",
            "[69] Ying Wei, Peilin Zhao, and Junzhou Huang. Meta-learning hyperparameter performance prediction with\n",
            "neural processes. In International Conference on Machine Learning , 2021.\n",
            "[70] Stratis Markou, James Requeima, Wessel P Bruinsma, Anna Vaughan, and Richard E Turner. Practical\n",
            "conditional neural processes via tractable dependent predictions. arXiv preprint arXiv:2203.08775 , 2022.\n",
            "[71] Zesheng Ye and Lina Yao. Contrastive conditional neural processes. In IEEE Conference on Computer\n",
            "Vision and Pattern Recognition , 2022.\n",
            "[72] Mingyu Kim, Kyeongryeol Go, and Se-Young Yun. Neural processes with stochastic attention: Paying\n",
            "more attention to the context dataset. arXiv preprint arXiv:2204.05449 , 2022.\n",
            "[73] Qi Wang, Marco Federici, and Herke van Hoof. Bridge the inference gaps of neural processes via\n",
            "expectation maximization. In International Conference on Learning Representations , 2023.\n",
            "[74] Xiaozhuang Song, Shun Zheng, Wei Cao, James Yu, and Jiang Bian. Efficient and effective multi-task\n",
            "grouping via meta learning on task combinations. Advances in Neural Information Processing Systems ,\n",
            "2022.\n",
            "[75] Richa Upadhyay, Prakash Chandra Chhipa, Ronald Phlypo, Rajkumar Saini, and Marcus Liwicki. Multi-\n",
            "task meta learning: learn how to adapt to unseen tasks. In 2023 International Joint Conference on Neural\n",
            "Networks (IJCNN) . IEEE, 2023.\n",
            "[76] Haoxiang Wang, Han Zhao, and Bo Li. Bridging multi-task learning and meta-learning: Towards efficient\n",
            "training and effective adaptation. In International Conference on Machine Learning , 2021.\n",
            "[77] Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain few-shot classifi-\n",
            "cation via learned feature-wise transformation. In International Conference on Learning Representations ,\n",
            "2020.\n",
            "[78] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at\n",
            "few-shot classification. arXiv preprint arXiv:1904.04232 , 2019.\n",
            "[79] Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana\n",
            "Rosing, and Rogerio Feris. A broader study of cross-domain few-shot learning. In European Conference\n",
            "on Computer Vision , 2020.\n",
            "[80] Yingjun Du, Xiantong Zhen, Ling Shao, and Cees G M Snoek. MetaNorm: Learning to normalize\n",
            "few-shot batches across domains. In International Conference on Learning Representations , 2021.\n",
            "[81] Debasmit Das, Sungrack Yun, and Fatih Porikli. Confess: A framework for single source cross-domain\n",
            "few-shot learning. In International Conference on Learning Representations , 2022.\n",
            "[82] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Cross-domain few-shot learning with task-specific adapters. In\n",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7161–7170,\n",
            "2022.\n",
            "[83] Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J Lim. Multimodal model-agnostic meta-learning\n",
            "via task-aware modulation. In Advances in neural information processing systems , 2019.\n",
            "[84] Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J Lim. Toward multimodal model-agnostic\n",
            "meta-learning. In arXiv preprint arXiv:1812.07172 , 2018.\n",
            "[85] Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-learning. In\n",
            "International Conference on Machine Learning , 2019.\n",
            "[86] Milad Abdollahzadeh, Touba Malekzadeh, and Ngai-Man Man Cheung. Revisit multimodal meta-learning\n",
            "through the lens of multi-task learning. In Advances in Neural Information Processing Systems , 2021.\n",
            "[87] Jiayi Chen and Aidong Zhang. Hetmaml: Task-heterogeneous model-agnostic meta-learning for few-shot\n",
            "learning across modalities. In Proceedings of the 30th ACM International Conference on Information &\n",
            "Knowledge Management , 2021.\n",
            "14[88] Chen Xing, Negar Rostamzadeh, Boris Oreshkin, and Pedro O O Pinheiro. Adaptive cross-modal few-shot\n",
            "learning. In Advances in Neural Information Processing Systems , 2019.\n",
            "[89] Frederik Pahde, Patrick Jähnichen, Tassilo Klein, and Moin Nabi. Cross-modal hallucination for few-shot\n",
            "fine-grained recognition. arXiv preprint arXiv:1806.05147 , 2018.\n",
            "[90] Frederik Pahde, Mihai Puscas, Tassilo Klein, and Moin Nabi. Multimodal prototypical networks for\n",
            "few-shot learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer\n",
            "Vision , 2021.\n",
            "[91] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing\n",
            "network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern\n",
            "Recognition , 2017.\n",
            "[92] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for\n",
            "multi-source domain adaptation. In IEEE International Conference on Computer Vision , 2019.\n",
            "[93] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint\n",
            "arXiv:2007.01434 , 2020.\n",
            "[94] Ze Wang, Zichen Miao, Xiantong Zhen, and Qiang Qiu. Learning to learn dense gaussian processes for\n",
            "few-shot learning. In Advances in Neural Information Processing Systems , 2021.\n",
            "[95] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via\n",
            "sequence modeling. arXiv preprint arXiv:2207.04179 , 2022.\n",
            "[96] Richa Upadhyay, Prakash Chandra Chhipa, Ronald Phlypo, Rajkumar Saini, and Marcus Liwicki. Multi-\n",
            "task meta learning: learn how to adapt to unseen tasks. arXiv preprint arXiv:2210.06989 , 2022.\n",
            "[97] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. Leaving no one behind: A multi-\n",
            "scenario multi-task meta learning approach for advertiser modeling. In Proceedings of the Fifteenth ACM\n",
            "International Conference on Web Search and Data Mining , 2022.\n",
            "[98] Kaidi Cao, Jiaxuan You, and Jure Leskovec. Relational multi-task learning: Modeling relations between\n",
            "data and tasks. In International Conference on Learning Representations , 2021.\n",
            "[99] Huaiwen Zhang, Shengsheng Qian, Quan Fang, and Changsheng Xu. Multi-modal meta multi-task\n",
            "learning for social media rumor detection. IEEE Transactions on Multimedia , 24:1449–1459, 2021.\n",
            "[100] Chu Han, Huasheng Yao, Bingchao Zhao, Zhenhui Li, Zhenwei Shi, Lei Wu, Xin Chen, Jinrong Qu,\n",
            "Ke Zhao, Rushi Lan, et al. Meta multi-task nuclei segmentation with fewer training samples. Medical\n",
            "Image Analysis , 2022.\n",
            "[101] Guan-Yuan Chen and Ya-Fen Yeh. Mmtl: The meta multi-task learning for aspect category sentiment\n",
            "analysis. In Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing ,\n",
            "2021.\n",
            "[102] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C\n",
            "Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In Proceedings\n",
            "of the IEEE/CVF international conference on computer vision , 2019.\n",
            "[103] Cuong C Nguyen, Thanh-Toan Do, and Gustavo Carneiro. Probabilistic task modelling for meta-learning.\n",
            "InUncertainty in Artificial Intelligence , 2021.\n",
            "[104] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,\n",
            "2013.\n",
            "[105] Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization\n",
            "trick. In Advances in neural information processing systems , 2015.\n",
            "[106] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n",
            "recognition. arXiv preprint arXiv:1409.1556 , 2014.\n",
            "[107] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\n",
            "InProceedings of the IEEE conference on computer vision and pattern recognition , 2016.\n",
            "[108] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n",
            "arXiv:1412.6980 , 2014.\n",
            "15[109] Huaxiu Yao, Linjun Zhang, and Chelsea Finn. Meta-learning with fewer tasks through task interpolation.\n",
            "arXiv preprint arXiv:2106.02695 , 2021.\n",
            "[110] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new\n",
            "domains. In European Conference on Computer Vision , 2010.\n",
            "[111] Brian Kulis, Kate Saenko, and Trevor Darrell. What you saw is not what you get: Domain adaptation\n",
            "using asymmetric kernel transforms. In CVPR 2011 . IEEE, 2011.\n",
            "[112] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n",
            "document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.\n",
            "[113] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain\n",
            "adaptation. In IEEE Conference on Computer Vision and Pattern Recognition , 2012.\n",
            "[114] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. Dataset Report ,\n",
            "2007.\n",
            "[115] Mateusz Buda, Ashirbani Saha, and Maciej A Mazurowski. Association of genomic subtypes of lower-\n",
            "grade gliomas with shape features automatically extracted by a deep learning algorithm. Computers in\n",
            "biology and medicine , 109:218–225, 2019.\n",
            "[116] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\n",
            "image segmentation. In International Conference on Medical Image Computing and Computer-Assisted\n",
            "Intervention , 2015.\n",
            "16Contents\n",
            "1 Introduction 1\n",
            "2 Methodology 2\n",
            "2.1 Modeling and Inference of Heterogeneous Neural Processes . . . . . . . . . . . . 3\n",
            "2.2 Transformer-Structured Inference Module . . . . . . . . . . . . . . . . . . . . . . 4\n",
            "3 Related Work 6\n",
            "4 Experiments 7\n",
            "4.1 Episodic Multi-Task Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "4.2 Episodic Multi-task Classification . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "4.3 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
            "5 Conclusion 10\n",
            "A Frequently Asked Questions 18\n",
            "B Properties of Valid Exchangeable Stochastic Processes 20\n",
            "B.1 Proof of Exchangeability Consistency . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
            "B.2 Proof of Marginalization Consistency . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
            "C Tractable and Scalable Optimization 22\n",
            "C.1 Derivation of Approximate ELBO for HNPs . . . . . . . . . . . . . . . . . . . . . 22\n",
            "C.2 Meta-Training Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
            "C.3 Meta-Test Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
            "D More Experimental Details 23\n",
            "D.1 Transformer-structured Inference Modules in Regression Scenarios . . . . . . . . . 23\n",
            "D.2 Backbone and Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
            "E Algorithm of the proposed HNPs 23\n",
            "F More Experimental Results under Episodic Multi-Task Setup 23\n",
            "F.1 Effects of More “Tasks” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
            "F.2 4task20way1shot v.s. 20way4shot . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n",
            "F.3 Comparisons with More Recent Works . . . . . . . . . . . . . . . . . . . . . . . . 24\n",
            "F.4 Comparisons on Another Benchmark Dataset . . . . . . . . . . . . . . . . . . . . 25\n",
            "G More Experimental Results under Conventional Multi-Task Setup 25\n",
            "G.1 Conventional Multi-Task Regression . . . . . . . . . . . . . . . . . . . . . . . . . 25\n",
            "G.2 Conventional Multi-Task Classification . . . . . . . . . . . . . . . . . . . . . . . . 26\n",
            "H Application to Brain Image Segmentation 27\n",
            "17A Frequently Asked Questions\n",
            "In this section, we list frequently asked questions from researchers who help proofread this manuscript.\n",
            "These raised questions might also be relevant for others and help in better understanding the paper, so\n",
            "we include more detailed discussions here.\n",
            "Connections between different settings. This work considers the multi-input multi-output setting\n",
            "of multi-task learning under the episodic training mechanism.\n",
            "As shown in Table 9, we use \"Heterogeneous tasks\" to distinguish the different branches of multi-task\n",
            "learning: (1) single-input multi-output (SIMO) considers different tasks which have the same input\n",
            "and different supervision information. (2) multi-input multi-output (MIMO) considers heterogeneous\n",
            "tasks, which have different inputs and follow different data distributions. All tasks are related since\n",
            "they share the target space. This setting encourages deep models to deal with the insufficient data of\n",
            "each task by aggregating the training data from related tasks in the spirit of data augmentation.\n",
            "Meanwhile, \"Episodic training\" is used to describe the data-feeding strategy. Multi-task meta-learning\n",
            "also benefits from episodic training, but it follows the SIMO setting in every single episode and cannot\n",
            "sufficiently handle heterogeneous tasks. In our work, episodic multi-task learning is designed based\n",
            "on the MIMO setting, suffering from distribution shifts between heterogeneous tasks. In addition,\n",
            "we note that conventional meta-learning follows the \"Episodic training\" mechanism but focuses on\n",
            "single-task learning in each episode. Thus, \"Heterogeneous tasks\" is not available here (-). More\n",
            "details are left in Table (9).\n",
            "Table 9: Connections between different settings. The symbols ✓and✗indicate whether or not the\n",
            "specific setting has the corresponding characteristic.\n",
            "Settings Methods Episodic training Heterogeneous tasks\n",
            "single-input multi-output (SIMO) [24–30] ✗ ✗\n",
            "multi-input multi-output (MIMO) [11, 10, 31–34] ✗ ✓\n",
            "conventional meta-learning [4–7, 12, 13] ✓ -\n",
            "multi-task meta-learning [96–98, 19, 99–101] ✓ ✗\n",
            "episodic multi-task learning This paper ✓ ✓\n",
            "Problem scope. In episodic multi-task learning, we restrict the scope of the problem to the case\n",
            "where tasks in the same episode are related and share the same target space. There are two main\n",
            "reasons: (1) we follow the MIMO setting of multi-task learning in every single episode, where the\n",
            "same target space assures the existence of the knowledge shared among tasks. (2) As demonstrated\n",
            "in recent works [ 102,103], meta-learning tasks generated from the same categories or taxonomic\n",
            "clusters are closer. This also implies that tasks with the same target space are related.\n",
            "Differences from other episodic single-task setups. Based on episodic training, there are several\n",
            "approaches related to the setting of episodic multi-task learning: (1) cross-domain few-shot learning\n",
            "addresses few-shot learning under domain shifts [ 77]. Several models [ 78,77,79–82] train a model\n",
            "on a single source domain or several source domains and then generalize it to other domains. In\n",
            "contrast, our research emphasizes the domain shifts within individual episodes rather than among\n",
            "them. (2) multimodal meta-learning extends few-shot learning from a single input-label domain to\n",
            "multiple different input-label domains [ 83]. These methods [ 84,83,56,85–87] design a meta-learner\n",
            "that can handle tasks from distinct distributions in sequence. Our work centers on simultaneously\n",
            "dealing with several related tasks within a meta-training or meta-test episode. (3) cross-modality\n",
            "few-shot learning [88–90] leverages semantic information (e.g., word vectors) to augment the\n",
            "performance of visual tasks and not among visual tasks only. The aforementioned approaches\n",
            "exclusively address single-task learning per episode, while our work concurrently tackles multiple\n",
            "heterogeneous and related tasks within each meta-training or meta-test episode. Intuitive comparisons\n",
            "with the approaches are shown in Figure 6.\n",
            "Different roles of global and local latent variables. In this paper, we introduce global latent\n",
            "representations and local latent parameters within a hierarchical architecture. Each type of them plays\n",
            "a distinct role in the proposed model: (1) Global latent representations provide rich task-specific\n",
            "information during the inference of all local latent parameters. This enables the model to generate\n",
            "18Figure 6: Differences from other episodic single-task setups . Each row corresponds to an episode.\n",
            "Different color denotes different categories; the same color with different shades represents different\n",
            "categories in the same task. Episodic multi-task learning is orthogonal to these setups since it\n",
            "concurrently tackles multiple heterogeneous and related tasks within each episode.\n",
            "task-specific decoders to handle heterogeneous tasks in a single episode effectively. (2) Local latent\n",
            "parameters with prediction-aware information constitute task-specific decoders. Each local latent\n",
            "parameter reveals the knowledge corresponding to a specific prediction across different tasks. This\n",
            "enhances the expressive power of the proposed model. In practice, we observe that global latent\n",
            "representations and local latent parameters complement each other when performing predictions in\n",
            "meta-test episodes.\n",
            "Advantages of the proposed hierarchical Bayes framework. We summarize the advantages of\n",
            "the proposed framework. (1) A hierarchical Bayesian framework with global and local latent variables\n",
            "yields a richer and more complex latent space to mitigate the expressiveness bottleneck, thus better\n",
            "parameterizing task-specific functions in stochastic processes. (2) Global and local latent variables\n",
            "capture epistemic uncertainty in representation and parameter levels, respectively, and show improved\n",
            "performance in our experiments.\n",
            "Roles of probabilistic HNPs and KL values in meta training. The probabilistic HNPs encode the\n",
            "context as the heterogeneous prior and reveal the uncertainty resulting from data insufficiency and the\n",
            "extent of observations in tasks. Additionally, minimizing KL terms encourages priors inferred from\n",
            "context sets to stay close to posteriors inferred from target sets, guiding more efficient conditional\n",
            "generation. In training processes, we observed the KL divergence value does not decrease to 0 after\n",
            "convergence, e.g., KL values are in the order of e-1 on Office-home. This is also part of traits in\n",
            "the NPs family, suggesting the approximate posterior and the approximate prior encode different\n",
            "conditional information during the generation of latent variables.\n",
            "Real-world examples or benchmarks for episodic multi-task learning. Episodic multi-task\n",
            "learning has several potential applications in the real world, such as autonomous driving and robotic\n",
            "manipulations. In detail, the autonomous driving system needs to deal with different and related\n",
            "sensor data in an environment. However, the driving environment constantly changes along with the\n",
            "weather, time, country, etc. Thus, fast adapting of the current multi-tasker to new environments is\n",
            "challenging in this application and our method can be a plausible solution for this challenge.\n",
            "19B Properties of Valid Exchangeable Stochastic Processes\n",
            "Here, we further demonstrate that HNPs are valid stochastic processes, as meeting the exchangeability\n",
            "andmarginalization consistency conditions [ 12]. As stated in [ 12]: the conditions, including (finite)\n",
            "exchangeability and marginalization, are sufficient to define a stochastic process with the help of the\n",
            "Kolmogorov extension theorem. Here we follow the notations in the main paper. For convenience,\n",
            "we provide the used symbols and the corresponding descriptions in Table 10.\n",
            "In this paper, we model the functional posterior distribution of the stochastic process by approximating\n",
            "the joint distribution over all target sets p(y1:M\n",
            "τ|x1:M\n",
            "τ,C1:M\n",
            "τ), which is conditioned on all context\n",
            "samples C1:M\n",
            "τ. To distinguish the order set among different tasks, we use nm\n",
            "τto denote the number\n",
            "of target samples corresponding to a specific task in the episode τ. For simplicity, we omit the\n",
            "meta-knowledge ωandν1:Min the formulations during the proof.\n",
            "Table 10: Notations and their corresponding descriptions in this paper.\n",
            "Notation Description\n",
            "(·)τ Variables correspond to an episode.\n",
            "(·)mVariables correspond to a single task.\n",
            "Im\n",
            "τ A single task in the episode τ, which is sampled from a specific task distribution.\n",
            "p(Im) The specific task distribution.\n",
            "M The number of task distributions and the number of tasks in a single episode.\n",
            "I1:M\n",
            "τ All heterogeneous tasks in the episode τ.\n",
            "C A context set.\n",
            "T A target set.\n",
            "¯x A context sample feature of the context set.\n",
            "x A target sample feature of the target set.\n",
            "x Set of all target sample features in the target set.\n",
            "¯y The ground truth of the context sample.\n",
            "y The ground truth of the target sample.\n",
            "y Set of the ground truth of all target samples in the target set.\n",
            "NC The numbers of context samples in the context set.\n",
            "NT The numbers of target samples in the target set.\n",
            "zm\n",
            "τ The introduced global latent representation for a given task.\n",
            "wm\n",
            "τ,1:O The introduced local latent parameters for a given task.\n",
            "B.1 Proof of Exchangeability Consistency\n",
            "We now provide the proof of Exchangeability Consistency : the joint prediction distribution is invariant\n",
            "to the permutation of the given multiple tasks and the corresponding samples in each task.\n",
            "Theorem B.1. (Exchangability) For finite n∗\n",
            "τ=PM\n",
            "m=1nm\n",
            "τ, ifπ∗\n",
            "τ={πm\n",
            "τ}M\n",
            "m=1is a permutation of\n",
            "{1,···, n∗}where πm\n",
            "τis a permutation of the corresponding order set {1,···, nm\n",
            "τ}, then:\n",
            "p(π∗\n",
            "τ(y1:M\n",
            "τ)|π∗\n",
            "τ(x1:M\n",
            "τ),C1:M\n",
            "τ) =p(y1:M\n",
            "τ|x1:M\n",
            "τ,C1:M\n",
            "τ),\n",
            "where π∗\n",
            "τ(y1:M\n",
            "τ) := ( π1\n",
            "τ(y1\n",
            "τ),···, πM\n",
            "τ(yM\n",
            "τ)) = ( yτ,π∗τ(1),···, yτ,π∗τ(n∗τ))andπ∗\n",
            "τ({x1:M\n",
            "τ}) :=\n",
            "(π1\n",
            "τ(x1\n",
            "τ),···, πM\n",
            "τ(xM\n",
            "τ)) = ( xτ,π∗τ(1),···, xτ,π∗τ(n∗τ)).\n",
            "20Proof.\n",
            "p(π∗\n",
            "τ(y1:M\n",
            "τ)|π∗\n",
            "τ(x1:M\n",
            "τ),C1:M\n",
            "τ)\n",
            "=Z Z\n",
            "p(π∗\n",
            "τ(y1:M\n",
            "τ)|π∗\n",
            "τ(x1:M\n",
            "τ),w1:M\n",
            "τ,1:O)\u0010MY\n",
            "m=1p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)\u0011\u0010 MY\n",
            "m=1p(zm\n",
            "τ|Cm\n",
            "τ)\u0011\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τ\n",
            "=Z Z\u0010n∗\n",
            "τY\n",
            "i=1p(yτ,π∗τ(i)|xτ,π∗τ(i),w1:M\n",
            "τ,1:O)\u0011\u0010 MY\n",
            "m=1p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)\u0011\u0010 MY\n",
            "m=1p(zm\n",
            "τ|Cm\n",
            "τ)\u0011\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τ\n",
            "=Z Z MY\n",
            "m=1nnm\n",
            "τY\n",
            "j=1p(ym\n",
            "τ,πmτ(j)|xm\n",
            "τ,πmτ(j),wm\n",
            "τ,1:O)p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)p(zm\n",
            "τ|Cm\n",
            "τ)o\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τ\n",
            "=Z Z MY\n",
            "m=1nnm\n",
            "τY\n",
            "j=1p(ym\n",
            "τ,j|xm\n",
            "τ,j,wm\n",
            "τ,1:O)p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)p(zm\n",
            "τ|Cm\n",
            "τ)o\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τ\n",
            "=p(y1:M\n",
            "τ|x1:M\n",
            "τ,C1:M\n",
            "τ).\n",
            "B.2 Proof of Marginalization Consistency\n",
            "We now aim to prove the Marginalization Consistency of the proposed model: if marginalizing out\n",
            "a part of the target set in each task, the marginal distribution remains the same as defined on the\n",
            "original target sets without the marginalized part.\n",
            "Theorem B.2. (Marginalization) Given ˆn∗\n",
            "τ=PM\n",
            "m=1ˆnm\n",
            "τ, where 1≤ˆn∗\n",
            "τ≤n∗\n",
            "τand for each task\n",
            "1≤ˆnm\n",
            "τ≤nm\n",
            "τ, the consistency is:\n",
            "Z\n",
            "p(y1:M\n",
            "τ|x1:M\n",
            "τ,C1:M\n",
            "τ)d(y1:M\n",
            "τ)ˆn∗τ+1:n∗τ=p((y1:M\n",
            "τ)1:ˆn∗τ|(x1:M\n",
            "τ)1:ˆn∗τ,C1:M\n",
            "τ),\n",
            "where (y1:M\n",
            "τ)1:ˆn∗τ= (( y1\n",
            "τ)1:ˆn1τ,···,(yM\n",
            "τ)1:ˆnMτ) = ( yτ,1,···, yτ,ˆn∗τ)and (x1:M\n",
            "τ)1:ˆn∗τ=\n",
            "((x1\n",
            "τ)1:ˆn1τ,···,(xM\n",
            "τ)1:ˆnMτ) = (xτ,1,···, xτ,ˆn∗τ).\n",
            "Proof.\n",
            "Z\n",
            "p(y1:M\n",
            "τ|x1:M\n",
            "τ,C1:M\n",
            "τ)d(y1:M\n",
            "τ)ˆn∗τ+1:n∗τ\n",
            "=Z Z Z\n",
            "p(y1:M\n",
            "τ|x1:M\n",
            "τ,w1:M\n",
            "τ,1:O)\u0010MY\n",
            "m=1p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)\u0011\u0010 MY\n",
            "m=1p(zm\n",
            "τ|Cm\n",
            "τ)\u0011\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τd(y1:M\n",
            "τ)ˆn∗τ+1:n∗τ\n",
            "=Z Z Z \u0010n∗\n",
            "τY\n",
            "i=1p(yτ,i|xτ,i,w1:M\n",
            "τ,1:O)\u0011\u0010 MY\n",
            "m=1p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)\u0011\u0010 MY\n",
            "m=1p(zm\n",
            "τ|Cm\n",
            "τ)\u0011\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τd(y1:M\n",
            "τ)ˆn∗τ+1:n∗τ\n",
            "=Z Z MY\n",
            "m=1nZnm\n",
            "τY\n",
            "j=1p(ym\n",
            "τ,j|xm\n",
            "τ,j,wm\n",
            "τ,1:O)p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)p(zm\n",
            "τ|Cm\n",
            "τ)d(ym\n",
            "τ)ˆnmτ+1:nmτo\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τ\n",
            "=Z Z MY\n",
            "m=1nˆnm\n",
            "τY\n",
            "j=1p(ym\n",
            "τ,j|xm\n",
            "τ,j,wm\n",
            "τ,1:O)p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)p(zm\n",
            "τ|Cm\n",
            "τ)\n",
            "Z nm\n",
            "τY\n",
            "j=ˆnmτ+1p(ym\n",
            "τ,j|xm\n",
            "τ,j,wm\n",
            "τ,1:O)d(ym\n",
            "τ)ˆnmτ+1:nmτo\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τ\n",
            "=Z Z MY\n",
            "m=1nˆnm\n",
            "τY\n",
            "j=1p(ym\n",
            "τ,j|xm\n",
            "τ,j,wm\n",
            "τ,1:O)p(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ)p(zm\n",
            "τ|Cm\n",
            "τ)o\n",
            "dw1:M\n",
            "τ,1:Odz1:M\n",
            "τ\n",
            "=p((y1:M\n",
            "τ)1:ˆn∗τ|(x1:M\n",
            "τ)1:ˆn∗τ,C1:M\n",
            "τ).\n",
            "21C Tractable and Scalable Optimization\n",
            "For the proposed HNPs, it is intractable to obtain the true joint posterior\n",
            "p(w1:M\n",
            "τ,1:O,z1:M\n",
            "τ|T1:M\n",
            "τ;ω, ν1:M)for each episode. Thus, we employ variational inference to\n",
            "optimize the designed model by approximating the true joint posterior in each episode. To do so, we\n",
            "introduce the variational joint posterior distribution:\n",
            "qθ,ϕ(w1:M\n",
            "τ,1:O,z1:M\n",
            "τ|T1:M\n",
            "τ;ω, ν1:M) =MY\n",
            "m=1qθ(zm\n",
            "τ|Tm\n",
            "τ;νm)qϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,T1:M\n",
            "τ;ω), (11)\n",
            "where qθ(zm\n",
            "τ|Tm\n",
            "τ;νm)andqϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,T1:M\n",
            "τ;ω)are variational posteriors of their corresponding\n",
            "latent variables. We note that variational posteriors are inferred from the target sets that are avail-\n",
            "able in the meta-training stage. The variational posteriors are parameterized as diagonal Gaussian\n",
            "distributions [ 104]. The inference networks θandϕare shared by the variational posteriors and their\n",
            "corresponding priors, following the protocol of vanilla NPs [12].\n",
            "C.1 Derivation of Approximate ELBO for HNPs\n",
            "By incorporating the variational posteriors in Eq. (11) into the modeling of HNPs in the main paper,\n",
            "we can derive the approximate ELBO LHNPs(ω, ν1:M, θ, ϕ)as follows:\n",
            "logp(T1:M\n",
            "τ|C1:M\n",
            "τ;ω, ν1:M)\n",
            "=MX\n",
            "m=1logp(Tm\n",
            "τ|C1:M\n",
            "τ;ω, νm)\n",
            "=MX\n",
            "m=1\u001a\n",
            "logZnZ\n",
            "p(ym\n",
            "τ|xm\n",
            "τ,wm\n",
            "τ,1:O)pϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω)dwm\n",
            "τ,1:Oo\n",
            "pθ(zm\n",
            "τ|Cm\n",
            "τ;νm)dzm\n",
            "τ\u001b\n",
            "=MX\n",
            "m=1\u001a\n",
            "logZnZ\n",
            "p(ym\n",
            "τ|xm\n",
            "τ,wm\n",
            "τ,1:O)pϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω)qϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,T1:M\n",
            "τ;ω)\n",
            "qϕ(wm\n",
            "τ,1:O|zmτ,T1:Mτ;ω)dwm\n",
            "τ,1:Oo\n",
            "pθ(zm\n",
            "τ|Cm\n",
            "τ;νm)qθ(zm\n",
            "τ|Tm\n",
            "τ;νm)\n",
            "qθ(zmτ|Tmτ;νm)dzm\n",
            "τ\u001b\n",
            "≥MX\n",
            "m=1\u001a\n",
            "Eqθ(zmτ|Tmτ;νm)n\n",
            "logZ\n",
            "p(ym\n",
            "τ|xm\n",
            "τ,wm\n",
            "τ,1:O)pϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω)qϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,T1:M\n",
            "τ;ω)\n",
            "qϕ(wm\n",
            "τ,1:O|zmτ,T1:Mτ;ω)dwm\n",
            "τ,1:Oo\n",
            "−DKL[qθ(zm\n",
            "τ|Tm\n",
            "τ;νm)||pθ(zm\n",
            "τ|Cm\n",
            "τ;νm)]\u001b\n",
            "≥MX\n",
            "m=1\u001a\n",
            "Eqθ(zmτ|Tmτ;νm)n\n",
            "Eqϕ(wm\n",
            "τ,1:O|zmτ,T1:Mτ;ω)[logp(ym\n",
            "τ|xm\n",
            "τ,wm\n",
            "τ,1:O)]\n",
            "−DKL[qϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,T1:M\n",
            "τ;ω)||pϕ(wm\n",
            "τ,1:O|zm\n",
            "τ,C1:M\n",
            "τ;ω)]o\n",
            "−DKL[qθ(zm\n",
            "τ|Tm\n",
            "τ;νm)||pθ(zm\n",
            "τ|Cm\n",
            "τ;νm)]\u001b\n",
            "=LHNPs(ω, ν1:M, θ, ϕ).\n",
            "(12)\n",
            "In general, when the variational joint posterior is flexible enough, the posterior approximation gap\n",
            "between the variational posterior and the intractable true posterior can be reduced to an arbitrarily\n",
            "small quantity [ 73]. In this case, maximizing the approximate ELBO increases the overall log\n",
            "likelihood in the proposed model accordingly. We construct task-specific decoders in a data-driven\n",
            "way by inferring local latent parameters wm\n",
            "1:Ofrom all context sets and the meta-knowledge. This\n",
            "enables our model to amortize the training cost of each task-specific decoder, further reducing the\n",
            "model’s over-fitting behaviors for episodic multi-task learning.\n",
            "C.2 Meta-Training Objective\n",
            "In practice, we consider the loss function as the negative approximate ELBO of HNPs as given in\n",
            "Eq. (12). By adopting Monte Carlo sampling [ 104,105], the meta-training objective for each episode\n",
            "22is:\n",
            "−LHNPs(ω, ν1:M, θ, ϕ)≈MX\n",
            "m=1\u001a1\n",
            "NzNzX\n",
            "i=1n1\n",
            "NwNwX\n",
            "j=1[−logp(ym\n",
            "τ|xm\n",
            "τ,wm\n",
            "τ,1:O(j))]\n",
            "+DKL[qϕ(wm\n",
            "τ,1:O|zm\n",
            "τ(i),T1:M\n",
            "τ;ω)||pϕ(wm\n",
            "τ,1:O|zm\n",
            "τ(i),C1:M\n",
            "τ;ω)]o\n",
            "+DKL[qθ(zm\n",
            "τ|Tm\n",
            "τ;νm)||pθ(zm\n",
            "τ|Cm\n",
            "τ;νm)]\u001b\n",
            ",(13)\n",
            "where zm\n",
            "τ(i)andwm\n",
            "τ,1:O(j)are sampled from their corresponding variational posteriors. NzandNw\n",
            "are the number of Monte Carlo samples for zm\n",
            "τandwm\n",
            "τ,1:O, respectively.\n",
            "C.3 Meta-Test Prediction\n",
            "At the meta-test stage, we perform predictions with the learned model on the target sets for a new\n",
            "episode τ∗, which involves the prior distributions of global and local latent variables. We again\n",
            "approximate the predictive distribution with Monte Carlo estimates:\n",
            "p(T1:M\n",
            "τ∗|C1:M\n",
            "τ∗;ω, ν1:M)≈MY\n",
            "m=1\u001a1\n",
            "NzNzX\n",
            "i=11\n",
            "NwNwX\n",
            "j=1p(ym\n",
            "τ∗|xm\n",
            "τ∗,wm\n",
            "τ∗,1:O(j))\u001b\n",
            ", (14)\n",
            "where wm\n",
            "τ∗,1:O(j)∼pϕ(wm\n",
            "τ∗,1:O|zm\n",
            "τ∗(i),C1:M\n",
            "τ∗;ω)andzm\n",
            "τ∗(i)∼pθ(zm\n",
            "τ∗|Cm\n",
            "τ∗;νm). Here the Monte\n",
            "Carlo samples follow their corresponding prior distributions since the target sets are unavailable\n",
            "during the meta-test.\n",
            "D More Experimental Details\n",
            "D.1 Transformer-structured Inference Modules in Regression Scenarios\n",
            "Here we present the implementation details of transformer-structured inference module θin regression\n",
            "scenarios. In a single episode τ, the module θencodes the task-specific information into each refined\n",
            "task-specific token νm, and then infers the prior distribution or the variational posterior distribution\n",
            "for the global latent representation zm\n",
            "τ. For episodic multi-task regression, the local latent parameters\n",
            "wm\n",
            "τ,1:Oconstruct a task-specific regressor during inference. We assume that the output of the decoder\n",
            "follows a Gaussian distribution for regression tasks. Thus, wm\n",
            "τ,1:Oare instantiated as parameters for\n",
            "generating the mean and variance of predictions.\n",
            "D.2 Backbone and Training Details\n",
            "Following the protocol of [ 10], we apply the pre-trained deep model as the backbone for the proposed\n",
            "method and baselines to extract the input features under the episodic multi-task classification setup.\n",
            "To be specific, we adopt VGGnet [ 106] for Office-Home , and its feature size is 4096 . We take\n",
            "ResNet18 [ 107] for DomainNet with input size 512. In practice, we train our method and baselines\n",
            "by the Adam optimizer [ 108] using an NVIDIA Tesla V100 GPU. The learning rate is initially set as\n",
            "1e−4and decreases with a factor of 0.5every 3,000iterations.\n",
            "E Algorithm of the proposed HNPs\n",
            "F More Experimental Results under Episodic Multi-Task Setup\n",
            "F.1 Effects of More “Tasks”\n",
            "To investigate the effects of more \"tasks\" in the episodic multi-task setup, we perform experiments\n",
            "on Office-Home under the Xway5way1shot setup by gradually increasing tasks during inference.\n",
            "Table 11 shows that the average accuracy increases with more tasks. The main reason is that more\n",
            "tasks can provide richer transferable information. Our model benefits from the positive transfer\n",
            "among tasks, and thus obtaining higher performance gain from more tasks.\n",
            "23Algorithm 1: Meta-training phase of HNPs.\n",
            "Input : Mdistinct and relevant task distributions p(I1:M), numbers of Monte Carlo samples\n",
            "NzandNw, learning rates α.\n",
            "Output : Meta-trained transformer inference module θandϕ, learnable tokens ω1:Oandν1:M.\n",
            "Initialize transformer inference module {θ, ν1:M};\n",
            "Initialize transformer inference module {ϕ, ω 1:O};\n",
            "while Meta-Training not Completed do\n",
            "// sample a meta-training episode indexed with τ.\n",
            "form= 1toMdo\n",
            "Sample a task Im\n",
            "τ∼p(Im), which shares the same target space Yτwith other tasks in\n",
            "the episode;\n",
            "Sample a context set Cm\n",
            "τand a target set Tm\n",
            "τfor the task Im\n",
            "τ;\n",
            "end\n",
            "// infer hierarchical latent variables.\n",
            "form= 1toMdo\n",
            "Apply transformer inference module θto infer prior and variational posterior\n",
            "distributions over zm\n",
            "τas Eq. (5), Eq. (6), and Eq. (7) in the main paper;\n",
            "Draw Nzsamples from the variational posterior, {zm\n",
            "τ(i)}Nz\n",
            "i=1;\n",
            "fori= 1toNzdo\n",
            "foro= 1toOdo\n",
            "Apply transformer inference module ϕto infer prior and variational posterior\n",
            "distributions over wm\n",
            "τ,oas Eq. (8), Eq. (9), and Eq. (10) in the main paper;\n",
            "Draw Nwsamples from the variational posterior, {wm\n",
            "τ,o(j)}Nw\n",
            "j=1;\n",
            "end\n",
            "end\n",
            "end\n",
            "// optimize the objective.\n",
            "Compute predictive distributions and minimize the empirical objective in Eq. (13) ;\n",
            "Update θ,ϕ,ω1:Oandν1:Mwith learning rate α.\n",
            "end\n",
            "F.2 4task20way1shot v.s. 20way4shot\n",
            "To show the effectiveness of the proposed method, we conduct experiments with the 20-way 4-shot\n",
            "setup, which needs to mix samples from all tasks in one episode. As shown in Table 12, MAML and\n",
            "Proto.Net perform better under the 20-way 4-shot but cannot outperform our method. The main reason\n",
            "is that our method can better handle distribution shifts among tasks by exploring task-relatedness\n",
            "rather than simply mixing them together.\n",
            "F.3 Comparisons with More Recent Works\n",
            "To compare the proposed method with more recent works, we perform experiments on Office-Home\n",
            "under the 4task5way1shot and 4task5way5shot setups. We provide some brief descriptions of two\n",
            "recent works as follows:\n",
            "[76] theoretically addresses the conclusion that MTL methods are powerful and efficient alternatives\n",
            "to GBML for meta-learning applications. However, our method inherits the advantages of multi-task\n",
            "learning and meta-learning: simultaneously generalizing meta-knowledge from past to new episodes\n",
            "and exploiting task-relatedness across heterogeneous tasks in every single episode. Thus, our method\n",
            "is more suitable for solving the data-insufficiency problem.\n",
            "[109] augments the task set in meta-learning through interpolation. Our method fully utilizes several\n",
            "observed tasks in a single episode rather than generating additional tasks.\n",
            "Table 13 shows that our method significantly outperforms other baselines with severely insufficient\n",
            "data, such as 1-shot. This is consistent with the conclusion obtained in the main paper.\n",
            "24Algorithm 2: Meta-test phase of HNPs.\n",
            "Input : Meta-trained transformer inference module θandϕ, learned tokens ω1:Oandν1:M,\n",
            "numbers of Monte Carlo samples NzandNw.\n",
            "Output : Prediction results.\n",
            "while Meta-Test not Completed do\n",
            "// given a meta-test episode indexed with τ∗.\n",
            "Collect the context sets of all tasks in the episode, C1:M\n",
            "τ∗.\n",
            "form= 1toMdo\n",
            "Apply transformer inference module θto infer prior distributions over zm\n",
            "τ∗as Eq. (5),\n",
            "Eq. (6), and Eq. (7) in the main paper;\n",
            "Draw Nzsamples from the prior distribution, {zm\n",
            "τ∗(i)}Nz\n",
            "i=1;\n",
            "fori= 1toNzdo\n",
            "foro= 1toOdo\n",
            "Apply transformer inference module ϕto infer the prior distribution over wm\n",
            "τ∗,o\n",
            "as Eq. (8), Eq. (9), and Eq. (10) in the main paper;\n",
            "Draw Nwsamples from the prior distribution, {wm\n",
            "τ∗,o(j)}Nw\n",
            "j=1;\n",
            "end\n",
            "end\n",
            "end\n",
            "Perform predictions on the target sets in Eq. (14) ;\n",
            "end\n",
            "Table 11: Performance comparisons on Office-Home under the Xtask5way1shot setup during\n",
            "inference.\n",
            "Number of tasks 1 2 3 4\n",
            "Average accuracy 64.33 ±0.85 70.75 ±0.67 74.95 ±0.57 76.29 ±0.51\n",
            "F.4 Comparisons on Another Benchmark Dataset\n",
            "We validate the performance of methods on the Office31 dataset [ 110,111] under the 3task5way1shot\n",
            "setup. This dataset contains 31 object categories in three domains: Amazon, DSLR, and Webcam.\n",
            "Table 14 shows our method outperforms baseline methods, demonstrating our model’s effectiveness\n",
            "in addressing the data insufficiency under the episodic setup.\n",
            "G More Experimental Results under Conventional Multi-Task Setup\n",
            "To show comparisons with existing multi-task models, which are designed for conventional multi-\n",
            "task learning, we extend the proposed HNPs to conventional multi-task learning settings for both\n",
            "regression and classification tasks by considering only one episode during training and inference.\n",
            "Under conventional multi-task settings (MIMO), we investigate their effectiveness in exploring shared\n",
            "knowledge when limited tasks and samples are available during training.\n",
            "G.1 Conventional Multi-Task Regression\n",
            "Dataset and Settings. We show the effectiveness of HNPs for conventional multi-task regression.\n",
            "We design experiments for rotation angle estimation on the Rotated MNIST dataset [ 112]. Each task\n",
            "is an angle estimation problem for a digit, and different tasks corresponding to individual digits are\n",
            "related because they share the same rotation angle space. Each image is rotated by 0◦through 90◦in\n",
            "intervals of 10◦, where the rotation angle is the regression target. We randomly choose 0.1%training\n",
            "samples per task per angle as the training set.\n",
            "We use the average of normalized mean squared errors (NMSE) of all tasks as the performance\n",
            "measurement. The lower NMSE, the better the performance. We provide 95% confidence intervals\n",
            "for the errors from five runs. Descriptions of baselines can be found in Section G.2.\n",
            "25Table 12: Performance comparisons under the 4task20way1shot and 20way4shot setups.\n",
            "Methods 4task20way1shot 20way4shot\n",
            "MAML 34.29±0.19 37 .23±0.25\n",
            "Proto.Net 32.72±0.18 37 .12±0.22\n",
            "Ours 51.82±0.23 -\n",
            "Table 13: Performance comparisons with more recent baselines.\n",
            "Methods 1-shot 5-shot\n",
            "MTL-bridge [76] 64.31±0.55 75 .10±0.51\n",
            "MLTI [109] 70.69±0.73 79 .59±0.58\n",
            "Ours 76.29±0.51 80 .80±0.42\n",
            "Results and Discussions. The experimental results are summarized in Table 15. The proposed\n",
            "HNPs outperform other counterpart methods by yielding a lower mean error. This demonstrates the\n",
            "effectiveness of HNPs in capturing task-relatedness for improved regression performance.\n",
            "G.2 Conventional Multi-Task Classification\n",
            "Datasets and Settings. Office-Caltech [113] contains ten categories shared between Office-31\n",
            "[110] and Caltech-256 [ 114]. One task uses data from Caltech-256, and the other tasks use data from\n",
            "Office-31, whose images were collected from three distinct domains/tasks, namely Amazon, Webcam\n",
            "and DSLR. There are 8∼151samples per category per task and 2,533images. ImageCLEF [10],\n",
            "the benchmark for the ImageCLEF domain adaptation challenge, contains 12common categories\n",
            "shared by four public datasets/tasks: Caltech-256, ImageNet ILSVRC 2012, Pascal VOC 2012, and\n",
            "Bing. There are 2,400images in total. Office-Home [91] mentioned in the main paper is also used\n",
            "under this setting.\n",
            "We adopt the standard evaluation protocols [ 10] for multi-task classification datasets. We randomly\n",
            "select 5%,10% and20% labeled data for training, which correspond to about 3, 6 and 12 samples\n",
            "per category per task, respectively. In this case, each task has insufficient training data for building\n",
            "a reliable classifier without overfitting. The average accuracy of all tasks is used for measuring the\n",
            "overall performance. We again provide 95% confidence intervals for the errors from five runs.\n",
            "Alternatives Methods. We conduct a thorough comparison with alternative multi-task learning\n",
            "models. Single-task learning (STL) is implemented by task-specific feature extractors and predictors\n",
            "without knowledge sharing among tasks. Basic multi-task learning (BMTL) shares feature extractors\n",
            "and adds task-specific predictors. We also define variational extensions of single-task learning (VSTL)\n",
            "and basic multi-task learning (VBMTL), which treat predictors as latent variables [ 11]. For a fair\n",
            "comparison, all the baseline methods mentioned above share the same architecture of the feature\n",
            "extractor and the train-test splits. We also compare the proposed HNPs to representative multi-task\n",
            "models. MTL-Uncertainty [ 24], MRN [ 10], LearnToBranch [ 48] are deep MTL methods, employing\n",
            "deep neural networks to construct information-sharing mechanisms for tasks. TCGBML [ 35],\n",
            "MTVIB [ 41] and VMTL [ 11] are probabilistic MTL methods, applying Bayes frameworks to model\n",
            "the relationships among tasks.\n",
            "Results and Discussions. We provide more comprehensive comparisons on Office-Home ,\n",
            "Office-Caltech andImageCLEF in Table 16. The best results are marked in bold. Our HNPs\n",
            "achieve competitive and even better performance on conventional multi-task classification datasets\n",
            "with different train-test splits. VSTL and VBMTL perform better than STL and BMTL, demonstrat-\n",
            "ing the benefits of Bayes frameworks. Compared with multi-task probabilistic baselines, including\n",
            "VBMTL, TCGBML, MTVIB and VMTL, our HNPs can model more complex functional distributions\n",
            "with more powerful priors by inferring both representations and parameters for predictive functions.\n",
            "Compared with VMTL [ 11], which neglects the hierarchical architecture of latent variables, the\n",
            "proposed HNPs show better performance. This demonstrates that by modeling the complex depen-\n",
            "dencies between heterogeneous context sets within the hierarchical Bayes framework, HNPs explore\n",
            "26Table 14: Performance comparisons on the Office31 dataset.\n",
            "Methods ERM Proto.Net CNPs NPs TNP-D Ours\n",
            "Average Accuracy 63.53±0.71 64 .54±0.64 49 .02±0.74 40 .52±0.75 69 .69±0.87 71 .89±0.52\n",
            "Table 15: Conventional multi-task regression (normalized mean squared errors) for rotation\n",
            "angle estimation.\n",
            "Methods Average NMSE\n",
            "STL .215±.001\n",
            "VSTL .224±.004\n",
            "BMTL .118±.003\n",
            "VBMTL .121±.003\n",
            "LearnToBranch [48] .109±.002\n",
            "VMTL [11] .110±.003\n",
            "HNPs .103±.001\n",
            "task-relatedness better. The hierarchical Bayes framework enables our model to explore the relevant\n",
            "knowledge even in the presence of distribution shifts among tasks.\n",
            "H Application to Brain Image Segmentation\n",
            "To show that HNPs have the potential to be helpful in settings other than categorization and regression,\n",
            "we apply the proposed HNPs to brain image segmentation.\n",
            "Dataset and Settings. We adopt a brain image dataset [ 115] with lower-grade gliomas collected\n",
            "from 110patients. The number of images varies among patients from 20to88. The goal is to segment\n",
            "the tumor in each brain image by predicting its contour.\n",
            "We reformulate the segmentation task as a pixel-wise regression problem, where each pixel corre-\n",
            "sponds to a regression task to predict the probability of this pixel belonging to the tumor. In doing\n",
            "so, the spatial correlation and dependency among pixels are effectively modeled by capturing task-\n",
            "relatedness. For the task m, we define Ωmas a local region centered at the spatial position m, which\n",
            "provides the local context information. In this case, the region centered at the pixel provides the local\n",
            "context information. Each task incorporates the knowledge provided by related tasks into the context\n",
            "of the predictive function. This offers an effective way to model the long-range interdependence of\n",
            "pixels in one image. In the implementation, we use U-Net [ 116] as the backbone and append our\n",
            "model to the last layer.\n",
            "Results and Discussions. The proposed HNPs surpass the baseline U-Net by 0.8% (91.2% v.s.\n",
            "90.6%) in terms of dice similarity coefficients (DSC) for the overall validation set. We provide the\n",
            "Table 16: Classification performance (average accuracy) on Office-Home ,Office-Caltech\n",
            "andImageCLEF .\n",
            "Office-Home Office-Caltech ImageCLEF\n",
            "Methods 5% 10% 20% 5% 10% 20% 5% 10% 20%\n",
            "STL 49.2±0.2 58.3±0.1 64.9±0.1 88.6±0.3 90.7±0.2 92.4±0.3 62.6±0.2 69.7±0.3 76.2±0.3\n",
            "VSTL 51.1±0.1 60.2±0.2 65.8±0.2 89.0±0.2 91.1±0.2 93.4±0.3 64.9±0.3 70.8±0.3 77.2±0.2\n",
            "BMTL 50.4±0.1 59.5±0.1 65.6±0.1 89.5±0.3 92.3±0.2 93.1±0.1 65.7±0.4 72.0±0.3 76.8±0.3\n",
            "VBMTL 51.3±0.1 60.9±0.1 67.0±0.2 90.8±0.6 93.2±0.2 93.5±0.1 67.1±0.3 73.0±0.7 78.0±0.2\n",
            "MTL-Uncertainty[24] 51.8±0.1 57.2±0.2 66.8±0.2 91.2±0.3 93.8±0.2 94.7±0.3 74.6±0.2 76.9±0.3 79.2±0.3\n",
            "MRN [10] 57.4±0.1 63.4±0.2 67.1±0.1 93.4±0.2 94.8±0.3 95.1±0.1 73.7±0.4 75.8±0.2 79.7±0.3\n",
            "LearnToBranch [48] 38.3±0.5 51.5±0.3 62.2±0.4 74.6±0.9 80.4±1.2 89.9±0.8 51.7±0.9 62.6±0.8 71.6±0.4\n",
            "TCGBML[35] 52.8±0.1 60.0±0.2 68.7±0.2 91.8±0.1 95.0±0.2 95.1±0.1 73.9±0.3 76.5±0.4 79.3±0.4\n",
            "MTVIB [41] 49.9±0.2 55.3±0.1 66.2±0.1 91.1±0.3 94.1±0.3 95.0±0.2 74.0±0.4 77.3±0.3 78.9±0.5\n",
            "VMTL [11] 58.3±0.1 65.0±0.0 69.2±0.2 93.8±0.1 95.3±0.0 95.2±0.1 76.2±0.3 77.9±0.2 80.2±0.1\n",
            "HNPs 60.0±0.1 66.2±0.2 70.9±0.2 94.6±0.1 95.4±0.1 95.8±0.1 76.4±0.1 79.5±0.1 80.9±0.1\n",
            "27Figure 7: Segmentation results by HNPs (bottom row) and U-Net (upper row) . The ground truth\n",
            "contours are in green, and the predicted ones are in red. The numbers are DSC scores computed\n",
            "against the ground truth. HNPs can predict contours closer to the ground truth ones, indicating the\n",
            "advantages of exploring spatial context information for image segmentation.\n",
            "predicted contours by HNPs (bottom row) and the U-Net (upper row) in Figure 7, where the green\n",
            "outline corresponds to the ground truth and the red to the segmentation output. This figure shows that\n",
            "HNPs predict contours closer to the ground truth. The results demonstrate the advantages of HNPs in\n",
            "exploring spatial-relatedness for medical segmentation.\n",
            "28Towards a Unified Conversational Recommendation System:\n",
            "Multi-task Learning via Contextualized Knowledge Distillation\n",
            "Yeongseo Jung Eunseo Jung Lei Chen\n",
            "The Hong Kong University of Science and Technology\n",
            "{yjungag, ejungab, leichen}@cse.ust.hk\n",
            "Abstract\n",
            "In Conversational Recommendation System\n",
            "(CRS), an agent is asked to recommend a set\n",
            "of items to users within natural language con-\n",
            "versations. To address the need for both con-\n",
            "versational capability and personalized recom-\n",
            "mendations, prior works have utilized sepa-\n",
            "rate recommendation and dialogue modules.\n",
            "However, such approach inevitably results in\n",
            "a discrepancy between recommendation re-\n",
            "sults and generated responses. To bridge\n",
            "the gap, we propose a multi-task learning\n",
            "for a unified CRS, where a single model\n",
            "jointly learns both tasks via Contextualized\n",
            "Knowledge Distillation (ConKD). We intro-\n",
            "duce two versions of ConKD: hard gate and\n",
            "soft gate . The former selectively gates between\n",
            "two task-specific teachers, while the latter in-\n",
            "tegrates knowledge from both teachers. Our\n",
            "gates are computed on-the-fly in a context-\n",
            "specific manner, facilitating flexible integration\n",
            "of relevant knowledge. Extensive experiments\n",
            "demonstrate that our single model significantly\n",
            "improves recommendation performance while\n",
            "enhancing fluency, and achieves comparable\n",
            "results in terms of diversity.1\n",
            "1 Introduction\n",
            "Natural language dialogue systems generally fall\n",
            "into either task-oriented system (Wen et al., 2016;\n",
            "Henderson et al., 2019; Peng et al., 2020) or open-\n",
            "domain dialogue system (Xing et al., 2016; Zhang\n",
            "et al., 2020; Adiwardana et al., 2020). Despite the\n",
            "same modality (conversation), the tasks differ in\n",
            "their objectives; the former aims to achieve certain\n",
            "tasks (e.g. booking hotels), while the latter engage\n",
            "in an open-ended dialogue.\n",
            "Conversational Recommendation (CR) is an\n",
            "emerging task in natural language dialogue, which\n",
            "combines the task-oriented and open-domain (Gao\n",
            "et al., 2021). The task aims to recommend proper\n",
            "1The code is available at https://github.com/\n",
            "yeongseoj/ConKD\n",
            "Figure 1: Illustration of evaluation mismatch. Previous\n",
            "methods evaluate recommendation and dialogue perfor-\n",
            "mance independently as separate models are trained for\n",
            "each task. In contrast, a single model is utilized for both\n",
            "tasks in ConKD.\n",
            "items to users through natural language conver-\n",
            "sations, and the model-generated conversation is\n",
            "expected to be fluent and suit the context. Unlike\n",
            "traditional recommendation systems, the interac-\n",
            "tive nature of multi-turn dialogue allows the agent\n",
            "to explore explicit user interests that may not be\n",
            "present in the user’s history. This advantage particu-\n",
            "larly stands out compared to the traditional models\n",
            "that have access to only few purchase histories or\n",
            "implicit feedback (e.g. click) from users.\n",
            "To generate appropriate responses containing\n",
            "recommended items aligned with a user’s taste, an\n",
            "agent needs to possess both recommendation and\n",
            "dialogue capabilities. Previous works addressed the\n",
            "issue with separate modules (Li et al., 2018; Chen\n",
            "et al., 2019; Zhou et al., 2020; Lu et al., 2021). A\n",
            "recommendation module learns user preferences\n",
            "based on conversation history and retrieve relevant\n",
            "items, while a dialogue module generates the final\n",
            "response sentences. In Conversational Recommen-\n",
            "dation System (CRS), a major problem lies in in-\n",
            "corporating the two separate modules. CommonarXiv:2310.18119v1  [cs.CL]  27 Oct 2023strategies for injecting recommendation ability into\n",
            "the responses include copy mechanism (Gu et al.,\n",
            "2016) and pointer network (Gulcehre et al., 2016).\n",
            "Despite these efforts, the prior approaches (Li\n",
            "et al., 2018; Chen et al., 2019; Zhou et al., 2020;\n",
            "Lu et al., 2021) have demonstrated a discrepancy\n",
            "between the separate modules: the results of a rec-\n",
            "ommendation model are not integrated in the gener-\n",
            "ated response as depicted in Figure 1. The dialogue\n",
            "module suggests “The Avengers”, while the recom-\n",
            "mendation module recommends “Titanic”, reveal-\n",
            "ing a clear disagreement between the two modules.\n",
            "Accordingly, the probability distribution of the rec-\n",
            "ommendation model is not directly reflected in the\n",
            "output of the dialogue model. Such mismatch is\n",
            "inevitable when CR is formulated as two separate\n",
            "tasks, failing to serve the original purpose of the\n",
            "task.\n",
            "To address this challenge, we propose a multi-\n",
            "task learning approach for a unified CRS using\n",
            "Contextualized Knowledge Distillation (ConKD).\n",
            "We build a CRS with a single model via knowledge\n",
            "transfer by two teacher models, a dialogue teacher\n",
            "and a recommendation teacher. However, combin-\n",
            "ing the knowledge is not straightforward due to the\n",
            "nature of CRS; the task differs in each time step\n",
            "depending on the context.\n",
            "In this light, we introduce two gating mecha-\n",
            "nisms, hard gate andsoft gate , to effectively fuse\n",
            "teachers’ knowledge. With hard gate, knowledge\n",
            "transfer comes solely from either of the teachers,\n",
            "while soft gate integrates both sources of knowl-\n",
            "edge. We introduce an adaptive nature in the gates\n",
            "which is context-specific and computed on-the-fly\n",
            "during forward propagation. To our knowledge,\n",
            "this is the first work to explicitly demonstrate the\n",
            "existence of the discrepancy, and provide a ded-\n",
            "icated training mechanism to address it. More-\n",
            "over, our approach offers the flexibility in selecting\n",
            "model architectures, enabling integration of diverse\n",
            "language and recommendation models.\n",
            "Extensive experiments conducted on a widely\n",
            "used benchmark dataset (Li et al., 2018) demon-\n",
            "strate that our single model significantly outper-\n",
            "forms baselines in terms of recommendation per-\n",
            "formance and response fluency, while achieving\n",
            "comparable results in response diversity.\n",
            "The contributions of our work can be summa-\n",
            "rized as follows:\n",
            "•We propose a multi-task learning approach for a\n",
            "unified CRS using Contextualized KnowledgeDistillation.\n",
            "•We introduce two versions of ConKD, employ-\n",
            "ing different gate mechanisms: hard gate and\n",
            "soft gate .\n",
            "•Our approach surpasses strong baseline mod-\n",
            "els in making coherent recommendations and\n",
            "fluent responses, while competitive results are\n",
            "observed in response diversity.\n",
            "2 Preliminaries & Related Work\n",
            "2.1 Open-Ended Conversational\n",
            "Recommendation System\n",
            "Conventional recommendation systems mainly fo-\n",
            "cus on building a static user preference based on\n",
            "previous histories, such as clicks, purchases, and\n",
            "ratings (Sarwar et al., 2001; Koren et al., 2009;\n",
            "Kuchaiev and Ginsburg, 2017; Kang and McAuley,\n",
            "2018). In such environment, where feedback from\n",
            "users is static, implicit, and sparse, recommen-\n",
            "dation systems have difficulty reflecting dynamic\n",
            "changes in users’ preferences as well as suffer the\n",
            "cold-start problem (Gao et al., 2021).\n",
            "ReDial (Li et al., 2018) is one of the first attempts\n",
            "at handling such issue; the work combines open-\n",
            "ended chit-chat with recommendation task, called\n",
            "Conversational Recommendation System (CRS).\n",
            "Specifically, let (x,y)be a dialogue sample, where\n",
            "x={x1, x2, ..., xm}is a set of previous dialogue\n",
            "turns. mis the lengths of turns in the dialogue his-\n",
            "tory and yis the corresponding response (ground\n",
            "truth). In each turn, a recommendation module is\n",
            "expected to provide an item set Iufor a user u∈U,\n",
            "while a dialogue module produces a response y\n",
            "based on a dialogue history x.\n",
            "To incorporate recommended items into a re-\n",
            "sponse, a copy mechanism (Gu et al., 2016) or\n",
            "pointer network (Gulcehre et al., 2016) is generally\n",
            "adopted in prior works (Li et al., 2018; Chen et al.,\n",
            "2019; Zhou et al., 2020; Lu et al., 2021; Zhou et al.,\n",
            "2023). In such methods, additional networks are\n",
            "trained to predict whether the next token is a word\n",
            "or an item by aggregating representations from rec-\n",
            "ommendation and dialogue modules.\n",
            "In the aim of improving a CRS, previous studies\n",
            "leverage external knowledge in training. In KBRD\n",
            "(Chen et al., 2019), an item-oriented knowledge\n",
            "graph is introduced as an auxiliary input to a rec-\n",
            "ommendation module. KGSF (Zhou et al., 2020)\n",
            "utilizes both word-level and item-level knowledge\n",
            "graphs in training a CRS. In addition to the knowl-Models Mismatch R@1 ReR@1 R@10 ReR@10 R@50 ReR@50\n",
            "KBRD 0.931 0.034 0.008 (76.47%) 0.168 0.040 (76.19 %) 0.360 0.096 (74.17%)\n",
            "KGSF 0.926 0.038 0.008 (78.95%) 0.183 0.043 (76.50%) 0.382 0.109 (73.33%)\n",
            "RevCore 0.971 0.052 0.006 (88.46%) 0.195 0.031 (84.10%) 0.341 0.077 (77.42%)\n",
            "Table 1: Comparisons of Recall ( R@k) by recommendation modules and Recall in Response ( ReR@ k) by dialogue\n",
            "modules. Numbers in parenthesis indicate relative decrease in recall score by dialogue module compared to the\n",
            "recall score by a recommendation module. The scores are averaged over three runs with random seeds.\n",
            "edge graphs, movie review data is utilized in\n",
            "RevCore (Lu et al., 2021), and the meta informa-\n",
            "tion of items is encoded in (Yang et al., 2022) to\n",
            "enrich item representation. However, these works\n",
            "employ separate modules to manage CRS, which\n",
            "inevitably introduces discrepancy issues.\n",
            "To address this problem, prompt-based learning\n",
            "strategies are introduced in (Wang et al., 2022b;\n",
            "Deng et al., 2023). Despite the unified architecture,\n",
            "these approaches fail to dynamically incorporate\n",
            "multiple recommendations into a response. RecIn-\n",
            "Dial (Wang et al., 2022a) introduces a vocabulary\n",
            "pointer and knowledge bias to produce a unified\n",
            "output by combining two modules.\n",
            "2.2 Knowledge Distillation\n",
            "The core idea behind Knowledge Distillation (KD)\n",
            "(Hinton et al., 2015) is transferring knowledge\n",
            "of a high-capacity teacher network to a relatively\n",
            "smaller student model. In knowledge distillation, a\n",
            "student network is guided by not only a one-hot en-\n",
            "coded ground-truth but also a soft target mapped by\n",
            "the teacher network (probability distribution). This\n",
            "is known to transfer a class-wise relation mapped\n",
            "by a teacher is commonly termed the dark knowl-\n",
            "edge. Given a data sample from a joint distribution\n",
            "(x, y)∈ X × Y , a student model is optimized by\n",
            "combining two cross-entropy terms.\n",
            "LKD(θ) =−|Y|X\n",
            "k=1γˆyklogPθ(yk|x)\n",
            "+ (1−γ)˜Pϕ(yk|x) log ˜Pθ(yk|x)(1)\n",
            "where|Y|andˆykdenote the number of classes and\n",
            "ak-th target label (one-hot encoded) respectively.\n",
            "γ, and ˜Pdenote a balancing parameter, and a prob-\n",
            "ability distribution scaled with a temperature. θand\n",
            "ϕare parameters of a student and teacher network\n",
            "respectively.\n",
            "3 Unified CRS via ConKD\n",
            "In this section, we first demonstrate the mismatch\n",
            "issue with preliminary experiments and introduce\n",
            "our approach that mitigates such problem.3.1 Preliminary Experiments\n",
            "In our preliminary experiments on REDIAL (Li et al.,\n",
            "2018) dataset2, we aim to identify the mismatch\n",
            "problem in evaluation. In Table 1, we compare the\n",
            "recommendation results from two separate modules\n",
            "using recall metrics: R@ k(Recall) and ReR@ k\n",
            "(Recall in Response) which evaluates the top- k\n",
            "items predicted by a recommendation module and\n",
            "a dialogue module respectively. In all metrics, a sig-\n",
            "nificant degradation is observed when recall is com-\n",
            "puted on the dialogue response. The relative de-\n",
            "creases in the performance are ranged from 73.33%\n",
            "to88.46%, implying that a large discrepancy exists\n",
            "between the outputs of recommendation modules\n",
            "and generated responses. However, incorporating\n",
            "the recommendation module’s outputs during in-\n",
            "ference does not provide the fundamental solution\n",
            "to the problem. Further discussion on this issue is\n",
            "provided in Appendix D.\n",
            "To address the discrepancy, we propose a multi-\n",
            "task learning approach for a unified conversa-\n",
            "tional recommendation system via Contextualized\n",
            "Knowledge Distillation (ConKD). ConKD consists\n",
            "of three key components: a dialogue teacher and\n",
            "a recommendation teacher as experts on each task,\n",
            "and a student model - a multi-task learner, as de-\n",
            "scribed in Figure 2.\n",
            "3.2 Recommendation Teacher\n",
            "A recommendation teacher models the item-user\n",
            "joint distribution and provides a set of items that\n",
            "suit a user’s preference. We adopt the model struc-\n",
            "ture of (Zhou et al., 2020), where an item-oriented\n",
            "Knowledge Graph (KG) (Bizer et al., 2009) and\n",
            "word-oriented KG (Speer et al., 2016) are encoded\n",
            "to build a user preference. To learn item represen-\n",
            "tations with structural and relational information,\n",
            "R-GCN (Schlichtkrull et al., 2017) is adopted for\n",
            "2Li et al. (2018) proposed a dataset and a model, which\n",
            "we term the dataset as REDIAL and the model as ReDial here-\n",
            "inafter.(a) Learning from a dialogue teacher ( λt= 0)\n",
            " (b) Learning from a recommendation teacher ( λt= 1)\n",
            "Figure 2: The main structure of the proposed contextualized knowledge distillation with the hard gate. D-Teacher\n",
            "and R-Teacher denote dialogue teacher and recommendation teacher respectively. IandVdenote item space and\n",
            "vocabulary space respectively. The dashed arrow indicates backward propagation. One can easily extend the above\n",
            "to the soft gate, where λtis continuous.\n",
            "the item-oriented KG as follows.\n",
            "h(l+1)\n",
            "e =σ(X\n",
            "r∈RX\n",
            "e′∈Ere1\n",
            "Ze,rW(l)\n",
            "rh(l)\n",
            "e′+W(l)h(l)\n",
            "e)\n",
            "(2)\n",
            "where h(l)\n",
            "edenotes the representation of node eat\n",
            "(l)-th layer and h(0)is the initial node embedding.\n",
            "Er\n",
            "eis the set of neighbor nodes for node eunder the\n",
            "relation r.W(l)\n",
            "randW(l)are learnable matrix for\n",
            "handling various edge types and self-loop respec-\n",
            "tively. Ze,ris a normalization constant. Similarly,\n",
            "word-oriented KG is encoded with the GCN (Kipf\n",
            "and Welling, 2016) and the description in detail is\n",
            "illustrated in Appendix B.\n",
            "Given the learned node embeddings, a user repre-\n",
            "sentation puis acquired by aggregating words v(x)\n",
            "and items n(x)that appear in previous dialogue\n",
            "turnsxas follows3.\n",
            "pu=β·v(x)+ (1−β)·n(x)\n",
            "β=σ(Wg[v(x);n(x)])(3)\n",
            "where Wgare learnable parameters for computing\n",
            "the balancing parameter β. Finally, a matching\n",
            "score between a user uand an item iis calculated\n",
            "as follows.\n",
            "Pψ(i) =softmax (pT\n",
            "uni) (4)\n",
            "where ψis model parameters optimized to max-\n",
            "imize the likelihood of predicting ground-truth\n",
            "items.\n",
            "3For the detailed aggregation process, please refer to Sec-\n",
            "tion 4.3 in (Zhou et al., 2020)3.3 Dialogue Teacher\n",
            "To handle the chit-chat task, we train a conditional\n",
            "language model that intakes dialogue history and\n",
            "generates a context-aware response. We explore\n",
            "two primary structural variations for our language\n",
            "model:\n",
            "•KGSF (Zhou et al., 2020): A standard trans-\n",
            "former (Vaswani et al., 2017) and a knowledge-\n",
            "enhanced transformer (Zhou et al., 2020) are\n",
            "utilized for each as an encoder and a decoder.\n",
            "•DialoGPT (Zhang et al., 2020): A transformer-\n",
            "based pre-trained language model (PLM) trained\n",
            "on a large-scale dialogue dataset.\n",
            "Both dialogue models are trained to maximize the\n",
            "likelihood of predicting the ground truth response\n",
            "as follows:\n",
            "L(ϕ) =−|Y|X\n",
            "j=1TX\n",
            "t=11\n",
            "Tˆyt,jlogPϕ(yt,j|y1:t−1,x)\n",
            "(5)\n",
            "where Tandjare the length of a response yand\n",
            "a token index respectively. Yis a union of the\n",
            "vocabulary set and item set ( Y=V∪I), and ϕ\n",
            "denotes the parameters of the dialogue model.\n",
            "3.4 Contextualized Knowledge Distillation\n",
            "We elaborate on how a student model learns both\n",
            "the recommendation and dialogue tasks. Specifi-\n",
            "cally, two gating mechanisms are introduced: dis-\n",
            "crete andcontinuous gate, which integrate knowl-\n",
            "edge between the two teachers in an adaptive man-\n",
            "ner.Hard Gate\n",
            "A student model is trained to minimize the gap\n",
            "between its conditional probability distribution and\n",
            "the conditional probabilities mapped by the teacher\n",
            "networks. However, it is not ideal to equally weight\n",
            "the knowledge from both teacher models at every\n",
            "time step as seen in the following response.\n",
            "y1:If you like romance movies, I would\n",
            "recommend Titanic .\n",
            "At the last time step, where the item Titanic ap-\n",
            "pears, knowledge of a recommendation teacher can\n",
            "help a student learn the recommendation ability.\n",
            "On the contrary, a student can learn to generate a\n",
            "coherent and suitable utterance by accessing knowl-\n",
            "edge from the dialogue model except the time of\n",
            "recommendation.\n",
            "Taking all these factors into account, we intro-\n",
            "duce a token-level hard gate between teacher net-\n",
            "works, where supervision solely comes from either\n",
            "of the teachers at each time step. To distinguish\n",
            "which knowledge of the two teachers to be trans-\n",
            "ferred, we aggregate the probability mass of item\n",
            "indexes mapped by the dialogue teacher in each\n",
            "time step . This is built on an assumption that the di-\n",
            "alogue model assigns a relatively large probability\n",
            "mass on item indexes at the time of recommenda-\n",
            "tion.\n",
            "Given the distribution Pϕ(yt|y1:t−1,x)mapped\n",
            "by the dialogue teacher, we calculate a sum of item\n",
            "probabilities which answers the question of “ is this\n",
            "time to recommend an item? \". Therefore, a time\n",
            "step-specific hard gate is computed on-the-fly dur-\n",
            "ing forward propagation as follows.\n",
            "λt=(\n",
            "0,ifP\n",
            "y′∈IPϕ(y′|y1:t−1,x)< η\n",
            "1,otherwise(6)\n",
            "whereIdenotes the set of items in the vocabulary,\n",
            "andηis a predefined threshold. When λtis com-\n",
            "puted to be 1, it is a clear indication that a CRS is\n",
            "expected to output a recommendation result. On\n",
            "the contrary, when the hard gate is 0, a dialogue\n",
            "teacher defines the current time step as a dialogue\n",
            "time step; hence, the CRS is expected to make a\n",
            "coherent chit-chat.\n",
            "Soft Gate\n",
            "The hard gate inevitably introduces a hyper-\n",
            "parameter ηdue to the thresholding approach. To\n",
            "remove the hyper-parameter search on η, we intro-\n",
            "duce a continuous gating mechanism. This can beapplied under the assumption that a sum of item\n",
            "probabilities mapped by the dialogue teacher re-\n",
            "flects the extent to which recommendation is ex-\n",
            "pected. Therefore, the aggregated mass answers the\n",
            "question of ‘ how much to learn the recommenda-\n",
            "tion ability at the time step ”. Based on the intuition,\n",
            "we introduce a soft gate as follows.\n",
            "λt=X\n",
            "y′∈IPϕ(y′|y1:t−1,x) (7)\n",
            "where the gate λttakes continuous values within\n",
            "the range of [0,1]. A gate close to 1 indicates that\n",
            "the system should focus more on recommendation,\n",
            "while a gate close to 0 suggests that the agent is\n",
            "expected to prioritize the conversation task.\n",
            "To validate our assumption regarding the behav-\n",
            "ior of the dialogue teacher, we conducted a prelimi-\n",
            "nary experiment using a smaller model, KGSF. We\n",
            "computed the average sum of item probabilities in\n",
            "a dialogue time step λvand in a recommendation\n",
            "time step λr. The computed value of λrwas found\n",
            "to be 0.653, while λvwas measured to be 0.023.\n",
            "These results support our assumption that the dia-\n",
            "logue teacher assigns relatively large probability\n",
            "mass to item indexes in recommendation time . We\n",
            "provide further discussion on the validity of λin\n",
            "Appendix C.\n",
            "The gating computation differs from the previ-\n",
            "ous gating approaches (Zhou et al., 2020; Li et al.,\n",
            "2018; Chen et al., 2019; Lu et al., 2021) in two\n",
            "ways : 1) we leverage a teacher distribution as a\n",
            "signal of gating, where the gates can be discrete\n",
            "λt∈ {0,1}or continuous λt∈[0,1]. 2) our gates\n",
            "are not learned but a simple sum of probabilities.\n",
            "Contextualized Knowledge Distillation Loss\n",
            "With the two pre-trained teachers and the gating\n",
            "mechanisms, we now introduce loss for contextual-\n",
            "ized knowledge distillation.\n",
            "KD losses for each task are formulated as fol-\n",
            "lows.\n",
            "LDIAL-KD (θ) =−|Y|X\n",
            "k=1TX\n",
            "t=11\n",
            "TPϕ(yt,k|y1:t−1,x)×\n",
            "logPθ(yt,k|y1:t−1,x)\n",
            "(8)\n",
            "LREC-KD (θ) =−|Y|X\n",
            "k=1TX\n",
            "t=11\n",
            "TPψ(yt,k|x)×\n",
            "logPθ(yt,k|y1:t−1,x)(9)where θis the parameter of the student. Then, the\n",
            "final KD losses for each task are derived as follows.\n",
            "LDIAL(θ) = (1 −γ)LNLL+γLDIAL-KD\n",
            "LREC(θ) = (1 −γ)LNLL+γLREC-KD(10)\n",
            "where γis the balancing parameter between ground\n",
            "truth and teacher distribution, and LNLLis the cross\n",
            "entropy with ground truth. Finally, the losses are\n",
            "aggregated with our gate λtper time step.\n",
            "L(θ) = (1 −λt)LDIAL+λtLREC (11)\n",
            "When the hard gate is applied, a supervision is\n",
            "made by either of the teachers with the discrete\n",
            "λt. On the other hand, the soft gate fuses knowl-\n",
            "edge from the two teachers with the λtbeing the\n",
            "weight. By optimizing the combined objective, a\n",
            "single model is capable of learning the dialogue\n",
            "and recommendation tasks simultaneously, allevi-\n",
            "ating the mismatch that comes from two separate\n",
            "modules in previous methods.\n",
            "An evident advantage of employing contextu-\n",
            "alized knowledge distillation lies in taking the\n",
            "class-wise relation into consideration beyond the\n",
            "observed data (Hinton et al., 2015). With a one-\n",
            "hot encoded label, a neural network is trained to\n",
            "maximize the difference between the ground-truth\n",
            "and remaining classes; the dark knowledge is over-\n",
            "looked with one-hot supervision where a single\n",
            "index is set to 1 and the remaining indexes to 0s. In\n",
            "our work, the dark knowledge from both teachers is\n",
            "engaged in an adaptive manner to generate a fluent\n",
            "and user-specific response.\n",
            "Special Tokens\n",
            "Under CR, a dialogue turn falls into either a turn\n",
            "with recommendation result or a turn for querying\n",
            "a user preference. In this light, to inject an extra\n",
            "signal to the student model, we add two special\n",
            "tokens, [REC] and[GEN] , at the beginning of each\n",
            "turn. During training , the ground truth prefix starts\n",
            "with either [REC] if the response includes an item ,\n",
            "or[GEN] if it is chit-chat. This explicit scheme en-\n",
            "ables the model to learn turn-specific actions based\n",
            "on the preceding context and generate appropriate\n",
            "sequences.\n",
            "During inference , we employ a pre-trained classi-\n",
            "fier to predict one of the two special tokens at each\n",
            "dialogue turn. The classifier is built with standardtransformer layers and optimized as follows.\n",
            "L(θcls) = E(k,x)∼D[−|K|X\n",
            "j=1logP(kj|x;θcls)]\n",
            "(12)\n",
            "where K={0,1}is the label set, and θclsdenotes\n",
            "the classifier’s parameters. The model learns to\n",
            "classify whether the current turn is intended for\n",
            "chit-chat or recommending an item, based on the\n",
            "dialogue history x.\n",
            "4 Experiments\n",
            "4.1 Dataset\n",
            "The proposed approach is tested on the recently\n",
            "introduced REDIAL (Recommendation Dialogues)\n",
            "dataset (Li et al., 2018). REDIAL is a conversation\n",
            "dataset which the dialogues are centered around\n",
            "recommendation, and the subject of the recommen-\n",
            "dation is movie. REDIAL contains 10,006 multi-turn\n",
            "dialogues, which amount to 182,150 utterances.\n",
            "The total number of unique movies in the dataset\n",
            "is 6,924, and the size of vocabulary is 23,928.\n",
            "4.2 Baselines\n",
            "ReDial (Li et al., 2018) consists of dialogue, rec-\n",
            "ommendation, and sentiment analysis modules.\n",
            "Pointer network (Gulcehre et al., 2016) is intro-\n",
            "duced to bridge the modules. KBRD (Chen et al.,\n",
            "2019) introduces item-oriented KG (Bizer et al.,\n",
            "2009) and the KG representation is added when\n",
            "building a logit for the dialogue module (Michel\n",
            "and Neubig, 2018). KGSF (Zhou et al., 2020) inte-\n",
            "grates word-oriented KG (Speer et al., 2016) and\n",
            "item-oriented KG (Bizer et al., 2009) for seman-\n",
            "tic alignment. KG-enhanced transformer and copy\n",
            "network (Gu et al., 2016) are employed. RevCore\n",
            "(Lu et al., 2021) incorporates movie-review data\n",
            "for review-enriched item representations and uti-\n",
            "lizes a copy network (Gu et al., 2016). DialoGPT\n",
            "(Zhang et al., 2020) is fine-tuned on the REDIAL\n",
            "dataset. RecInDial (Wang et al., 2022a) finetunes\n",
            "DialoGPT-small with R-GCN(Schlichtkrull et al.,\n",
            "2017) and introduces a vocabulary pointer and\n",
            "knowledge-aware bias to generate unified outputs.\n",
            "4.3 Evaluation Metrics\n",
            "To validate the recommendation performance, we\n",
            "employ a top- kevaluation approach with kval-\n",
            "ues of 1, 10, and 50. Consistent with prior re-\n",
            "search (Zhou et al., 2020; Lu et al., 2021), we re-\n",
            "port Recall@ k(R@k) in Section 3.1. However,Models ReR@1 ReR@10 ReR@50 PrR@1 PrR@10 PrR@50 F1@1 F1@10 F1@50 Rec Ratio\n",
            "REDIAL 0.002 0.017 0.039 0.002 0.021 0.048 0.002 0.019 0.043 0.014\n",
            "KBRD 0.008 0.040 0.096 0.011 0.052 0.126 0.009 0.045 0.109 0.317\n",
            "KGSF 0.008 0.043 0.109 0.009 0.048 0.123 0.009 0.045 0.116 0.445\n",
            "REVCORE 0.006 0.031 0.077 0.014 0.075 0.183 0.008 0.044 0.109 0.203\n",
            "DialoGPT 0.011 0.070 0.172 0.011 0.071 0.174 0.011 0.070 0.173 0.462\n",
            "RecInDial 0.017 0.088 0.203 0.022 0.114 0.264 0.020 0.099 0.229 0.438\n",
            "KGSF + ConKD (hard) 0.023 0.110 0.249 0.024 0.113 0.257 0.023 0.111 0.253 0.499\n",
            "KGSF + ConKD (soft) 0.022 0.105 0.241 0.023 0.111 0.255 0.023 0.108 0.248 0.479\n",
            "DialoGPT + ConKD (hard) 0.022 0.120 0.250 0.021 0.112 0.235 0.022 0.116 0.243 0.525\n",
            "DialoGPT + ConKD (soft) 0.019 0.101 0.219 0.02 0.104 0.226 0.019 0.102 0.222 0.505\n",
            "Table 2: Automatic evaluation results on recommendation task. The scores are averaged over three runs with random\n",
            "seeds. ReR@ kandPrR@ kare recall and precision in response for the top kitems. F1@kis the harmonic mean\n",
            "ofReR@ kandPrR@ k.Rec Ratio is the ratio of recommendation turns to total turns. All scores are computed\n",
            "on final outputs predicted by dialogue modules. Bold and underlined numbers denote the best and second-best\n",
            "performance, respectively.\n",
            "ModelsQualitative Quantitative\n",
            "Flu Info Cohe Average PPL DIST-2 DIST-3 DIST-4\n",
            "REDIAL 1.772 0.334 1.194 1.100 17.577 0.075 0.123 0.166\n",
            "KBRD 1.640 0.729 1.321 1.230 19.039 0.123 0.223 0.304\n",
            "KGSF 1.811 0.502 1.433 1.249 11.191 0.168 0.305 0.420\n",
            "REVCORE 1.854 0.512 1.187 1.184 9.283 0.098 0.170 0.235\n",
            "DialoGPT 1.766 0.781 1.580 1.376 17.552 0.206 0.419 0.595\n",
            "RecInDial 1.812 0.726 1.606 1.381 5.858 0.065 0.124 0.183\n",
            "KGSF + ConKD (hard) 1.831 0.856 1.571 1.419 8.886 0.138 0.249 0.344\n",
            "KGSF + ConKD (soft) 1.858 0.878 1.644 1.460 8.689 0.132 0.236 0.326\n",
            "DialoGPT + ConKD (hard) 1.894 0.859 1.688 1.480 12.412 0.179 0.344 0.489\n",
            "DialoGPT + ConKD (soft) 1.870 0.829 1.591 1.430 12.336 0.180 0.350 0.505\n",
            "Table 3: Qualitative and quantitative evaluation results on conversation task. The qualitative scores are averaged over\n",
            "three hired annotators. Flu,Info andCohe indicate fluency, informativeness, and coherence of model-generated\n",
            "responses. Average is the average of the quantities. In quantitative method, scores are averaged over three runs\n",
            "with random seeds. PPL is the perplexity of dialogue calculated by a language model, and DIST -nis the distinct\n",
            "n-gram in corpus level.\n",
            "given the conversational nature of the task, it is\n",
            "crucial to evaluate recommendation performance\n",
            "within generated responses . We introduce Recall in\n",
            "Response (ReR@ k) following Liang et al. (2021)\n",
            "and Wang et al. (2022a), with a refined calcula-\n",
            "tion approach to ensure scores range within [0,1].\n",
            "Specifically, we take the average of correct item pre-\n",
            "dictions over the total number of item predictions\n",
            "instead of responses containing items. Addition-\n",
            "ally, we introduce Precision in Response (PrR@ k)\n",
            "and compute the harmonic mean of ReR@ kand\n",
            "PrR@ k, denoted as F1@ k. Furthermore, we assess\n",
            "the system’s ability to make active recommenda-\n",
            "tions by introducing the recommendation turn ratio,\n",
            "calculated as the number of dialogue turns with\n",
            "recommended items over the total dialogue turns.\n",
            "To evaluate dialogue performance, we report per-\n",
            "plexity (PPL) and distinct n-grams (Li et al., 2016)\n",
            "(DIST), assessing the fluency and the diversity of\n",
            "generated responses, respectively. In prior studies,\n",
            "DIST was computed by counting distinct n-gramsat the corpus-level and averaging them over sen-\n",
            "tences, which can lead to scores greater than 1. To\n",
            "address this, we have updated the metric calcula-\n",
            "tion to count distinct n-grams and calculate rates at\n",
            "the corpus-level, ensuring the scores fall within the\n",
            "range of 0 to 1. The results evaluated on original\n",
            "metrics are illustrated in Appendix F.\n",
            "Recent studies find that the n-gram based evalu-\n",
            "ation methods may not be sufficient to assess the\n",
            "performance of a language model (Zhang* et al.,\n",
            "2020). Therefore, we conduct human evaluation\n",
            "to comprehensively assess model performance as\n",
            "done in previous works (Zhou et al., 2020; Wang\n",
            "et al., 2022a). Detailed human evaluation setup is\n",
            "described in Appendix E.\n",
            "4.4 Results\n",
            "Evaluation of Recommendation\n",
            "In Table 2, we present the recommendation perfor-\n",
            "mance of the models. The results clearly demon-\n",
            "strate that models with ConKD (hard) consistentlyachieve the highest scores in F1@ k, indicating su-\n",
            "perior performance in the recommendation task.\n",
            "Notably, KGSF integrated with ConKD doubles the\n",
            "scores compared to KGSF, while DialoGPT with\n",
            "ConKD achieves the scores 1.5 times as high as\n",
            "DialoGPT. These improvements are observed not\n",
            "only in single predictions, but also in top- 10and\n",
            "top-50predictions, indicating superior user-item\n",
            "mapping. We hypothesize that such gain stems\n",
            "from the “dark knowledge” distilled from the rec-\n",
            "ommendation teacher within our framework. This\n",
            "knowledge encompasses inter-class relations that\n",
            "are absent in the one-hot encoded hard targets but\n",
            "are expressed through the probability distribution\n",
            "provided by the recommendation teacher. Further-\n",
            "more, the models with ConKD make active recom-\n",
            "mendations, as depicted by the high recommenda-\n",
            "tion ratio, reflecting their focus on task-oriented\n",
            "conversation. Among our gating mechanisms, the\n",
            "hard gate outperforms the soft gate, which can be\n",
            "attributed to the stronger supervision made by the\n",
            "hard gate; a student is guided solely by the recom-\n",
            "mendation teacher in recommendation time.\n",
            "Evaluation of Dialogue Generation\n",
            "In addition to the recommendation performances,\n",
            "ConKD exhibits comparable results in conversa-\n",
            "tion metrics, as shown in Table 3. Under quanti-\n",
            "tative evaluation, the proposed models outperform\n",
            "the backbone models in PPL, indicating enhanced\n",
            "fluency of responses. We observed that RecIn-\n",
            "Dial tends to generate relatively simple responses\n",
            "without active engagement, resulting in lower PPL\n",
            "scores. Regarding the slight decrease in the DIST\n",
            "metric compared to the backbone models in our\n",
            "results, two important observations should be high-\n",
            "lighted: 1) The base models fail to effectively ad-\n",
            "dress the recommendation task in their responses,\n",
            "and 2) DIST scores alone are insufficient for evalu-\n",
            "ating the quality of model-generated responses.\n",
            "These findings are supported by the results of\n",
            "qualitative evaluation, where the single models\n",
            "with ConKD outperform all baselines in average\n",
            "scores. Specifically, when applied to KGSF, the\n",
            "proposed methods significantly enhance informa-\n",
            "tiveness and coherence. This implies our training\n",
            "mechanism performs consistently regardless of the\n",
            "model size, aligning with the automatic evaluation\n",
            "results. We also observe that ConKD-soft outper-\n",
            "forms ConKD-hard with KGSF as the backbone,\n",
            "and the opposite holds true for DialoGPT. This dis-\n",
            "crepancy is attributed to the model capacity of theModels F1@1 F1@10 F1@50\n",
            "RecInDial 0.087 0.283 0.424\n",
            "DialoGPT + ConKD (hard) 0.124 0.331 0.465\n",
            "Table 4: Recommendation results evaluated on contex-\n",
            "tullay relevant items.\n",
            "dialogue teacher, which influences the gate value. It\n",
            "suggests that the choice between hard and soft gates\n",
            "depends on the capacity of the dialogue teacher.\n",
            "4.5 Quality of Recommendation\n",
            "In CRS, evaluating recommendations based solely\n",
            "on a single ground-truth item may not fully capture\n",
            "a model’s potential, as user preferences can span\n",
            "a wide range of items. To address this, we expand\n",
            "the evaluation by considering contextually relevant\n",
            "items. These relevant items are those located within\n",
            "a 2-hop distance from previously mentioned items\n",
            "in the item-oriented KG (Bizer et al., 2009), shar-\n",
            "ing attributes such as genre and actor. We compare\n",
            "two models, RecInDial and DialoGPT + ConKD\n",
            "(hard), both of which use the same backbone model\n",
            "to produce unified outputs. Table 4 shows that\n",
            "ConKD consistently outperforms RecInDial across\n",
            "all metrics, with a significant improvement over the\n",
            "single-item evaluation results in Table 2. This high-\n",
            "lights ConKD’s capability not only to recommend\n",
            "a gold item but also to comprehend the underly-\n",
            "ing knowledge structures, even in the absence of a\n",
            "knowledge graph.\n",
            "4.6 Efficiency Comparison\n",
            "Ensuring real-time efficiency is of importance in\n",
            "enhancing the user experience in CRS. This sec-\n",
            "tion compares the inference speeds of three mod-\n",
            "els: DialoGPT, DialoGPT + ConKD (hard), and\n",
            "RecInDial. DialoGPT and DialoGPT+ConKD\n",
            "(hard) achieve inference latencies of 5.464ms and\n",
            "5.306ms per token, respectively. In contrast, RecIn-\n",
            "Dial incurs a slightly higher latency of 6.100ms\n",
            "per token. This additional latency in RecInDial can\n",
            "be attributed to the computation of the knowledge-\n",
            "aware bias and vocabulary pointer. The compo-\n",
            "nents involve making decisions between generat-\n",
            "ing items or words in every time step. In ConKD,\n",
            "a language model handles the knowledge-aware\n",
            "recommendations without sacrificing efficiency.\n",
            "4.7 Ablations\n",
            "To verify the efficacy of each component intro-\n",
            "duced in this work, we conduct ablation studiesModels F1@1 F1@10 F1@50 Rec Ratio\n",
            "Vanilla 0.012 0.062 0.155 0.337\n",
            "(+) D 0.017 0.063 0.165 0.334\n",
            "(+) R 0.019 0.066 0.146 0.293\n",
            "(+) D&R 0.020 0.093 0.200 0.313\n",
            "(+) D&R&ST 0.023 0.111 0.253 0.499\n",
            "λt↔0.5 0.021 0.105 0.245 0.471\n",
            "Table 5: Ablations on the recommendation task. (+)\n",
            "indicates adding corresponding components to Vanilla,\n",
            "a model without ConKD. D and R refer to the dialogue\n",
            "teacher and recommendation teacher. ST is the special\n",
            "token mechanism, and ↔indicates replacement. Hard\n",
            "gate is applied for combining the KGSF teachers.\n",
            "Models DIST-1 DIST-2 DIST-3 DIST-4 PPL\n",
            "Vanilla 0.029 0.100 0.176 0.246 7.538\n",
            "(+) D 0.029 0.100 0.172 0.241 7.071\n",
            "(+) R 0.019 0.066 0.115 0.164 12.331\n",
            "(+) D&R 0.028 0.097 0.166 0.232 6.349\n",
            "(+) D&R&ST 0.035 0.138 0.249 0.344 8.886\n",
            "λt↔0.5 0.031 0.121 0.219 0.306 9.211\n",
            "Table 6: Ablations on the dialogue task.\n",
            "using several variants of our model. The results are\n",
            "depicted in Table 5 and 6.\n",
            "We observed that the role of teachers signifi-\n",
            "cantly affects the performance of a student. Learn-\n",
            "ing solely from the recommendation teacher en-\n",
            "hances recommendation performance but comes\n",
            "at the cost of PPL and DIST scores. In contrast,\n",
            "learning from the dialogue teacher improves flu-\n",
            "ency. When both teachers are present within the\n",
            "training framework, the student’s performance im-\n",
            "proves in both tasks, highlighting the effectiveness\n",
            "of knowledge transfer through our gating mecha-\n",
            "nism. Scores decline in all metrics when we replace\n",
            "our gate with a static value of 0.5. Lastly, the spe-\n",
            "cial token also brings meaningful gains, depicted\n",
            "with the increased F1@ kand DIST scores.\n",
            "4.8 Case Study\n",
            "The responses generated by the baselines and\n",
            "ConKD are shown in Table 7. In the conversa-\n",
            "tion, the user has expressed a clear preference for\n",
            "action movies and has previously mentioned actors\n",
            "“bruce willis” and“Tom Cruise” . However, the\n",
            "three baselines generate chit-chat responses with-\n",
            "out recommendations. Although KGSF suggests a\n",
            "movie, the response lacks coherence and fluency.\n",
            "DialoGPT generates a coherent response but recom-\n",
            "mends “Mr.& Mrs.Smith” , which does not align\n",
            "with the user’s preference; neither Bruce Willis nor\n",
            "Tom Cruise are associated with it. On the other\n",
            "hand, our models produce apt responses that areCase 1\n",
            "Context\n",
            "User Hi!\n",
            "Agent How are you?\n",
            "User I like action movies. I like Bruce Willis . He is\n",
            "a great actor. Or Tom Cruise . I am doing well,\n",
            "thanks for asking.\n",
            "Model Output\n",
            "ReDial What can I help you find today ?\n",
            "KBRD What would you like?\n",
            "KGSF I’m doing well. Edge of Tomorrow\n",
            "RevCore I am not sure, but I will have to give you some\n",
            "good ideas.\n",
            "DialoGPT Have you seen the movie Mr.& Mrs.Smith ? It\n",
            "is a really good movie...!\n",
            "KGSF* Have you seen Edge of Tomorrow ?\n",
            "KGSF** I like action movies too. Have you seen Edge of\n",
            "Tomorrow ?\n",
            "DialoGPT* Have you seen Mission: Impossible ?\n",
            "DialoGPT** Have you seen Mission: Impossible ?\n",
            "Ground Truth Have you seen Gladiator ?\n",
            "Table 7: Sample outputs generated by the baselines and\n",
            "ConKD. * and ** indicate a model with ConKD (hard)\n",
            "and ConKD (soft) respectively. The last three dialogue\n",
            "turns are depicted for context. Bold words indicate\n",
            "user’s preference and recommended items.\n",
            "both fluent and informative; it makes user-specific\n",
            "recommendations, suggesting Edge of Tomorrow\n",
            "andMission: Impossible , both of which are action\n",
            "movies featuring Tom Cruise . Notably, these rec-\n",
            "ommendations are more aligned with the user’s ex-\n",
            "pressed preferences compared to the ground-truth\n",
            "movie “Gladiator” , which is an action movie with-\n",
            "out the mentioned actors. Additional examples are\n",
            "provided in the Appendix G.\n",
            "5 Conclusion\n",
            "In this study, we introduce contextualized knowl-\n",
            "edge distillation with hard gate and soft gate. In\n",
            "hard gate, a student is either learned from a rec-\n",
            "ommendation teacher or from a dialogue teacher\n",
            "with a discrete value, while the soft gate fuses\n",
            "knowledge from both teachers, removing a hyper-\n",
            "parameter from the hard gate. The gates are com-\n",
            "puted in a context-specific manner by aggregating\n",
            "the probability mass on the interest set (a movie\n",
            "item set in our experiment). Our work verifies\n",
            "the idea in the popular benchmark dataset and the\n",
            "result illustrates the superior performance of our\n",
            "approach compared to strong baselines. In addi-\n",
            "tion, human evaluation mainly conforms with the\n",
            "automatic evaluation, demonstrating that the pro-\n",
            "posed approach is a well-balanced model with both\n",
            "recommendation and chit-chat ability.Limitations\n",
            "This work is grounded on the student-teacher\n",
            "framework, hence requiring additional computa-\n",
            "tion when obtaining knowledge of a teacher; our\n",
            "approach requires two teachers, one for dialogue\n",
            "and one for recommendation. This extra compu-\n",
            "tation can be a burden in an environment lack of\n",
            "resource. Nonetheless, the proposed approach uti-\n",
            "lizes a single model for inference. Furthermore, our\n",
            "approach requires the teachers to be well-trained.\n",
            "This, however, is also a shared problem within KD\n",
            "training.\n",
            "Ethical Consideration\n",
            "Since REDIAL dataset (Li et al., 2018) contains\n",
            "multi-turn dialogue histories, the dataset, by nature,\n",
            "may pose a privacy issue. If a dialogue teacher in\n",
            "our framework learns such information, the student\n",
            "in our framework can also learn to output private\n",
            "information while in conversation. Such issue may\n",
            "possibly be handled by employing a privacy clas-\n",
            "sifier model, where a model is trained to identify\n",
            "certain outputs containing private information of a\n",
            "user.\n",
            "Acknowledgements\n",
            "Lei Chen’s work is partially supported by National\n",
            "Science Foundation of China (NSFC) under Grant\n",
            "No. U22B2060, the Hong Kong RGC GRF Project\n",
            "16213620, CRF Project C2004-21GF, RIF Project\n",
            "R6020-19, AOE Project AoE/E-603/18, Theme-\n",
            "based project TRS T41-603/20R, China NSFC\n",
            "No. 61729201, Guangdong Basic and Applied\n",
            "Basic Research Foundation 2019B151530001,\n",
            "Hong Kong ITC ITF grants MHX/078/21 and\n",
            "PRP/004/22FX, Microsoft Research Asia Collabo-\n",
            "rative Research Grant and HKUST-Webank joint\n",
            "research lab grants.\n",
            "References\n",
            "Daniel Adiwardana, Minh-Thang Luong, David R. So,\n",
            "Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\n",
            "Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\n",
            "and Quoc V . Le. 2020. Towards a human-like open-\n",
            "domain chatbot.\n",
            "Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sören\n",
            "Auer, Christian Becker, Richard Cyganiak, and Se-\n",
            "bastian Hellmann. 2009. Dbpedia - a crystallization\n",
            "point for the web of data. Journal of Web Semantics ,\n",
            "7(3):154–165. The Web of Data.Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding,\n",
            "Yukuo Cen, Hongxia Yang, and Jie Tang. 2019. To-\n",
            "wards knowledge-based recommender dialog system.\n",
            "Yang Deng, Wenxuan Zhang, Weiwen Xu, Wenqiang\n",
            "Lei, Tat-Seng Chua, and Wai Lam. 2023. A unified\n",
            "multi-task learning framework for multi-goal con-\n",
            "versational recommender systems. ACM Trans. Inf.\n",
            "Syst., 41(3).\n",
            "Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten\n",
            "de Rijke, and Tat-Seng Chua. 2021. Advances and\n",
            "challenges in conversational recommender systems:\n",
            "A survey. AI Open , 2:100–126.\n",
            "Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li.\n",
            "2016. Incorporating copying mechanism in sequence-\n",
            "to-sequence learning. In Proceedings of the 54th An-\n",
            "nual Meeting of the Association for Computational\n",
            "Linguistics (Volume 1: Long Papers) , pages 1631–\n",
            "1640, Berlin, Germany. Association for Computa-\n",
            "tional Linguistics.\n",
            "Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,\n",
            "Bowen Zhou, and Yoshua Bengio. 2016. Pointing\n",
            "the unknown words. In Proceedings of the 54th An-\n",
            "nual Meeting of the Association for Computational\n",
            "Linguistics (Volume 1: Long Papers) , pages 140–149,\n",
            "Berlin, Germany. Association for Computational Lin-\n",
            "guistics.\n",
            "Matthew Henderson, Ivan Vuli ´c, Daniela Gerz, Iñigo\n",
            "Casanueva, Paweł Budzianowski, Sam Coope, Geor-\n",
            "gios Spithourakis, Tsung-Hsien Wen, Nikola Mrkši ´c,\n",
            "and Pei-Hao Su. 2019. Training neural response se-\n",
            "lection for task-oriented dialogue systems.\n",
            "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\n",
            "Distilling the knowledge in a neural network.\n",
            "Wang-Cheng Kang and Julian McAuley. 2018. Self-\n",
            "attentive sequential recommendation. In 2018 IEEE\n",
            "International Conference on Data Mining (ICDM) ,\n",
            "pages 197–206.\n",
            "Diederik P. Kingma and Jimmy Ba. 2014. Adam: A\n",
            "method for stochastic optimization.\n",
            "Thomas N. Kipf and Max Welling. 2016. Semi-\n",
            "supervised classification with graph convolutional\n",
            "networks.\n",
            "Yehuda Koren, Robert Bell, and Chris V olinsky. 2009.\n",
            "Matrix factorization techniques for recommender sys-\n",
            "tems. Computer , 42(8):30–37.\n",
            "Oleksii Kuchaiev and Boris Ginsburg. 2017. Training\n",
            "deep autoencoders for collaborative filtering.\n",
            "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\n",
            "and Bill Dolan. 2016. A diversity-promoting ob-\n",
            "jective function for neural conversation models. In\n",
            "Proceedings of the 2016 Conference of the North\n",
            "American Chapter of the Association for Computa-\n",
            "tional Linguistics: Human Language Technologies ,\n",
            "pages 110–119, San Diego, California. Association\n",
            "for Computational Linguistics.Raymond Li, Samira Kahou, Hannes Schulz, Vincent\n",
            "Michalski, Laurent Charlin, and Chris Pal. 2018. To-\n",
            "wards deep conversational recommendations.\n",
            "Zujie Liang, Huang Hu, Can Xu, Jian Miao, Yingy-\n",
            "ing He, Yining Chen, Xiubo Geng, Fan Liang, and\n",
            "Daxin Jiang. 2021. Learning neural templates for\n",
            "recommender dialogue system.\n",
            "Yu Lu, Junwei Bao, Yan Song, Zichen Ma, Shuguang\n",
            "Cui, Youzheng Wu, and Xiaodong He. 2021.\n",
            "Revcore: Review-augmented conversational recom-\n",
            "mendation.\n",
            "Paul Michel and Graham Neubig. 2018. Extreme adap-\n",
            "tation for personalized neural machine translation.\n",
            "Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun\n",
            "Li, Jinchao Li, Michael Zeng, and Jianfeng Gao.\n",
            "2020. Few-shot natural language generation for task-\n",
            "oriented dialog.\n",
            "Badrul Sarwar, George Karypis, Joseph Konstan, and\n",
            "John Riedl. 2001. Item-based collaborative filtering\n",
            "recommendation algorithms. In Proceedings of the\n",
            "10th International Conference on World Wide Web ,\n",
            "WWW ’01, page 285–295, New York, NY , USA.\n",
            "Association for Computing Machinery.\n",
            "Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem,\n",
            "Rianne van den Berg, Ivan Titov, and Max Welling.\n",
            "2017. Modeling relational data with graph convolu-\n",
            "tional networks.\n",
            "Robyn Speer, Joshua Chin, and Catherine Havasi. 2016.\n",
            "Conceptnet 5.5: An open multilingual graph of gen-\n",
            "eral knowledge.\n",
            "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
            "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
            "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
            "you need.\n",
            "Lingzhi Wang, Huang Hu, Lei Sha, Can Xu, Daxin\n",
            "Jiang, and Kam-Fai Wong. 2022a. RecInDial: A uni-\n",
            "fied framework for conversational recommendation\n",
            "with pretrained language models. In Proceedings of\n",
            "the 2nd Conference of the Asia-Pacific Chapter of the\n",
            "Association for Computational Linguistics and the\n",
            "12th International Joint Conference on Natural Lan-\n",
            "guage Processing (Volume 1: Long Papers) , pages\n",
            "489–500, Online only. Association for Computational\n",
            "Linguistics.\n",
            "Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin\n",
            "Zhao. 2022b. Towards unified conversational rec-\n",
            "ommender systems via knowledge-enhanced prompt\n",
            "learning. In Proceedings of the 28th ACM SIGKDD\n",
            "Conference on Knowledge Discovery and Data Min-\n",
            "ing. ACM.\n",
            "Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Mil-\n",
            "ica Gasic, Lina M. Rojas-Barahona, Pei-Hao Su, Ste-\n",
            "fan Ultes, and Steve Young. 2016. A network-based\n",
            "end-to-end trainable task-oriented dialogue system.Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,\n",
            "Ming Zhou, and Wei-Ying Ma. 2016. Topic aware\n",
            "neural response generation.\n",
            "Bowen Yang, Cong Han, Yu Li, Lei Zuo, and Zhou\n",
            "Yu. 2022. Improving conversational recommenda-\n",
            "tion systems’ quality with context-aware item meta-\n",
            "information. In Findings of the Association for Com-\n",
            "putational Linguistics: NAACL 2022 , pages 38–48,\n",
            "Seattle, United States. Association for Computational\n",
            "Linguistics.\n",
            "Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\n",
            "Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-\n",
            "uating text generation with bert. In International\n",
            "Conference on Learning Representations .\n",
            "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\n",
            "Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\n",
            "Liu, and Bill Dolan. 2020. Dialogpt: Large-scale\n",
            "generative pre-training for conversational response\n",
            "generation.\n",
            "Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang\n",
            "Zhou, Ji-Rong Wen, and Jingsong Yu. 2020. Improv-\n",
            "ing conversational recommender systems via knowl-\n",
            "edge graph based semantic fusion.\n",
            "Yuanhang Zhou, Kun Zhou, Wayne Xin Zhao, Cheng\n",
            "Wang, Peng Jiang, and He Hu. 2023. C2-crs: Coarse-\n",
            "to-fine contrastive learning for conversational recom-\n",
            "mender system.\n",
            "A Implementation Details\n",
            "Combinations of KGSF and ConKD. We mainly\n",
            "follow KGSF (Zhou et al., 2020) for the basic\n",
            "model structure of the two teacher and student\n",
            "models. For the dialogue teacher and student, we\n",
            "employ an encoder-decoder structure, with each\n",
            "module consisting of 2 transformer layers. The\n",
            "hidden dimension size of both models is 300. Our\n",
            "work excludes the copy mechanism used in KGSF.\n",
            "In the recommendation model, we utilize 1-layer\n",
            "GNN networks trained with word-oriented and\n",
            "item-oriented KGs as inputs. The normalization\n",
            "constant Ze,ris set to 1. The token classifier is\n",
            "a transformer-based encoder with a classification\n",
            "head. We employ the Adam optimizer (Kingma\n",
            "and Ba, 2014) and apply gradient clipping for sta-\n",
            "ble training. Our chosen hyper-parameters include\n",
            "a batch size of 32, a learning rate of 1e-3, and train-\n",
            "ing for 200 epochs. Furthermore, for ConKD, we\n",
            "set the ηandγto 0.3 and 0.6, respectively.\n",
            "Combinations of DialoGPT and ConKD. We\n",
            "utilize DialoGPT-small (Zhang et al., 2020) as\n",
            "the backbone for the dialogue teacher and student\n",
            "model. DialoGPT consists of 12 transformer lay-\n",
            "ers and the hidden dimension size of the model is768. The recommendation teacher and token clas-\n",
            "sifier in the DialoGPT+ConKD models follow the\n",
            "same settings as those in the KGSF+ConKD mod-\n",
            "els. For hyper-parameters, we use a batch size of\n",
            "32, a learning rate of 1e-3, and train for 20 epochs.\n",
            "For ConKD, we set the ηandγto 0.6 and 0.4,\n",
            "respectively.\n",
            "During inference, we only use the student model\n",
            "for the response generation in both settings.\n",
            "B Graph Convolutional Network (GCN)\n",
            "GCN is adopted to encode the word-oriented KG\n",
            "ConceptNet (Speer et al., 2016). A triple in the\n",
            "KG is denoted by (w1, r, w 2), where w1andw2\n",
            "are word nodes, and ris a relationship between\n",
            "the nodes. The node features are updated with an\n",
            "aggregation function as follows:\n",
            "H(l)=ReLU(˜D−1\n",
            "2˜A˜D−1\n",
            "2H(l−1)W(l))(13)\n",
            "H(l)∈ Rn×dandW(l)denote the node represen-\n",
            "tations and a learnable matrix at the l-th layer re-\n",
            "spectively. nis the number of nodes and ddenotes\n",
            "the dimension size of node features. ˜A=A+I\n",
            "is the adjacency matrix of the graph with self-loop,\n",
            "where Iis the identity matrix. ˜D=P\n",
            "j˜Aijrefers\n",
            "to a degree matrix.\n",
            "C Validity of λ\n",
            "To explore the validity of the λ, we illustrate varia-\n",
            "tions of the response y1introduced in 3.4.\n",
            "y2:If you like romance movies, I would\n",
            "recommend youTitanic.\n",
            "y3:If you like romance movies, I would\n",
            "recommend some romantic comedies.\n",
            "After the word recommend , the vocab ( youand\n",
            "some ) can be replaced with the item Titanic in the\n",
            "y1. Therefore, the average value λrof 0.653 is\n",
            "acceptable in soft gate, indicating the mass reflects\n",
            "the level to which recommendation is expected .\n",
            "D Item Refilling\n",
            "We discuss a two-step inference, in which a re-\n",
            "sponse is first generated, and items in the response\n",
            "are refilled with the output of a recommendation\n",
            "module. The recommendation module predicts\n",
            "a probability distribution in each dialogue turn,\n",
            "which remains fixed at each time step during the\n",
            "response generation. Hence, the module cannot\n",
            "directly handle the variable number of items in aresponse . Additionally, the output does not reflect\n",
            "the dynamic changes of the context , which fails to\n",
            "provide a context-aware recommendation as seen\n",
            "in the following responses.\n",
            "y4:Blended withAdam Sandler is a fav\n",
            "of mine.\n",
            "y5:Love Actually withAdam Sandler is\n",
            "a fav of mine.\n",
            "y4andy5are generated under the original setting\n",
            "and the two-step inference setting respectively4.\n",
            "The dialogue module generates the coherent and in-\n",
            "formative response y4, describing the actor Adam\n",
            "Sandler who stars in the movie Blended . On the\n",
            "other hand, the item refilling fails to incorporate a\n",
            "suitable item into the response, which causes a fac-\n",
            "tual error; there is no relationship between Adam\n",
            "Sandler and the movie Love Actually . This in-\n",
            "dicates that the simple integration cannot be the\n",
            "fundamental solution for the mismatch issues; it\n",
            "leads to semantic discrepancy between the recom-\n",
            "mendations and generated responses. In our unified\n",
            "system, the output distribution over items dynami-\n",
            "cally changes depending on the context, generating\n",
            "coherent and user-specific responses. This is done\n",
            "by a single model in a single step, thereby reducing\n",
            "the model size and inference time.\n",
            "E Human Evaluation Setup\n",
            "We engaged three annotators who were tasked with\n",
            "assessing model outputs given a dialogue history.\n",
            "100 model outputs from each model are randomly\n",
            "sampled and collected, the total being 1000 dia-\n",
            "logue turns. The annotators score each model out-\n",
            "put from the range of 0 to 2 on the level of informa-\n",
            "tiveness, coherence, and fluency of model output.\n",
            "The following instructions were provided to guide\n",
            "annotators in their assessments:\n",
            "Fluency: Fluency encapsulates the naturalness\n",
            "of the generated text. It involves an assessment\n",
            "of how the output adheres to linguistic standards,\n",
            "avoiding grammatical flaws. Annotators should\n",
            "evaluate the syntactic flow, word choice, and over-\n",
            "all readability. The scores should be shown as 0, 1,\n",
            "and 2, where each indicates “not fluent”, “readable\n",
            "but with some flaws”, and “fluent”, respectively.\n",
            "Informativeness: The informativeness encom-\n",
            "passes the model’s ability to convey relevant and\n",
            "4The samples are generated by KGSF. We leverage top k\n",
            "sampling to handle the variable number of items in responses,\n",
            "where kis set to 1.accurate information. Annotators should assess the\n",
            "depth and accuracy of the conveyed information.\n",
            "The scores need to be displayed 0, 1, and 2 where\n",
            "each corresponds to “information is missing or in-\n",
            "correct”, “information is included but insufficient\n",
            "or partially inaccurate”, and “comprehensive and\n",
            "accurate information”, respectively.\n",
            "Coherence: Coherence entails the harmonious\n",
            "integration of the model’s output within the evolv-\n",
            "ing conversation. Annotators should assess how\n",
            "well the model comprehends and adheres to the\n",
            "conversation’s theme, avoiding abrupt shifts and\n",
            "ensuring a natural conversational flow. The scores\n",
            "should be valued using 0, 1, and 2. Each rating\n",
            "represents “awkward conversation flow”, “make\n",
            "sense but somewhat disconnected”, and “coherent”,\n",
            "respectively.\n",
            "F Results Evaluated Using Original\n",
            "Metrics\n",
            "Models DIST-2 DIST-3 DIST-4 ReR@1 ReR@10 ReR@50\n",
            "RecInDial 0.413 0.663 0.815 0.023 0.118 0.273\n",
            "Ours 1.326 2.217 2.704 0.03 0.161 0.337\n",
            "Table 8: Dialogue and recommendation performance\n",
            "evaluated on the conventional metric. DialoGPT +\n",
            "ConKD (hard) is employed for ours.\n",
            "G Additional Cases\n",
            "Our models generate diverse movies that differ\n",
            "from the ground truth but align with the user’s pref-\n",
            "erences. For example, in Case 2, when the user re-\n",
            "quests old classics , our models suggest Gone with\n",
            "the Wind (1939) ,It’s a Wonderful Life (1946) ,\n",
            "The Big Lebowski (1998) ,The Outsiders (1967)\n",
            "andDriving Miss Daisy (1989) , all of which are\n",
            "considered old classics. In contrast, other baselines\n",
            "fail to provide recommendation, except for KBRD.\n",
            "In the Case 3, when the user expresses a prefer-\n",
            "ence for family friendly movies like Peter Rabbit ,\n",
            "andFinding Dory , our models recommend Beauty\n",
            "and the Beast ,Jumanji ,Coco , and Troll all of\n",
            "which are family-friendly, with three of them being\n",
            "animations. This contrasts with other baselines that\n",
            "produce contextually incorrect responses without\n",
            "recommendations or mention Peter Rabbit again,\n",
            "which the user had previously mentioned in the\n",
            "dialogue context.Case 2\n",
            "Context\n",
            "Agent Hey! What is a movie you love?\n",
            "User Hello, I am looking for a old classic . What would\n",
            "you suggest?\n",
            "Model Output\n",
            "ReDial Hello\n",
            "KBRD Have you seen Jumanji ?\n",
            "KGSF I can recommend some movies for you?\n",
            "RevCore I love comedy movies!\n",
            "DialoGPT Do you have a favorite actor or actress? I love\n",
            "how many types of movies today.\n",
            "KGSF* Have you seen Gone with the Wind ? Or maybe\n",
            "something like It’s a Wonderful Life ?\n",
            "KGSF** Have you seen The Big Lebowski ?\n",
            "DialoGPT* I would suggest The Outsiders It’s a classic !\n",
            "DialoGPT** Have you seen Driving Miss Daisy ?\n",
            "Ground Truth The Gold Rush andInvasion of the Body\n",
            "Snatchers are classics.\n",
            "Case 3\n",
            "Context\n",
            "User Hello! Do you know of any family friendly movies?\n",
            "Agent Good Morning have you seen The Sixth Sense ?\n",
            "User Yes, but it is too scary for my children . something\n",
            "like, Peter Rabbit we saw that and it was cute.\n",
            "Agent A old classic will be Finding Nemo .\n",
            "User Oh yes, and Finding Dory .\n",
            "Model Output\n",
            "ReDial I liked it a lot.\n",
            "KBRD Glad to help. Have a great day!\n",
            "KGSF I also recommend watching Captain Underpants:\n",
            "The First Epic Movie again. It’s pretty good I can\n",
            "think of some people haven’t seen\n",
            "RevCore I think you would like it .\n",
            "DialoGPT I have not seen Peter Rabbit but I have heard good\n",
            "things about it\n",
            "KGSF* I also really liked Beauty and the Beast\n",
            "KGSF** I also recommend watching the new Jumanji . It\n",
            "was cute.\n",
            "DialoGPT* Coco was good too\n",
            "DialoGPT** I also recommend Troll . If your looking at animals\n",
            "that one is really good too!\n",
            "Ground Truth I saw Peter Rabbit with my daughter she loved it\"\n",
            "Table 9: Additional sample outputs generated by the\n",
            "baselines and ConKD. * and ** indicate a model with\n",
            "ConKD (hard) and ConKD (soft) respectively. The last\n",
            "four dialogue turns are depicted for context. Bold words\n",
            "indicate user’s preference and recommended items.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.create_documents([all_text])"
      ],
      "metadata": {
        "id": "KxOVn-G5J0hD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "directory = 'index_store'\n",
        "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
        "vector_index.save_local(directory)"
      ],
      "metadata": {
        "id": "87G_3hCQKFhC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
        "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
        "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(),\n",
        "                                           chain_type=\"stuff\",\n",
        "                                           retriever=retriever,\n",
        "                                           return_source_documents=True\n",
        "                                           )"
      ],
      "metadata": {
        "id": "jKzEdhPtLQLk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(question):\n",
        "  response_object = qa_interface(question)\n",
        "  question = response_object['query']\n",
        "  response = response_object['result']\n",
        "  # print(f\"Query: {question}\")\n",
        "  # print(f\"Response: {response}\")\n",
        "  return response\n",
        "query = 'Summarize this research paper'\n",
        "get_response(query)"
      ],
      "metadata": {
        "id": "HY1k5GRvyyiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "79abcffb-3041-40c2-c950-5c7e8347c675"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This research paper focuses on the development of a methodology for extracting building details from complex urban environmental scenes. The authors propose a spatial sampling scheme that selects representative samples for training a multi-task network. The study aims to enhance predictive performance while minimizing data preparation costs. The paper discusses related work, describes the methodology, outlines the experimental framework, presents and discusses the experimental findings, and offers conclusions. The authors also acknowledge the limitations of their work and discuss potential negative societal impacts. The primary contributions of the study include the proposed sampling scheme and the assessment of modeling approaches for building outcome prediction. The code and dataset obtained from this work are available for public access. The research was supported by funding from the National Natural Science Foundation of China.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts and Responses"
      ],
      "metadata": {
        "id": "OMO6S37uyHFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"Please create a summary of  for the budget speech of 1947 1948.\"\n",
        "get_response(Query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KtCgUSYhcabr",
        "outputId": "17f38364-cb25-46a2-f556-7e32ae37dd01"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I don't have access to specific documents or speeches from the past. My training data only includes general knowledge and information up until 2021. I recommend referring to historical archives or relevant sources for the budget speech of 1947-1948.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"What is Multi task learning?\"\n",
        "get_response(Query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "09QBZM_Hehv8",
        "outputId": "06011671-5c4e-4d63-ba57-895ad8cf6fde"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multi-task learning (MTL) is a machine learning approach that involves training a single model to perform multiple related tasks simultaneously. Instead of training separate models for each task, MTL aims to leverage the common information shared among the tasks to improve the overall performance. The idea is that by jointly optimizing the model for multiple tasks, the learned features can capture the underlying relationships and dependencies, leading to better generalization and improved performance on each individual task. MTL can be particularly useful in scenarios where labeled data for each task is limited or expensive to obtain, as it allows for knowledge transfer and data sharing across tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"How are transformers useful in MTL?\"\n",
        "get_response(Query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dAN-F3Z9a9A1",
        "outputId": "5f2e07da-96e2-431a-d065-83a49c2dfdc5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There is no specific mention of transformers in the provided context. Therefore, it is not possible to determine how transformers are useful in Multi-Task Learning (MTL) based on the given information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Query = ''' Various data imbalances that naturally arise in a multi-territory personalized recommender system can lead to a significant item bias\n",
        "for globally prevalent items. A locally popular item can be overshadowed by a globally prevalent item. Moreover, users’ viewership\n",
        "patterns/statistics can drastically change from one geographic location to another which may suggest to learn specific user embeddings.\n",
        "In this paper, we propose a multi-task learning (MTL) technique, along with an adaptive upsampling method to reduce popularity bias\n",
        "in multi-territory recommendations. Our proposed framework is designed to enrich training examples with active users representation\n",
        "through upsampling, and capable of learning geographic-based user embeddings by leveraging MTL. Through experiments, we\n",
        "demonstrate the effectiveness of our framework in multiple territories compared to a baseline not incorporating our proposed\n",
        "techniques. Noticeably, we show improved relative gain of up to 65.27% in PR-AUC metric. A case study is presented to demonstrate\n",
        "the advantages of our methods in attenuating the popularity bias of global items'''\n",
        "get_response(Query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "RIIiud3Ua85l",
        "outputId": "1a53814e-1b00-4be4-b470-a39e54b33a77"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper proposes a multi-task learning (MTL) technique and an adaptive upsampling method to reduce popularity bias in multi-territory recommendations. The proposed framework aims to enrich training examples with active user representation through upsampling and learn geographic-based user embeddings using MTL. The experiments conducted show the effectiveness of the framework in multiple territories compared to a baseline without incorporating the proposed techniques, with an improved relative gain of up to 65.27% in the PR-AUC metric. Additionally, a case study is presented to demonstrate the advantages of the methods in attenuating the popularity bias of global items.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}